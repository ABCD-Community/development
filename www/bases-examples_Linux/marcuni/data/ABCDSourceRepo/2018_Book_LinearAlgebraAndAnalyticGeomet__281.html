<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled-9</title>
</head>
<body><div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics
</p>
<p>Giovanni&nbsp;Landi&nbsp;&middot; Alessandro&nbsp;Zampini
</p>
<p>Linear Algebra 
and Analytic 
Geometry 
for Physical 
Sciences</p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics</p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Lecture Notes in Physics (ULNP) publishes authoritative texts covering
</p>
<p>topics throughout pure and applied physics. Each title in the series is suitable as a basis for
</p>
<p>undergraduate instruction, typically containing practice problems, worked examples, chapter
</p>
<p>summaries, and suggestions for further reading.
</p>
<p>ULNP titles must provide at least one of the following:
</p>
<p>&bull; An exceptionally clear and concise treatment of a standard undergraduate subject.
</p>
<p>&bull; A solid undergraduate-level introduction to a graduate, advanced, or non-standard subject.
</p>
<p>&bull; A novel perspective or an unusual approach to teaching a subject.
</p>
<p>ULNP especially encourages new, original, and idiosyncratic approaches to physics teaching
</p>
<p>at the undergraduate level.
</p>
<p>The purpose of ULNP is to provide intriguing, absorbing books that will continue to be the
</p>
<p>reader&rsquo;s preferred reference throughout their academic career.
</p>
<p>Series editors
</p>
<p>Neil Ashby
</p>
<p>University of Colorado, Boulder, CO, USA
</p>
<p>William Brantley
</p>
<p>Department of Physics, Furman University, Greenville, SC, USA
</p>
<p>Matthew Deady
</p>
<p>Physics Program, Bard College, Annandale-on-Hudson, NY, USA
</p>
<p>Michael Fowler
</p>
<p>Department of Physics, University of Virginia, Charlottesville, VA, USA
</p>
<p>Morten Hjorth-Jensen
</p>
<p>Department of Physics, University of Oslo, Oslo, Norway
</p>
<p>Michael Inglis
</p>
<p>Department of Physical Sciences, SUNY Suffolk County Community College,
</p>
<p>Selden, NY, USA
</p>
<p>More information about this series at http://www.springer.com/series/8917</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/8917">http://www.springer.com/series/8917</a></div>
</div>
<div class="page"><p/>
<p>Giovanni Landi &bull; Alessandro Zampini
</p>
<p>Linear Algebra and Analytic
Geometry for Physical
Sciences
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Giovanni Landi
University of Trieste
Trieste
Italy
</p>
<p>Alessandro Zampini
INFN Sezione di Napoli
Napoli
Italy
</p>
<p>ISSN 2192-4791 ISSN 2192-4805 (electronic)
Undergraduate Lecture Notes in Physics
ISBN 978-3-319-78360-4 ISBN 978-3-319-78361-1 (eBook)
https://doi.org/10.1007/978-3-319-78361-1
</p>
<p>Library of Congress Control Number: 2018935878
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
</p>
<p>publication does not imply, even in the absence of a specific statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
</p>
<p>jurisdictional claims in published maps and institutional affiliations.
</p>
<p>Printed on acid-free paper
</p>
<p>This Springer imprint is published by the registered company Springer International Publishing AG
part of Springer Nature
</p>
<p>The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland</p>
<p/>
</div>
<div class="page"><p/>
<p>To our families</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Vectors and Coordinate Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Applied Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 Coordinate Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
</p>
<p>1.3 More Vector Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
</p>
<p>1.4 Divergence, Rotor, Gradient and Laplacian . . . . . . . . . . . . . . . . 15
</p>
<p>2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>2.1 Definition and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>2.2 Vector Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
</p>
<p>2.3 Linear Combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
</p>
<p>2.4 Bases of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
</p>
<p>2.5 The Dimension of a Vector Space . . . . . . . . . . . . . . . . . . . . . . 33
</p>
<p>3 Euclidean Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.1 Scalar Product, Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.2 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>3.3 Orthonormal Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
</p>
<p>3.4 Hermitian Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>4 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>4.1 Basic Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>4.2 The Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
</p>
<p>4.3 Reduced Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
</p>
<p>4.4 Reduction of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>4.5 The Trace of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
</p>
<p>5 The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
</p>
<p>5.1 A Multilinear Alternating Mapping . . . . . . . . . . . . . . . . . . . . . 69
</p>
<p>5.2 Computing Determinants via a Reduction Procedure . . . . . . . . . 74
</p>
<p>5.3 Invertible Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>6 Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>6.1 Basic Notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>6.2 The Space of Solutions for Reduced Systems . . . . . . . . . . . . . . 81
</p>
<p>6.3 The Space of Solutions for a General Linear System . . . . . . . . 84
</p>
<p>6.4 Homogeneous Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . 94
</p>
<p>7 Linear Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>7.1 Linear Transformations and Matrices . . . . . . . . . . . . . . . . . . . . 97
</p>
<p>7.2 Basic Notions on Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
</p>
<p>7.3 Kernel and Image of a Linear Map . . . . . . . . . . . . . . . . . . . . . 104
</p>
<p>7.4 Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
</p>
<p>7.5 Computing the Kernel of a Linear Map . . . . . . . . . . . . . . . . . . 108
</p>
<p>7.6 Computing the Image of a Linear Map . . . . . . . . . . . . . . . . . . 111
</p>
<p>7.7 Injectivity and Surjectivity Criteria . . . . . . . . . . . . . . . . . . . . . . 114
</p>
<p>7.8 Composition of Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 116
</p>
<p>7.9 Change of Basis in a Vector Space . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>8 Dual Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>8.1 The Dual of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>8.2 The Dirac&rsquo;s Bra-Ket Formalism . . . . . . . . . . . . . . . . . . . . . . . . 128
</p>
<p>9 Endomorphisms and Diagonalization . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>9.1 Endomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>9.2 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . 133
</p>
<p>9.3 The Characteristic Polynomial of an Endomorphism . . . . . . . . . 138
</p>
<p>9.4 Diagonalisation of an Endomorphism . . . . . . . . . . . . . . . . . . . . 143
</p>
<p>9.5 The Jordan Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
</p>
<p>10 Spectral Theorems on Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>10.1 Orthogonal Matrices and Isometries . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>10.2 Self-adjoint Endomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . 156
</p>
<p>10.3 Orthogonal Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
</p>
<p>10.4 The Diagonalization of Self-adjoint Endomorphisms . . . . . . . . . 163
</p>
<p>10.5 The Diagonalization of Symmetric Matrices . . . . . . . . . . . . . . . 167
</p>
<p>11 Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>11.1 Skew-Adjoint Endomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>11.2 The Exponential of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 178
</p>
<p>11.3 Rotations in Two Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . 180
</p>
<p>11.4 Rotations in Three Dimensions . . . . . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>11.5 The Lie Algebra so&eth;3&THORN; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
11.6 The Angular Velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
</p>
<p>11.7 Rigid Bodies and Inertia Matrix . . . . . . . . . . . . . . . . . . . . . . . . 194
</p>
<p>viii Contents</p>
<p/>
</div>
<div class="page"><p/>
<p>12 Spectral Theorems on Hermitian Spaces . . . . . . . . . . . . . . . . . . . . . 197
</p>
<p>12.1 The Adjoint Endomorphism . . . . . . . . . . . . . . . . . . . . . . . . . . 197
</p>
<p>12.2 Spectral Theory for Normal Endomorphisms . . . . . . . . . . . . . . 203
</p>
<p>12.3 The Unitary Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
</p>
<p>13 Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>13.1 Quadratic Forms on Real Vector Spaces . . . . . . . . . . . . . . . . . . 213
</p>
<p>13.2 Quadratic Forms on Complex Vector Spaces . . . . . . . . . . . . . . 222
</p>
<p>13.3 The Minkowski Spacetime . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>13.4 Electro-Magnetism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
</p>
<p>14 Affine Linear Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
</p>
<p>14.1 Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
</p>
<p>14.2 Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>14.3 General Linear Affine Varieties and Parallelism . . . . . . . . . . . . 245
</p>
<p>14.4 The Cartesian Form of Linear Affine Varieties . . . . . . . . . . . . . 249
</p>
<p>14.5 Intersection of Linear Affine Varieties . . . . . . . . . . . . . . . . . . . 258
</p>
<p>15 Euclidean Affine Linear Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 269
</p>
<p>15.1 Euclidean Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
</p>
<p>15.2 Orthogonality Between Linear Affine Varieties . . . . . . . . . . . . . 272
</p>
<p>15.3 The Distance Between Linear Affine Varieties . . . . . . . . . . . . . 276
</p>
<p>15.4 Bundles of Lines and of Planes . . . . . . . . . . . . . . . . . . . . . . . . 283
</p>
<p>15.5 Symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
</p>
<p>16 Conic Sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>16.1 Conic Sections as Geometric Loci . . . . . . . . . . . . . . . . . . . . . . 293
</p>
<p>16.2 The Equation of a Conic in Matrix Form . . . . . . . . . . . . . . . . . 298
</p>
<p>16.3 Reduction to Canonical Form of a Conic: Translations . . . . . . . 301
</p>
<p>16.4 Eccentricity: Part 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
</p>
<p>16.5 Conic Sections and Kepler Motions . . . . . . . . . . . . . . . . . . . . . 309
</p>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations . . . . . . . . . 310
</p>
<p>16.7 Eccentricity: Part 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
</p>
<p>16.8 Why Conic Sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
</p>
<p>Appendix A: Algebraic Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
</p>
<p>Contents ix</p>
<p/>
</div>
<div class="page"><p/>
<p>Introduction
</p>
<p>This book originates from a collection of lecture notes that the first author prepared
</p>
<p>at the University of Trieste with Michela Brundu, over a span of fifteen years,
</p>
<p>together with the more recent one written by the second author. The notes were
</p>
<p>meant for undergraduate classes on linear algebra, geometry and more generally
</p>
<p>basic mathematical physics delivered to physics and engineering students, as well
</p>
<p>as mathematics students in Italy, Germany and Luxembourg.
</p>
<p>The book is mainly intended to be a self-contained introduction to the theory of
</p>
<p>finite-dimensional vector spaces and linear transformations (matrices) with their
</p>
<p>spectral analysis both on Euclidean and Hermitian spaces, to affine Euclidean
</p>
<p>geometry as well as to quadratic forms and conic sections.
</p>
<p>Many topics are introduced and motivated by examples, mostly from physics.
</p>
<p>They show how a definition is natural and how the main theorems and results are
</p>
<p>first of all plausible before a proof is given. Following this approach, the book
</p>
<p>presents a number of examples and exercises, which are meant as a central part in
</p>
<p>the development of the theory. They are all completely solved and intended both to
</p>
<p>guide the student to appreciate the relevant formal structures and to give in several
</p>
<p>cases a proof and a discussion, within a geometric formalism, of results from
</p>
<p>physics, notably from mechanics (including celestial) and electromagnetism.
</p>
<p>Being the book intended mainly for students in physics and engineering, we
</p>
<p>tasked ourselves not to present the mathematical formalism per se. Although we
</p>
<p>decided, for clarity's sake of our readers, to organise the basics of the theory in the
</p>
<p>classical terms of definitions and the main results as theorems or propositions, we
</p>
<p>do often not follow the standard sequential form of definition&mdash;theorem&mdash;corollary
</p>
<p>&mdash;example and provided some two hundred and fifty solved problems given as
</p>
<p>exercises.
</p>
<p>Chapter 1 of the book presents the Euclidean space used in physics in terms of
</p>
<p>applied vectors with respect to orthonormal coordinate system, together with the
</p>
<p>operation of scalar, vector and mixed product. They are used both to describe the
</p>
<p>motion of a point mass and to introduce the notion of vector field with the most
</p>
<p>relevant differential operators acting upon them.
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapters 2 and 3 are devoted to a general formulation of the theory of
</p>
<p>finite-dimensional vector spaces equipped with a scalar product, while the Chaps. 4
</p>
<p>&ndash;6 present, via a host of examples and exercises, the theory of finite rank matrices
</p>
<p>and their use to solve systems of linear equations.
</p>
<p>These are followed by the theory of linear transformations in Chap. 7. Such a
</p>
<p>theory is described in Chap. 8 in terms of the Dirac&rsquo;s Bra-Ket formalism, providing
</p>
<p>a link to a geometric&ndash;algebraic language used in quantum mechanics.
</p>
<p>The notion of the diagonal action of an endomorphism or a matrix (the problem
</p>
<p>of diagonalisation and of reduction to the Jordan form) is central in this book, and it
</p>
<p>is introduced in Chap. 9.
</p>
<p>Again with many solved exercises and examples, Chap. 10 describes the spectral
</p>
<p>theory for operators (matrices) on Euclidean spaces, and (in Chap. 11) how it allows
</p>
<p>one to characterise the rotations in classical mechanics. This is done by introducing
</p>
<p>the Euler angles which parameterise rotations of the physical three-dimensional
</p>
<p>space, the notion of angular velocity and by studying the motion of a rigid body
</p>
<p>with its inertia matrix, and formulating the description of the motion with respect to
</p>
<p>different inertial observers, also giving a characterisation of polar and axial vectors.
</p>
<p>Chapter 12 is devoted to the spectral theory for matrices acting on Hermitian
</p>
<p>spaces in order to present a geometric setting to study a finite level quantum
</p>
<p>mechanical system, where the time evolution is given in terms of the unitary group.
</p>
<p>All these notions are related with the notion of Lie algebra and to the exponential
</p>
<p>map on the space of finite rank matrices.
</p>
<p>In Chap. 13, we present the theory of quadratic forms. Our focus is the
</p>
<p>description of their transformation properties, so to give the notion of signature,
</p>
<p>both in the real and in the complex cases. As the most interesting example of a
</p>
<p>non-Euclidean quadratic form, we present the Minkowski spacetime from special
</p>
<p>relativity and the Maxwell equations.
</p>
<p>In Chaps. 14 and 15, we introduce through many examples the basics of the
</p>
<p>Euclidean affine linear geometry and develop them in the study of conic sections, in
</p>
<p>Chap. 16, which are related to the theory of Kepler motions for celestial body in
</p>
<p>classical mechanics. In particular, we show how to characterise a conic by means of
</p>
<p>its eccentricity.
</p>
<p>A reader of this book is only supposed to know about number sets, more
</p>
<p>precisely the natural, integer, rational and real numbers and no additional prior
</p>
<p>knowledge is required. To try to be as much self-contained as possible, an appendix
</p>
<p>collects a few basic algebraic notions, like that of group, ring and field and maps
</p>
<p>between them that preserve the structures (homomorphisms), and polynomials in
</p>
<p>one variable. There are also a few basic properties of the field of complex numbers
</p>
<p>and of the field of (classes of) integers modulo a prime number.
</p>
<p>Giovanni Landi
</p>
<p>Alessandro Zampini
</p>
<p>Trieste, Italy
</p>
<p>Napoli, Italy
</p>
<p>May 2018
</p>
<p>xii Introduction</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
</p>
<p>Vectors and Coordinate Systems
</p>
<p>The notion of a vector, or more precisely of a vector applied at a point, originates in
</p>
<p>physics when dealing with an observable quantity. By this or simply by observable,
</p>
<p>onemeans anything that can bemeasured in the physical space&mdash;the space of physical
</p>
<p>events&mdash; via a suitable measuring process. Examples are the velocity of a point
</p>
<p>particle, or its acceleration, or a force acting on it. These are characterised at the
</p>
<p>point of application by a direction, an orientation and a modulus (or magnitude). In
</p>
<p>the following pages we describe the physical space in terms of points and applied
</p>
<p>vectors, and use these to describe the physical observables related to the motion of a
</p>
<p>point particle with respect to a coordinate system (a reference frame). The geometric
</p>
<p>structures introduced in this chapter will be more rigorously analysed in the next
</p>
<p>chapters.
</p>
<p>1.1 Applied Vectors
</p>
<p>We refer to the common intuition of a physical space made of points, where the
</p>
<p>notions of straight line between two points and of the length of a segment (or equiv-
</p>
<p>alently of distance of two points) are assumed to be given. Then, a vector v can be
</p>
<p>denoted as
</p>
<p>v = B &minus; A or v = AB,
</p>
<p>where A, B are two points of the physical space. Then, A is the point of application
</p>
<p>of v, its direction is the straight line joining B to A, its orientation the one of the arrow
</p>
<p>pointing from A towards B, and its modulus the real number ‖B &minus; A‖ = ‖A &minus; B‖,
</p>
<p>that is the length (with respect to a fixed unit) of the segment AB.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_1
</p>
<p>1</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_1&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_1&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>2 1 Vectors and Coordinate Systems
</p>
<p>Fig. 1.1 The parallelogram rule
</p>
<p>If S denotes the usual three dimensional physical space, we denote by
</p>
<p>W
3 = {B &minus; A | A, B &isin; S}
</p>
<p>the collection of all applied vectors at any point of S and by
</p>
<p>V
3
A = {B &minus; A | B &isin; S}
</p>
<p>the collection of all vectors applied at A in S. Then
</p>
<p>W
3 =
</p>
<p>⋃
</p>
<p>A&isin;S
</p>
<p>V
3
A.
</p>
<p>Remark 1.1.1 Once fixed a point O in S, one sees that there is a bijection between
</p>
<p>the set V3O = {B &minus; O | B &isin; S} and S itself. Indeed, each point B in S uniquely
</p>
<p>determines the element B &minus; O in V3O , and each element B &minus; O in V
3
O uniquely
</p>
<p>determines the point B in S.
</p>
<p>It is well known that the so called parallelogram rule defines in V3O a sum of
</p>
<p>vectors, where
</p>
<p>(A &minus; O)+ (B &minus; O) = (C &minus; O),
</p>
<p>with C the fourth vertex of the parallelogram whose other three vertices are A, O ,
</p>
<p>B, as shown in Fig. 1.1.
</p>
<p>The vector 0 = O &minus; O is called the zero vector (or null vector); notice that its
</p>
<p>modulus is zero, while its direction and orientation are undefined.
</p>
<p>It is evident that V3O is closed with respect to the notion of sum defined above.
</p>
<p>That such a sum is associative and abelian is part of the content of the proposition
</p>
<p>that follows.
</p>
<p>Proposition 1.1.2 The datum (V3O ,+, 0) is an abelian group.
</p>
<p>Proof Clearly the zero vector 0 is the neutral (identity) element for the sum in V3O ,
</p>
<p>that added to any vector leave the latter unchanged. Any vector A &minus; O has an inverse</p>
<p/>
</div>
<div class="page"><p/>
<p>1.1 Applied Vectors 3
</p>
<p>Fig. 1.2 The opposite of a vector: A
&prime;
&minus; O = &minus;(A &minus; O)
</p>
<p>Fig. 1.3 The associativity of the vector sum
</p>
<p>with respect to the sum (that is, any vector has an opposite vector) given by A&prime; &minus; O ,
</p>
<p>where A&prime; is the symmetric point to A with respect to O on the straight line joining
</p>
<p>A to O (see Fig. 1.2).
</p>
<p>From its definition the sum of two vectors is a commutative operation. For the
</p>
<p>associativity we give a pictorial argument in Fig. 1.3. �
</p>
<p>There is indeed more structure. The physical intuition allows one to consider
</p>
<p>multiples of an applied vector. Concerning the collection V3O , this amounts to define
</p>
<p>an operation involving vectors applied in O and real numbers, which, in order not to
</p>
<p>create confusion with vectors, are called (real) scalars.
</p>
<p>Definition 1.1.3 Given the scalar λ &isin; R and the vector A &minus; O &isin; V3O , the product
</p>
<p>by a scalar
</p>
<p>B &minus; O = λ(A &minus; O)
</p>
<p>is the vector such that:
</p>
<p>(i) A, B, O are on the same (straight) line,
</p>
<p>(ii) B &minus; O and A &minus; O have the same orientation if λ &gt; 0, while A &minus; O and
</p>
<p>B &minus; O have opposite orientations if λ &lt; 0,
</p>
<p>(iii) ‖B &minus; O‖ = |λ| ‖A &minus; O‖.
</p>
<p>The main properties of the operation of product by a scalar are given in the
</p>
<p>following proposition.
</p>
<p>Proposition 1.1.4 For any pair of scalars λ,&micro; &isin; R and any pair of vectors
</p>
<p>A &minus; O, B &minus; O &isin; V3O , it holds that:</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Vectors and Coordinate Systems
</p>
<p>Fig. 1.4 The scaling λ(C &minus; O) = (C &prime; &minus; O) with λ &gt; 1
</p>
<p>1. λ(&micro;(A &minus; O)) = (λ&micro;)(A &minus; O),
</p>
<p>2. 1(A &minus; O) = A &minus; O,
</p>
<p>3. λ ((A &minus; O)+ (B &minus; O)) = λ(A &minus; O) + λ(B &minus; O),
</p>
<p>4. (λ+ &micro;)(A &minus; O) = λ(A &minus; O)+ &micro;(A &minus; O).
</p>
<p>Proof 1. Set C &minus; O = λ (&micro;(A &minus; O)) and D &minus; O = (λ&micro;)(A &minus; O). If one of
</p>
<p>the scalars λ,&micro; is zero, one trivially has C &minus; O = 0 and D &minus; O = 0, so
</p>
<p>Point 1. is satisfied. Assume now that λ �= 0 and &micro; �= 0. Since, by definition,
</p>
<p>both C and D are points on the line determined by O and A, the vectors C &minus; O
</p>
<p>and D &minus; O have the same direction. It is easy to see that C &minus; O and D &minus; O
</p>
<p>have the same orientation: it will coincide with the orientation of A &minus; O or not,
</p>
<p>depending on the sign of the product λ&micro; �= 0. Since |λ&micro;| = |λ||&micro;| &isin; R, one has
</p>
<p>‖C &minus; O‖ = ‖D &minus; O‖.
</p>
<p>2. It follows directly from the definition.
</p>
<p>3. SetC &minus; O = (A &minus; O)+ (B &minus; O) andC &prime; &minus; O = (A&prime; &minus; O)+ (B &prime; &minus; O),
</p>
<p>with A&prime; &minus; O = λ(A &minus; O) and B &prime; &minus; O = λ(B &minus; O).
</p>
<p>We verify that λ(C &minus; O) = C &prime; &minus; O (see Fig. 1.4).
</p>
<p>Since O A is parallel to O A&prime; by definition, then BC is parallel to B &prime;C &prime;; O B is
</p>
<p>indeed parallel to O B &prime;, so that the planar angles Ô BC and Ô B &prime;C &prime; are equal.
</p>
<p>Also λ(O B) = O B &prime;, λ(O A) = O A&prime;, and λ(BC) = B &prime;C &prime;. It follows that the
</p>
<p>triangles O BC and O B &prime;C &prime; are similar: the vector OC is then parallel OC &prime; and
</p>
<p>they have the same orientation, with ‖OC &prime;‖ = λ ‖OC‖. From this we obtain
</p>
<p>OC &prime; = λ(OC).
</p>
<p>4. The proof is analogue to the one in point 3. �
</p>
<p>What we have described above shows that the operations of sum and product by a
</p>
<p>scalar give V3O an algebraic structure which is richer than that of abelian group. Such
</p>
<p>a structure, that we shall study in detail in Chap. 2, is called in a natural way vector
</p>
<p>space.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Coordinate Systems 5
</p>
<p>1.2 Coordinate Systems
</p>
<p>The notion of coordinate system is well known.We rephrase its main aspects in terms
</p>
<p>of vector properties.
</p>
<p>Definition 1.2.1 Given a line r , a coordinate system � on it is defined by a point
</p>
<p>O &isin; r and a vector i = A &minus; O , where A &isin; r and A �= O .
</p>
<p>The point O is called the origin of the coordinate system, the norm ‖A &minus; O‖ is
</p>
<p>the unit of measure (or length) of �, with i the basis unit vector. The orientation of
</p>
<p>i is the orientation of the coordinate system �.
</p>
<p>A coordinate system � provides a bijection between the points on the line r and
</p>
<p>R. Any point P &isin; r singles out the real number x such that P &minus; O = x i; viceversa,
</p>
<p>for any x &isin; R one has the point P &isin; r defined by P &minus; O = x i. One says that P
</p>
<p>has coordinate x , and we shall denote it by P = (x), with respect to the coordinate
</p>
<p>system � that is also denoted as (O; x) or (O; i).
</p>
<p>Definition 1.2.2 Given a plane α, a coordinate system� on it is defined by a point
</p>
<p>O &isin; α and a pair of non zero distinct (and not having the same direction) vectors
</p>
<p>i = A &minus; O and j = B &minus; O with A, B &isin; α, and ‖A &minus; O‖ = ‖B &minus; O‖.
The point O is the origin of the coordinate system, the (common) norm of the
</p>
<p>vectors i, j is the unit length of �, with i, j the basis unit vectors. The system is
</p>
<p>oriented in such a way that the vector i coincides with j after an anticlockwise
</p>
<p>rotation of angle φ with 0 &lt; φ &lt; π. The line defined by O and i, with its given
</p>
<p>orientation, is usually referred to as a the abscissa axis, while the one defined by O
</p>
<p>and j, again with its given orientation, is called ordinate axis.
</p>
<p>As before, it is immediate to see that a coordinate system � on α allows one to
</p>
<p>define a bijection between points on α and ordered pairs of real numbers. Any
</p>
<p>P &isin; α uniquely provides, via the parallelogram rule (see Fig. 1.5), the ordered
</p>
<p>pair (x, y) &isin; R2 with P &minus; O = x i + yj; conversely, for any given ordered pair
</p>
<p>(x, y) &isin; R2, one defines P &isin; α as given by P &minus; O = x i + yj.
</p>
<p>With respect to �, the elements x &isin; R and y &isin; R are the coordinates of P ,
</p>
<p>and this will be denoted by P = (x, y). The coordinate system � will be denoted
</p>
<p>(O; i, j) or (O; x, y).
</p>
<p>Fig. 1.5 The bijection P(x, y) &harr; P &minus; O = x i + yj in a plane</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Vectors and Coordinate Systems
</p>
<p>Definition 1.2.3 Acoordinate system� = (O; i, j) on a planeα is called an orthog-
</p>
<p>onal cartesian coordinate system if φ = π/2, where φ is as before the width of the
</p>
<p>anticlockwise rotation under which i coincides with j.
</p>
<p>In order to introduce a coordinate system for the physical three dimensional
</p>
<p>space, we start by considering three unit-length vectors inV3O given as u = U &minus; O,
</p>
<p>v = V &minus; O, w = W &minus; O , andwe assume the points O,U, V, W not to be on the
</p>
<p>same plane. This means that any two vectors, u and v say, determine a plane which
</p>
<p>does not contain the third point, say W . Seen from W , the vector u will coincide
</p>
<p>with v under an anticlockwise rotation by an angle that we denote by ûv.
</p>
<p>Definition 1.2.4 An ordered triple (u, v,w) of unit vectors in V3O which do not lie
</p>
<p>on the same plane is called right-handed if the three angles ûv, v̂w, ŵu, defined by
</p>
<p>the prescription above are smaller than π. Notice that the order of the vectors matters.
</p>
<p>Definition 1.2.5 A coordinate system � for the space S is given by a point O &isin; S
</p>
<p>and three non zero distinct (and not lying on the same plane) vectors i = A &minus; O,
</p>
<p>j = B &minus; O andk = C &minus; O , with A, B,C &isin; S, and ‖A &minus; O‖ = ‖B &minus; O‖ =
</p>
<p>‖C &minus; O‖ and (i, j,k) giving a right-handed triple.
</p>
<p>The point O is the origin of the coordinate system, the common length of the
</p>
<p>vectors i, j,k is the unit measure in �, with i, j,k the basis unit vectors. The line
</p>
<p>defined by O and i, with its orientation, is the abscissa axis, that defined by O and j
</p>
<p>is the ordinate axis, while the one defined by O and k is the quota axis.
</p>
<p>With respect to the coordinate system �, one establishes, via V3O , a bijection
</p>
<p>between ordered triples of real numbers and points in S. One has
</p>
<p>P &harr; P &minus; O &harr; (x, y, z)
</p>
<p>with P &minus; O = x i + yj + zk as in Fig. 1.6. The real numbers x, y, z are the com-
</p>
<p>ponents (or coordinates) of the applied vector P &minus; O , and this will be denoted by
</p>
<p>P = (x, y, z). Accordingly, the coordinate system will be denoted by
</p>
<p>� = (O; i, j,k) = (O; x, y, z). The coordinate system� is called cartesianorthog-
</p>
<p>onal if the vectors i, j,k are pairwise orthogonal.
</p>
<p>By writing v = P &minus; O , it is convenient to denote by vx , vy, vz the components
</p>
<p>of v with respect to a cartesian coordinate system �, so to have
</p>
<p>v = vx i + vyj + vzk.
</p>
<p>In order to simplify the notations, we shall also write this as
</p>
<p>v = (vx , vy, vz),
</p>
<p>implicitly assuming that such components of v refer to the cartesian coordinate sys-
</p>
<p>tem (O; i, j,k). Clearly the components of a given vector v depend on the particular
</p>
<p>coordinate system one is using.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Coordinate Systems 7
</p>
<p>Fig. 1.6 The bijection P(x, y, z) &harr; P &minus; O = x i + yj + zk in the space
</p>
<p>Exercise 1.2.6 One has
</p>
<p>1. The zero (null) vector 0 = O &minus; O has components (0, 0, 0)with respect to any
</p>
<p>coordinate system whose origin is O , and it is the only vector with this property.
</p>
<p>2. Given a coordinate system� = (O; i, j,k), the basis unit vectors have compo-
</p>
<p>nents
</p>
<p>i = (1, 0, 0) , j = (0, 1, 0) , k = (0, 0, 1).
</p>
<p>3. Given a coordinate system� = (O; i, j,k) for the space S, we call coordinate
</p>
<p>plane each plane determined by a pair of axes of�. We have v = (a, b, 0), with
</p>
<p>a, b &isin; R, if v is on the plane xy, v&prime; = (0, b&prime;, c&prime;) if v&prime; is on the plane yz, and
</p>
<p>v&prime;&prime; = (a&prime;&prime;, 0, c&prime;&prime;) if v&prime;&prime; is on the plane xz.
</p>
<p>Example 1.2.7 Themotion of a pointmass in three dimensional space is described by
</p>
<p>a map t &isin; R �&rarr; x(t) &isin; V3O where t represents the time variable and x(t) is the posi-
</p>
<p>tion of the point mass at time t . With respect to a coordinate system� = (O; x, y, z)
</p>
<p>we then write
</p>
<p>x(t) = (x(t), y(t), z(t)) or equivalently x(t) = x(t)i + y(t)j + z(t)k.
</p>
<p>The corresponding velocity is a vector applied in x(t), that is v(t) &isin; V3x(t), with
</p>
<p>components
</p>
<p>v(t) = (vx (t), vy(t), vz(t)) =
dx(t)
</p>
<p>dt
= (
</p>
<p>dx
</p>
<p>dt
,
dy
</p>
<p>dt
,
dz
</p>
<p>dt
),
</p>
<p>while the acceleration is the vector a(t) &isin; V3x(t) with components
</p>
<p>a(t) =
dv(t)
</p>
<p>dt
= (
</p>
<p>dx2
</p>
<p>dt2
,
d2y
</p>
<p>dt2
,
d2z
</p>
<p>dt2
).</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Vectors and Coordinate Systems
</p>
<p>One also uses the notations
</p>
<p>v =
dx
</p>
<p>dt
= ẋ and a =
</p>
<p>d2x
</p>
<p>dt2
= v̇ = ẍ.
</p>
<p>In the newtonian formalism for the dynamics, a force acting on the given point
</p>
<p>mass is a vector applied in x(t), that is F &isin; V3x(t) with components F = (Fx , Fy, Fz),
</p>
<p>and the second law of dynamics is written as
</p>
<p>m a = F
</p>
<p>where m &gt; 0 is the value of the inertial mass of the moving point mass. Such a
</p>
<p>relation can be written component-wise as
</p>
<p>m
d2x
</p>
<p>dt2
= Fx , m
</p>
<p>d2y
</p>
<p>dt2
= Fy, m
</p>
<p>d2z
</p>
<p>dt2
= Fz .
</p>
<p>Acoordinate system forS allows one to express the operations of sum and product
</p>
<p>by a scalar in V3O in terms of elementary algebraic expressions.
</p>
<p>Proposition 1.2.8 With respect to the coordinate system � = (O; i, j,k), let us
</p>
<p>consider the vectors v = vx i + vyj + vzk and w = wx i + wyj + wzk, and the scalar
</p>
<p>λ &isin; R. One has:
</p>
<p>(1) v + w = (vx + wx )i + (vy + wy)j + (vz + wz)k,
</p>
<p>(2) λv = λvx i + λvyj + λvzk.
</p>
<p>Proof (1) Sincev + w = (vx i + vyj + vzk)+ (wx i + wyj + wzk), byusing the com-
</p>
<p>mutativity and the associativity of the sum of vectors applied at a point, one has
</p>
<p>v + w = (vx i + wx i)+ (vyj + wyj)+ (vzk + wzk).
</p>
<p>Being the product distributive over the sum, this can be regrouped as in the
</p>
<p>claimed identity.
</p>
<p>(2) Along the same lines as (1). �
</p>
<p>Remark 1.2.9 By denoting v = (vx , vy, vz) and w = (wx , wy, wz), the identities
</p>
<p>proven in the proposition above are written as
</p>
<p>(vx , vy, vz)+ (wx , wy, wz) = (vx + wx , vy + wy, vz + wz),
</p>
<p>λ(vx , vy, vz) = (λvx ,λvy,λvz).
</p>
<p>This suggests a generalisation we shall study in detail in the next chapter. If we
</p>
<p>denote by R3 the set of ordered triples of real numbers, and we consider a pair of</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Coordinate Systems 9
</p>
<p>elements (x1, x2, x3) and (y1, y2, y3) in R
3, with λ &isin; R, one can introduce a sum of
</p>
<p>triples and a product by a scalar:
</p>
<p>(x1, x2, x3)+ (y1, y2, y3) = (x1 + y1, x2 + y2, x3 + y3),
</p>
<p>λ(x1, x2, x3) = (λx1,λx2,λx3).
</p>
<p>1.3 More Vector Operations
</p>
<p>In this section we recall the notions&mdash;originating in physics&mdash;of scalar product,
</p>
<p>vector product and mixed products.
</p>
<p>Before we do this, as an elementary consequence of the Pythagora&rsquo;s theorem, one
</p>
<p>has the following (see Fig. 1.6)
</p>
<p>Proposition 1.3.1 Let v = (vx , vy, vz) be an arbitrary vector in V
3
O with respect to
</p>
<p>the cartesian orthogonal coordinate system (O; i, j, z). One has
</p>
<p>‖v‖ =
</p>
<p>&radic;
v2x + v
</p>
<p>2
y + v
</p>
<p>2
z .
</p>
<p>Definition 1.3.2 Let us consider a pair of vectors v,w &isin; V3O . The scalar product of
</p>
<p>v and w, denoted by v &middot; w, is the real number
</p>
<p>v &middot; w = ‖v‖ ‖w‖ cosα
</p>
<p>with α = v̂w the plane angle defined by v and w. Since cosα = cos(&minus;α), for this
</p>
<p>definition one has cos v̂w = cos ŵv.
</p>
<p>The definition of a scalar product for vectors in V2O is completely analogue.
</p>
<p>Remark 1.3.3 The following properties follow directly from the definition.
</p>
<p>(1) If v = 0, then v &middot; w = 0.
</p>
<p>(2) If v, w are both non zero vectors, then
</p>
<p>v &middot; w = 0 &lArr;&rArr; cosα = 0 &lArr;&rArr; v &perp; w.
</p>
<p>(3) For any v &isin; V3O , it holds that:
</p>
<p>v &middot; v = ‖v‖2
</p>
<p>and moreover
</p>
<p>v &middot; v = 0 &lArr;&rArr; v = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Vectors and Coordinate Systems
</p>
<p>(4) From (2), (3), if (O; i, j,k) is an orthogonal cartesian coordinate system, then
</p>
<p>i &middot; i = j &middot; j = k &middot; k = 1, i &middot; j = j &middot; k = k &middot; i = 0.
</p>
<p>Proposition 1.3.4 For any choice of u, v,w &isin; V3O and λ &isin; R, the following identi-
</p>
<p>ties hold.
</p>
<p>(i) v &middot; w = w &middot; v,
</p>
<p>(ii) (λv) &middot; w = v &middot; (λw) = λ(v &middot; w),
</p>
<p>(iii) u &middot; (v + w) = u &middot; v + u &middot; w.
</p>
<p>Proof (i) From the definition one has
</p>
<p>v &middot; w = ‖v‖ ‖w‖ cos v̂w = ‖w‖ ‖v‖ cos ŵv = w &middot; v.
</p>
<p>(ii) Setting a = (λv) &middot; w, b = v &middot; (λw) and c = λ(v &middot; w), from the Definition 1.3.2
</p>
<p>and the properties of the norm of a vector, one has
</p>
<p>a = (λv) &middot; w = ‖λv‖ ‖w‖ cosα&prime; = |λ|‖v‖ ‖w‖ cosα&prime;
</p>
<p>b = v &middot; (λw) = ‖v‖ ‖λw‖ cosα&prime;&prime; = ‖v‖ |λ|‖w‖ cosα&prime;&prime;
</p>
<p>c = λ(v &middot; w) = λ(‖v‖ ‖w‖ cosα) = λ‖v‖ ‖w‖ cosα
</p>
<p>where α&prime; = (̂λv)w, α&prime;&prime; = v̂(λw) and α = v̂w. If λ = 0, then a = b = c = 0.
</p>
<p>If λ &gt; 0, then |λ| = λ and α = α&prime; = α&prime;&prime;; from the commutativity and the
</p>
<p>associativity of the product in R, this gives that a = b = c. If λ &lt; 0, then
</p>
<p>|λ| = &minus;λ andα&prime; = α&prime;&prime; = π &minus; α, thus giving cosα&prime; = cosα&prime;&prime; = &minus; cosα. These
</p>
<p>read a = b = c.
</p>
<p>(iii) We sketch the proof for parallel u, v,w. Under this condition, the result depends
</p>
<p>on the relative orientations of the vectors. If u, v,w have the same orientation,
</p>
<p>one has
</p>
<p>u &middot; (v + w) = ‖u‖ ‖v + w‖
</p>
<p>= ‖u‖(‖v‖ + ‖w‖)
</p>
<p>= ‖u‖ ‖v‖ + ‖u‖ ‖w‖
</p>
<p>= u &middot; v + u &middot; w.
</p>
<p>If v and w have the same orientation, which is not the orientation of u, one has
</p>
<p>u &middot; (v + w) = &minus;‖u‖ ‖v + w‖
</p>
<p>= &minus;‖u‖(‖v‖ + ‖w‖)
</p>
<p>= &minus;‖u‖ ‖v‖ &minus; ‖u‖ ‖w‖
</p>
<p>= u &middot; v + u &middot; w.
</p>
<p>We leave the reader to explicitly prove the other cases. �</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 More Vector Operations 11
</p>
<p>Byexpressing vectors inV3O in terms of an orthogonal cartesian coordinate system,
</p>
<p>the scalar product has an expression that will allow us to define the scalar product of
</p>
<p>vectors in the more general situation of euclidean spaces.
</p>
<p>Proposition 1.3.5 Given (O; i, j,k), an orthogonal cartesian coordinate system for
</p>
<p>S; with vectors v = (vx , vy, vz) and w = (wx , wy, wz) in V
3
O , one has
</p>
<p>v &middot; w = vxwx + vywy + vzwz .
</p>
<p>Proof Withv = vx i + vyj + vzk andw = wx i + wyj + wzk, fromProposition1.3.4,
</p>
<p>one has
</p>
<p>v &middot; w = (vx i + vyj + vzk) &middot; (wx i + wyj + wzk)
</p>
<p>= vxwx i &middot; i + vywx j &middot; i + vzwx k &middot; i
</p>
<p>+ vxwy i &middot; j + vywyj &middot; j + vzwyk &middot; j + vxwzi &middot; k + vywzj &middot; k + vzwzk &middot; k.
</p>
<p>The result follows directly from (4) in Remark 1.3.3, that is i &middot; j = j &middot; k = k &middot; i = 0
</p>
<p>as well as i &middot; i = j &middot; j = k &middot; k = 1. �
</p>
<p>Exercise 1.3.6 With respect to a given cartesian orthogonal coordinate system, con-
</p>
<p>sider the vectors v = (2, 3, 1) and w = (1,&minus;1, 1). We verify they are orthogonal.
</p>
<p>From (2) in Remark 1.3.3 this is equivalent to show that v &middot; w = 0. From Proposition
</p>
<p>1.3.5, one has v &middot; w = 2 &middot; 1+ 3 &middot; (&minus;1)+ 1 &middot; 1 = 0.
</p>
<p>Example 1.3.7 If the map x(t) : R &ni; t �&rarr; x(t) &isin; V3O describes the motion (notice
</p>
<p>that the range of the map gives the trajectory) of a point mass (with mass m), its
</p>
<p>kinetic energy is defined by
</p>
<p>T =
1
</p>
<p>2
m ‖v(t)‖2.
</p>
<p>With respect to an orthogonal coordinate system � = (O; i, j,k), given
</p>
<p>v(t) = (vx (t), vy(t), vz(t)) as in the Example 1.2.7, we have from the Proposi-
</p>
<p>tion 1.3.5 that
</p>
<p>T =
1
</p>
<p>2
m (v2x + v
</p>
<p>2
y + v
</p>
<p>2
z ).
</p>
<p>Also the following notion will be generalised in the context of euclidean spaces.
</p>
<p>Definition 1.3.8 Given two non zero vectors v and w in V3O , the orthogonal projec-
</p>
<p>tion of v along w is defined as the vector vw in V
3
O given by
</p>
<p>vw =
v &middot; w
</p>
<p>‖w‖2
w.
</p>
<p>As the first part of Fig. 1.7 displays, vw is parallel to w.
</p>
<p>From the identities proven in Proposition 1.3.4 one easily has</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Vectors and Coordinate Systems
</p>
<p>Fig. 1.7 Orthogonal projections
</p>
<p>Proposition 1.3.9 For any u, v,w &isin; V3O , the following identities hold:
</p>
<p>(a) (u + v)w = uw + vw,
</p>
<p>(b) v &middot; w = vw &middot; w = wv &middot; v .
</p>
<p>The point (a) is illustrated by the second part of the Fig. 1.7.
</p>
<p>Remark 1.3.10 The scalar product we have defined is a map
</p>
<p>σ : V3O &times; V
3
O &minus;&rarr; R, σ(v,w) = v &middot; w.
</p>
<p>Also, the scalar product of vectors on a plane is a map σ : V2O &times; V
2
O &minus;&rarr; R.
</p>
<p>Definition 1.3.11 Let v,w &isin; V3O . The vector product between v and w, denoted by
</p>
<p>v &and; w, is defined as the vector in V3O whose modulus is
</p>
<p>‖v &and; w‖ = ‖v‖ ‖w‖ sinα,
</p>
<p>where α = v̂w, with 0 &lt; α &lt; π is the angle defined by v e w; the direction of v &and; w
</p>
<p>is orthogonal to both v and w; and its orientation is such that (v,w, v &and; w) is a
</p>
<p>right-handed triple as in Definition 1.2.4.
</p>
<p>Remark 1.3.12 The following properties follow directly from the definition.
</p>
<p>(i) if v = 0 then v &and; w = 0,
</p>
<p>(ii) if v and w are both non zero then
</p>
<p>v &and; w = 0 &lArr;&rArr; sinα = 0 &lArr;&rArr; v ‖ w,
</p>
<p>(one trivially has v &and; v = 0),
</p>
<p>(iii) if (O; i, j,k) is an orthogonal cartesian coordinate system, then
</p>
<p>i &and; j = k = &minus;j &and; i, j &and; k = i = &minus;k &and; j, k &and; i = j = &minus;i &and; k.
</p>
<p>We omit to prove the following proposition.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 More Vector Operations 13
</p>
<p>Proposition 1.3.13 For any u, v,w &isin; V3O and λ &isin; R, the following identities holds:
</p>
<p>(i) v &and; w = &minus;w &and; v,
</p>
<p>(ii) (λv) &and; w = v &and; (λw) = λ(v &and; w)
</p>
<p>(iii) u &and; (v + w) = u &and; v + u &and; w,
</p>
<p>Exercise 1.3.14 With respect to a given cartesian orthogonal coordinate system,
</p>
<p>consider in V3O the vectors v = (1, 0,&minus;1) e w = (&minus;2, 0, 2). To verify that they are
</p>
<p>parallel, we recall the abov e result (ii) in the Remark 1.3.12 and compute, using the
</p>
<p>Proposition 1.3.15, that v &and; w = 0.
</p>
<p>Proposition 1.3.15 Let v = (vx , vy, vz) and w = (wx , wy, wz) be elements in V
3
O
</p>
<p>with respect to a given cartesian orthogonal coordinate system. It is
</p>
<p>v &and; w = (vywz &minus; vzwy, vzwx &minus; vxwz, vxwy &minus; vywx ).
</p>
<p>Proof Given the Remark 1.3.12 and the Proposition 1.3.13, this comes as an easy
</p>
<p>computation. �
</p>
<p>Remark 1.3.16 The vector product defines a map
</p>
<p>τ : V3O &times; V
3
O &minus;&rarr; V
</p>
<p>3
O , τ (v,w) = v &and; w.
</p>
<p>Clearly, such a map has no meaning on a plane.
</p>
<p>Example 1.3.17 By slightly extending the Definition 1.3.11, one can use the vec-
</p>
<p>tor product for additional notions coming from physics. Following Sect. 1.1, we
</p>
<p>consider vectors u,w as elements in W3, that is vectors applied at arbitrary
</p>
<p>points in the physical three dimensional space S, with components u = (ux , u y, uz)
</p>
<p>and w = (wx , wy, wz) with respect to a cartesian orthogonal coordinate system
</p>
<p>� = (O; i, j,k). In parallelwithProposition1.3.15,wedefine τ : W3 &times; W3 &rarr; W3
</p>
<p>as
</p>
<p>u &and; w = (u ywz &minus; uzwy, uzwx &minus; uxwz, uxwy &minus; u ywx ).
</p>
<p>If u &isin; V3x is a vector applied at x, its momentum with respect to a point x
&prime; &isin; S is the
</p>
<p>vector in W3 defined by
</p>
<p>M = (x &minus; x&prime;) &and; u.
</p>
<p>In particular, if u = F is a force acting on a point mass in x, its momentum is
</p>
<p>M = (x &minus; x&prime;) &and; F.
</p>
<p>If x(t) &isin; V3O describes the motion of a point mass (with mass m &gt; 0), whose
</p>
<p>velocity is v(t), then its corresponding angular momentum with respect to a point x&prime;
</p>
<p>is defined by
</p>
<p>Lx&prime;(t) = (x(t)&minus; x
&prime;) &and; mv(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Vectors and Coordinate Systems
</p>
<p>Exercise 1.3.18 The angular momentum is usually definedwith respect to the origin
</p>
<p>of the coordinate system�, giving LO(t) = x(t) &and; mv(t). If we consider a circular
</p>
<p>uniform motion
</p>
<p>x(t) =
(
</p>
<p>x(t) = r cos(ωt), y(t) = r sin(ωt), z(t) = 0
)
,
</p>
<p>with r &gt; 0 the radius of the trajectory and ω &isin; R the angular velocity, then
</p>
<p>v(t) =
(
</p>
<p>vx (t) = &minus;rω sin(ωt), y(t) = rω cos(ωt), vz(t) = 0
)
</p>
<p>so that
</p>
<p>LO(t) = (0, 0,mrω).
</p>
<p>Thus, a circular motion on the xy plane has angular momentum along the z axis.
</p>
<p>Definition 1.3.19 Given an ordered triple u, v,w &isin; V3O , their mixed product is the
</p>
<p>real number
</p>
<p>u &middot; (v &and; w).
</p>
<p>Proposition 1.3.20 Given a cartesian orthogonal coordinate system in S with
</p>
<p>u = (ux , u y, uz), v = (vx , vy, vz) and w = (wx , wy, wz) in V
3
O , one has
</p>
<p>u &middot; (v &and; w) = ux (vywz &minus; vzwy)+ u y(vzwx &minus; vxwz)+ uz(vxwy &minus; vywx ).
</p>
<p>Proof It follows immediately by Propositions 1.3.5 and 1.3.15. �
</p>
<p>In the space S, the vector product between u &and; w is the area of the parallelogram
</p>
<p>defined by u and v, while the mixed product u &middot; (v &and; w) give the volume of the
</p>
<p>parallelepiped defined by u, v,w.
</p>
<p>Proposition 1.3.21 Given u, v,w &isin; V3O .
</p>
<p>1. Denote α = v̂w the angle defined by v and w. Then, the area A of the parallelo-
</p>
<p>gram whose edges are u and v, is given by
</p>
<p>A = ‖v‖ ‖w‖ sinα = ‖v &and; w‖.
</p>
<p>2. Denote θ = ̂u(v &and; w) the angle defined by u and v &and; w. Then the volume V of
</p>
<p>the parallelepiped whose edges are u, v,w, is given by
</p>
<p>V = A‖u‖ cos θ = ‖u &middot; v &and; w‖.
</p>
<p>Proof The claim is evident, as shown in the Figs. 1.8 and 1.9. �</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Divergence, Rotor, Gradient and Laplacian 15
</p>
<p>Fig. 1.8 The area of the parallelogramm with edges v and w
</p>
<p>Fig. 1.9 The volume of the parallelogramm with edges v,w,u
</p>
<p>1.4 Divergence, Rotor, Gradient and Laplacian
</p>
<p>We close this chapter by describing how the notion of vector applied at a point also
</p>
<p>allows one to introduce a definition of a vector field.
</p>
<p>The intuition coming from physics requires to consider, for each point x in the
</p>
<p>physical space S, a vector applied at x. We describe it as a map
</p>
<p>S &ni; x �&rarr; A(x) &isin; V3x .
</p>
<p>With respect to a given cartesian orthogonal reference system for S we can write
</p>
<p>this in components as x = (x1, x2, x3) and A(x) = (A1(x), A2(x), A3(x)) and one
</p>
<p>can act on a vector field with partial derivatives (first order differential operators),
</p>
<p>&part;a = (&part;/&part;xa) with a = 1, 2, 3, defined as usual by
</p>
<p>&part;a(xb) = (δab), with δab =
</p>
<p>{
1 if a = b
</p>
<p>0 if a �= b
.
</p>
<p>Then, (omitting the explicit dependence of A on x) one defines
</p>
<p>divA =
</p>
<p>3&sum;
</p>
<p>k=1
</p>
<p>(&part;k Ak) &isin; R
</p>
<p>rot A = (&part;2 A3 &minus; &part;3A2)i + (&part;3A1 &minus; &part;1A3)j + (&part;1A2 &minus; &part;2 A1)k &isin; V
3
x .</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1 Vectors and Coordinate Systems
</p>
<p>By introducing the triple &nabla; = (&part;1, &part;2, &part;3), such actions can be formally written as a
</p>
<p>scalar product and a vector product, that is
</p>
<p>divA = &nabla; &middot; A
</p>
<p>rot A = &nabla; &and; A .
</p>
<p>Furthermore, if f : S &rarr; R is a real valued function defined on S, that is a (real)
</p>
<p>scalar field on S, one has the grad operator
</p>
<p>grad f = &nabla; f = (&part;1 f, &part;2 f, &part;3 f )
</p>
<p>as well as the Laplacian operator
</p>
<p>&nabla;2 f = div(&nabla; f ) =
( 3&sum;
</p>
<p>k=1
</p>
<p>&part;k&part;k
)
</p>
<p>f = &part;21 f + &part;
2
2 f + &part;
</p>
<p>2
3 f .
</p>
<p>Exercise 1.4.1 The properties of the mixed products yields a straightforward proof
</p>
<p>of the identity
</p>
<p>div(rot A) = &nabla; &middot; (&nabla; &and; A) = 0 ,
</p>
<p>for any vector fieldA. On the other hand, a direct computation shows also the identity
</p>
<p>rot (grad f ) = &nabla; &and; (grad f ) = 0 ,
</p>
<p>for any scalar field f .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
</p>
<p>Vector Spaces
</p>
<p>The notion of vector space can be defined over any field K. We shall mainly consider
</p>
<p>the case K = R and briefly mention the case K = C. Starting from our exposition,
</p>
<p>it is straightforward to generalise to any field.
</p>
<p>2.1 Definition and Basic Properties
</p>
<p>The model of the construction is the collection of all vectors in the space applied at
</p>
<p>a point with the operations of sum and multiplication by a scalar, as described in the
</p>
<p>Chap. 1.
</p>
<p>Definition 2.1.1 A non empty set V is called a vector space over R (or a real vector
</p>
<p>space or an R-vector space) if there are defined two operations,
</p>
<p>(a) an internal one: a sum of vectors s : V &times; V &rarr; V ,
</p>
<p>V &times; V &ni; (v, v&prime;) �&rarr; s(v, v&prime;) = v + v&prime;,
</p>
<p>(b) an exterior one: the product by a scalar p : R &times; V &rarr; V
</p>
<p>R &times; V &ni; (k, v) �&rarr; p(k, v) = kv,
</p>
<p>and these operations are required to satisfy the following conditions:
</p>
<p>(1) There exists an element 0V &isin; V , which is neutral for the sum, such that
</p>
<p>(V,+, 0V ) is an abelian group.
</p>
<p>For any k, k &prime; &isin; R and v, v&prime; &isin; V one has
</p>
<p>(2) (k + k &prime;)v = kv + k &prime;v
</p>
<p>(3) k(v + v&prime;) = kv + kv&prime;
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_2
</p>
<p>17</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_2&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_2&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>18 2 Vector Spaces
</p>
<p>(4) k(k &prime;v) = (kk &prime;)v
</p>
<p>(5) 1v = v, with 1 = 1R.
</p>
<p>The elements of a vector space are called vectors; the element 0V is the zero or null
</p>
<p>vector. A vector space is also called a linear space.
</p>
<p>Remark 2.1.2 Given the properties of a group (see A.2.9), the null vector 0V and the
</p>
<p>opposite &minus;v to any vector v are (in any given vector space) unique. The sums can
</p>
<p>be indeed simplified, that is v + w = v + u =&rArr; w = u. Such a statement is easily
</p>
<p>proven by adding to both terms in v + w = v + u the element &minus;v and using the
</p>
<p>associativity of the sum.
</p>
<p>As already seen in Chap. 1, the collections V2O (vectors in a plane) and V
3
O (vectors
</p>
<p>in the space) applied at the point O are real vector spaces. The bijection V3O &larr;&rarr; R
3
</p>
<p>introduced in the Definition 1.2.5, together with the Remark 1.2.9, suggest the natural
</p>
<p>definitions of sum and product by a scalar for the set R3 of ordered triples of real
</p>
<p>numbers.
</p>
<p>Proposition 2.1.3 The collection R3 of triples of real numbers together with the
</p>
<p>operations defined by
</p>
<p>I. (x1, x2, x3)+ (y1, y2, y3) = (x1 + y1, x2 + y2, x3 + y3), for any (x1, x2, x3),
</p>
<p>(y1, y2, y3) &isin; R
3,
</p>
<p>II. a(x1, x2, x3) = (ax1, ax2, ax3), for any a &isin; R, (x1, x2, x3) &isin; R
3,
</p>
<p>is a real vector space.
</p>
<p>Proof We verify that the conditions given in the Definition 2.1.1 are satisfied. We
</p>
<p>first notice that (a) and (b) are fullfilled, since R3 is closed with respect to the
</p>
<p>operations in I. and II. of sum and product by a scalar. The neutral element for the
</p>
<p>sum is 0R3 = (0, 0, 0), since one clearly has
</p>
<p>(x1, x2, x3)+ (0, 0, 0) = (x1, x2, x3).
</p>
<p>The datum (R3,+, 0R3) is an abelian group, since one has
</p>
<p>&bull; The sum (R3,+) is associative, from the associativity of the sum in R:
</p>
<p>(x1, x2, x3)+ ((y1, y2, y3)+ (z1, z2, z3))
</p>
<p>= (x1, x2, x3)+ (y1 + z1, y2 + z2, y3 + z3)
</p>
<p>= (x1 + (y1 + z1), x2 + (y2 + z2), x3 + (y3 + z3))
</p>
<p>= ((x1 + y1)+ z1, (x2 + y2)+ z2, (x3 + y3)+ z3)
</p>
<p>= (x1 + y1, x2 + y2, x3 + y3)+ (z1, z2, z3)
</p>
<p>= ((x1, x2, x3)+ (y1, y2, y3))+ (z1, z2, z3).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Definition and Basic Properties 19
</p>
<p>&bull; From the identity
</p>
<p>(x1, x2, x3)+ (&minus;x1,&minus;x2,&minus;x3) = (x1 &minus; x1, x2 &minus; x2, x3 &minus; x3) = (0, 0, 0)
</p>
<p>one has (&minus;x1,&minus;x2,&minus;x3) as the opposite in R
3 of the element (x1, x2, x3).
</p>
<p>&bull; The group (R3,+) is commutative, since the sum in R is commutative:
</p>
<p>(x1, x2, x3)+ (y1, y2, y3) = (x1 + y1, x2 + y2, x3 + y3)
</p>
<p>= (y1 + x1, y2 + x2, y3 + x3)
</p>
<p>= (y1, y2, y3)+ (x1, x2, x3).
</p>
<p>We leave to the reader the task to show that the conditions (1), (2), (3), (4) in Defi-
</p>
<p>nition 2.1.1 are satisfied: for any λ,λ&prime; &isin; R and any (x1, x2, x3), (y1, y2, y3) &isin; R
3 it
</p>
<p>holds that
</p>
<p>1. (λ+ λ&prime;)(x1, x2, x3) = λ(x1, x2, x3)+ λ
&prime;(x1, x2, x3)
</p>
<p>2. λ((x1, x2, x3)+ (y1, y2, y3)) = λ(x1, x2, x3)+ λ(y1, y2, y3)
</p>
<p>3. λ(λ&prime;(x1, x2, x3)) = (λλ
&prime;)(x1, x2, x3)
</p>
<p>4. 1(x1, x2, x3) = (x1, x2, x3). �
</p>
<p>The previous proposition can be generalised in a natural way. If n &isin; N is a positive
</p>
<p>natural number, one defines the n-th cartesian product of R, that is the collection of
</p>
<p>ordered n-tuples of real numbers
</p>
<p>R
n = {X = (x1, . . . , xn) : xk &isin; R},
</p>
<p>and the following operations, with a &isin; R, (x1, . . . , xn), (y1, . . . , yn) &isin; R
n:
</p>
<p>In. (x1, . . . , xn)+ (y1, . . . , yn) = (x1 + y1, . . . , xn + yn)
</p>
<p>IIn. a(x1, . . . , xn) = (ax1, . . . , axn).
</p>
<p>The previous proposition can be directly generalised to the following.
</p>
<p>Proposition 2.1.4 With respect to the above operations, the set Rn is a vector space
</p>
<p>over R.
</p>
<p>The elements in Rn are called n-tuples of real numbers. With the notation
</p>
<p>X = (x1, . . . , xn) &isin; R
n , the scalar xk , with k = 1, 2, . . . , n, is the k-th component
</p>
<p>of the vector X .
</p>
<p>Example 2.1.5 As in the Definition A.3.3, consider the collection of all polynomials
</p>
<p>in the indeterminate x and coefficients in R, that is
</p>
<p>R[x] =
{
</p>
<p>f (x) = a0 + a1x + a2x
2 + &middot; &middot; &middot; + an x
</p>
<p>n : ak &isin; R, n &ge; 0
}
</p>
<p>,
</p>
<p>with the operations of sum and product by a scalar λ &isin; R defined, for any pair of
</p>
<p>elements in R[x], f (x) = a0 + a1x + a2x
2 + &middot; &middot; &middot; + an x
</p>
<p>n and g(x) = b0 + b1x +
</p>
<p>b2x
2 + &middot; &middot; &middot; + bm x
</p>
<p>m , component-wise by</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Vector Spaces
</p>
<p>Ip. f (x)+ g(x) = a0 + b0 + (a1 + b1)x + (a2 + b2)x
2 + &middot; &middot; &middot;
</p>
<p>IIp. λ f (x) = λa0 + λa1x + λa2x
2 + &middot; &middot; &middot; + λan x
</p>
<p>n .
</p>
<p>Endowed with the previous operations, the set R[x] is a real vector space; R[x] is
</p>
<p>indeed closed with respect to the operations above. The null polynomial, denoted by
</p>
<p>0R[x] (that is the polynomial with all coefficients equal zero), is the neutral element
</p>
<p>for the sum. The opposite to the polynomial f (x) = a0 + a1x + a2x
2 + &middot; &middot; &middot; + an x
</p>
<p>n
</p>
<p>is the polynomial (&minus;a0 &minus; a1x &minus; a2x
2 &minus; &middot; &middot; &middot; &minus; an x
</p>
<p>n) &isin; R[x] that one denotes by
</p>
<p>&minus; f (x). We leave to the reader to prove that (R[x],+, 0R[x]) is an abelian group and
</p>
<p>that all the additional conditions in Definition 2.1.1 are fulfilled.
</p>
<p>Exercise 2.1.6 We know from the Proposition A.3.5 that R[x]r , the subset in R[x]
</p>
<p>of polynomials with degree not larger than a fixed r &isin; N, is closed under addition
</p>
<p>of polynomials. Since the degree of the polynomial λ f (x) coincides with the degree
</p>
<p>of f (x) for any λ 
= 0, we see that also the product by a scalar, as defined in IIp.
</p>
<p>above, is defined consistently on R[x]r . It is easy to verify that also R[x]r is a real
</p>
<p>vector space.
</p>
<p>Remark 2.1.7 The proof that Rn , R[x] and R[x]r are vector space over R relies on
</p>
<p>the properties of R as a field (in fact a ring, since the multiplicative inverse in R does
</p>
<p>not play any role).
</p>
<p>Exercise 2.1.8 The set Cn , that is the collection of ordered n-tuples of complex
</p>
<p>numbers, can be given the structure of a vector space over C. Indeed, both the
</p>
<p>operations In. and IIn. considered in the Proposition 2.1.3 when intended for complex
</p>
<p>numbers make perfectly sense:
</p>
<p>Ic. (z1, . . . , zn)+ (w1, . . . , wn) = (z1 + w1, . . . , zn + wn)
</p>
<p>IIc. c(z1, . . . , zn) = (cz1, . . . , czn)
</p>
<p>with c &isin; C, and (z1, . . . , zn), (w1, . . . , wn) &isin; C
n .
</p>
<p>The reader is left to show that Cn is a vector space over C.
</p>
<p>The space Cn can also be given a structure of vector space over R, by noticing
</p>
<p>that the product of a complex number by a real number is a complex number. This
</p>
<p>means that Cn is closed with respect to the operations of (component-wise) product
</p>
<p>by a real scalar. The condition IIc. above makes sense when c &isin; R.
</p>
<p>We next analyse some elementary properties of general vector spaces.
</p>
<p>Proposition 2.1.9 Let V be a vector space over R. For any k &isin; R and any v &isin; V it
</p>
<p>holds that:
</p>
<p>(i) 0Rv = 0V ,
</p>
<p>(ii) k0V = 0V ,
</p>
<p>(iii) if kv = 0V then it is either k = 0R or v = 0V ,
</p>
<p>(iv) (&minus;k)v = &minus;(kv) = k(&minus;v).
</p>
<p>Proof (i) From 0Rv = (0R + 0R)v = 0Rv + 0Rv, since the sums can be simpli-
</p>
<p>fied, one has that 0Rv = 0V .
</p>
<p>(ii) Analogously: k0V = k(0V + 0V ) = k0V + k0V which yields k0V = 0V .</p>
<p/>
</div>
<div class="page"><p/>
<p>2.1 Definition and Basic Properties 21
</p>
<p>(iii) Let k 
= 0, so k&minus;1 &isin; R exists. Then, v = 1v = k&minus;1kv = k&minus;10V = 0V , with the
</p>
<p>last equality coming from (ii).
</p>
<p>(iv) Since the product is distributive over the sum, from (i) it follows that
</p>
<p>kv + (&minus;k)v = (k + (&minus;k))v = 0Rv = 0V that is the first equality. For the sec-
</p>
<p>ond, one writes analogously kv + k(&minus;v) = k(v &minus; v) = k0V = 0V �
</p>
<p>Relations (i), (ii), (iii) above are more succinctly expressed by the equivalence:
</p>
<p>kv = 0V &lArr;&rArr; k = 0R or v = 0V .
</p>
<p>2.2 Vector Subspaces
</p>
<p>Among the subsets of a real vector space, of particular relevance are those which
</p>
<p>inherit from V a vector space structure.
</p>
<p>Definition 2.2.1 Let V be a vector space over R with respect to the sum s and the
</p>
<p>product p as given in the Definition 2.1.1. Let W &sube; V be a subset of V . One says
</p>
<p>that W is a vector subspace of V if the restrictions of s and p to W equip W with
</p>
<p>the structure of a vector space over R.
</p>
<p>In order to establish whether a subset W &sube; V of a vector space is a vector subspace,
</p>
<p>the following can be seen as criteria.
</p>
<p>Proposition 2.2.2 Let W be a non empty subset of the real vector space V . The
</p>
<p>following conditions are equivalent.
</p>
<p>(i) W is a vector subspace of V ,
</p>
<p>(ii) W is closed with respect to the sum and the product by a scalar, that is
</p>
<p>(a) w + w&prime; &isin; W , for any w,w&prime; &isin; W ,
</p>
<p>(b) kw &isin; W , for any k &isin; R and w &isin; W ,
</p>
<p>(iii) kw + k &prime;w&prime; &isin; W , for any k, k &prime; &isin; R and any w,w&prime; &isin; W .
</p>
<p>Proof The implications (i) =&rArr; ii) and (ii) =&rArr; (iii) are obvious from the definition.
</p>
<p>(iii) =&rArr; (ii): By taking k = k &prime; = 1 one obtains (a), while to show point (b) one
</p>
<p>takes k &prime; = 0R.
</p>
<p>(ii) =&rArr; (i): Notice that, by hypothesis, W is closed with respect to the sum and
</p>
<p>product by a scalar. Associativity and commutativity hold in W since they hold in V .
</p>
<p>One only needs to prove that W has a neutral element 0W and that, for such a
</p>
<p>neutral element, any vector in W has an opposite in W . If 0V &isin; W , then 0V is the
</p>
<p>zero element in W : for any w &isin; W one has 0V + w = w + 0V = w since w &isin; V ;
</p>
<p>from ii, (b) one has 0Rw &isin; W for any w &isin; W ; from the Proposition 2.1.9 one has
</p>
<p>0Rw = 0V ; collecting these relations, one concludes that 0V &isin; W . If w &isin; W , again
</p>
<p>from the Proposition 2.1.9 one gets that &minus;w = (&minus;1)w &isin; W . �</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Vector Spaces
</p>
<p>Exercise 2.2.3 Both W = {0V } &sub; V and W = V &sube; V are trivial vector subspaces
</p>
<p>of V .
</p>
<p>Exercise 2.2.4 We have already seen that R[x]r &sube; R[x] are vector spaces with
</p>
<p>respect to the same operations, so we may conclude that R[x]r is a vector subspace
</p>
<p>of R[x].
</p>
<p>Exercise 2.2.5 Let v &isin; V a non zero vector in a vector space, and let
</p>
<p>L(v) = {av : a &isin; R} &sub; V
</p>
<p>be the collection of all multiples of v by a real scalar. Given the elements w = av
</p>
<p>and w&prime; = a&prime;v in L(v), from the equality
</p>
<p>αw + α&prime;w&prime; = (αa + α&prime;a&prime;)v &isin; L(v)
</p>
<p>for any α,α&prime; &isin; R, we see that, from the Proposition 2.2.2, L(v) is a vector subspace
</p>
<p>of V , and we call it the (vector) line generated by v.
</p>
<p>Exercise 2.2.6 Consider the following subsets W &sub; R2:
</p>
<p>1. W1 = {(x, y) &isin; R
2 : x &minus; 3y = 0},
</p>
<p>2. W2 = {(x, y) &isin; R
2 : x + y = 1},
</p>
<p>3. W3 = {(x, y) &isin; R
2 : x &isin; N},
</p>
<p>4. W4 = {(x, y) &isin; R
2 : x2 &minus; y = 0}.
</p>
<p>From the previous exercise, one sees that W1 is a vector subspace since
</p>
<p>W1 = L((3, 1)). On the other hand, W2, W3, W4 are not vector subspaces of R
2. The
</p>
<p>zero vector (0, 0) /&isin; W2; while W3 and W4 are not closed with respect to the product
</p>
<p>by a scalar, since, for example, (1, 0) &isin; W3 but
1
2
(1, 0) = ( 1
</p>
<p>2
, 0) /&isin; W3. Analogously,
</p>
<p>(1, 1) &isin; W4 but 2(1, 1) = (2, 2) /&isin; W4.
</p>
<p>The next step consists in showing how, given two or more vector subspaces of a
</p>
<p>real vector space V , one can define new vector subspaces of V via suitable operations.
</p>
<p>Proposition 2.2.7 The intersection W1 &cap; W2 of any two vector subspaces W1 and
</p>
<p>W2 of a real vector space V is a vector subspace of V .
</p>
<p>Proof Consider a, b &isin; R and v,w &isin; W1 &cap; W2. From the Propostion 2.2.2 it follows
</p>
<p>that av + bw &isin; W1 since W1 is a vector subspace, and also that av + bw &isin; W2 for
</p>
<p>the same reason. As a consequence, one has av + bw &isin; W1 &cap; W2. �
</p>
<p>Remark 2.2.8 In general, the union of two vector subspaces of V is not a vector
</p>
<p>subspace of V . As an example, the Fig. 2.1 shows that, if L(v) and L(w) are generated
</p>
<p>by different v,w &isin; R2, then L(v) &cup; L(w) is not closed under the sum, since it does
</p>
<p>not contain the sum v + w, for instance.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Vector Subspaces 23
</p>
<p>Fig. 2.1 The vector line L(v + w) with respect to the vector lines L(v) and L(w)
</p>
<p>Proposition 2.2.9 Let W1 and W2 be vector subspaces of the real vector space V
</p>
<p>and let W1 + W2 denote
</p>
<p>W1 + W2 = {v &isin; V | v = w1 + w2; w1 &isin; W1, w2 &isin; W2} &sub; V .
</p>
<p>Then W1 + W2 is the smallest vector subspace of V which contains the union
</p>
<p>W1 &cup; W2.
</p>
<p>Proof Let a, a&prime; &isin; R and v, v&prime; &isin; W1 + W2; this means that there exist w1, w
&prime;
1 &isin; W1
</p>
<p>and w2, w
&prime;
2 &isin; W2, so that v = w1 + w2 and v
</p>
<p>&prime; = w&prime;1 + w
&prime;
2. Since both W1 and W2
</p>
<p>are vector subspaces of V , from the identity
</p>
<p>av + a&prime;v&prime; = aw1 + aw2 + a
&prime;w&prime;1 + a
</p>
<p>&prime;w&prime;2 = (aw1 + a
&prime;w&prime;1)+ (aw2 + a
</p>
<p>&prime;w&prime;2),
</p>
<p>one has aw1 + a
&prime;w&prime;1 &isin; W1 and aw2 + a
</p>
<p>&prime;w&prime;2 &isin; W2. It follows that W1 + W2 is a vector
</p>
<p>subspace of V .
</p>
<p>It holds that W1 + W2 &supe; W1 &cup; W2: if w1 &isin; W1, it is indeed w1 = w1 + 0V in
</p>
<p>W1 + W2; one similarly shows that W2 &sub; W1 + W2.
</p>
<p>Finally, let Z be a vector subspace of V containing W1 &cup; W2; then for any
</p>
<p>w1 &isin; W1 and w2 &isin; W2 it must be w1 + w2 &isin; Z . This implies Z &supe; W1 + W2, and
</p>
<p>then W1 + W2 is the smallest of such vector subspaces Z . �
</p>
<p>Definition 2.2.10 If W1 and W2 are vector subspaces of the real vector space V the
</p>
<p>vector subspace W1 + W2 of V is called the sum of W1 e W2.
</p>
<p>The previous proposition and definition are easily generalised, in particular:
</p>
<p>Definition 2.2.11 If W1, . . . , Wn are vector subspaces of the real subspace V , the
</p>
<p>vector subspace
</p>
<p>W1 + &middot; &middot; &middot; + Wn = {v &isin; V | v = w1 + &middot; &middot; &middot; + wn; wi &isin; Wi , i = 1, . . . , n}
</p>
<p>of V is the sum of W1, . . . , Wn .</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Vector Spaces
</p>
<p>Definition 2.2.12 Let W1 and W2 be vector subspaces of the real vector space V . The
</p>
<p>sum W1 + W2 is called direct if W1 &cap; W2 = {0V }. A direct sum is denoted W1 &oplus; W2.
</p>
<p>Proposition 2.2.13 Let W1, W2 be vector subspaces of the real vector space V .
</p>
<p>Their sum W = W1 + W2 is direct if and only if any element v &isin; W1 + W2 has a
</p>
<p>unique decomposition as v = w1 + w2 with wi &isin; Wi , i = 1, 2.
</p>
<p>Proof We first suppose that the sum W1 + W2 is direct, that is W1 &cap; W2 = {0V }. If
</p>
<p>there exists an elementv &isin; W1 + W2 withv = w1 + w2 = w
&prime;
1 + w
</p>
<p>&prime;
2, andwi , w
</p>
<p>&prime;
i &isin; Wi ,
</p>
<p>then w1 &minus; w
&prime;
1 = w
</p>
<p>&prime;
2 &minus; w2 and such an element would belong to both W1 and W2.
</p>
<p>This would then be zero, since W1 &cap; W2 = {0V }, and then w1 = w
&prime;
1 and w2 = w
</p>
<p>&prime;
2.
</p>
<p>Suppose now that any element v &isin; W1 + W2 has a unique decomposition
</p>
<p>v = w1 + w2 with wi &isin; Wi , i = 1, 2. Let v &isin; W1 &cap; W2; then v &isin; W1 and v &isin; W2
which gives 0V = v &minus; v &isin; W1 + W2, so the zero vector has a unique decomposition.
</p>
<p>But clearly also 0V = 0V + 0V and being the decomposition for 0V unique, this gives
</p>
<p>v = 0V . �
</p>
<p>These proposition gives a natural way to generalise the notion of direct sum to an
</p>
<p>arbitrary number of vector subspaces of a given vector space.
</p>
<p>Definition 2.2.14 Let W1, . . . , Wn be vector subspaces of the real vector space V .
</p>
<p>The sum W1 + &middot; &middot; &middot; + Wn is called direct if any of its element has a unique decom-
</p>
<p>position as v = w1 + &middot; &middot; &middot; + wn with wi &isin; Wi , i = 1, . . . , n. The direct sum vector
</p>
<p>subspace is denoted W1 &oplus; &middot; &middot; &middot; &oplus; Wn .
</p>
<p>2.3 Linear Combinations
</p>
<p>We have seen in Chap. 1 that, given a cartesian coordinate system � = (O; i, j,k)
</p>
<p>for the space S, any vector v &isin; V3O can be written as v = ai + bj + ck. One says
</p>
<p>that v is a linear combination of i, j, k. From the Definition 1.2.5 we also know that,
</p>
<p>given �, the components (a, b, c) are uniquely determined by v. For this one says
</p>
<p>that i, j, k are linearly independent. In this section we introduce these notions for an
</p>
<p>arbitrary vector space.
</p>
<p>Definition 2.3.1 Let v1, . . . , vn be arbitrary elements of a real vector space V . A vec-
</p>
<p>tor v &isin; V is a linear combination of v1, . . . , vn if there exist n scalarsλ1, . . . ,λn &isin; R,
</p>
<p>such that
</p>
<p>v = λ1v1 + &middot; &middot; &middot; + λnvn.
</p>
<p>The collection of all linear combinations of the vectors v1, . . . , vn is denoted by
</p>
<p>L(v1, . . . , vn). If I &sube; V is an arbitrary subset of V , by L(I ) one denotes the col-
</p>
<p>lection of all possible linear combinations of vectors in I , that is
</p>
<p>L(I ) = {λ1v1 + &middot; &middot; &middot; + λnvn | λi &isin; R, vi &isin; I, n &ge; 0}.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Linear Combinations 25
</p>
<p>The set L(I ) is also called the linear span of I .
</p>
<p>Proposition 2.3.2 The space L(v1, . . . , vn) is a vector subspace of V , called the
</p>
<p>space generated by v1, . . . , vn or the linear span of the vectors v1, . . . , vn .
</p>
<p>Proof After Proposition 2.2.2, it is enough to show that L(v1, . . . , vn) is closed
</p>
<p>for the sum and the product by a scalar. Let v,w &isin; L(v1, . . . , vn); it is then
</p>
<p>v = λ1v1 + &middot; &middot; &middot; + λnvn and w = &micro;1v1 + &middot; &middot; &middot; + &micro;nvn , for scalars λ1, . . . ,λn and
</p>
<p>&micro;1, . . . ,&micro;n . Recalling point (2) in the Definition 2.1.1, one has
</p>
<p>v + w = (λ1 + &micro;1)v1 + &middot; &middot; &middot; + (λn + &micro;n)vn &isin; L(v1, . . . , vn).
</p>
<p>Next, let α &isin; R. Again from the Definition 2.1.1 (point 4)), one has αv = (αλ1)v1 +
</p>
<p>&middot; &middot; &middot; + (αλn)vn , which gives αv &isin; L(v1, . . . , vn). �
</p>
<p>Exercise 2.3.3 The following are two examples for the notion just introduced.
</p>
<p>(1) Clearly one has V2O = L(i, j) and V
3
O = L(i, j,k).
</p>
<p>(2) Let v = (1, 0,&minus;1) and w = (2, 0, 0) be two vectors in R3; it is easy to see that
</p>
<p>L(v,w) is a proper subset of R3. For example, the vector
</p>
<p>u = (0, 1, 0) /&isin; L(v,w). If u were in L(v,w), there should be α,β &isin; R such
</p>
<p>that
</p>
<p>(0, 1, 0) = α(1, 0,&minus;1)+ β(2, 0, 0) = (α+ 2β, 0,&minus;α).
</p>
<p>No choice of α,β &isin; R can satisfy this vector identity, since the second com-
</p>
<p>ponent equality would give 1 = 0, independently of α,β.
</p>
<p>It is interesting to explore which subsets I &sube; V yield L(I ) = V . Clearly, one has
</p>
<p>V = L(V ). The example (1) above shows that there are proper subsets I &sub; V
</p>
<p>whose linear span coincides with V itself. We already know that V2O = L(i, j) and
</p>
<p>that V3O = L(i, j,k): both V
3
O and V
</p>
<p>2
O are generated by a finite number of (their)
</p>
<p>vectors. This is not always the case, as the following exercise shows.
</p>
<p>Exercise 2.3.4 The real vector space R[x] is not generated by a finite num-
</p>
<p>ber of vectors. Indeed, let f1(x), . . . , fn(x) &isin; R[x] be arbitrary polynomials. Any
</p>
<p>p(x) &isin; L( f1, . . . , fn) is written as
</p>
<p>p(x) = λ1 f1(x)+ &middot; &middot; &middot; + λn fn(x)
</p>
<p>with suitable λ1, . . . ,λn &isin; R. If one writes di = deg( fi ) and d = max{d1, . . . , dn},
</p>
<p>from Remark A.3.5 one has that
</p>
<p>deg(p(x)) = deg(λ1 f1(x)+ &middot; &middot; &middot; + λn fn(x)) &le; max{d1, . . . , dn} = d.
</p>
<p>This means that any polynomial of degree d + 1 or higher is not contained in
</p>
<p>L( f1, . . . , fn). This is the case for any finite n, giving a finite d; we conclude that, if
</p>
<p>n is finite, any L(I ) with I = ( f1(x), . . . , fn(x)) is a proper subset of R[x] which
</p>
<p>can then not be generated by a finite number of polynomials.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Vector Spaces
</p>
<p>On the other hand, R[x] is indeed the linear span of the infinite set
</p>
<p>{1, x, x2, . . . , x i , . . . }.
</p>
<p>Definition 2.3.5 A vector space V over R is said to be finitely generated if
</p>
<p>there exists a finite number of elements v1, . . . , vn in V which are such that
</p>
<p>V = L(v1, . . . , vn). In such a case, the set {v1, . . . , vn} is called a system of gen-
</p>
<p>erators for V .
</p>
<p>Proposition 2.3.6 Let I &sube; V and v &isin; V . It holds that
</p>
<p>L({v} &cup; I ) = L(I ) &lArr;&rArr; v &isin; L(I ).
</p>
<p>Proof &ldquo; &rArr;&rdquo; Let us assume that L({v} &cup; I ) = L(I ). Since v &isin; L({v} &cup; I ), then
</p>
<p>v &isin; L(I ).
</p>
<p>&ldquo; &lArr;&rdquo; We shall prove the claim under the hypothesis that we have a finite
</p>
<p>system {v1, . . . , vn}. The inclusion L(I ) &sube; L({v} &cup; I ) is obvious. To prove the
</p>
<p>inclusion L({v} &cup; I ) &sube; L(I ), consider an arbitrary element w &isin; L({v} &cup; I ), so that
</p>
<p>w = αv + &micro;1v1 + &middot; &middot; &middot; + &micro;nvn . By the hypothesis, v &isin; L(I ) so it is
</p>
<p>v = λ1v1 + &middot; &middot; &middot; + λnvn . We can then write
</p>
<p>w = α(λ1v1 + &middot; &middot; &middot; + λnvn)+ &micro;1v1 + &middot; &middot; &middot; + &micro;nvn.
</p>
<p>From the properties of the sum of vectors in V , one concludes thatw &isin; L(v1, . . . , vn)
</p>
<p>= L(I ). �
</p>
<p>Remark 2.3.7 From the previous proposition one has also the identity
</p>
<p>L(v1, . . . , vn, 0V ) = L(v1, . . . , vn)
</p>
<p>for any v1, . . . , vn &isin; V .
</p>
<p>If I is a system of generators for V , the next question to address is whether I
</p>
<p>contains a minimal set of generators for V , that is whether there exists a set J &sub; I
</p>
<p>(with J 
= I ) such that L(J ) = L(I ) = V . The answer to this question leads to the
</p>
<p>notion of linear independence for a set of vectors.
</p>
<p>Definition 2.3.8 Given a collection I = {v1, . . . , vn} of vectors in a real vector space
</p>
<p>V , the elements of I are called linearly independent on R, and the system I is said
</p>
<p>to be free, if the following implication holds,
</p>
<p>λ1v1 + &middot; &middot; &middot; + λnvn = 0V =&rArr; λ1 = &middot; &middot; &middot; = λn = 0R.
</p>
<p>That is, if the only linear combination of elements of I giving the zero vector is the
</p>
<p>one whose coefficients are all zero.
</p>
<p>Analogously, an infinite system I &sube; V is said to be free if any of its finite subsets
</p>
<p>is free.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Linear Combinations 27
</p>
<p>The vectors v1, . . . , vn &isin; V are said to be linearly dependent if they are not
</p>
<p>linearly independent, that is if there are scalars (λ1, . . . ,λn) 
= (0, . . . , 0) such that
</p>
<p>λ1v1 + &middot; &middot; &middot; + λnvn = 0V .
</p>
<p>Exercise 2.3.9 It is clear that i, j,k are linearly independent in V3O , while the vec-
</p>
<p>tors v1 = i + j, v2 = j &minus; k and v3 = 2i &minus; j + 3k are linearly dependent, since one
</p>
<p>computes that 2v1 &minus; 3v2 &minus; v3 = 0.
</p>
<p>Proposition 2.3.10 Let V be a real vector space and I = {v1, . . . , vn} be a collec-
</p>
<p>tion of vectors in V . The following properties hold true:
</p>
<p>(i) if 0V &isin; I , then I is not free,
</p>
<p>(ii) I is not free if and only if one of the elements vi is a linear combination of the
</p>
<p>other elements v1, . . . , vi&minus;1, vi+1, . . . , vn ,
</p>
<p>(iii) if I is not free, then any J &supe; I is not free,
</p>
<p>(iv) if I is free, then any J such that J &sube; I is free; that is any subsystem of a free
</p>
<p>system is free.
</p>
<p>Proof i) Without loss of generality we suppose that v1 = 0V . Then, one has
</p>
<p>1Rv1 + 0Rv2 + &middot; &middot; &middot; + 0Rvn = 0V ,
</p>
<p>which amounts to say that the zero vector can be written as a linear combination
</p>
<p>of elements in I with a non zero coefficients.
</p>
<p>(ii) Suppose I is not free. Then, there exists scalars (λ1, . . . ,λn) 
= (0, . . . , 0) giv-
</p>
<p>ing the combination λ1v1 + &middot; &middot; &middot; + λnvn = 0V . Without loss of generality take
</p>
<p>λ1 
= 0; so λ1 is invertible and we can write
</p>
<p>v1 = λ
&minus;1
1 (&minus;λ2v2 &minus; &middot; &middot; &middot; &minus; λnvn) &isin; L(v2, . . . , vn).
</p>
<p>In order to prove the converse, we start by assuming that a vector vi is a linear
</p>
<p>combination
</p>
<p>vi = λ1v1 + &middot; &middot; &middot; + λi&minus;1vi&minus;1 + λi+1vi+1 + &middot; &middot; &middot; + λnvn.
</p>
<p>This identity can be written in the form
</p>
<p>λ1v1 + &middot; &middot; &middot; + λi&minus;1vi&minus;1 &minus; vi + λi+1vi+1 + &middot; &middot; &middot; + λnvn = 0V .
</p>
<p>The zero vector is then written as a linear combination with coefficients not all
</p>
<p>identically zero, since the coefficient of vi is &minus;1. This amounts to say that the
</p>
<p>system I is not free.
</p>
<p>We leave the reader to show the obvious points (iii) and (iv). �</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Vector Spaces
</p>
<p>2.4 Bases of a Vector Space
</p>
<p>Given a real vector space V , in this section we determine its smallest possible systems
</p>
<p>of generators, together with their cardinalities.
</p>
<p>Proposition 2.4.1 Let V be a real vector space, with v1, . . . , vn &isin; V . The following
</p>
<p>facts are equivalent:
</p>
<p>(i) the elements v1, . . . , vn are linearly independent,
</p>
<p>(ii) v1 
= 0V and, for any i &ge; 2, the vector vi is not a linear combination of
</p>
<p>v1, . . . , vi&minus;1.
</p>
<p>Proof The implication (i) =&rArr; (ii) directly follows from the Proposition 2.3.10.
</p>
<p>To show the implication (ii) =&rArr; (i) we start by considering a combination
</p>
<p>λ1v1 + &middot; &middot; &middot; + λnvn = 0V . Under the hypothesis, vn is not a linear combination of
</p>
<p>v1, . . . , vn&minus;1, so it must be λn = 0: were it not, one could write vn =
</p>
<p>λ&minus;1n (&minus;λ1v1 &minus; &middot; &middot; &middot; &minus; λn&minus;1vn&minus;1). We are then left with λ1v1 + &middot; &middot; &middot; + λn&minus;1vn&minus;1 = 0V ,
</p>
<p>and an analogous reasoning leads to λn&minus;1 = 0. After n &minus; 1 similar steps, one has
</p>
<p>λ1v1 = 0; since v1 
= 0 by hypothesis, it must be (see 2.1.5) that λ1 = 0. �
</p>
<p>Theorem 2.4.2 Any finite system of generators for a vector space V contains a free
</p>
<p>system of generators for V .
</p>
<p>Proof Let I = {v1, . . . , vs} be a system of generators for a real vector space V .
</p>
<p>Recalling the Remark 2.3.7, we can take vi 
= 0 for any i = 1, . . . , s. We define
</p>
<p>iteratively a system of subsets of I , as follows:
</p>
<p>&bull; take I1 = I = {v1, . . . , vs},
</p>
<p>&bull; if v2 &isin; L(v1), take I2 = I1 \ {v2}; if v2 
= L(v1), take I2 = I1,
</p>
<p>&bull; if v3 &isin; L(v1, v2), take I3 = I2 \ {v3}; if v3 
= L(v1, v2), take I3 = I2,
</p>
<p>&bull; Iterate the steps above.
</p>
<p>The whole procedure consists in examining any element in the starting I1 = I , and
</p>
<p>deleting it if it is a linear combination of the previous ones. After s steps, one ends
</p>
<p>up with a chain I1 &supe; &middot; &middot; &middot; &supe; Is &supe; I .
</p>
<p>Notice that, for any j = 2, . . . , s, it is L(I j ) = L(I j&minus;1). It is indeed either I j =
</p>
<p>I j&minus;1 (which makes the claim obvious) or I j&minus;1 = I j &cup; {v j }, withv j &isin; L(v1, . . . , v j&minus;1)
</p>
<p>&sube; L(I j&minus;1); from Proposition 2.3.6, it follows that L(I j ) = L(I j&minus;1).
</p>
<p>One has then L(I ) = L(I1) = &middot; &middot; &middot; = L(Is), and Is is a system of generators of
</p>
<p>V . Since no element in Is is a linear combination of the previous ones, the Proposi-
</p>
<p>tion 2.4.1 shows that Is is free. �
</p>
<p>Definition 2.4.3 Let V be a real vector space. An ordered system of vectors I =
</p>
<p>(v1, . . . , vn) in V is called a basis of V if I is a free system of generators for V , that
</p>
<p>is V = L(v1, . . . , vn) and v1, . . . , vn are linearly independent.
</p>
<p>Corollary 2.4.4 Any finite system of generators for a vector space contains (at least)
</p>
<p>a basis. This means also that any finitely generated vector space has a basis.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Bases of a Vector Space 29
</p>
<p>Proof It follows directly from the Theorem 2.4.2. �
</p>
<p>Exercise 2.4.5 Consider the vector space R3 and the system of vectors I = {v1, . . . , v5}
</p>
<p>with
</p>
<p>v1 = (1, 1,&minus;1), v2 = (&minus;2,&minus;2, 2), v3 = (2, 0, 1), v4 = (1,&minus;1, 2), v5 = (0, 1, 1).
</p>
<p>Following Theorem 2.4.2, we determine a basis for L(v1, v2, v3, v4, v5).
</p>
<p>&bull; At the first step I1 = I .
</p>
<p>&bull; Since v2 = &minus;2v1, so that v2 &isin; L(v1), delete v2 and take I2 = I1 \ {v2}.
</p>
<p>&bull; One has v3 /&isin; L(v1), so keep v3 and take I3 = I2.
</p>
<p>&bull; One has v4 &isin; L(v1, v3) if and only if there exist α,β &isin; R such that v4 =
</p>
<p>αv1 + βv3, that is (1,&minus;1, 2) = (α+ 2β,α,&minus;α+ β). By equating components,
</p>
<p>one has α = &minus;1, β = 1. This shows that v4 = &minus;v1 + v3 &isin; L(v1, v3); therefore
</p>
<p>delete v4 and take I4 = I3 \ {v4}.
</p>
<p>&bull; Similarly one shows that v5 /&isin; L(v1, v3). A basis for L(I ) is then I5 = I4 =
</p>
<p>(v1, v3, v5).
</p>
<p>The next theorem characterises free systems.
</p>
<p>Theorem 2.4.6 A system I = {v1, . . . , vn} of vectors in V is free if and only if any
</p>
<p>element in L(v1, . . . , vn) can be written in a unique way as a linear combination of
</p>
<p>the elements v1, . . . , vn .
</p>
<p>Proof We assume that I is free and that L(v1, . . . , vn) contains a vector, say v, which
</p>
<p>has two linear decompositions with respect to the vectors vi :
</p>
<p>v = λ1v1 + &middot; &middot; &middot; + λnvn = &micro;1v1 + &middot; &middot; &middot; + &micro;nvn.
</p>
<p>This identity would give (λ1 &minus; &micro;1)v1 + &middot; &middot; &middot; + (λn &minus; &micro;n)vn = 0V ; since the elements
</p>
<p>vi are linearly independent it would read
</p>
<p>λ1 &minus; &micro;1 = 0, &middot; &middot; &middot; , λn &minus; &micro;n = 0,
</p>
<p>that is λi = &micro;i for any i = 1, . . . , n. This says that the two linear expressions above
</p>
<p>coincide and v is written in a unique way.
</p>
<p>We assume next that any element in L(v1, . . . , vn) as a unique linear decomposi-
</p>
<p>tion with respect to the vectors vi . This means that the zero vector 0V &isin; L(v1, . . . , vn)
</p>
<p>has the unique decomposition 0V = 0Rv1 + &middot; &middot; &middot; + 0Rvn . Let us consider the expres-
</p>
<p>sion λ1v1 + &middot; &middot; &middot; + λnvn = 0V ; since the linear decomposition of 0V is unique, it
</p>
<p>is λi = 0 for any i = 1, . . . , n. This says that the vectors v1, . . . , vn are linearly
</p>
<p>independent. �
</p>
<p>Corollary 2.4.7 Let v1, . . . , vn be elements of a real vector space V . The system
</p>
<p>I = (v1, . . . , vn) is a basis for V if and only if any element v &isin; V can be written in
</p>
<p>a unique way as v = λ1v1 + &middot; &middot; &middot; + λnvn .</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Vector Spaces
</p>
<p>Definition 2.4.8 Let I = (v1, . . . , vn) be a basis for the real vector space V . Any
</p>
<p>v &isin; V is then written as a linear combination v = λ1v1 + &middot; &middot; &middot; + λnvn is a unique
</p>
<p>way. The scalars λ1, . . . ,λn (which are uniquely determined by Corollary 2.4.7) are
</p>
<p>called the components of v on the basis I . We denote this by
</p>
<p>v = (λ1, . . . ,λn)I .
</p>
<p>Remark 2.4.9 Notice that we have taken a free system in a vector space V and
</p>
<p>a system of generators for V not to be ordered sets while on the other hand, the
</p>
<p>Definition 2.4.3 refers to a basis as an ordered set. This choice is motivated by the
</p>
<p>fact that it is more useful to consider the components of a vector on a given basis
</p>
<p>as an ordered array of scalars. For example, if I = (v1, v2) is a basis for V , so it is
</p>
<p>J = (v2, v1). But one considers I equivalent to J as systems of generators for V ,
</p>
<p>not as bases.
</p>
<p>Exercise 2.4.10 With E = (i, j) and E &prime; = (j, i) two bases for V2O , the vector v =
</p>
<p>2i + 3j has the following components
</p>
<p>v = (2, 3)E = (3, 2)E &prime;
</p>
<p>when expressed with respect to them.
</p>
<p>Remark 2.4.11 Consider the real vector space Rn and the vectors
</p>
<p>e1 = (1, 0, . . . , 0),
</p>
<p>e2 = (0, 1, . . . , 0),
</p>
<p>...
</p>
<p>en = (0, 0, . . . , 1).
</p>
<p>Since any element v = (x1, . . . , xn) can be uniquely written as
</p>
<p>(x1, . . . , xn) = x1e1 + &middot; &middot; &middot; + xnen,
</p>
<p>the system E = (e1, . . . , en) is a basis for R
n .
</p>
<p>Definition 2.4.12 The system E = (e1, . . . , en) above is called the canonical basis
</p>
<p>for Rn .
</p>
<p>The canonical basis for R2 is E = (e1, e2), with e1 = (1, 0) and e2 = (0, 1); the
</p>
<p>canonical basis for R3 is E = (e1, e2, e3), with e1 = (1, 0, 0), e2 = (0, 1, 0) and
</p>
<p>e3 = (0, 0, 1).
</p>
<p>Remark 2.4.13 We have meaningfully introduced the notion of a canonical basis for
</p>
<p>R
n . Our analysis so far should nonetheless make it clear that for an arbitrary vector</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Bases of a Vector Space 31
</p>
<p>space V over R there is no canonical choice of a basis. The exercises that follow
</p>
<p>indeed show that some vector spaces have bases which appear more natural than
</p>
<p>others, in a sense.
</p>
<p>Exercise 2.4.14 We refer to the Exercise 2.1.8 and consider C as a vector space
</p>
<p>over R. As such it is generated by the two elements 1 and i since any complex
</p>
<p>number can be written as z = a + ib, with a, b &isin; R. Since the elements 1, i are
</p>
<p>linearly independent over R they are a basis over R for C.
</p>
<p>As already seen in the Exercise 2.1.8, Cn is a vector space both over C and over R.
</p>
<p>As a C-vector space, Cn has canonical basis E = (e1, . . . , en), where the elements
</p>
<p>ei are given as in the Remark 2.4.11. For example, the canonical basis for C
2 is
</p>
<p>E = (e1, e2), with e1 = (1, 0), e2 = (0, 1).
</p>
<p>As a real vector space, Cn has no canonical basis. It is natural to consider for it
</p>
<p>the following basis B = (b1, c1 . . . , bn, cn), made of the 2n following elements,
</p>
<p>b1 = (1, 0, . . . , 0), c1 = (i, 0, . . . , 0),
</p>
<p>b2 = (0, 1, . . . , 0), c2 = (0, i, . . . , 0),
</p>
<p>...
</p>
<p>bn = (0, 0, . . . , 1), cn = (0, 0, . . . , i).
</p>
<p>For C2 such a basis is B = (b1, c1, b2, c2), with b1 = (1, 0), c1 = (i, 0), and
</p>
<p>b2 = (0, 1), c2 = (0, i).
</p>
<p>Exercise 2.4.15 The real vector space R[x]r has a natural basis given by all the
</p>
<p>monomials (1, x, x2, . . . , xr ) with degree less than r , since any element
</p>
<p>p(x) &isin; R[x]r can be written in a unique way as
</p>
<p>p(x) = a0 + a1x + a2x
2 + &middot; &middot; &middot; ar x
</p>
<p>r ,
</p>
<p>with ai &isin; R.
</p>
<p>Remark 2.4.16 We have seen in Chap. 1 that, by introducing a cartesian coordinate
</p>
<p>system in V3O and with the notion of components for the vectors, the vector space
</p>
<p>operations in V3O can be written in terms of operations among components. This fact
</p>
<p>is generalised in the following way.
</p>
<p>Let I = (v1, . . . , vn) be a basis for V . Let v,w &isin; V , with v = (λ1, . . . ,λn)I and
</p>
<p>w = (&micro;1, . . . ,&micro;n)I the corresponding components with respect to I . We compute
</p>
<p>the components, with respect to I , of the vectors v + w. We have
</p>
<p>v + w = (λ1v1 + &middot; &middot; &middot; + λnvn)+ (&micro;1v1 + &middot; &middot; &middot; + &micro;nvn)
</p>
<p>= (λ1 + &micro;1)v1 + &middot; &middot; &middot; + (λn + &micro;n)vn,
</p>
<p>so we can write
</p>
<p>v + w = (λ1 + &micro;1, . . . ,λn + &micro;n)I .</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 Vector Spaces
</p>
<p>Next, with a &isin; R we also have
</p>
<p>av = a(λ1v1 + &middot; &middot; &middot; + λnvn) = (aλ1)v1 + &middot; &middot; &middot; + (aλn)vn,
</p>
<p>so we can write
</p>
<p>av = (aλ1, . . . , aλn)I .
</p>
<p>If z = av + bw with, z = (ξ1, . . . , ξn)I , it is immediate to see that
</p>
<p>(ξ1, . . . , ξn)I = (aλ1 + b&micro;1, . . . , aλn + b&micro;n)I
</p>
<p>or equivalently
</p>
<p>ξi = aλi + b&micro;i , for any i = 1, . . . , n.
</p>
<p>Proposition 2.4.17 Let V be a vector space over R, and I = (v1, . . . , vn) a basis
</p>
<p>for V . Consider a system
</p>
<p>w1 = (λ11, . . . ,λ1n)I , w2 = (λ21, . . . ,λ2n)I , . . . , ws = (λs1, . . . ,λsn)I
</p>
<p>of vectors in V , and denote z = (ξ1, . . . , ξn)I . One has that
</p>
<p>z = a1w1 + &middot; &middot; &middot; + asws &lArr;&rArr; ξi = a1λ1i + &middot; &middot; &middot; + asλsi for any i = 1, . . . , n.
</p>
<p>The i-th component of the linear combination z of the vectors wk , is given by the
</p>
<p>same linear combination of the i-th components of the vectors wk .
</p>
<p>Proof It comes as a direct generalisation of the previous remark. �
</p>
<p>Corollary 2.4.18 With the same notations as before, one has that
</p>
<p>(a) the vectors w1, . . . , ws are linearly independent in V if and only if the corre-
</p>
<p>sponding n-tuples of components (λ11, . . . ,λ1n), . . . , (λs1, . . . ,λsn) are linearly
</p>
<p>independent in Rn ,
</p>
<p>(b) the vectors w1, . . . , ws form a system of generators for V if and only if the cor-
</p>
<p>responding n-tuples of components (λ11, . . . ,λ1n), . . . , (λs1, . . . ,λsn) generate
</p>
<p>R
n .
</p>
<p>A free system can be completed to a basis for a given vector space.
</p>
<p>Theorem 2.4.19 Let V be a finitely generated real vector space. Any free finite
</p>
<p>system is contained in a basis for V .
</p>
<p>Proof Let I = {v1, . . . , vs} be a free system for the real vector space V . By the
</p>
<p>Corollary 2.4.4, V has a basis, that we denote B = (e1, . . . , en). The set I &cup; B =
</p>
<p>{v1, . . . , vs, e1, . . . , en} obviously generates V . By applying the procedure given
</p>
<p>in the Theorem 2.4.2, the first s vectors are not deleted, since they are linearly
</p>
<p>independent by hypothesis; the subsystem B&prime; one ends up with at the end of the
</p>
<p>procedure will then be a basis for V that contains I . �</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 The Dimension of a Vector Space 33
</p>
<p>2.5 The Dimension of a Vector Space
</p>
<p>The following (somewhat intuitive) result is given without proof.
</p>
<p>Theorem 2.5.1 Let V be a vector space over R with a basis made of n elements.
</p>
<p>Then,
</p>
<p>(i) any free system I in V contains at most n elements,
</p>
<p>(ii) any system of generators for V has at least n elements,
</p>
<p>(iii) any basis for V has n elements.
</p>
<p>This theorem makes sure that the following definition is consistent.
</p>
<p>Definition 2.5.2 If there exists a positive integer n &gt; 0, such that the real vector
</p>
<p>space V has a basis with n elements, we say that V has dimension n, and write
</p>
<p>dim V = n. If V is not finitely generated we set dim V = &infin;. If V = {0V } we set
</p>
<p>dim V = 0.
</p>
<p>Exercise 2.5.3 Following what we have extensively described above, it is clear that
</p>
<p>dim V2O = 2 and dim V
3
O = 3. Also dim R
</p>
<p>n = n, with dim R = 1, and we have that
</p>
<p>dim R[x] = &infin; while dim R[x]r = r + 1. Referring to the Exercise 2.4.14, one has
</p>
<p>that dimC C
n = n while dimR C
</p>
<p>n = 2n.
</p>
<p>We omit the proof of the following results.
</p>
<p>Proposition 2.5.4 Let V be a n-dimensional vector space, and W a vector subspace
</p>
<p>of V . Then, dim(W ) &le; n, while dim(W ) = n if and only if W = V .
</p>
<p>Corollary 2.5.5 Let V be a n-dimensional vector space, and v1, . . . , vn &isin; V . The
</p>
<p>following facts are equivalent:
</p>
<p>(i) the system (v1, . . . , vn) is a basis for V ,
</p>
<p>(ii) the system {v1, . . . , vn} is free,
</p>
<p>(iii) the system {v1, . . . , vn} generates V .
</p>
<p>Theorem 2.5.6 (Grassmann) Let V a finite dimensional vector space, with U and
</p>
<p>W two vector subspaces of V . It holds that
</p>
<p>dim(U + W ) = dim(U )+ dim(W )&minus; dim(U &cap; W ).
</p>
<p>Proof Denote r = dim(U ), s = dim(W ) and p = dim(U &cap; W ). We need to show
</p>
<p>that U + W has a basis with r + s &minus; p elements.
</p>
<p>Let (v1, . . . , vp) be a basis for U &cap; W . By the Theorem 2.4.19 such a free sys-
</p>
<p>tem can be completed to a basis (v1, . . . , vp, u1, . . . , ur&minus;p) for U and to a basis
</p>
<p>(v1, . . . , vp, w1, . . . , ws&minus;p) for W .
</p>
<p>We then show that I = (v1, . . . , vp, u1, . . . , ur&minus;p, w1, . . . , ws&minus;p) is a basis for
</p>
<p>the vector space U + W . Since any vector in U + W has the form u + w, with u &isin; U
</p>
<p>and w &isin; W , and since u is a linear combination of v1, . . . , vp, u1, . . . , ur&minus;p, whilew</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2 Vector Spaces
</p>
<p>is a linear combination of v1, . . . , vp, w1, . . . , ws&minus;p, the system I generates U + W .
</p>
<p>Next, consider the combination
</p>
<p>α1v1 + &middot; &middot; &middot; + αpvp + β1u1 + &middot; &middot; &middot; + βr&minus;pur&minus;p + γ1w1 + &middot; &middot; &middot; + γs&minus;pws&minus;p = 0V .
</p>
<p>Denoting for brevity v =
&sum;p
</p>
<p>i=1 αivi , u =
&sum;r&minus;p
</p>
<p>j=1 β j u j andw =
&sum;s&minus;p
</p>
<p>k=1 γkwk , we write
</p>
<p>this equality as
</p>
<p>v + u + w = 0V ,
</p>
<p>with v &isin; U &cap; W, u &isin; U, w &isin; W . Since v, u &isin; U , then w = &minus;v &minus; u &isin; U ; so w &isin;
</p>
<p>U &cap; W . This implies
</p>
<p>w = γ1w1 + &middot; &middot; &middot; + γs&minus;pws&minus;p = λ1v1 + &middot; &middot; &middot; + λpvp
</p>
<p>for suitable scalars λi : in fact we know that {v1, . . . , vp, w1, . . . , ws&minus;p} is a free
</p>
<p>system, so any γk must be zero. We need then to prove that, from
</p>
<p>α1v1 + &middot; &middot; &middot; + αpvp + β1u1 + &middot; &middot; &middot; + βr&minus;pur&minus;p = 0V
</p>
<p>it follows that all the coefficients αi and β j are zero. This is true, since (v1, . . . , vp,
</p>
<p>u1, . . . , ur&minus;p) is a basis for U . Thus I is a free system. �
</p>
<p>Corollary 2.5.7 Let W1 and W2 be vector subspaces of V . If W1 &oplus; W2 can be
</p>
<p>defined, then
</p>
<p>dim(W1 &oplus; W2) = dim(W1)+ dim(W2).
</p>
<p>Also, if B1 = (w
&prime;
1, . . . , w
</p>
<p>&prime;
s) and B2 = (w
</p>
<p>&prime;&prime;
1 , . . . , w
</p>
<p>&prime;&prime;
r ) are basis for W1 and W2 respec-
</p>
<p>tively, a basis for W1 &oplus; W2 is given by B = (w
&prime;
1, . . . , w
</p>
<p>&prime;
s, w
</p>
<p>&prime;&prime;
1 , . . . , w
</p>
<p>&prime;&prime;
r ).
</p>
<p>Proof By the Grassmann theorem, one has
</p>
<p>dim(W1 + W2)+ dim(W1 &cap; W2) = dim(W1)+ dim(W2)
</p>
<p>and from the Definition 2.2.12 we also have dim(W1 &cap; W2) = 0, which gives the
</p>
<p>first claim.
</p>
<p>With the basis B1 and B2 one considers B = B1 &cup; B2 which obviously generates
</p>
<p>W1 &oplus; W2. The second claim directly follows from the Corollary 2.5.5. �
</p>
<p>The following proposition is a direct generalization.
</p>
<p>Proposition 2.5.8 Let W1, . . . , Wn be subspaces of a real vector space V and let
</p>
<p>the direct sum W1 &oplus; &middot; &middot; &middot; &oplus; Wn be defined. One has that
</p>
<p>dim(W1 &oplus; &middot; &middot; &middot; &oplus; Wn) = dim(W1)+ &middot; &middot; &middot; + dim(Wn).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
</p>
<p>Euclidean Vector Spaces
</p>
<p>When dealing with vectors of V3O in Chap. 1, we have somehow implicitly used the
</p>
<p>notions of length for a vector and of orthogonality of vectors as well as amplitude of
</p>
<p>plane angle between vectors. In order to generalise all of this, in the present chapter
</p>
<p>we introduce the structure of scalar product for any vector space, thus coming to the
</p>
<p>notion of euclidean vector space. A scalar product allows one to speak, among other
</p>
<p>things, of orthogonality of vectors or of the length of a vector in an arbitrary vector
</p>
<p>space.
</p>
<p>3.1 Scalar Product, Norm
</p>
<p>We start by recalling, through an example, how the vector space R3 can be endowed
</p>
<p>with a euclidean scalar product using the usual scalar product in the space V3O .
</p>
<p>Example 3.1.1 The usual scalar product in V3O , under the isomorphism R
3 ≃ V3O
</p>
<p>(see the Proposition 1.3.9), induces a map
</p>
<p>&middot; : R3 &times; R3 &minus;&rarr; R
</p>
<p>defined as
</p>
<p>(x1, x2, x3) &middot; (y1, y2, y3) = x1 y1 + x2 y2 + x3 y3.
</p>
<p>For vectors (x1, x2, x3), (y1, y2, y3), (z1, z2, z3) &isin; R3 and scalars a, b &isin; R, the fol-
lowing properties are easy to verify.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_3
</p>
<p>35</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_3&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_3&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>36 3 Euclidean Vector Spaces
</p>
<p>(i) Symmetry, that is:
</p>
<p>(x1, x2, x3) &middot; (y1, y2, y3) = x1 y1 + x2 y2 + x3 y3
= y1x1 + y2x2 + y3x3 = (y1, y2, y3) &middot; (x1, x2, x3).
</p>
<p>(ii) Linearity, that is:
</p>
<p>(a(x1, x2, x3) + b(y1, y2, y3)) &middot; (z1, z2, z3)
= (ax1 + by1)z1 + (ax2 + by2)z2 + (ax3 + by3)z3
= a(x1z1 + x2z2 + x3z3) + b(y1z1 + y2z2 + by3z3)
= a(x1, x2, x3) &middot; (z1, z2, z3) + b(y1, y2, y3) &middot; (z1, z2, z3).
</p>
<p>(iii) Non negativity, that is:
</p>
<p>(x1, x2, x3) &middot; (x1, x2, x3) = x21 + x
2
2 + x
</p>
<p>2
3 &ge; 0.
</p>
<p>(iv) Non degeneracy, that is:
</p>
<p>(x1, x2, x3) &middot; (x1, x2, x3) = 0 &hArr; (x1, x2, x3) = (0, 0, 0).
</p>
<p>These last two properties are summarised by saying that the scalar product in R3
</p>
<p>is positive definite.
</p>
<p>The above properties suggest the following definition.
</p>
<p>Definition 3.1.2 Let V be a finite dimensional real vector space. A scalar product
</p>
<p>on V is a map
</p>
<p>&middot; : V &times; V &minus;&rarr; R (v,w) �&rarr; v &middot; w
</p>
<p>that fulfils the following properties. For any v,w, v1, v2 &isin; V and a1, a2 &isin; R it holds
that:
</p>
<p>(i) v &middot; w = w &middot; v,
(ii) (a1v1 + a2v2) &middot; w = a1(v1 &middot; w) + a2(v2 &middot; w),
</p>
<p>(iii) v &middot; v &ge; 0,
(iv) v &middot; v = 0 &hArr; v = 0V .
</p>
<p>A finite dimensional real vector space V equipped with a scalar product will be
</p>
<p>denoted (V, &middot;) and will be referred to as a euclidean vector space.
Clearly the properties (i) and (ii) in the previous definition allows one to prove
</p>
<p>that the scalar product map &middot; is linear also with respect to the second argument.
A scalar product is then a suitable bilinear symmetric map, also called a bilinear
</p>
<p>symmetric real form since its range is in R.
</p>
<p>Exercise 3.1.3 It is clear that the scalar product considered in V3O satisfies the condi-
</p>
<p>tions given in the Definition 3.1.2. The map in the Example 3.1.1 is a scalar product on
</p>
<p>the vector space R3. This scalar product is not unique. Indeed, consider for instance
</p>
<p>p : R3 &times; R3 &minus;&rarr; R given by</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Scalar Product, Norm 37
</p>
<p>p((x1, x2, x3), (y1, y2, y3)) = 2x1 y1 + 3x2 y2 + x3 y3.
</p>
<p>It is easy to verify that such a map p is bilinear and symmetric. With v = (v1, v2, v3),
from p(v, v) = 2v21 + 3v22 + v23 one has p(v, v) &ge; 0 and p(v, v) = 0 &hArr; v = 0. We
have then that p is a scalar product on R3.
</p>
<p>Definition 3.1.4 On Rn there is a canonical scalar product
</p>
<p>&middot; : Rn &times; Rn &minus;&rarr; R
</p>
<p>defined by
</p>
<p>(x1, . . . , xn) &middot; (y1, . . . , yn) = x1 y1 + &middot; &middot; &middot; + xn yn =
n
</p>
<p>&sum;
</p>
<p>j=1
</p>
<p>x j y j .
</p>
<p>The datum (Rn, &middot;) is referred to as the canonical euclidean space and denoted En .
</p>
<p>The following lines sketch the proof that the above map satisfies the conditions
</p>
<p>of Definition 3.1.2.
</p>
<p>(i) (x1, . . . , xn) &middot; (y1, . . . , yn) =
&sum;n
</p>
<p>j=1
x j y j
</p>
<p>=
&sum;n
</p>
<p>j=1
y j x j = (y1, . . . , yn) &middot; (x1, . . . , xn),
</p>
<p>(ii) left to the reader,
</p>
<p>(iii) (x1, . . . , xn) &middot; (x1, . . . , xn) =
&sum;n
</p>
<p>i=1 x
2
i &ge; 0,
</p>
<p>(iv) (x1, . . . , xn) &middot; (x1, . . . , xn) = 0 &hArr;
&sum;n
</p>
<p>i=1 x
2
i = 0 &hArr; xi = 0, &forall;i &hArr;
</p>
<p>(x1, . . . , xn) = (0, . . . , 0).
</p>
<p>Definition 3.1.5 Let (V, &middot;) be a finite dimensional euclidean vector space. The map
</p>
<p>‖ &minus; ‖ : V &minus;&rarr; R, v �&rarr; ‖v‖ =
&radic;
</p>
<p>v &middot; v
</p>
<p>is called norm. For any v &isin; V , the real number ‖v‖ is the norm or the length of the
vector v.
</p>
<p>Exercise 3.1.6 The norm of a vector v = (x1, . . . , xn) in En = (Rn, &middot;) is
</p>
<p>‖(x1, . . . , xn)‖ =
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>x2i .
</p>
<p>In particular, for E3 one has ‖(x1, x2, x3)‖ =
&radic;
</p>
<p>x21 + x22 + x23 .
</p>
<p>The proof of the following proposition is immediate.
</p>
<p>Proposition 3.1.7 Let (V, &middot;) be a finite dimensional euclidean vector space. For any
v &isin; V and any a &isin; R, the following properties hold:</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Euclidean Vector Spaces
</p>
<p>(1) ‖v‖ &ge; 0,
(2) ‖v‖ = 0 &hArr; v = 0V ,
(3) ‖av‖ = |a| ‖v‖.
</p>
<p>Proposition 3.1.8 Let (V, &middot;) be a finite dimensional euclidean vector space. For any
v,w &isin; V the following inequality holds:
</p>
<p>|v &middot; w| &le; ‖v‖ ‖w‖.
</p>
<p>This is called the Schwarz inequality.
</p>
<p>Proof If either v = 0V or w = 0V the claim is obvious, so we may assume that both
vectors v,w �= 0V . Set a = ‖w‖ and b = ‖v‖; from (iii) in the Definition 3.1.2, one
can write
</p>
<p>0 &le; ‖av &plusmn; bw‖2 = (av &plusmn; bw) &middot; (av &plusmn; bw)
= a2‖v‖2 &plusmn; 2ab(v &middot; w) + b2‖w‖2
</p>
<p>= 2ab(‖v‖‖w‖ &plusmn; v &middot; w).
</p>
<p>Since both a, b are real positive scalars, the above expression reads
</p>
<p>∓ v &middot; w &le; ‖v‖ ‖w‖
</p>
<p>which is the claim. �
</p>
<p>Definition 3.1.9 The Schwarz inequality can be written in the form
</p>
<p>|v &middot; w|
‖v‖ ‖w‖
</p>
<p>&le; 1, that is &minus; 1 &le;
v &middot; w
</p>
<p>‖v‖ ‖w‖
&le; 1.
</p>
<p>Then one can define then angle α between the vectors v,w, by requiring that
</p>
<p>v &middot; w
‖v‖ ‖w‖
</p>
<p>= cos α
</p>
<p>with 0 &le; α &le; π. Notice the analogy between such a definition and the one in Defi-
nition (1.3.2) for the geometric vectors in V3O .
</p>
<p>Proposition 3.1.10 Let (V, &middot;) be a finite dimensional euclidean vector space. For
any v,w &isin; V the following inequality holds:
</p>
<p>‖v + w‖ &le; ‖v‖ + ‖w‖.
</p>
<p>This is called the triangle, or Minkowski inequality.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 Scalar Product, Norm 39
</p>
<p>Proof From the definition of the norm and the Schwarz inequality in Proposi-
</p>
<p>tion 3.1.8, one has v &middot; w &le; |v &middot; w| &le; ‖v‖ ‖w‖. The following relations are imme-
diate,
</p>
<p>‖v + w‖2 = (v + w) &middot; (v + w)
= ‖v‖2 + 2(v &middot; w) + ‖w‖2
</p>
<p>&le; ‖v‖2 + 2‖v‖ ‖w‖ + ‖w‖2
</p>
<p>= (‖v‖ + ‖w‖)2
</p>
<p>and prove the claim. �
</p>
<p>3.2 Orthogonality
</p>
<p>As mentioned, with a scalar product one generalises the notion of orthogonality
</p>
<p>between vectors and then between vector subspaces.
</p>
<p>Definition 3.2.1 Let (V, &middot;) be a finite dimensional euclidean vector space. Two vec-
tors v,w &isin; V are said to be orthogonal if v &middot; w = 0.
</p>
<p>Proposition 3.2.2 Let (V, &middot;) be a finite dimensional euclidean vector space, and let
w1, &middot; &middot; &middot; , ws and v be vectors in V . If v is orthogonal to each wi , then v is orthogonal
to any vector in the linear span L(w1, . . . , ws).
</p>
<p>Proof From the bilinearity of the scalar product, one has
</p>
<p>v &middot; (λ1w1 + &middot; &middot; &middot; + λsws) = λ1(v &middot; w1) + &middot; &middot; &middot; + λs(v &middot; ws).
</p>
<p>The right hand side of such expression is obviously zero under the hypothesis of
</p>
<p>orthogonality, that is v &middot; wi = 0 for any i . �
</p>
<p>Proposition 3.2.3 Let (V, &middot;) be a finite dimensional euclidean vector space. If
v1, . . . , vs is a collection of non zero vectors which are mutually orthogonal, that is
</p>
<p>vi &middot; v j = 0 for i �= j , then the vectors v1, . . . , vs are linearly independent.
</p>
<p>Proof Let us equate to the zero vector a linear combination of the vectors v1, . . . , vs ,
</p>
<p>that is, let
</p>
<p>λ1v1 + &middot; &middot; &middot; + λsvs = 0V .
</p>
<p>For vi &isin; {v1, . . . , vs}, we have
</p>
<p>0 = vi &middot; (λ1v1 + &middot; &middot; &middot; + λsvs) = λ1(vi &middot; v1) + &middot; &middot; &middot; + λs(vi &middot; vs) = λi ‖vi‖2.
</p>
<p>Being vi �= 0V it must be λi = 0. One gets λ1 = . . . = λs = 0, with the same
argument for any vector vi . �</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Euclidean Vector Spaces
</p>
<p>Definition 3.2.4 Let (V, &middot;) be a finite dimensional euclidean vector space. If W &sube; V
is a vector subspace of V , then the set
</p>
<p>W&perp; = {v &isin; V : s v &middot; w = 0,&forall;w &isin; W }
</p>
<p>is called the orthogonal complement to W .
</p>
<p>Proposition 3.2.5 Let W &sube; V be a vector subspace of a euclidean vector space
(V, &middot;). Then,
</p>
<p>(i) W&perp; is a vector subspace of V ,
</p>
<p>(ii) W &cap; W&perp; = {0V }, and the sum between W and W&perp; is direct.
</p>
<p>Proof (i) Let v1, v2 &isin; W&perp;, that is v1 &middot; w = 0 and v2 &middot; w = 0 for any w &isin; W . With
arbitrary scalars λ1,λ2 &isin; R, one has
</p>
<p>(λ1v1 + λ2v2) &middot; w = λ1(v1 &middot; w) + λ2(v2 &middot; w) = 0
</p>
<p>for any w &isin; W ; thus λ1v1 + λ2v2 &isin; W&perp;. The claim follows by recalling the
Proposition 2.2.2.
</p>
<p>(ii) If w &isin; W &cap; W&perp;, then w &middot; w = 0, which then gives w = 0V . �
</p>
<p>Remark 3.2.6 Let W = L(w1, . . . , ws) &sub; V . One has
</p>
<p>W&perp; = {v &isin; V | v &middot; wi = 0,&forall;i = 1, . . . , s}.
</p>
<p>The inclusion W&perp; &sube; L(w1, . . . , vs) is obvious, while the opposite inclusion
L(w1, . . . , ws) &sube; W&perp; follows from the Proposition 3.2.2.
</p>
<p>Exercise 3.2.7 Consider the vector subspace W = L((1, 0, 1)) &sub; E3. From the pre-
vious remark we have
</p>
<p>W&perp; = {(x, y, z) &isin; E3 | (x, y, z) &middot; (1, 0, 1) = 0} = {(x, y, z) &isin; E3 | x + z = 0},
</p>
<p>that is W&perp; = L((1, 0,&minus;1), (0, 1, 0)).
</p>
<p>Exercise 3.2.8 Let W &sub; E4 be defined by
</p>
<p>W = L((1,&minus;1, 1, 0), (2, 1, 0, 1)).
</p>
<p>By recalling the Proposition 3.2.3 and the Corollary 2.5.7 we know that the
</p>
<p>orthogonal subspace W&perp; has dimension 2. From the Remark 3.2.6, it is given by
</p>
<p>W&perp; =
{
</p>
<p>(x, y, z, t) &isin; E4 :
{
</p>
<p>(x, y, z, t) &middot; (1,&minus;1, 1, 0) = 0
(x, y, z, t) &middot; (2, 1, 0, 1) = 0
</p>
<p>}
</p>
<p>,
</p>
<p>that is by the common solutions of the following linear equations,</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Orthogonality 41
</p>
<p>x &minus; y + z = 0
2x + y + t = 0 .
</p>
<p>Such solutions can be written as
</p>
<p>{
</p>
<p>z = y &minus; x
t = &minus;2x &minus; y
</p>
<p>for arbitrary values of x, y. By choosing, for example, (x, y) = (1, 0) and (x, y) =
(0, 1), for the orthogonal subspace W&perp; one can show that W&perp; = L((1, 0,&minus;1,&minus;2),
(0, 1, 1,&minus;1)) (this kind of examples and exercises will be clearer after studying
homogeneous linear systems of equations).
</p>
<p>3.3 Orthonormal Basis
</p>
<p>We have seen in Chap. 2 that the orthogonal cartesian coordinate system (O, i, j, k)
</p>
<p>for the vector space V3O can be seen as having a basis whose vectors are mutually
</p>
<p>orthogonal and have norm one.
</p>
<p>In this section we analyse how to select in a finite dimensional euclidean vector
</p>
<p>space (V, &middot;), a basis whose vectors are mutually orthogonal and have norm one.
</p>
<p>Definition 3.3.1 Let I = {v1, . . . , vr } be a system of vectors of a vector space V . If
V is endowed with a scalar product, I is called orthonormal if
</p>
<p>vi &middot; v j = δi j =
{
</p>
<p>1 if i = j
0 if i �= j .
</p>
<p>Remark 3.3.2 From the Proposition 3.2.3 one has that any orthonormal system of
</p>
<p>vectors if free, that is its vectors are linearly independent.
</p>
<p>Definition 3.3.3 A basis B for (V, &middot;) is called orthonormal if it is an orthonormal
system.
</p>
<p>By such a definition, the basis (i, j, k) of V3O as well as the canonical basis for E
n
</p>
<p>are orthonormal.
</p>
<p>Remark 3.3.4 Let B = (e1, . . . , en) be an orthonormal basis for (V, &middot;) and let v &isin; V .
The vector v can be written with respect to B as
</p>
<p>v = (v &middot; e1)e1 + &middot; &middot; &middot; + (v &middot; en)en.
</p>
<p>Indeed, from
</p>
<p>v = a1e1 + &middot; &middot; &middot; + anen</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 Euclidean Vector Spaces
</p>
<p>one can consider the scalar products of v with each ei , and the orthogonality of these
</p>
<p>yields
</p>
<p>a1 = v &middot; e1, . . . , an = v &middot; en .
</p>
<p>Thus the components of a vector with respect to an orthonormal basis are given by
</p>
<p>the scalar products of the vector with the corresponding basis elements.
</p>
<p>Definition 3.3.5 Let B = (e1, . . . , en) be an orthonormal basis for (V, &middot;). With
v &isin; V , the vectors
</p>
<p>(v &middot; e1)e1, . . . , (v &middot; en)en,
</p>
<p>which give a linear decomposition of v, are called the orthogonal projections of v
</p>
<p>along e1, . . . , en .
</p>
<p>The next proposition shows that in an any finite dimensional real vector space
</p>
<p>(V, &middot;), with respect to an orthonormal basis for V the scalar product has the same
form than the canonical scalar product in En .
</p>
<p>Proposition 3.3.6 Let B = (e1, . . . , en) be an orthonormal basis for (V, &middot;). With
v,w &isin; V , let it be v = (a1, . . . , an)B and w = (b1, . . . , bn)B. Then one has
</p>
<p>v &middot; w = a1b1 + &middot; &middot; &middot; + anbn.
</p>
<p>Proof This follows by using the bilinearity of the scalar product and the relations
</p>
<p>ei &middot; e j = δi j . �
</p>
<p>Any finite dimensional real vector space can be shown to admit an orthonormal
</p>
<p>basis. This is done via the so called Gram-Schmidt orthonormalisation method. Its
</p>
<p>proof is constructive since, out of any given basis, the method provides an explicit
</p>
<p>orthonormal basis via linear algebra computations.
</p>
<p>Proposition 3.3.7 (Gram-Schmidt method) Let B = (v1, . . . , vn) be a basis for the
finite dimensional euclidean space (V, &middot;). The vectors
</p>
<p>e1 =
v1
</p>
<p>‖v1‖
,
</p>
<p>e2 =
v2 &minus; (v2 &middot; e1)e1
‖v2 &minus; (v2 &middot; e1)e1‖
</p>
<p>,
</p>
<p>...
</p>
<p>en =
vn &minus;
</p>
<p>&sum;n&minus;1
i=1 (vn &middot; ei )ei
</p>
<p>‖vn &minus;
&sum;n&minus;1
</p>
<p>i=1 (vn &middot; ei )ei‖
</p>
<p>form an orthonormal basis (e1, . . . , en) for V .</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Orthonormal Basis 43
</p>
<p>Proof We start by noticing that ‖e j‖ = 1, for j = 1, . . . , n, from the way these
vectors are defined. The proof of orthogonality is done by induction. As induction
</p>
<p>basis we prove explicitly that e1 &middot; e2 = 0. Being e1 &middot; e1 = 1, one has
</p>
<p>e1 &middot; e2 =
e1 &middot; v2 &minus; (v2 &middot; e1)e1 &middot; e1
‖v1‖ ‖v2 &minus; (v2 &middot; e1)e1‖
</p>
<p>= 0 .
</p>
<p>We then assume that e1, . . . , eh are pairwise orthogonal (this is the inductive
</p>
<p>hypothesis) and show that e1, . . . , eh+1 are pairwise orthogonal. Consider an integer
</p>
<p>k such that 1 &le; k &le; h. Then,
</p>
<p>eh+1 &middot; ek =
vh+1 &minus;
</p>
<p>&sum;h
i=1(vh+1 &middot; ei )ei
</p>
<p>‖vh+1 &minus;
&sum;h
</p>
<p>i=1(vh+1 &middot; ei )ei‖
&middot; ek
</p>
<p>=
vh+1 &middot; ek &minus;
</p>
<p>&sum;h
i=1 ((vh+1 &middot; ei )(ei &middot; ek))
</p>
<p>‖vh+1 &minus;
&sum;h
</p>
<p>i=1(vh+1 &middot; ei )ei‖
</p>
<p>=
vh+1 &middot; ek &minus; vh+1 &middot; ek
</p>
<p>‖vh+1 &minus;
&sum;h
</p>
<p>i=1(vh+1 &middot; ei )ei‖
= 0
</p>
<p>where the last equality follows from the inductive hypothesis ei &middot; ek = 0. The system
(e1, . . . , en) is free by Remark 3.3.2, thus giving an orthonormal basis for V . �
</p>
<p>Exercise 3.3.8 Let V = L(v1, v2) &sub; E4, withv1 = (1, 1, 0, 0), andv2 = (0, 2, 1, 1).
With the Gram-Schmidt orthogonalization method, we obtain an orthonormal basis
</p>
<p>for V . Firstly, we have
</p>
<p>e1 =
v1
</p>
<p>‖v1‖
=
</p>
<p>1
&radic;
</p>
<p>2
(1, 1, 0, 0) .
</p>
<p>Set f2 = v2 &minus; (v2 &middot; e1)e1. We have then
</p>
<p>f2 = (0, 2, 1, 1) &minus;
(
</p>
<p>(0, 2, 1, 1) &middot;
1
&radic;
</p>
<p>2
(1, 1, 0, 0)
</p>
<p>)
</p>
<p>1
&radic;
</p>
<p>2
(1, 1, 0, 0)
</p>
<p>= (0, 2, 1, 1) &minus; (1, 1, 0, 0)
= (&minus;1, 1, 1, 1).
</p>
<p>Then, the second vector e2 = f2‖ f2‖ is
</p>
<p>e2 =
1
</p>
<p>2
(&minus;1, 1, 1, 1) .
</p>
<p>Theorem 3.3.9 Any finite dimensional euclidean vector space (V, &middot;) admits an
orthonormal basis.
</p>
<p>Proof Since V is finite dimensional, by the Corollary 2.4.4 it has a basis, which can
</p>
<p>be orthonormalised using the Gram-Schmidt method. �</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Euclidean Vector Spaces
</p>
<p>Theorem 3.3.10 Let (V, &middot;) be finite dimensional with {e1, . . . , er } an orthonor-
mal system of vectors of V . The system can be completed to an orthonormal basis
</p>
<p>(e1, . . . , er , er+1, . . . , en) for V .
</p>
<p>Proof From the Theorem 2.4.19 the free system {e1, . . . , er } can be completed to a
basis for V , say
</p>
<p>B = (e1, . . . , er , vr+1, . . . , vn).
</p>
<p>The Gram-Schmidt method for the system B does not alter the first r vectors, and
</p>
<p>provides an orthonormal basis for V . �
</p>
<p>Corollary 3.3.11 Let (V, &middot;) have finite dimension n and let W be a vector subspace
of V . Then,
</p>
<p>(1) dim(W ) + dim(W&perp;) = n,
(2) V = W &oplus; W&perp;,
(3) (W&perp;)&perp; = W .
</p>
<p>Proof
</p>
<p>(1) Let (e1, . . . , er ) be an orthonormal basis for W completed (by the theorem
</p>
<p>above) to an orthonormal basis (e1, . . . , er , er+1, . . . , en) for V . Since the vec-
</p>
<p>tors er+1, . . . , en are then orthogonal to the vectors e1, . . . , er , they are (see the
</p>
<p>Definition 3.2.1) orthogonal to any vector in W , so er+1, . . . , en &isin; W&perp;. This
gives dim(W&perp;) &ge; n &minus; r , that is dim(W ) + dim(W&perp;) &ge; n. From the Defini-
tion 3.2.4 the sum of W and W&perp; is direct, so, recalling the Corollary 2.5.7,
</p>
<p>one has dim(W ) + dim(W&perp;) = dim(W &oplus; W&perp;) &le; n, thus proving the claim.
(2) From (1) we have dim(W &oplus; W&perp;) = dim(W ) + dim(W&perp;) = n = dim(V ); thus
</p>
<p>W &oplus; W&perp; = V .
(3) We start by proving the inclusion (W&perp;)&perp; &supe; W .
</p>
<p>By definition, it is (W&perp;)&perp; = {v &isin; V | v &middot; w = 0, &forall;w &isin; W&perp;}. If v &isin; W , then
v &middot; w = 0 for any w &isin; W&perp;, thus W &sube; (W&perp;)&perp;. Apply now the result in point
1) to W&perp;: one has
</p>
<p>dim(W&perp;) + dim((W&perp;)&perp;) = n.
</p>
<p>This inequality, together with the point 1) gives dim((W&perp;)&perp;) = dim(W ); the
spaces W and (W&perp;)&perp; are each other subspace with the same dimension, thus
</p>
<p>they coincide. �
</p>
<p>It is worth stressing that for the identity (W&perp;)&perp; = W it is crucial that the vector
space V be finite dimensional. For infinite dimensional vector spaces in general only
</p>
<p>the inclusion (W&perp;)&perp; &supe; W holds.
</p>
<p>Exercise 3.3.12 In Exercise 3.2.7 we considered the subspace of E3 given by W =
L((1, 0, 1)), and computed W&perp; = L((1, 0,&minus;1), (0, 1, 0)). It is immediate to verify
that
</p>
<p>dim(W ) + dim(W&perp;) = 1 + 2 = 3 = dim(E3).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Hermitian Products 45
</p>
<p>3.4 Hermitian Products
</p>
<p>The canonical scalar product in Rn can be naturally extended to the complex vector
</p>
<p>space Cn with a minor modification.
</p>
<p>Definition 3.4.1 The canonical hermitian product on Cn is the map
</p>
<p>&middot; : Cn &times; Cn &minus;&rarr; C
</p>
<p>defined by
</p>
<p>(z1, . . . , zn) &middot; (w1, . . . , wn) = z̄1w1 + &middot; &middot; &middot; + z̄nwn
</p>
<p>where z̄ denotes the complex conjugate of z (see the Sect. A.5). The datum (Cn, &middot;) is
called the canonical hermitian vector space of dimension n.
</p>
<p>The following proposition&mdash;whose straightforward proof we omit&mdash;generalises
</p>
<p>to the complex case the properties of the canonical scalar product on Rn shown
</p>
<p>after Definition 3.1.4. For easy of notation, we shall denote the vectors in Cn by
</p>
<p>z = (z1, . . . , zn).
</p>
<p>Proposition 3.4.2 For any z, w, v &isin; Cn and a, b &isin; C, the following properties hold:
</p>
<p>(i) w &middot; z = z &middot; w ,
(ii) (az + bw) &middot; v = ā(z &middot; v) + b̄(w &middot; v)
</p>
<p>while v &middot; (az + bw) = a(v &middot; z) + b(v &middot; w) ,
(iii) z &middot; z =
</p>
<p>&sum;n
i=1 |zi |2 &ge; 0 ,
</p>
<p>(iv) z &middot; z = 0 &hArr; z = (0, . . . , 0) &isin; Cn .
</p>
<p>Notice that the complex conjugation for the first entry of the hermitian scalar prod-
</p>
<p>uct implies that the hermitian product of a vector with itself is a real positive number.
</p>
<p>It is this number that gives the real norm of a complex vector z = (z1, . . . , zn),
defined as
</p>
<p>‖z‖ =
&radic;
</p>
<p>(z1, . . . , zn) &middot; (z1, . . . , zn) =
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>&radic;
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>|zi |2 .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
</p>
<p>Matrices
</p>
<p>Matrices are an important tool when dealing with many problems, notably the theory
</p>
<p>of systems of linear equations and the study of maps (operators) between vector
</p>
<p>spaces. This chapter is devoted to their basic notions and properties.
</p>
<p>4.1 Basic Notions
</p>
<p>Definition 4.1.1 A matrix M with entries in R (or a real matrix) is a collection
</p>
<p>of elements ai j &isin; R, with i = 1, . . . , m; j = 1, . . . , n and m, n &isin; N, displayed as
</p>
<p>follows
</p>
<p>M =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
...
</p>
<p>am1 am2 . . . amn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The matrix M above is said to be made of m row vectors in Rn , that is
</p>
<p>R1 = (a11, . . . , a1n) , . . . , Rm = (am1, . . . , amn)
</p>
<p>or by n column vectors in Rm , that is
</p>
<p>C1 = (a11, . . . , am1) , . . . , Cn = (a1n, . . . , amn).
</p>
<p>Thus the matrix M above is a m &times; n-matrix (m rows Ri &isin; R
n and n columns
</p>
<p>Ri &isin; R
n). As a shorthand, by M = (ai j ) we shall denote a matrix M with entry
</p>
<p>ai j at the i-th row and j-th column. We denote by R
m,n the collection of all
</p>
<p>m &times; n-matrices whose entries are in R.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_4
</p>
<p>47</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_4&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_4&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>48 4 Matrices
</p>
<p>Remark 4.1.2 It is sometime useful to consider a matrix M &isin; Rm,n as the collection
</p>
<p>of its n columns, or as the collection of its m rows, that is to write
</p>
<p>M = (C1 C2 . . . Cn) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>R1
R2
...
</p>
<p>Rm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>An element M &isin; R1,n is called a n-dimensional row matrix,
</p>
<p>M = (a11 a12 . . . a1n)
</p>
<p>while an element M &isin; Rm,1 is called a m-dimensional column matrix,
</p>
<p>M =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11
a21
...
</p>
<p>am1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>A square matrices of order n is a n &times; n matrix, that is an element in Rn,n . An element
</p>
<p>M &isin; R1,1 is a scalar, that is a single element in R. If A = (ai j ) &isin; R
n,n is a square
</p>
<p>matrix, the entries (a11, a22, . . . , ann) give the (principal) diagonal of A.
</p>
<p>Example 4.1.3 The bold typeset entries in
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 2
</p>
<p>&minus;1 0 3
</p>
<p>2 4 7
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>give the diagonal of A.
</p>
<p>Proposition 4.1.4 The set Rm,n is a real vector space whose dimension is mn. With
</p>
<p>A = (ai j ), B = (bi j ) &isin; R
m,n and λ &isin; R, the vector space operations are defined by
</p>
<p>A + B = (ai j + bi j ) , λA = (λai j ).
</p>
<p>Proof We task the reader to show that Rm,n equipped with the above defined oper-
</p>
<p>ations is a vector space. We only remark that the zero element in Rm,n is given by
</p>
<p>the matrix A with entries ai j = 0R; such a matrix is also called the null matrix and
</p>
<p>denoted 0Rm,n .
</p>
<p>In order to show that the dimension of Rm,n is mn, consider the elementary
</p>
<p>m &times; n-matrices
</p>
<p>Ers = (e
(rs)
jk ) , with e
</p>
<p>(rs)
jk =
</p>
<p>{
</p>
<p>1 if ( j, k) = (r, s)
</p>
<p>0 if ( j, k) �= (r, s)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Basic Notions 49
</p>
<p>Thus the matrix Ers has entries all zero but for the entry (r, s) which is 1. Clearly
</p>
<p>there are mn of them and it is immediate to show that they form a basis for Rm,n . �
</p>
<p>Exercise 4.1.5 Let A =
</p>
<p>(
</p>
<p>1 2 &minus;1
</p>
<p>0 &minus;1 1
</p>
<p>)
</p>
<p>&isin; R2,3. One computes
</p>
<p>(
</p>
<p>1 2 &minus;1
</p>
<p>0 &minus;1 1
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>1 0 0
</p>
<p>0 0 0
</p>
<p>)
</p>
<p>+ 2
</p>
<p>(
</p>
<p>0 1 0
</p>
<p>0 0 0
</p>
<p>)
</p>
<p>&minus;
</p>
<p>(
</p>
<p>0 0 1
</p>
<p>0 0 0
</p>
<p>)
</p>
<p>+ 0
</p>
<p>(
</p>
<p>0 0 0
</p>
<p>1 0 0
</p>
<p>)
</p>
<p>&minus;
</p>
<p>(
</p>
<p>0 0 0
</p>
<p>0 1 0
</p>
<p>)
</p>
<p>+
</p>
<p>(
</p>
<p>0 0 0
</p>
<p>0 0 1
</p>
<p>)
</p>
<p>= E11 + 2E12 &minus; E13 &minus; E22 + E23.
</p>
<p>In addition to forming a vector space, matrices of &lsquo;matching size&rsquo; can be multiplied.
</p>
<p>Definition 4.1.6 If A = (ai j ) &isin; R
m,n and B = (b jk) &isin; R
</p>
<p>n,p the product between A
</p>
<p>and B is the matrix in Rm,p defined by
</p>
<p>C = (cik) = AB &isin; R
m,p , where cik = R
</p>
<p>(A)
i &middot; C
</p>
<p>(B)
k =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>ai j b jk,
</p>
<p>with i = 1, . . . , m and k = 1, . . . , p. Here R
(A)
i &middot; C
</p>
<p>(B)
k denotes the scalar product in
</p>
<p>R
n between the i-th row vector R
</p>
<p>(A)
i of A with the k-th column vector C
</p>
<p>(B)
k of B.
</p>
<p>Remark 4.1.7 Notice that the product AB&mdash;also called the row by column product&mdash;
</p>
<p>is defined only if the number of columns of A equals the number of rows of B.
</p>
<p>Exercise 4.1.8 Consider the matrices
</p>
<p>A =
</p>
<p>(
</p>
<p>1 2 &minus;1
</p>
<p>3 0 1
</p>
<p>)
</p>
<p>&isin; R2,3, B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2
</p>
<p>2 1
</p>
<p>3 4
</p>
<p>⎞
</p>
<p>⎠ &isin; R3,2.
</p>
<p>One has AB = C = (cik) &isin; R
2,2 with
</p>
<p>C =
</p>
<p>(
</p>
<p>2 0
</p>
<p>6 10
</p>
<p>)
</p>
<p>.
</p>
<p>On the other hand, B A = C &prime; = (c&prime;st ) &isin; R
3,3 with
</p>
<p>C &prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>7 2 1
</p>
<p>5 4 &minus;1
</p>
<p>15 6 1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Clearly, comparing C with C &prime; is meaningless, since they are in different spaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 4 Matrices
</p>
<p>Remark 4.1.9 With A &isin; Rm,n and B &isin; Rp,q , the product AB is defined only if
</p>
<p>n = p, giving a matrix AB &isin; Rm,q . It is clear that the product B A is defined only if
</p>
<p>m = q and in such a case one has B A &isin; Rp,n . When both the conditions m = q and
</p>
<p>n = p are satisfied both products are defined. This is the case of the matrices A and
</p>
<p>B in the previous exercise.
</p>
<p>Let us consider the space Rn,n of square matrices of order n. If A, B are in Rn,n then
</p>
<p>both AB and B A are square matrices of order n. An example shows that in general
</p>
<p>one has AB �= B A. If
</p>
<p>A =
</p>
<p>(
</p>
<p>1 2
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>, B =
</p>
<p>(
</p>
<p>1 &minus;1
</p>
<p>1 0
</p>
<p>)
</p>
<p>,
</p>
<p>one computes that
</p>
<p>AB =
</p>
<p>(
</p>
<p>3 &minus;1
</p>
<p>0 &minus;1
</p>
<p>)
</p>
<p>�= B A =
</p>
<p>(
</p>
<p>0 3
</p>
<p>1 2
</p>
<p>)
</p>
<p>.
</p>
<p>Thus the product of matrices is non commutative. We shall say that two matrices
</p>
<p>A, B &isin; Rn,n commute if AB = B A. On the other hand, the associativity of the prod-
</p>
<p>uct in R and its distributivity with respect to the sum, allow one to prove analogous
</p>
<p>properties for the space of matrices.
</p>
<p>Proposition 4.1.10 The following identities hold:
</p>
<p>(i) A(BC) = (AB)C, for any A &isin; Rm,n, B &isin; Rn,p, C &isin; Rp,q ,
</p>
<p>(ii) A(B + C) = AB + AC, for any A &isin; Rm,n, B, C &isin; Rn,p ,
</p>
<p>(iii) λ(AB) = (λA)B = A(λB), for any A &isin; Rm,n, B &isin; Rn,p, λ &isin; R.
</p>
<p>Proof (i) Consider three matrices A = (aih) &isin; R
m,n , B = (bhk) &isin; R
</p>
<p>n,p and
</p>
<p>C = (ck j ) &isin; R
p,q . From the definition of row by column product one has
</p>
<p>AB = (dik)with dik =
&sum;n
</p>
<p>h=1 aihbhk , while BC = (eh j )with eh j =
&sum;p
</p>
<p>k=1 bhkck j .
</p>
<p>The i j-entries of (AB)C and A(BC) are
</p>
<p>p
&sum;
</p>
<p>k=1
</p>
<p>dikck j =
</p>
<p>p
&sum;
</p>
<p>k=1
</p>
<p>(
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aihbhk
</p>
<p>)
</p>
<p>ck j =
</p>
<p>p
&sum;
</p>
<p>k=1
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>(aihbhkck j ),
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aiheh j =
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aih
</p>
<p>(
</p>
<p>p
&sum;
</p>
<p>k=1
</p>
<p>bhkck j
</p>
<p>)
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>p
&sum;
</p>
<p>k=1
</p>
<p>(aihbhkck j ).
</p>
<p>These two expressions coincide (the last equality on both lines follows from
</p>
<p>the distributivity in R of the product with respect to the sum).
</p>
<p>(ii) Take matrices A = (aih) &isin; R
m,n , B = (bh j ) &isin; R
</p>
<p>n,p and C = (ch j ) &isin; R
n,p. The
</p>
<p>equality A(B + C) = AB + AC is proven again by a direct computation of
</p>
<p>the i j-entry for both sides:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Basic Notions 51
</p>
<p>[A(B + C)]i j =
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aih(bh j + ch j )
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aihbh j +
</p>
<p>n
&sum;
</p>
<p>h=1
</p>
<p>aihch j
</p>
<p>= [AB]i j + [AC]i j
</p>
<p>= [AB + AC]i j .
</p>
<p>(iii) This is immediate. �
</p>
<p>The matrix product in Rn,n is inner and it has a neutral element, a multiplication unit.
</p>
<p>Definition 4.1.11 The unit matrix of order n, denoted by In , is the element in R
n,n
</p>
<p>given by
</p>
<p>In = (δi j ) , with δi j =
</p>
<p>{
</p>
<p>1 if i = j
</p>
<p>0 if i �= j
.
</p>
<p>The diagonal entries of In are all 1, while its off-diagonal entries are all zero.
</p>
<p>Remark 4.1.12 It is easy to prove that, with A &isin; Rm,n , one has
</p>
<p>AIn = A and Im A = A.
</p>
<p>Proposition 4.1.13 The space Rn,n of square matrices of order n, endowed with the
</p>
<p>sum and the product as defined above, is a non abelian ring.
</p>
<p>Proof Recall the definition of a ring given in A.1.6. The matrix product is an inner
</p>
<p>operation in Rn,n , so the claim follows from the fact that (Rn,n,+, 0Rn,n ) is an abelian
</p>
<p>group and the results of the Proposition 4.1.10. �
</p>
<p>Definition 4.1.14 A matrix A &isin; Rn,n is said to be invertible (also non-singular or
</p>
<p>non-degenerate) if there exists a matrix B &isin; Rn,n , such that AB = B A = In . Such
</p>
<p>a matrix B is denoted by A&minus;1 and is called the inverse of A.
</p>
<p>Definition 4.1.15 If a matrix is non invertible, then it is called singular or degener-
</p>
<p>ate.
</p>
<p>Exercise 4.1.16 An element of the ring Rn,n needs not be invertible. The matrix
</p>
<p>A =
</p>
<p>(
</p>
<p>1 1
</p>
<p>0 1
</p>
<p>)
</p>
<p>&isin; R2,2
</p>
<p>is invertible, with inverse
</p>
<p>A&minus;1 =
</p>
<p>(
</p>
<p>1 &minus;1
</p>
<p>0 1
</p>
<p>)
</p>
<p>as it can be easily checked. On the other hand, the matrix</p>
<p/>
</div>
<div class="page"><p/>
<p>52 4 Matrices
</p>
<p>A&prime; =
</p>
<p>(
</p>
<p>1 0
</p>
<p>k 0
</p>
<p>)
</p>
<p>&isin; R2,2
</p>
<p>is non invertible, for any value of the parameter k &isin; R. It is easy to verify that the
</p>
<p>matrix equation
(
</p>
<p>1 0
</p>
<p>k 0
</p>
<p>)(
</p>
<p>x y
</p>
<p>z t
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>has no solutions.
</p>
<p>Proposition 4.1.17 The subset of invertible elements in Rn,n is a group with respect
</p>
<p>to the matrix product. It is called the general linear group of order n and is denoted
</p>
<p>by GL(n, R) or simply by GL(n).
</p>
<p>Proof Recall the definition of a group in A.2.7. We observe first that if A and B are
</p>
<p>both invertible then AB is invertible since AB(B&minus;1 A&minus;1) = (B&minus;1 A&minus;1)AB = In; this
</p>
<p>means that (AB)&minus;1 = B&minus;1 A&minus;1 so GL(n) is closed under the matrix product. It is
</p>
<p>evident that I&minus;1n = In and that if A is invertible, then A is the inverse of A
&minus;1, thus
</p>
<p>the latter is invertible. �
</p>
<p>Notice that since AB is in general different from B A the group GL(n) is non abelian.
</p>
<p>As an example, the non commuting matrices A and B considered in the Remark 4.1.9
</p>
<p>are both invertible.
</p>
<p>Definition 4.1.18 Given A = (ai j ) &isin; R
m,n its transpose, denoted by tA, is the matrix
</p>
<p>obtained from A when exchanging its rows with its columns, that is tA = (bi j ) &isin; R
n,m
</p>
<p>with bi j = a j i .
</p>
<p>Exercise 4.1.19 The matrix
</p>
<p>A =
</p>
<p>(
</p>
<p>1 2 &minus;1
</p>
<p>3 0 1
</p>
<p>)
</p>
<p>&isin; R2,3
</p>
<p>has transpose tA &isin; R3,2 given by
</p>
<p>tA =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 3
</p>
<p>2 0
</p>
<p>&minus;1 1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Proposition 4.1.20 The following identities hold:
</p>
<p>(i) t(A + B) = tA + tB, for any A, B &isin; Rm,n ,
</p>
<p>(ii) t(AB) = tB tA, for A &isin; Rm,n and B &isin; Rn,p,
</p>
<p>(iii) if A &isin; GL(n) then tA &isin; GL(n) that is, if A is invertible its transpose is invertible
</p>
<p>as well with (tA)&minus;1 = t(A&minus;1).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Basic Notions 53
</p>
<p>Proof (i) It is immediate.
</p>
<p>(ii) Given A = (ai j ) and B = (bi j ), denote
tA = (a&prime;i j ) and
</p>
<p>tB = (b&prime;i j ) with a
&prime;
i j = a j i
</p>
<p>and b&prime;i j = b j i . If AB = (ci j ), then ci j =
&sum;n
</p>
<p>h=1 aihbh j . The i j-element in
t (AB)
</p>
<p>is then
&sum;n
</p>
<p>h=1 a jhbhi ; the i j-element in
tB tA is
</p>
<p>&sum;n
h=1 b
</p>
<p>&prime;
iha
</p>
<p>&prime;
h j and these elements
</p>
<p>clearly coincide, for any i and j .
</p>
<p>(iii) It is enough to show that t (A&minus;1) tA = In . From (ii) one has indeed
</p>
<p>t (A&minus;1) tA = t (AA&minus;1) = tIn = In.
</p>
<p>This finishes the proof. �
</p>
<p>Definition 4.1.21 A square matrix of order n, A = (ai j ) &isin; R
n,n , is said to be sym-
</p>
<p>metric if tA = A that is, if for any i, j it holds that ai j = a j i .
</p>
<p>Exercise 4.1.22 The matrix A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 &minus;1
</p>
<p>2 0 1
</p>
<p>&minus;1 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠ is symmetric.
</p>
<p>4.2 The Rank of a Matrix
</p>
<p>Definition 4.2.1 Let A = (ai j ) be a matrix in R
m,n . We have seen that the m rows
</p>
<p>of A,
</p>
<p>R1 = (a11, . . . , a1n),
</p>
<p>...
</p>
<p>Rm = (am1, . . . , amn)
</p>
<p>are elements (vectors, indeed) in Rn . By R(A) we denote the vector subspace of Rn
</p>
<p>generated by the vectors R1, . . . , Rm that is,
</p>
<p>R(A) = L(R1, . . . , Rm).
</p>
<p>We call R(A) the row space of A. Analogously, given the columns
</p>
<p>C1 =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11
a21
...
</p>
<p>am1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, &middot; &middot; &middot; , Cn =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a1n
a2n
...
</p>
<p>amn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>of A, we define the vector subspace C(A) in Rm ,
</p>
<p>C(A) = L(C1, . . . , Cn)</p>
<p/>
</div>
<div class="page"><p/>
<p>54 4 Matrices
</p>
<p>as the column space of A.
</p>
<p>Remark 4.2.2 Clearly C(tA) = R(A) since the columns of tA are the rows of A.
</p>
<p>Theorem 4.2.3 Given A = (ai j ) &isin; R
m,n one has that dim(R(A)) = dim(C(A)).
</p>
<p>Proof Since A is fixed, to simplify notations we set R = R(A) and C = C(A).
</p>
<p>The first step is to show that dim(C) &le; dim(R). Let dim R = r ; up to a permuta-
</p>
<p>tion, we can take the first r rows in A as linearly independent. The remaining rows
</p>
<p>Rr+1, . . . , Rm are elements in R = L(R1, . . . , Rr ) and we can write
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>R1
...
</p>
<p>Rr
&sum;r
</p>
<p>i=1 λ
r+1
i Ri
</p>
<p>...
&sum;r
</p>
<p>i=1 λ
m
i Ri
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 &middot; &middot; &middot; a1n
...
</p>
<p>...
</p>
<p>ar1 &middot; &middot; &middot; arn
&sum;r
</p>
<p>i=1 λ
r+1
i ai1 &middot; &middot; &middot;
</p>
<p>&sum;r
i=1 λ
</p>
<p>r+1
i ain
</p>
<p>...
...
</p>
<p>&sum;r
i=1 λ
</p>
<p>m
i ai1 &middot; &middot; &middot;
</p>
<p>&sum;r
i=1 λ
</p>
<p>m
i ain
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>for suitable scalarsλ
j
</p>
<p>i (with i &isin; 1, . . . , r, and j &isin; r + 1, . . . , m). Given h = 1, . . . , n,
</p>
<p>consider the h-th column,
</p>
<p>Ch =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a1h
a2h
...
</p>
<p>arh
&sum;r
</p>
<p>i=1 λ
r+1
i aih
</p>
<p>...
&sum;r
</p>
<p>i=1 λ
m
i aih
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= a1h
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1
</p>
<p>0
...
</p>
<p>0
</p>
<p>λ
r+1
1
...
</p>
<p>λ
m
1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>+ &middot; &middot; &middot; + arh
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0
</p>
<p>0
...
</p>
<p>1
</p>
<p>λ
r+1
r
</p>
<p>...
</p>
<p>λ
m
r
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>This means that C is generated by the r columns
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1
</p>
<p>0
...
</p>
<p>0
</p>
<p>λ
r+1
1
...
</p>
<p>λ
m
1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, &middot; &middot; &middot; ,
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0
</p>
<p>0
...
</p>
<p>1
</p>
<p>λ
r+1
r
</p>
<p>...
</p>
<p>λ
m
r
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>so we have dim(C) &le; r = dim R. By exchanging the rows with columns, a similar
</p>
<p>argument shows also that dim(C) &ge; dim(R) thus the claim. �
</p>
<p>This theorem shows that dim(R(A)) = dim(C(A)) is an integer number that char-
</p>
<p>acterises A.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 The Rank of a Matrix 55
</p>
<p>Definition 4.2.4 Given a matrix A &isin; Rm,n , its rank is the number
</p>
<p>rk(A) = dim(C(A)) = dim(R(A))
</p>
<p>that is the common dimension of its space of rows, or columns.
</p>
<p>Corollary 4.2.5 For any A &isin; Rm,n one has rk(A) = rk(tA).
</p>
<p>Proof This follows from Remark 4.2.2 since C(tA) = R(A). �
</p>
<p>It is clear that rk(A) &le; min(m, n).
</p>
<p>Definition 4.2.6 A matrix A &isin; Rm,n has maximal rank if rk(A) = min(m, n).
</p>
<p>Our task next is to give methods to compute the rank of a given matrix. We first
</p>
<p>identify a class of matrices whose rank is easy to determine.
</p>
<p>Remark 4.2.7 It is immediate to convince one-self that the rank of a matrix A does not
</p>
<p>change by enlarging it with an arbitrary number of zero rows or columns. Moreover,
</p>
<p>if a matrix B is obtained from a matrix A by a permutation of either its rows or
</p>
<p>columns, that is, if it is
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>R1
R2
...
</p>
<p>Rm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>and B =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Rσ(1)
Rσ(2)
</p>
<p>...
</p>
<p>Rσ(m)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>(where σ denotes a permutation of m objects) or if
</p>
<p>A = (C1, . . . , Cn) and B
&prime; = (Cσ&prime;(1), . . . , Cσ&prime;(n))
</p>
<p>(where σ&prime; denotes a permutation of n objects), then rk(A) = rk(B) = rk(B &prime;). These
</p>
<p>equalities are true since the dimension of a vector space does not depend on the
</p>
<p>ordering of its basis.
</p>
<p>Definition 4.2.8 A square matrix A = (ai j ) &isin; R
n,n is called diagonal if ai j = 0 for
</p>
<p>i �= j .
</p>
<p>Exercise 4.2.9 The following matrix is diagonal,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 0 0 0
</p>
<p>0 2 0 0
</p>
<p>0 0 0 0
</p>
<p>0 0 0 &minus;3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Its rows and columns are vectors in R4, with R1 = e1, R2 = 2e2, R3 = 0, R4 = &minus;3e4
with respect to the canonical basis. As a consequence R(A) = L(R1, R2, R3, R4) =
</p>
<p>L(e1, e2, e4) so that rk(A) = 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 Matrices
</p>
<p>The rank of a diagonal matrix of order n coincides with the number of its non zero
</p>
<p>diagonal elements, since, as the previous exercise shows, its non zero rows or columns
</p>
<p>correspond to multiples of vectors of the canonical basis of Rn . Beside the diagonal
</p>
<p>ones, a larger class of matrices for which the rank is easy to compute is given in the
</p>
<p>following definition.
</p>
<p>Definition 4.2.10 Let A = (ai j ) be a square matrix in R
n,n . The matrix A is called
</p>
<p>upper triangular if ai j = 0 for i &gt; j . An upper triangular matrix for which ai i �= 0
</p>
<p>for any i , is called a complete upper triangular matrix.
</p>
<p>Exercise 4.2.11 Given
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 3
</p>
<p>0 0 2
</p>
<p>0 0 &minus;1
</p>
<p>⎞
</p>
<p>⎠ , B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 3
</p>
<p>0 2 2
</p>
<p>0 0 &minus;1
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>then A is upper triangular and B is complete upper triangular.
</p>
<p>Theorem 4.2.12 Let A &isin; Rn,n be a complete upper triangular matrix. Then,
</p>
<p>rk(A) = n.
</p>
<p>Proof Let
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 &middot; &middot; &middot; a1n
0 a22 &middot; &middot; &middot; a2n
...
</p>
<p>...
...
</p>
<p>0 0 &middot; &middot; &middot; ann
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>To prove the claim we show that the n columns C1, . . . , Cn of A are linearly inde-
</p>
<p>pendent. The equation λ1C1 + &middot; &middot; &middot; + λnCn = 0 can be written in the form
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ1a11 + &middot; &middot; &middot; + λn&minus;1a1n&minus;1 + λna1n
...
</p>
<p>λn&minus;1an&minus;1 n&minus;1 + λnan&minus;1 n
λnann
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0
...
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Equating term by term, one has for the n-th component λnann = 0, which gives
</p>
<p>λn = 0 since ann �= 0. For the (n &minus; 1)-th component, one has
</p>
<p>λn&minus;1an&minus;1,n&minus;1 + λnan&minus;1,n = 0
</p>
<p>which gives, from λn = 0 and an&minus;1,n&minus;1 �= 0, that λn&minus;1 = 0. This can be extended
</p>
<p>step by step to all components, thus getting λn = λn&minus;1 = &middot; &middot; &middot; = λ1 = 0. �
</p>
<p>The notion of upper triangularity can be extended to non square matrices.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 The Rank of a Matrix 57
</p>
<p>Definition 4.2.13 A matrix A = (ai j ) &isin; R
m,n is called upper triangular if it satisfies
</p>
<p>ai j = 0 for i &gt; j and complete upper triangular if it is upper triangular with ai i �= 0
</p>
<p>for any i .
</p>
<p>Remark 4.2.14 Given a matrix A &isin; Rm,n set p = min(m, n). If A is a complete
</p>
<p>upper triangular matrix, the submatrix B made by the first p rows of A when m &gt; n,
</p>
<p>or the first p columns of A when m &lt; n, is a square complete upper triangular matrix
</p>
<p>of order p.
</p>
<p>Exercise 4.2.15 The following matrices are complete upper triangular:
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 0 &minus;3
</p>
<p>0 2 0
</p>
<p>0 0 &minus;1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, A&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 3 9
</p>
<p>0 2 0 7
</p>
<p>0 0 4 &minus;3
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The submatrices
</p>
<p>B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;3
</p>
<p>0 2 0
</p>
<p>0 0 &minus;1
</p>
<p>⎞
</p>
<p>⎠ , B &prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 3
</p>
<p>0 2 0
</p>
<p>0 0 4
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>are (square) complete upper triangular as mentioned in the previous remark.
</p>
<p>Corollary 4.2.16 If A &isin; Rm,n is a complete upper triangular matrix then rk(A) =
</p>
<p>min(m, n).
</p>
<p>Proof We consider two cases.
</p>
<p>&bull; n &ge; m. One has rk(A) &le; min(m, n) = m, with
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 a13 . . . a1m&minus;1 a1m &lowast; . . . &lowast;
</p>
<p>0 a22 a23 . . . a2m&minus;1 a2m &lowast; . . . &lowast;
</p>
<p>0 0 a33 . . . a3m&minus;1 a3m &lowast; . . . &lowast;
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 0 . . . 0 amm &lowast; . . . &lowast;
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Let B be the submatrix of A given by the its first m columns. Since B is
</p>
<p>(Remark 4.2.14) a complete upper triangular square matrix of order m, the columns
</p>
<p>C1, . . . , Cm are linearly independent. This means that rk(A) &ge; m and the claim
</p>
<p>follows.
</p>
<p>&bull; n &lt; m. One has rk(A) &le; min(m, n) = n, with</p>
<p/>
</div>
<div class="page"><p/>
<p>58 4 Matrices
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 a13 . . . a1n
0 a22 a23 . . . a2n
0 0 a33 . . . a3n
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 0 . . . ann
0 0 0 . . . 0
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 0 . . . 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>By deleting all zero rows, one gets a matrix of the previous type, thus
</p>
<p>rk(A) = n. �
</p>
<p>The matrices A and A&prime; in the Exercise 4.2.15 are both complete upper triangular.
</p>
<p>Their rank is 3.
</p>
<p>Remark 4.2.17 The notions introduced in the present section can be formulated by
</p>
<p>considering columns instead of rows. One has:
</p>
<p>&bull; A matrix A &isin; Rm,n is called lower triangular if ai j = 0 for i &lt; j . A lower trian-
</p>
<p>gular matrix is called complete if ai i �= 0 for any i .
</p>
<p>&bull; Given A &isin; Rm,n , one has that A is (complete) upper triangular if and only if tA is
</p>
<p>(complete) lower triangular.
</p>
<p>&bull; If A &isin; Rm,n is a complete lower triangular matrix then rk(A) = min(m, n).
</p>
<p>4.3 Reduced Matrices
</p>
<p>Definition 4.3.1 A matrix A &isin; Rm,n is said to be reduced by rows if any non zero
</p>
<p>row has a non zero element such that the entries below it are all zero. Such an element,
</p>
<p>which is not necessarily unique if m &le; n, is called the pivot of its row.
</p>
<p>Exercise 4.3.2 The matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 3
</p>
<p>0 0 0
</p>
<p>2 0 0
</p>
<p>0 0 &minus;1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>is reduced by row. The pivot element for the first row is 1, the pivot element for the
</p>
<p>third row is 2, the pivot element for the fourth row is &minus;1. Note that rk(A) = 3 since
</p>
<p>the three non zero rows are linearly independent.
</p>
<p>Exercise 4.3.3 Any complete upper triangular matrix is reduced by rows.
</p>
<p>Theorem 4.3.4 The rank of a matrix A which is reduced by row coincides with the
</p>
<p>number of its non zero rows. Indeed, the non zero rows of a reduced by rows matrix
</p>
<p>are linearly independent.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Reduced Matrices 59
</p>
<p>Proof Let A be a reduced by rows matrix and let A&prime; be the submatrix of A obtained
</p>
<p>by deleting the zero rows of A. From the Remark 4.2.7, rk(A&prime;) = rk(A). Let A&prime;&prime;
</p>
<p>be the matrix obtained by A&prime; by the following permutation of its columns: the first
</p>
<p>column of A&prime;&prime; is the column of A&prime; containing the pivot element for the first row of
</p>
<p>A&prime;, the second column of A&prime;&prime; is the column of A&prime; containing the pivot element for the
</p>
<p>second row of A&prime; and so on. By such a permutation A&prime;&prime; turns out to be a complete
</p>
<p>upper triangular matrix and again from the Remark 4.2.7 it is rk(A&prime;) = rk(A&prime;&prime;). Since
</p>
<p>A&prime;&prime; is complete upper triangular its rank is given by the number of its rows, the rank
</p>
<p>of A is given by the number of non zero rows of A. �
</p>
<p>Since the proof of such a theorem is constructive, an example clarifies it.
</p>
<p>Example 4.3.5 Let us consider the following matrix A which is reduced by rows (its
</p>
<p>pivot elements are bold typed):
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 1 1
</p>
<p>0 0 2 &minus;1
</p>
<p>2 0 0 0
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The first column of A&prime; is the column containing the pivot element for the first row
</p>
<p>of A, the second column of A&prime; is the column containing the pivot element for the
</p>
<p>second row of A and so on. The matrix A&prime; is then
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&minus;1 1 1 1
</p>
<p>0 2 0 &minus;1
</p>
<p>0 0 2 0
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>and A&prime; is complete upper triangular; so rk(A) = rk(A&prime;) = 4.
</p>
<p>Remark 4.3.6 As we noticed in Remark 4.2.17, the notions introduced above can be
</p>
<p>formulated by exchanging the role of the columns with that of the rows of a matrix.
</p>
<p>&bull; A matrix A &isin; Rm,n is said to be reduced by columns if any non zero column has
</p>
<p>a non zero element such that the entries at its right are all zero. Such an element,
</p>
<p>which is not necessarily unique, is called the pivot of its column.
</p>
<p>&bull; If A is a reduced by columns matrix its rank coincides with the number of its non
</p>
<p>zero columns. The non zero columns are linearly independent.
</p>
<p>&bull; By mimicking the proof of the Theorem 4.3.4 it is clear that a matrix A is reduced
</p>
<p>by rows if and only if tA is reduced by column.</p>
<p/>
</div>
<div class="page"><p/>
<p>60 4 Matrices
</p>
<p>4.4 Reduction of Matrices
</p>
<p>In the previous section we have learnt how to compute the rank of a reduced matrix.
</p>
<p>In this section we outline a procedure that associates to any given matrix a reduced
</p>
<p>matrix having the same rank.
</p>
<p>We shall consider the following set of transformations acting on the rows of a matrix.
</p>
<p>They are called elementary transformations of rows and their action preserves the
</p>
<p>vector space structure of the space of rows.
</p>
<p>&bull; (λ) The transformation Ri �&rarr; λRi that replace the row Ri with its multiple λRi ,
</p>
<p>with R &ni; λ �= 0,
</p>
<p>&bull; (e) The transformation Ri &harr; R j , that exchanges the rows Ri and R j ,
</p>
<p>&bull; (D) The transformation Ri �&rarr; Ri + a R j that replace the row Ri with the linear
</p>
<p>combination Ri + a R j , with a &isin; R and i �= j .
</p>
<p>Given a matrix A &isin; Rm,n the matrix A&prime; &isin; Rm,n is said to be row-transformed from
</p>
<p>A if A&prime; is obtained from A by the action of a finite number of the elementary trans-
</p>
<p>formations (λ), (e) and (D) listed above.
</p>
<p>Proposition 4.4.1 Let A &isin; Rm,n and A&prime; &isin; Rm,n be row-transformed form A. Then
</p>
<p>R(A) = R(A&prime;) as vector spaces and rk(A) = rk(A&prime;).
</p>
<p>Proof It is obvious that for an elementary transformation (e) or (λ) the vector spaces
</p>
<p>R(A) and R(A&prime;) coincide. Let us take A&prime; to be row-transformed from A by a trans-
</p>
<p>formation (D). Since
</p>
<p>R(A) = L(R1, . . . , Ri&minus;1, Ri , Ri+1, . . . , Rm)
</p>
<p>and
</p>
<p>R(A&prime;) = L(R1, . . . , Ri&minus;1, Ri + a R j , Ri+1, . . . , Rm)
</p>
<p>it is clear that R(A&prime;) &sube; R(A). To prove the opposite inclusion, R(A) &sube; R(A&prime;), it is
</p>
<p>enough to show that the row Ri in A is in the linear span of the rows of A
&prime;. Indeed
</p>
<p>Ri = (Ri + a R j ) &minus; a R j , thus the claim. �
</p>
<p>Exercise 4.4.2 Let
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>2 1 &minus;1
</p>
<p>&minus;1 1 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We act on A with the following (D) elementary transformations:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Reduction of Matrices 61
</p>
<p>A
R2 �&rarr;R2&minus;2R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;3
</p>
<p>&minus;1 1 0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>A&prime;
R&prime;3 �&rarr;R
</p>
<p>&prime;
3+R
</p>
<p>&prime;
1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;3
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>A&prime;&prime;
R&prime;&prime;3 �&rarr;R
</p>
<p>&prime;&prime;
3&minus;R
</p>
<p>&prime;&prime;
2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;3
</p>
<p>0 0 4
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The matrix A&prime;&prime;&prime; is reduced by rows with rk(A&prime;&prime;&prime;) = 3. From the proposition above, we
</p>
<p>conclude that rk(A) = rk(A&prime;&prime;&prime;) = 3. This exercise shows how the so called Gauss&rsquo;
</p>
<p>algorithm works.
</p>
<p>Proposition 4.4.3 Given any matrix A it is always possible to find a finite sequence
</p>
<p>of type (D) elementary transformations whose action results in a matrix (say B)
</p>
<p>which is reduced by rows.
</p>
<p>Proof Let A = (ai j ) &isin; R
m,n . We denote by Ri the first non zero row in A and by ai j
</p>
<p>the first non zero element in Ri . In order to obtain a matrix A
&prime; such that the elements
</p>
<p>under ai j are zero one acts with the following (D) transformation
</p>
<p>Rk �&rarr; Rk &minus; ak j ai j
&minus;1 Ri , for any k &gt; i .
</p>
<p>We denote such a transformed matrix by A&prime; = (a&prime;i j ). Notice that the first i rows in
</p>
<p>A&prime; coincide with the first i rows in A, with all the elements in the column j below
</p>
<p>the element a&prime;i j = ai j being null. Next, let R
&prime;
h be the first non zero row in A
</p>
<p>&prime; with
</p>
<p>h &gt; i and let a&prime;hp be the first non zero element in R
&prime;
h . As before we now act with the
</p>
<p>following (D) elementary transformation
</p>
<p>R&prime;k &minus;&rarr; R
&prime;
k &minus; a
</p>
<p>&prime;
kpa
</p>
<p>&prime;
hp
</p>
<p>&minus;1
R&prime;h, for any k &gt; h.
</p>
<p>Let A&prime;&prime; the matrix obtained with this transformation and iterate. It is clear that a finite
</p>
<p>number of iterations of this procedure yield a matrix B which is&mdash;by construction&mdash;
</p>
<p>reduced by row. �
</p>
<p>With the expression of reduction by rows of a matrix A we mean a finite sequence of
</p>
<p>elementary transformations on the rows of A whose final image is a matrix A&prime; which
</p>
<p>is reduced by rows.
</p>
<p>Remark 4.4.4 The proof of the Proposition 4.4.3 made use only of type (D) trans-
</p>
<p>formations. It is clear that, depending on the specific elements of the matrix one is
</p>
<p>considering, it can be easier to use also type (e) and (λ) transformations. The claim
</p>
<p>of the Proposition 4.4.1 does not change.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 4 Matrices
</p>
<p>Exercise 4.4.5 Let us reduce by rows the following matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 0 0
</p>
<p>0 1 2 &minus;1
</p>
<p>0 0 0 9
</p>
<p>1 3 1 5
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>This matrix can be reduced as in the proof of the Proposition 4.4.3 by type (D)
</p>
<p>transformations alone. A look at it shows that it is convenient to swap the first row
</p>
<p>with the fourth. We have
</p>
<p>A
R1&harr;R4
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 3 1 5
</p>
<p>0 1 2 &minus;1
</p>
<p>0 0 0 9
</p>
<p>0 1 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= B.
</p>
<p>It is evident that the matrix B is already reduced by row so we can write
</p>
<p>rk(A) = rk(B) = 4.
</p>
<p>Exercise 4.4.6 Let us consider the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 &minus;1 1
</p>
<p>3 1 1 &minus;1
</p>
<p>0 1 1 9
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>To reduce A we start with the type (D) transformation R2 �&rarr; R2 &minus; 3/2R1, that leads
</p>
<p>to
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 &minus;1 1
</p>
<p>0 &minus;1/2 5/2 &minus;5/2
</p>
<p>0 1 1 9
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Since we are interested in computing the rank of the matrix A in order to avoid
</p>
<p>non integers matrix entries (which would give heavier computations) we can instead
</p>
<p>reduce by rows the matrix A&prime; as
</p>
<p>A&prime;
R2 �&rarr;2R2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 &minus;1 1
</p>
<p>0 &minus;1 5 &minus;5
</p>
<p>0 1 1 9
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>A&prime;&prime;
R&prime;3 �&rarr;R
</p>
<p>&prime;
2+R
</p>
<p>&prime;
3
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 &minus;1 1
</p>
<p>0 &minus;1 5 &minus;5
</p>
<p>0 0 6 4
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The matrix A&prime;&prime;&prime; is upper triangular so we have rk(A) = 3.
</p>
<p>The method of reducing by rows a matrix can be used to select a basis for a vec-
</p>
<p>tor space V given as a linear span of a system of vectors in some Rn , that is</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Reduction of Matrices 63
</p>
<p>V = L(v1, . . . , vr ). To this end, given the vectors v1, . . . , vr spanning V , one con-
</p>
<p>siders the matrix A with rows v1, . . . , vr or alternatively a matrix B with columns
</p>
<p>v1, . . . , vr :
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v1
...
</p>
<p>vr
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
, B = (v1 &middot; &middot; &middot; vr ) .
</p>
<p>One then has R(A) = V using A, which is reduced by rows to a matrix
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>w1
...
</p>
<p>wr
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
.
</p>
<p>Clearly V = R(A) = R(A&prime;) and dim(V ) = dim(R(A)) = rk(A) = rk(A&prime;). That is
</p>
<p>dim(V ) is the number of non zero rows in A&prime; and these non zero rows in A&prime; are a
</p>
<p>basis for V .
</p>
<p>Exercise 4.4.7 In R4 consider the system of vectors I = {v1, v2, v3, v4, v5} with
</p>
<p>v1 = (1,&minus;1, 2, 1),v2 = (&minus;2, 2,&minus;4,&minus;2),v3 = (1, 1, 1,&minus;1),v4 = (&minus;1, 3,&minus;3,&minus;3),
</p>
<p>v5 = (1, 2, 1, 2). We would like to
</p>
<p>(a) exhibit a basis B for V = L(I ) &sub; R4, with B &sub; I ,
</p>
<p>(b) complete B to a basis C for R4.
</p>
<p>For point (a) we let A be the matrix whose rows are the vectors in I that is,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v1
v2
v3
v4
v5
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>&minus;2 2 &minus;4 &minus;2
</p>
<p>1 1 1 &minus;1
</p>
<p>&minus;1 3 &minus;3 &minus;3
</p>
<p>1 2 1 2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>We reduce the matrix A by rows using the following transformations:
</p>
<p>A
</p>
<p>R2 �&rarr;R2+2R1
</p>
<p>R3 �&rarr;R3&minus;R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R4 �&rarr;R4+R1
</p>
<p>R5 �&rarr;R5&minus;R1
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 0 0 0
</p>
<p>0 2 &minus;1 &minus;2
</p>
<p>0 2 &minus;1 &minus;2
</p>
<p>0 3 &minus;1 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>A&prime;
R&prime;4 �&rarr;R
</p>
<p>&prime;
4&minus;R
</p>
<p>&prime;
3
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R&prime;5 �&rarr;2R
&prime;
5&minus;3R
</p>
<p>&prime;
3
</p>
<p>A&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 0 0 0
</p>
<p>0 2 &minus;1 &minus;2
</p>
<p>0 0 0 0
</p>
<p>0 0 1 8
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 4 Matrices
</p>
<p>As a result we have rk(A) = 3 and then dim(V ) = 3. A basis for V is for example
</p>
<p>given by the three non zero rows in A&prime;&prime; since R(A) = R(A&prime;&prime;). The basis B is made by
</p>
<p>the vectors in I corresponding to the three non zero rows in A&prime;&prime; that is B = (v1, v3, v5).
</p>
<p>Cleary, with the transformations given above one has also that
</p>
<p>B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>v1
v3
v5
</p>
<p>⎞
</p>
<p>⎠ �&rarr; B &prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 2 &minus;1 &minus;2
</p>
<p>0 0 1 8
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>To complete the basis B to a basis for R4 one can use the vectors of the canonical
</p>
<p>basis. From the form of the matrix B &prime; it is clear that it suffices to add the vector e4 to
</p>
<p>the three row vectors in B to meet the requirement:
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v1
v3
v5
e4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 2 &minus;1 &minus;2
</p>
<p>0 0 1 8
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>We can conclude that C = (v1, v3, v5, e4).
</p>
<p>Exercise 4.4.8 Let I = {v1, v2, v3, v4} &sub; R
4 be given by v1 = (0, 1, 2, 1),
</p>
<p>v2 = (0, 1, 1, 1), v3 = (0, 2, 3, 2), v4 = (1, 2, 2, 1). With V = L(I ) &sub; R
4:
</p>
<p>(a) determine a basis B for V , with B &sub; I ,
</p>
<p>(b) complete B to a basis C for R4.
</p>
<p>Let A be the matrix whose rows are given by the vectors in I that is,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v1
v2
v3
v4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 2 1
</p>
<p>0 1 1 1
</p>
<p>0 2 3 2
</p>
<p>1 2 2 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>After swapping R1 &harr; R4, the matrix can be reduced following the lines above,
</p>
<p>leading to
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 2 1
</p>
<p>0 1 1 1
</p>
<p>0 2 3 2
</p>
<p>0 1 2 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>R3 �&rarr; R3&minus;2R2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R4 �&rarr; R4 &minus; R2
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 2 1
</p>
<p>0 1 1 1
</p>
<p>0 0 1 0
</p>
<p>0 0 1 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R4 �&rarr; R4 &minus; R3
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 2 1
</p>
<p>0 1 1 1
</p>
<p>0 0 1 0
</p>
<p>0 0 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Reduction of Matrices 65
</p>
<p>We can then take B = (v4, v2, v3). Analogously to what we did in the Exercise 4.4.7,
</p>
<p>we have
⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v4
v2
v3
e4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 2 1
</p>
<p>0 1 1 1
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>and such a matrix shows that we can tale C = (v4, v2, v3, e4) as a basis for R
4.
</p>
<p>Exercise 4.4.9 Consider again the set I given in the previous exercise. We now look
</p>
<p>for a basis B &sub; I via the constructive proof of the Theorem 2.4.2. The reduction by
</p>
<p>rows procedure can be used in this case as well. Start again with
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>v1
v2
v3
v4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 2 1
</p>
<p>0 1 1 1
</p>
<p>0 2 3 2
</p>
<p>1 2 2 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The swap operated in Exercise 4.4.8 is not admissible with the procedure in the
</p>
<p>Theorem 2.4.2 so we use type (D) transformations:
</p>
<p>A
</p>
<p>R2 �&rarr; R2&minus;R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 �&rarr; R3 &minus; 2R2
</p>
<p>R4 &minus; R4 &minus; R1
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 2 1
</p>
<p>0 0 &minus;1 0
</p>
<p>0 0 &minus;1 0
</p>
<p>1 1 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R&prime;3 �&rarr; R
&prime;
3 &minus; R
</p>
<p>&prime;
2
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>0 1 2 1
</p>
<p>0 0 &minus;1 0
</p>
<p>0 0 0 0
</p>
<p>1 1 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>These computations show that R&prime;3 &minus; R
&prime;
2 = 0, R
</p>
<p>&prime;
3 = R3 &minus; 2R1 and R
</p>
<p>&prime;
2 = R2 &minus; R1.
</p>
<p>From these relations we have that R3 &minus; R2 &minus; R1 = 0 which is equivalent to
</p>
<p>v3 = v1 + v2: this shows that v3 is a linear combination of v1 and v2, so we recover
</p>
<p>the set {v1, v2, v4} as a basis for L(I ).
</p>
<p>The method we just illustrated in order to exhibit the basis of a vector subspace of
</p>
<p>R
n can be used with any vector space: the entries of the relevant matrix will be given
</p>
<p>by the components of a system of vectors with respect to a fixed basis.
</p>
<p>Exercise 4.4.10 Let V = L(I ) &sub; R2,3 with I = {M1, M2, M3, M4} given by
</p>
<p>M1 =
</p>
<p>(
</p>
<p>1 1 1
</p>
<p>0 1 0
</p>
<p>)
</p>
<p>, M2 =
</p>
<p>(
</p>
<p>1 2 1
</p>
<p>0 1 1
</p>
<p>)
</p>
<p>,
</p>
<p>M3 =
</p>
<p>(
</p>
<p>2 3 2
</p>
<p>0 2 1
</p>
<p>)
</p>
<p>, M4 =
</p>
<p>(
</p>
<p>0 1 1
</p>
<p>0 1 &minus;1
</p>
<p>)
</p>
<p>;
</p>
<p>(a) exhibit a basis B for V , with B &sub; I ,
</p>
<p>(b) complete B to a basis C for R2,3.</p>
<p/>
</div>
<div class="page"><p/>
<p>66 4 Matrices
</p>
<p>In order to use the reduction method we need to represent the matrices M1, M2, M3, M4
as row vectors. The components of these vectors will be given by the components
</p>
<p>of the matrices in a basis. This we may take to be the basis E = (Ei j | i = 1, 2;
</p>
<p>j = 1, 2, 3) of R2,3 made of elementary matrices as introduced in the proof of the
</p>
<p>Proposition 4.1.4. One has, for example,
</p>
<p>M1 = E11 + E12 + E13 + E22 = (1, 1, 1, 0, 1, 0)E .
</p>
<p>Proceeding analogously we write the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>M1
M2
M3
M4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 1 1 0 1 0
</p>
<p>1 2 1 0 1 1
</p>
<p>2 3 2 0 2 1
</p>
<p>0 1 1 0 1 &minus;1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>With a suitable reduction we have
</p>
<p>A �&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 1 1 0 1 0
</p>
<p>0 1 0 0 0 1
</p>
<p>0 1 0 0 0 1
</p>
<p>0 1 1 0 1 &minus;1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 1 1 0 1 0
</p>
<p>0 1 0 0 0 1
</p>
<p>0 0 0 0 0 0
</p>
<p>0 2 1 0 1 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>from which we have B = (M1, M2, M4).
</p>
<p>We complete B to a basis C for R2,3 by considering 3 elements in E and the same
</p>
<p>reduction:
⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>M1
M2
M4
E13
E21
E22
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 1 1 0 1 0
</p>
<p>0 1 0 0 0 1
</p>
<p>0 2 1 0 1 0
</p>
<p>0 0 1 0 0 0
</p>
<p>0 0 0 1 0 0
</p>
<p>0 0 0 0 1 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Since this matrix is reduced by row, the vectors {M1, M2, M4, E13, E21, E22} are 6
</p>
<p>linearly independent vectors in R2,3 (whose dimension is 6). This is enough to say
</p>
<p>that they give a basis C for R2,3 completing B.
</p>
<p>4.5 The Trace of a Matrix
</p>
<p>We end this chapter with another useful notion for square matrices.
</p>
<p>Definition 4.5.1 The trace of a square matrix is the function tr : Rn,n &rarr; R defined
</p>
<p>as follows. If A = (ai j ) its trace is given by</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 The Trace of a Matrix 67
</p>
<p>tr(A) = a11 + a22 + &middot; &middot; &middot; + ann =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>a j j .
</p>
<p>That is, the trace of a matrix is the sum of its diagonal elements.
</p>
<p>The following proposition proves an important property of the trace function for a
</p>
<p>matrix.
</p>
<p>Proposition 4.5.2 With A = (ai j ) and B = (bi j ) &isin; R
n,n it holds that
</p>
<p>tr(AB) = tr(B A).
</p>
<p>Proof The entry (i, j) in AB is (AB)i j =
&sum;n
</p>
<p>k=1 aikbk j , while the entry (i, j) in B A
</p>
<p>is (B A)i j =
&sum;n
</p>
<p>k=1 bikak j . From the row by column product of square matrices one
</p>
<p>obtaines
</p>
<p>tr(AB) =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>(AB) j j =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>a jkbk j
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>bk j a jk
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>(B A)kk = tr(B A),
</p>
<p>which is the claim. �
</p>
<p>Because of the above property one says that the trace is cyclic.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
</p>
<p>The Determinant
</p>
<p>The notion of determinant of a matrix plays an important role in linear algebra.
</p>
<p>While the rank measures the linear independence of the row (or column) vectors of a
</p>
<p>matrix, the determinant (which is defined only for square matrices) is used to control
</p>
<p>the invertibility of a matrix and in explicitly constructing the inverse of an invertible
</p>
<p>matrix.
</p>
<p>5.1 A Multilinear Alternating Mapping
</p>
<p>The determinant can be defined as an abstract function by using multilinear algebra.
</p>
<p>We shall define it constructively and using a recursive procedure.
</p>
<p>Definition 5.1.1 The determinant of a 2 &times; 2 matrix is the map
</p>
<p>det : R2,2 &rarr; R , A �&rarr; det(A) = |A|
</p>
<p>defined as
</p>
<p>A =
</p>
<p>(
</p>
<p>a11 a12
a21 a22
</p>
<p>)
</p>
<p>�&rarr; det(A) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a11 a12
a21 a22
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= a11a22 &minus; a12a21.
</p>
<p>The above definition shows that the determinant can be though of as a function
</p>
<p>of the column vectors of A = (C1, C2), that is
</p>
<p>det : R2 &times; R2 &rarr; R , (C1, C2) �&rarr; a11a22 &minus; a12a21.
</p>
<p>It is immediate to see that the map det is bilinear on the column of A, that is
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_5
</p>
<p>69</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_5&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_5&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>70 5 The Determinant
</p>
<p>det(λC1 + λ
&prime;C &prime;1, C2) = λ det(C1, C2) + λ
</p>
<p>&prime; det(C &prime;1, C2)
</p>
<p>det(C1,λC2 + λ
&prime;C &prime;2) = λ det(C1, C2) + λ
</p>
<p>&prime; det(C1, C
&prime;
2) (5.1)
</p>
<p>for any C1, C
&prime;
1, C2, C
</p>
<p>&prime;
2 &isin; R
</p>
<p>2 and any λ,λ&prime; &isin; R.
</p>
<p>The map det is indeed alternating (or skew-symmetric), that is
</p>
<p>det(C2, C1) = &minus; det(C1, C2). (5.2)
</p>
<p>From (5.2) the determinant of A vanishes if the columns C1 and C2 coincide.
</p>
<p>More generally, det(A) = 0 if C2 = λC1 for λ &isin; R, since, from (5.1)
</p>
<p>det(C1, C2) = det(C1,λC1) = λ det(C1, C1) = 0.
</p>
<p>Since the determinant map is bilinear and alternating, one also has
</p>
<p>det(C1 + λC2, C2) = det(C1, C2) + det(λC2, C2) = det(C1, C2).
</p>
<p>Exercise 5.1.2 Given the canonical basis (e1, e2) for R
2, we compute
</p>
<p>det(e1, e1) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 1
</p>
<p>0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= 0, det(e1, e2) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= 1
</p>
<p>det(e2, e1) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= &minus;1, det(e2, e2) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0 0
</p>
<p>1 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= 0.
</p>
<p>We generalise the definition of determinant to 3 &times; 3 and further to n &times; n matrices.
</p>
<p>Definition 5.1.3 Given a 3 &times; 3 matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>a11 a12 a13
a21 a22 a23
a31 a32 a33
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>one defines det : R3,3 &rarr; R as
</p>
<p>det(A) = |A| =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a11 a12 a13
a21 a22 a23
a31 a32 a33
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>(5.3)
</p>
<p>= a11
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a22 a23
a32 a33
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus; a12
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a21 a23
a31 a33
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>+ a13
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a21 a22
a31 a32
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= a11a22a33 &minus; a11a23a32 &minus; a12a21a33 + a12a23a31 + a13a21a32 &minus; a13a22a31.
</p>
<p>Exercise 5.1.4 Let us compute the determinant of the following matrix,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;1
</p>
<p>1 1 &minus;1
</p>
<p>2 1 0
</p>
<p>⎞
</p>
<p>⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 A Multilinear Alternating Mapping 71
</p>
<p>Using the first row as above one gets:
</p>
<p>det(A) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 &minus;1
</p>
<p>1 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 1
</p>
<p>2 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= 2.
</p>
<p>It is evident that the map det can be read, as we showed above, as defined on the
</p>
<p>column vectors of A = (C1, C2, C3), that is
</p>
<p>det : R3 &times; R3 &times; R3 &rarr; R , (C1, C2, C3) �&rarr; det(A).
</p>
<p>Remark 5.1.5 It is easy to see that the map det defined in (5.3) is multilinear, that
</p>
<p>is it is linear in each column argument. Also, for any swap of the columns of A,
</p>
<p>det(A) changes its sign. This means that (5.3) is an alternating map (this property
</p>
<p>generalises the skew-symmetry of the det map on 2 &times; 2 matrices). For example,
</p>
<p>det(C2, C1, C3) = &minus; det(C1, C2, C3),
</p>
<p>with analogous relations holding for any swap of the columns of A. Then det(A) = 0
</p>
<p>if one of the columns of A is a multiple of the others, like in
</p>
<p>det(C1, C2,λC2) = λ det(C1, C2, C2) = &minus;λ det(C1, C2, C2) = 0.
</p>
<p>More generally det(A) = 0 if one of the columns of A is a linear combination of the
</p>
<p>others as in
</p>
<p>det(λC2 + &micro;C3, C2, C3) = λ det(C2, C2, C3) + &micro; det(C3, C2, C3) = 0.
</p>
<p>Exercise 5.1.6 If (e1, e2, e3) is the canonical basis for R
3, generalising Exer-
</p>
<p>cise 5.1.2 one finds det(ei , ei , ei ) = 0, det(ei , ei , e j ) = 0 and det(e1, e2, e3) =
</p>
<p>det(I3) = 1, with I3 the 3 &times; 3 unit matrix.
</p>
<p>We have seen that the determinant of a 3 &times; 3 matrix A makes use of the deter-
</p>
<p>minant of a 2 &times; 2 matrix: such a determinant is given as the alternating sum of the
</p>
<p>elements in the first row of A, times the determinant of suitable 2 &times; 2 submatrices
</p>
<p>in A. This procedure is generalised to define the determinant of n &times; n matrices.
</p>
<p>Definition 5.1.7 Consider the matrix A = (ai j ) &isin; R
n,n , or
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
...
</p>
<p>an1 an2 . . . ann
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>For any pair (i, j) we denote by Ai j the (n &minus; 1) &times; (n &minus; 1) submatrix of A obtained
</p>
<p>by erasing the i-th row and the j-th column of A, Firstly, the number det(Ai j ) is</p>
<p/>
</div>
<div class="page"><p/>
<p>72 5 The Determinant
</p>
<p>called the minor of the element ai j . Then the cofactor αi j of the element ai j (or
</p>
<p>associated with ai j ) is defined as
</p>
<p>αi j = (&minus;1)
i+ j det(Ai j ).
</p>
<p>Exercise 5.1.8 With A &isin; R3,3 given by
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;1
</p>
<p>3 &minus;2 &minus;1
</p>
<p>2 5 0
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>we easily compute for instance,
</p>
<p>A11 =
</p>
<p>(
</p>
<p>&minus;2 &minus;1
</p>
<p>5 0
</p>
<p>)
</p>
<p>, A12 =
</p>
<p>(
</p>
<p>3 &minus;1
</p>
<p>2 0
</p>
<p>)
</p>
<p>and
</p>
<p>α11 = (&minus;1)
1+1|A11| = 5, α12 = (&minus;1)
</p>
<p>1+2|A12| = &minus;2.
</p>
<p>Definition 5.1.9 Let A = (ai j ) &isin; R
n,n . One defines its determinant by the formula
</p>
<p>det(A) = a11α11 + a12α12 + &middot; &middot; &middot; + a1nα1n. (5.4)
</p>
<p>Such an expression is also referred to as the expansion of the determinant of the
</p>
<p>matrix A with respect to its first row.
</p>
<p>The above definition is recursive: the determinant of a n &times; n matrix involves
</p>
<p>the determinants of a (n &minus; 1) &times; (n &minus; 1) matrices, starting from the definition of the
</p>
<p>determinant of a 2 &times; 2 matrix. The Definition 5.1.3 is indeed the expansion with
</p>
<p>respect to the first row as written in (5.4).
</p>
<p>That the determinant det(A) of a matrix A can be equivalently defined in terms
</p>
<p>of a similar expansion with respect to any row or column of A is the content of the
</p>
<p>following important theorem, whose proof we omit.
</p>
<p>Theorem 5.1.10 (Laplace) For any i = 2, . . . , n it holds that
</p>
<p>det(A) = ai1αi1 + ai2αi2 + &middot; &middot; &middot; + ainαin. (5.5)
</p>
<p>This expression is called the expansion of the determinant of A with respect to its
</p>
<p>i-th row.
</p>
<p>For any j = 1, . . . , n, it holds that
</p>
<p>det(A) = a1 jα1 j + a2 jα2 j + &middot; &middot; &middot; + anjαnj (5.6)
</p>
<p>and this expression is the expansion of the determinant of A with respect to its j -th
</p>
<p>column.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 A Multilinear Alternating Mapping 73
</p>
<p>The expansions (5.5) or (5.6) are called the cofactor expansion of the determinant
</p>
<p>with respect to the corresponding row or column.
</p>
<p>Exercise 5.1.11 Let In &isin; R
n,n be the n &times; n unit matrix. It is immediate to compute
</p>
<p>det(In) = 1.
</p>
<p>From the Laplace theorem the following statement is obvious.
</p>
<p>Corollary 5.1.12 Let A &isin; Rn,n . Then det(tA) = det(A).
</p>
<p>Also, from the Laplace theorem it is immediate to see that det(A) = 0 if A
</p>
<p>has a null column or a null row. We can still think of the determinant of the
</p>
<p>matrix A as a function defined on its columns. If A = (C1, &middot; &middot; &middot; , Cn), one has
</p>
<p>det(A) = det(C1, . . . , Cn), that is
</p>
<p>det : Rn &times; &middot; &middot; &middot; &times; Rn &rarr; R, (C1, . . . , Cn) �&rarr; det(A).
</p>
<p>The following result, that can be proven by using the Definition 5.1.9, generalises
</p>
<p>properties already seen for the matrices of order two and three.
</p>
<p>Proposition 5.1.13 Let A = (C1, &middot; &middot; &middot; , Cn) &isin; R
n,n . One has the following proper-
</p>
<p>ties:
</p>
<p>(i) For any λ,λ&prime; &isin; R and C &prime;1 &isin; R
n , it holds that
</p>
<p>det
(
</p>
<p>λC1 + λ
&prime;C &prime;1, C2, . . . , Cn
</p>
<p>)
</p>
<p>= λ det(C1, C2, . . . , Cn) + λ
&prime; det(C &prime;1, C2, . . . , Cn).
</p>
<p>Analogous properties hold for any other column of A.
</p>
<p>(ii) If A&prime; = (Cσ(1), . . . , Cσ(n)), where σ = (σ(1), . . . ,σ(n)) is a permutation of the
</p>
<p>columns transforming A �&rarr; A&prime;, it holds that
</p>
<p>det(A&prime;) = (&minus;1)σ det(A),
</p>
<p>where (&minus;1)σ is the parity of the permutation σ, that is (&minus;1)σ = 1 if σ is given
</p>
<p>by an even number of swaps, while (&minus;1)σ = &minus;1 if σ is given by an odd number
</p>
<p>of swaps.
</p>
<p>Corollary 5.1.14 Let A = (C1, &middot; &middot; &middot; , Cn) &isin; R
n,n . Then,
</p>
<p>(i) det(λC1, C2, . . . , Cn) = λ det(A),
</p>
<p>(ii) if Ci = C j for any pair i, j , then det(A) = 0,
</p>
<p>(iii) det(α2C2 + &middot; &middot; &middot; + αnCn, C2, . . . , Cn) = 0; that is the determinant of a matrix
</p>
<p>A is zero if a column of A is a linear combination of its other columns,
</p>
<p>(iv) det(C1 + α2C2 + &middot; &middot; &middot; + αnCn, C2, . . . , Cn) = det(A).</p>
<p/>
</div>
<div class="page"><p/>
<p>74 5 The Determinant
</p>
<p>Proof (i) it follows from the Proposition 5.1.13, with λ&prime; = 0,
</p>
<p>(ii) if Ci = C j , the odd permutation σ which swaps Ci with C j does not change the
</p>
<p>matrix A; then from the Proposition 5.1.13, det(A) = &minus; det(A) &rArr; det(A) = 0,
</p>
<p>(iii) from 5.1.13 we can write
</p>
<p>det(α2C2 + &middot; &middot; &middot; + αnCn, C2, . . . , Cn) =
</p>
<p>n
&sum;
</p>
<p>i=2
</p>
<p>αi det(Ci , C2, . . . , Cn) = 0
</p>
<p>since, by point (ii), one has det(Ci , C2, . . . , Cn) = 0 for any i = 2, . . . , n,
</p>
<p>(iv) from the previous point we have
</p>
<p>det(C1 + α2C2 + &middot; &middot; &middot; + αnCn, C2, . . . , Cn)
</p>
<p>= det(C1, C2, . . . , Cn) +
</p>
<p>n
&sum;
</p>
<p>i=2
</p>
<p>αi det(Ci , C2, . . . , Cn) = det(A).
</p>
<p>This concludes the proof. �
</p>
<p>Remark 5.1.15 From the Laplace theorem it follows that the determinant of A is an
</p>
<p>alternating and multilinear function even when it is defined via the expansion with
</p>
<p>respect to the rows of A.
</p>
<p>We conclude this section with the next useful theorem, whose proof we omit.
</p>
<p>Theorem 5.1.16 (Binet) Given A, B &isin; Rn,n it holds that
</p>
<p>det(AB) = det(A) det(B). (5.7)
</p>
<p>5.2 Computing Determinants via a Reduction Procedure
</p>
<p>The Definition 5.1.9 and the Laplace theorem allow one to compute the determinant of
</p>
<p>any square matrix. In this section we illustate how the reduction procedure studied
</p>
<p>in the previous chapter can be used when computing a determinant. We start by
</p>
<p>considering upper triangular matrices.
</p>
<p>Proposition 5.2.1 Let A = (ai j ) &isin; R
n,n . If A is diagonal then,
</p>
<p>det(A) = a11a22 &middot; &middot; &middot; ann .
</p>
<p>More generally, if A is an upper (respectively a lower) triangular matrix,
</p>
<p>det(A) = a11a22 &middot; &middot; &middot; ann .</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Computing Determinants via a Reduction Procedure 75
</p>
<p>Proof The claim for a diagonal matrix is evident. With A an upper (respectively
</p>
<p>a lower) triangular matrix, by expanding det(A) with respect to the first column
</p>
<p>(respectively row) the submatrix A11 is upper (respectively lower) triangular. The
</p>
<p>result then follows by a recursive argument. �
</p>
<p>Remark 5.2.2 In Sect. 4.4 we defined the type (s), (λ) and (D) elementary transfor-
</p>
<p>mations on the rows of a matrix. If A is a square matrix, transformed under one of
</p>
<p>these transformations into the matrix A&prime;, we have the following results:
</p>
<p>&bull; (s) : det(A&prime;) = &minus; det(A) (Proposition 5.1.13),
</p>
<p>&bull; (λ) : det(A&prime;) = λ det(A) (Corollary 5.1.14),
</p>
<p>&bull; (D) : det(A&prime;) = det(A) (Corollary 5.1.14).
</p>
<p>It is evident that the above relations are valid when A is mapped into A&prime; with ele-
</p>
<p>mentary transformations on its columns.
</p>
<p>Exercise 5.2.3 Let us use row transformations on the matrix A:
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>2 1 1
</p>
<p>1 2 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>R2 �&rarr; R2 &minus; 2R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 �&rarr;R3 &minus; R1
</p>
<p>A&prime;
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 &minus;1 3
</p>
<p>0 1 2
</p>
<p>⎞
</p>
<p>⎠ &minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R&prime;3 �&rarr;R
&prime;
3 + R
</p>
<p>&prime;
2
</p>
<p>A&prime;&prime;
</p>
<p>A&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 &minus;1 3
</p>
<p>0 0 5
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Since we have used only type (D) transformations, from the Remark 5.2.2
</p>
<p>det(A) = det(A&prime;&prime;) and from Proposition 5.2.1 we have det(A&prime;&prime;) = 1 &middot; (&minus;1) &middot; 5 = &minus;5.
</p>
<p>Exercise 5.2.4 Via a sequence of elementary transformations,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 1
</p>
<p>1 2 &minus;1
</p>
<p>1 1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C1 &harr;C2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;
</p>
<p>A&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>2 1 &minus;1
</p>
<p>1 1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>R&prime;2 �&rarr; R
&prime;
2 &minus; 2R
</p>
<p>&prime;
1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R&prime;3 �&rarr; R
&prime;
3 &minus; R
</p>
<p>&prime;
1
</p>
<p>A&prime;&prime;
</p>
<p>A&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;3
</p>
<p>0 1 0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>R&prime;&prime;3 �&rarr; R
&prime;&prime;
3 &minus; R
</p>
<p>&prime;&prime;
2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr; A&prime;&prime;&prime;
</p>
<p>A&prime;&prime;&prime; =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;3
</p>
<p>0 0 3
</p>
<p>⎞
</p>
<p>⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>76 5 The Determinant
</p>
<p>Since we used once a type (s) transformation det(A) = &minus; det(A&prime;&prime;&prime;) = &minus;3.
</p>
<p>Remark 5.2.5 The sequence of transformations defined in the Exercise 5.2.3 does
</p>
<p>not alter the space of rows of the matrix A, that is R(A) = R(A&prime;&prime;). The sequence of
</p>
<p>transformations defined in the Exercise 5.2.4 does alter both the spaces of rows and
</p>
<p>of columns of the matrix A.
</p>
<p>Proposition 5.2.6 Let A = (ai j ) &isin; R
n,n be reduced by rows and without null rows.
</p>
<p>It holds that
</p>
<p>det(A) = (&minus;1)σa1,σ(1) &middot; &middot; &middot; an,σ(n)
</p>
<p>where ai,σ(i) is the pivot element of the i-th row and σ is the permutation of the
</p>
<p>columns mapping A into the corresponding (complete) upper triangular matrix.
</p>
<p>Proof Let B = (bi j ) &isin; R
n,n be the complete upper triangular matrix obtained
</p>
<p>from A with the permutation σ. From the Proposition 5.1.13 we have det(A) =
</p>
<p>(&minus;1)σ det(B), with (&minus;1)σ the parity of σ. From the Proposition 5.2.1 we have
</p>
<p>det(B) = b11b22 &middot; &middot; &middot; bnn , with b11 = a1,σ(1), . . . , bnn = an,σ(n) by construction, thus
</p>
<p>obtaining the claim. �
</p>
<p>The above proposition suggests that a sequence of type (D) transformations on the
</p>
<p>rows of a square matrix simplifies the computation of its determinant. We summarise
</p>
<p>this suggestion as a remark.
</p>
<p>Remark 5.2.7 In order to compute the determinant of the matrix A &isin; Rn,n:
</p>
<p>&bull; riduce A by row with only type (D) transformations to a matrix A&prime;; this is
</p>
<p>always possible from the Proposition 4.4.3. Then det(A) = det(A&prime;) from the
</p>
<p>Remark 5.2.2;
</p>
<p>&bull; compute the determinant of A&prime;. Then,
</p>
<p>&ndash; if A&prime; has a null row, from the Corollary 5.1.14 one has det(A&prime;) = 0;
</p>
<p>&ndash; if A&prime; has no null rows, from the Proposition 5.2.6 one has
</p>
<p>det(A&prime;) = (&minus;1)σa&prime;1,σ(1) &middot; &middot; &middot; a
&prime;
n,σ(n)
</p>
<p>with σ = (σ(1), . . . ,σ(n)).
</p>
<p>Again, the result continues to hold by exchanging rows with columns.
</p>
<p>Exercise 5.2.8 With the above method we have the following equalities,
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 2 1 &minus;1
</p>
<p>0 1 1 1
</p>
<p>&minus;1 &minus;1 1 1
</p>
<p>1 2 0 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 2 1 &minus;1
</p>
<p>0 1 1 1
</p>
<p>0 1 2 0
</p>
<p>0 0 &minus;1 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 2 1 &minus;1
</p>
<p>0 1 1 1
</p>
<p>0 0 1 &minus;1
</p>
<p>0 0 &minus;1 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1 2 1 &minus;1
</p>
<p>0 1 1 1
</p>
<p>0 0 1 &minus;1
</p>
<p>0 0 0 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Invertible Matrices 77
</p>
<p>5.3 Invertible Matrices
</p>
<p>We now illustrate some use of the determinant in the study of invertible matrices.
</p>
<p>Proposition 5.3.1 Given A &isin; Rn,n , it holds that
</p>
<p>det(A) = 0 &hArr; rk(A) &lt; n.
</p>
<p>Proof
</p>
<p>(&lArr;) : By hypothesis, the system of the n columns C1, . . . , Cn of A is not free, so
</p>
<p>there is least a column of A which is a linear combination of the other columns.
</p>
<p>From the Corollary 5.1.14 it is then det(A) = 0.
</p>
<p>(&rArr;) : Suppose rk(A) = n. With this assumption A could be reduced by row to a
</p>
<p>matrix A&prime; having no null rows since rk(A) = rk(A&prime;) = n. From the Propo-
</p>
<p>sition 5.2.6, det(A&prime;) is the product of the pivot elements in A&prime; and since by
</p>
<p>hypothesis they would be non zero, we would have det(A&prime;) 
= 0 and from the
</p>
<p>Remark 5.2.2 det(A) = det(A&prime;) 
= 0 thus contradicting the hypothesis. �
</p>
<p>Remark 5.3.2 The equivalence in the above proposition can be stated as
</p>
<p>det(A) 
= 0 &hArr; rk(A) = n.
</p>
<p>Proposition 5.3.3 A matrix A = (ai j ) &isin; R
n,n is invertible (or non-singular) if and
</p>
<p>only if
</p>
<p>det(A) 
= 0.
</p>
<p>Proof If A is invertible, the matrix inverse A&minus;1 exists with AA&minus;1 = In . From the
</p>
<p>Binet theorem, this yields det(A) det(A&minus;1) = det(In) = 1 or det(A
&minus;1) =
</p>
<p>(det(A))&minus;1 
= 0.
</p>
<p>If det(A) 
= 0, the inverse of A is the matrix B = (bi j ) with elements
</p>
<p>bi j =
1
</p>
<p>det(A)
α j i
</p>
<p>and α j i the cofactor of a j i as in the Definition 5.1.7. Indeed, an explicit computation
</p>
<p>shows that
</p>
<p>(AB)rs =
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>arkbks =
1
</p>
<p>det(A)
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>arkαsk =
</p>
<p>{
</p>
<p>det(A)
</p>
<p>det(A)
= 1 if r = s
</p>
<p>0 if r 
= s
.
</p>
<p>The result for r = s is just the cofactor expansion of the determinant given by the
</p>
<p>Laplace theorem in Theorem 5.1.10, while the result for r 
= s is known as the second
</p>
<p>Laplace theorem (whose discussion we omit). The above amounts to AB = In so
</p>
<p>that A is invertible with B = A&minus;1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>78 5 The Determinant
</p>
<p>Notice that in the inverse matrix B there is an index transposition, that is up to
</p>
<p>the determinant factor, the element bi j of B is given by the cofactor α j i of A.
</p>
<p>Exercise 5.3.4 Let us compute the inverse of the matrix
</p>
<p>A =
</p>
<p>(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
</p>
<p>.
</p>
<p>This is possible if and only if |A| = ad &minus; bc 
= 0. In such a case,
</p>
<p>A&minus;1 =
1
</p>
<p>|A|
</p>
<p>(
</p>
<p>α11 α21
α12 α22
</p>
<p>)
</p>
<p>,
</p>
<p>with α11 = d, α21 = &minus;b, α12 = &minus;c, α22 = a, so that we get the final result,
</p>
<p>A&minus;1 =
1
</p>
<p>|A|
</p>
<p>(
</p>
<p>d &minus;b
</p>
<p>&minus;c a
</p>
<p>)
</p>
<p>.
</p>
<p>Exercise 5.3.5 Let us compute the inverse of the matrix A from the Exercise 5.1.4,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;1
</p>
<p>1 1 &minus;1
</p>
<p>2 1 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>From the computation there det(A) = 2, explicit computations show that
</p>
<p>α11 = (+) 1 α12 = (&minus;) 2 α13 = (+) (&minus;1)
</p>
<p>α21 = (&minus;) 1 α22 = (+) 2 α23 = (&minus;) 1
</p>
<p>α31 = (+) 1 α32 = (&minus;) 0 α33 = (+) 1
</p>
<p>.
</p>
<p>It is then easy to find that
</p>
<p>A&minus;1 =
1
</p>
<p>2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 1
</p>
<p>&minus;2 2 0
</p>
<p>&minus;1 &minus;1 1
</p>
<p>⎞
</p>
<p>⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
</p>
<p>Systems of Linear Equations
</p>
<p>Linear equations and system of them are ubiquitous and an important tool in all
</p>
<p>of physics. In this chapter we shall present a systematic approach to them and to
</p>
<p>methods for their solutions.
</p>
<p>6.1 Basic Notions
</p>
<p>Definition 6.1.1 An equation in n unknown variables x1, . . . , xn with coefficients
</p>
<p>in R is called linear if it has the form
</p>
<p>a1x1 + &middot; &middot; &middot; + an xn = b,
</p>
<p>with ai &isin; R and b &isin; R. A solution for such a linear equation is an n-tuple of real
</p>
<p>numbers (α1, . . . ,αn) &isin; R
n which, when substituted for the unknowns, yield an
</p>
<p>&lsquo;identity&rsquo;, that is
</p>
<p>a1α1 + &middot; &middot; &middot; + anαn = b.
</p>
<p>Exercise 6.1.2 It is easy to see that the element (2, 6, 1) &isin; R3 is a solution for the
</p>
<p>equation with real coefficients given by
</p>
<p>3x1 &minus; 2x2 + 7x3 = 1.
</p>
<p>Clearly, this is not the only solution for the equation: the element ( 1
3
, 0, 0) is for
</p>
<p>instance a solution of the same equation.
</p>
<p>Definition 6.1.3 A collection of m linear equations in the n unknown variables
</p>
<p>x1, . . . , xn and with real coefficients is called a linear system of m equations in n
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_6
</p>
<p>79</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_6&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_6&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>80 6 Systems of Linear Equations
</p>
<p>unknowns. We shall adopt the following notation
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>a11x1 + a12x2 + . . . + a1n xn = b1
a21x1 + a22x2 + . . . + a2n xn = b2
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>am1x1 + am2x2 + . . . + amn xn = bm
</p>
<p>.
</p>
<p>A solution for a given linear system is an n-tuple (α1, . . . ,αn) in R
n which
</p>
<p>simultaneously solves each equation of the system. The collection of the solutions of
</p>
<p>the system � is then a subset of Rn , denoted by S� and called the space of solutions
</p>
<p>of �.
</p>
<p>A system � is called compatible or solvable if its space of solutions is non void,
</p>
<p>S� �= &empty;; it will be said to be incompatible if S� = &empty;.
</p>
<p>Exercise 6.1.4 The element (1,&minus;1) &isin; R2 is a solution of the system
</p>
<p>{
</p>
<p>x + y = 0
</p>
<p>x &minus; y = 2
.
</p>
<p>The following system
{
</p>
<p>x + y = 0
</p>
<p>x + y = 1
.
</p>
<p>has no solutions.
</p>
<p>In the present chapter we study conditions under which a linear system is com-
</p>
<p>patible and in such a case find methods to determine its space of solutions. We shall
</p>
<p>make a systematic use of the matrix formalism described in the previous Chaps. 4
</p>
<p>and 5.
</p>
<p>Definition 6.1.5 There are two matrices naturally associated to the linear system �
</p>
<p>as given in the Definition 6.1.3:
</p>
<p>1. the matrix of the coefficients of �, A = (ai j ) &isin; R
m,n ,
</p>
<p>2. the matrix of the inhomogeneous terms of �, B = t (b1, . . . , bm) &isin; R
m,1.
</p>
<p>The complete or augmented matrix of the linear system � is given by
</p>
<p>(A, B) = (ai j | bi ) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
...
</p>
<p>am1 am2 . . . amn
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>b1
b2
...
</p>
<p>bm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>By using these matrices the system � can be represented as follows</p>
<p/>
</div>
<div class="page"><p/>
<p>6.1 Basic Notions 81
</p>
<p>� :
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
...
</p>
<p>am1 am2 . . . amn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x1
x2
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>b1
b2
...
</p>
<p>bm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>or more succinctly as
</p>
<p>� : AX = B
</p>
<p>where the array of unknowns is written as X = t (x1, . . . , xn) and (abusing notations)
</p>
<p>thought to be an element in Rn,1.
</p>
<p>Definition 6.1.6 Two linear systems � : AX = B and �&prime; : A&prime;X = B &prime; are called
</p>
<p>equivalent if their spaces of solutions coincide, that is � &sim; �&prime; if S� = S�&prime; . Notice
</p>
<p>that the vector of unknowns for the two systems is the same.
</p>
<p>Remark 6.1.7 The linear systems AX = B and A&prime;X = B &prime; are trivially equivalent
</p>
<p>&bull; if (A&prime;, B &prime;) results from (A, B) by adding null rows,
</p>
<p>&bull; if (A&prime;, B &prime;) is given by a row permutation of (A, B).
</p>
<p>The following linear systems are evidently equivalent:
</p>
<p>{
</p>
<p>x + y = 0
</p>
<p>x &minus; y = 2
,
</p>
<p>{
</p>
<p>x &minus; y = 2
</p>
<p>x + y = 0
.
</p>
<p>Remark 6.1.8 Notice that for a permutation of the columns of the matrix of its
</p>
<p>coefficients a linear system � changes to a system that is in general not equivalent
</p>
<p>to the starting one. As an example, consider the compatible linear system AX = B
</p>
<p>given in Exercise 6.1.4. If the columns of A are swapped one has
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 1
</p>
<p>1 &minus;1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0
</p>
<p>2
</p>
<p>)
</p>
<p>C1 &harr;C2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;1 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0
</p>
<p>2
</p>
<p>)
</p>
<p>= (A&prime;, B).
</p>
<p>One checks that the solution (1,&minus;1) of the starting system is not a solution for
</p>
<p>the system A&prime;X = B.
</p>
<p>6.2 The Space of Solutions for Reduced Systems
</p>
<p>Definition 6.2.1 A linear system AX = B is called reduced if the matrix A of its
</p>
<p>coefficients is reduced by rows in the sense of Sect. 4.4. Solving a reduced system is
</p>
<p>quite elementary, as the following exercises show.
</p>
<p>Exercise 6.2.2 Let the linear system � be given by</p>
<p/>
</div>
<div class="page"><p/>
<p>82 6 Systems of Linear Equations
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + y + 2z = 4
</p>
<p>y &minus; 2z = &minus;3
</p>
<p>z = 2
</p>
<p>with (A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 2
</p>
<p>0 1 &minus;2
</p>
<p>0 0 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>4
</p>
<p>&minus;3
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>It is reduced, and has the only solution (x, y, z) = (&minus;1, 1, 2). This is easily found
</p>
<p>by noticing that the third equation gives z = 2. By inserting this value into the second
</p>
<p>equation one has y = 1, and by inserting both these values into the first equation one
</p>
<p>eventually gets x = &minus;1.
</p>
<p>Exercise 6.2.3 To solve the linear system
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>2x + y + 2z + t = 1
</p>
<p>2x + 3y &minus; z = 3
</p>
<p>x + z = 0
</p>
<p>with (A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 2 1
</p>
<p>2 3 &minus;1 0
</p>
<p>1 0 1 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>3
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>one proceeds as in the previous exercise. The last equation gives z = &minus;x . By setting
</p>
<p>x = τ , one gets the solutions (x, y, z, t) = (τ ,&minus;τ + 1,&minus;τ , τ ) with τ &isin; R. Clearly
</p>
<p>� has an infinite number of solutions: the space of solutions for � is bijective to
</p>
<p>elements τ &isin; R.
</p>
<p>Exercise 6.2.4 The linear system � : AX = B, with
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1
</p>
<p>0 &minus;1 2
</p>
<p>0 0 3
</p>
<p>0 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>3
</p>
<p>1
</p>
<p>2
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>is trivially not compatible since the last equation would give 0 = 1.
</p>
<p>Remark 6.2.5 If A is reduced by row, the Exercises 6.2.2 and 6.2.3 show that one first
</p>
<p>determines the value of the unknown corresponding to the pivot (special) element
</p>
<p>of the bottom row and then replaces such unknown by its value in the remaining
</p>
<p>equations. This amounts to delete, or eliminate one of the unknowns. Upon iterating
</p>
<p>this procedure one completely solves the system. This procedure is showed in the
</p>
<p>following displays where the pivot elements are bold typed:
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 2
</p>
<p>0 1 &minus;2
</p>
<p>0 0 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>4
</p>
<p>&minus;3
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Here one determines z at first then y and finally x . As for the Exercise 6.2.3, one
</p>
<p>writes
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 2 1
</p>
<p>2 3 &minus;1 0
</p>
<p>1 0 1 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>3
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>where one determines z, then y and after those one determines t .</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 The Space of Solutions for Reduced Systems 83
</p>
<p>The previous exercises suggest the following method that we describe as a propo-
</p>
<p>sition.
</p>
<p>Proposition 6.2.6 (The method of eliminations) Let � : AX = B be a reduced sys-
</p>
<p>tem.
</p>
<p>(1) From the Remark 6.1.7 we may assume that (A, B) has no null rows.
</p>
<p>(2) If A has null rows they correspond to equations like 0 = bi with bi �= 0 since
</p>
<p>the augmented matrix (A, B) has no null rows. This means that the system is not
</p>
<p>compatible, S� = &empty;.
</p>
<p>(3) If A has no null rows, then m &le; n. Since A is reduced, it has m pivot elements, so
</p>
<p>its rank is m. Starting from the bottom row one can then determine the unknown
</p>
<p>corresponding to the pivot element and then, by substituting such an unknown
</p>
<p>in the remaining equations, iterate the procedure thus determining the space of
</p>
<p>solutions.
</p>
<p>We describe the general procedure when A is a complete upper triangular matrix.
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 a13 . . . a1m &lowast; . . . &lowast; a1n
0 a22 a23 . . . a2m &lowast; . . . &lowast; a2n
0 0 a33 . . . a3m &lowast; . . . &lowast; a3n
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 0 . . . amm &lowast; . . . &lowast; amn
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>b1
b2
b3
...
</p>
<p>bm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>with all diagonal elements ai i �= 0. The equation corresponding to the bottom line
</p>
<p>of the matrix is
</p>
<p>amm xm + amm+1xm+1 + &middot; &middot; &middot; + amn xn = bm
</p>
<p>with amm �= 0. By dividing both sides of the equation by amm , one has
</p>
<p>xm = a
&minus;1
mm(bm &minus; amm+1xm+1 + &middot; &middot; &middot; &minus; amn xn).
</p>
<p>Then xm is a function of xm+1, . . . , xn . From the (m &minus; 1)-th row one analogously
</p>
<p>obtains
</p>
<p>xm&minus;1 = a
&minus;1
m&minus;1m&minus;1(bm&minus;1 &minus; am&minus;1m xm &minus; am&minus;1m+1xm+1 + &middot; &middot; &middot; &minus; am&minus;1n xn).
</p>
<p>By replacing xm with its value (as a function of xm+1, . . . , xn) previously deter-
</p>
<p>mined, one writes xm&minus;1 as a function of the last unknowns xm+1, . . . , xn . The natural
</p>
<p>iterations of this process leads to write the unknowns xm&minus;2, xm&minus;3, . . . , x1 as functions
</p>
<p>of the remaining ones xm+1, . . . , xn .
</p>
<p>Remark 6.2.7 Since the m unknowns x1, . . . , xm can be expressed as functions of
</p>
<p>the remaining ones, the n &minus; m unknowns xm+1, . . . , xn , the latter are said to be free
</p>
<p>unknowns. By choosing an arbitrary numerical value for them, xm+1 = λ1, . . . , xn =
</p>
<p>λn&minus;m , with λi &isin; R, one obtains a solution, since the matrix A is reduced, of the linear
</p>
<p>system. This allows one to define a bijection</p>
<p/>
</div>
<div class="page"><p/>
<p>84 6 Systems of Linear Equations
</p>
<p>R
n&minus;m &hArr; S�
</p>
<p>where n is the number of unknowns of � and m = rk(A). One usually labels this
</p>
<p>result by saying that the linear system has &infin;n&minus;m solutions.
</p>
<p>6.3 The Space of Solutions for a General Linear System
</p>
<p>One of the possible methods to solve a general linear system AX = B uses the
</p>
<p>notions of row reduction for a matrix as described in Sect. 4.4. From the definition
</p>
<p>at the beginning of that section one has the following proposition.
</p>
<p>Theorem 6.3.1 Let � : AX = B be a linear system, and let (A&prime;, B &prime;) be a trans-
</p>
<p>formed by row matrix of (A, B). The linear systems � and the transformed one
</p>
<p>�&prime; : A&prime;X = B &prime; are equivalent.
</p>
<p>Proof We denote as usual A = (ai j ) and B =
t (b1, . . . , bm). If (A
</p>
<p>&prime;, B &prime;) is obtained
</p>
<p>from (A, B) under a type (e) elementary transformation, the claim is obvious as seen
</p>
<p>in Remark 6.1.7. If (A&prime;, B &prime;) is obtained from (A, B) under a type (λ) transformation
</p>
<p>by the row Ri the claim follows by noticing that, for any λ �= 0, the linear equation
</p>
<p>ai1x1 + &middot; &middot; &middot; + ain xn = bi
</p>
<p>is equivalent to the equation
</p>
<p>λai1x1 + &middot; &middot; &middot; + λain xn = λbi .
</p>
<p>Let now (A&prime;, B &prime;) be obtained from (A, B) via a type (D) elementary transforma-
</p>
<p>tion,
</p>
<p>Ri &#13;&rarr; Ri + λR j
</p>
<p>with j �= i . To be definite we take i = 2 and j = 1. We then have
</p>
<p>(A&prime;, B &prime;) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>R1
R2 + λR1
</p>
<p>...
</p>
<p>Rm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Let us assume that α = (α1, . . . ,αn) is a solution for �, that is
</p>
<p>ai1α1 + &middot; &middot; &middot; + ainαn = bi
</p>
<p>for any i = 1, . . . ,m. That all but the second equation of �&prime; are solved by α is</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Space of Solutions for a General Linear System 85
</p>
<p>obvious; it remains to verify whether α solves also the second equation in it that is,
</p>
<p>to show that
</p>
<p>(a21 + λa11)x1 + &middot; &middot; &middot; + (a2n + λa1n)xn = b2 + λb1.
</p>
<p>If we add the equation for i = 2 to λ times the equation for i = 1, we obtain
</p>
<p>(a21 + λa11)α1 + &middot; &middot; &middot; + (a2n + λa1n)αn = b2 + λb1
</p>
<p>thus (α1, . . . ,αn) is a solution for �
&prime; and S� &sube; S�&prime; . The inclusion S�&prime; &sube; S� is
</p>
<p>proven in an analogous way. �
</p>
<p>By using the above theorem one proves a general method to solve linear systems
</p>
<p>known as Gauss&rsquo; elimination method or Gauss&rsquo; algorithm.
</p>
<p>Theorem 6.3.2 The space S� of the solutions of the linear system � : AX = B is
</p>
<p>determined via the following steps.
</p>
<p>(1) Reduce by rows the matrix (A, B) to (A&prime;, B &prime;) with A&prime; reduced by row.
</p>
<p>(2) Using the method given in the Proposition 6.2.6 determine the space S�&prime; of the
</p>
<p>solutions for the system �&prime; : A&prime;X = B &prime;.
</p>
<p>(3) From the Theorem 6.3.1 it is � &sim; �&prime; that is S� = S�&prime; .
</p>
<p>Exercise 6.3.3 Let us solve the following linear system
</p>
<p>� =
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>2x + y + z = 1
</p>
<p>x &minus; y &minus; z = 0
</p>
<p>x + 2y + 2z = 1
</p>
<p>whose complete matrix is
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 1
</p>
<p>1 &minus;1 &minus;1
</p>
<p>1 2 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>By reducing such a matrix by rows, we have
</p>
<p>(A, B)
</p>
<p>R2 &#13;&rarr; R2+R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;2R1
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 1
</p>
<p>3 0 0
</p>
<p>&minus;3 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>1
</p>
<p>&minus;1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3+R2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 1
</p>
<p>3 0 0
</p>
<p>0 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>1
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠ = (A&prime;, B &prime;).
</p>
<p>Since A&prime; is reduced the linear system �&prime; : A&prime;X = B &prime; is reduced and then solvable
</p>
<p>by the Gauss&rsquo; method. We have</p>
<p/>
</div>
<div class="page"><p/>
<p>86 6 Systems of Linear Equations
</p>
<p>�&prime; :
</p>
<p>{
</p>
<p>2x + y + z = 1
</p>
<p>3x = 1
=&rArr;
</p>
<p>{
</p>
<p>y + z = 1
3
</p>
<p>x = 1
3
</p>
<p>.
</p>
<p>It is now clear that one unknown is free so the linear system has &infin;1 solutions.
</p>
<p>By choosing z = λ the space S� of solutions for � is
</p>
<p>S� = {(x, y, z) &isin; R
3 | (x, y, z) = ( 1
</p>
<p>3
, 1
</p>
<p>3
&minus; λ,λ), λ &isin; R}.
</p>
<p>On the other end, by choosing y = α the space S� can be written as
</p>
<p>S� = {(x, y, z) &isin; R
3 | (x, y, z) = ( 1
</p>
<p>3
,α, 1
</p>
<p>3
&minus; α), α &isin; R}.
</p>
<p>It is obvious that we are representing the same subset S� &sub; R
3 in two different
</p>
<p>ways.
</p>
<p>Notice that the number of free unknowns is the difference between the total number
</p>
<p>of unknowns and the rank of the matrix A.
</p>
<p>Exercise 6.3.4 Let us solve the following linear system,
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + y &minus; z = 0
</p>
<p>2x &minus; y = 1
</p>
<p>y + 2z = 2
</p>
<p>whose complete matrix is
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>2 &minus;1 0
</p>
<p>0 1 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The reduction procedure gives
</p>
<p>(A, B)
</p>
<p>R2 &#13;&rarr; R2&minus;2R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 &minus;3 2
</p>
<p>0 1 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;R2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 &minus;3 2
</p>
<p>0 4 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>0
</p>
<p>1
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ = (A&prime;, B &prime;).
</p>
<p>Since A&prime; is reduced the linear system �&prime; : A&prime;X = B &prime; is reduced with no free
</p>
<p>unknowns. This means that S�&prime; (and then S�) has &infin;
0 = 1 solution. The Gauss&rsquo;
</p>
<p>method provides us a way to find such a solution, namely
</p>
<p>�&prime; :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x &minus; y + z = 0
</p>
<p>&minus;3y + 2z = 1
</p>
<p>4y = 1
</p>
<p>=&rArr;
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x &minus; z = &minus; 1
4
</p>
<p>2z = 7
4
</p>
<p>y = 1
4
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Space of Solutions for a General Linear System 87
</p>
<p>This gives S� = {(x, y, z) = (
5
8
, 1
</p>
<p>4
, 7
</p>
<p>8
)}. Once more the number of free unknowns
</p>
<p>is the difference between the total number of unknowns and the rank of the matrix A.
</p>
<p>The following exercise shows how to solve a linear system with one coefficient
</p>
<p>given by a real parameter instead of a fixed real number. By solving such a system we
</p>
<p>mean to analyse the conditions on the parameter under which the system is solvable
</p>
<p>and to provide its space of solutions as depending on the possible values of the
</p>
<p>parameter.
</p>
<p>Exercise 6.3.5 Let us study the following linear system,
</p>
<p>�λ :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x + 2y + z + t = &minus;1
</p>
<p>x + y &minus; z + 2t = 1
</p>
<p>2x + λy + λt = 0
</p>
<p>&minus;λy &minus; 2z + λt = 2
</p>
<p>with λ &isin; R. When the complete matrix for such a system is reduced, particular care
</p>
<p>must be taken for some critical values of λ. We have
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1 1
</p>
<p>1 1 &minus;1 2
</p>
<p>2 λ 0 λ
</p>
<p>0 &minus;λ &minus;2 λ
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>1
</p>
<p>0
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>R2 &#13;&rarr; R2&minus;R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;2R1
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1 1
</p>
<p>0 &minus;1 &minus;2 1
</p>
<p>0 λ &minus; 4 &minus;2 λ &minus; 2
</p>
<p>0 &minus;λ &minus;2 λ
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>2
</p>
<p>2
</p>
<p>2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>R3 &#13;&rarr; R3&minus;R2
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R4&rarr;R4&minus;R2
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1 1
</p>
<p>0 &minus;1 &minus;2 1
</p>
<p>0 λ &minus; 3 0 λ &minus; 3
</p>
<p>0 &minus;λ + 1 0 λ &minus; 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>2
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= (A&prime;, B &prime;).
</p>
<p>The transformations R3 &#13;&rarr; R3 + R4, then R3 &#13;&rarr;
1
2
</p>
<p>R3 and finally R4 &#13;&rarr; R4 +
</p>
<p>(1 &minus; λ)R3 give a further reduction of (A
&prime;, B &prime;) as
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1 1
</p>
<p>0 &minus;1 &minus;2 1
</p>
<p>0 &minus;1 0 λ &minus; 2
</p>
<p>0 &minus;λ + 1 0 λ &minus; 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>2
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>&#13;&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 2 1 1
</p>
<p>0 &minus;1 &minus;2 1
</p>
<p>0 &minus;1 0 λ &minus; 2
</p>
<p>0 0 0 a44
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>2
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= (A&prime;&prime;, B &prime;&prime;)
</p>
<p>with a44 = (1 &minus; λ)(λ &minus; 3). Notice that the last transformation is meaningful for any
</p>
<p>λ &isin; R. In the reduced form (A&prime;&prime;, B &prime;&prime;) we have that R4 is null if and only if either
</p>
<p>λ = 3 or λ = 1. For such values of the parameter λ either R3 or R4 in A
&prime; is indeed
</p>
<p>null. We can now conclude that �λ is solvable for any value of λ &isin; R and we have</p>
<p/>
</div>
<div class="page"><p/>
<p>88 6 Systems of Linear Equations
</p>
<p>&bull; If λ &isin; {1, 3} then a44 = 0, so rk(A) = 3 and �λ has &infin;
1 solutions,
</p>
<p>&bull; If λ /&isin; {1, 3} then a44 �= 0, so rk(A) = 4 and �λ has a unique solution.
</p>
<p>We can now study the following three cases:
</p>
<p>(a) λ /&isin; {1, 3}, that is
</p>
<p>�λ :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x + 2y + z + t = &minus;1
</p>
<p>&minus;y &minus; 2z + t = 2
</p>
<p>&minus;y + (λ &minus; 2)t = 0
</p>
<p>(λ &minus; 3)(λ &minus; 1)t = 0
</p>
<p>.
</p>
<p>From our assumption, we have that a44 = (λ &minus; 3)(λ &minus; 1) �= 0 so we get t = 0.
</p>
<p>By using the Gauss&rsquo; method we then write
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x = 0
</p>
<p>z = &minus;1
</p>
<p>y = 0
</p>
<p>t = 0
</p>
<p>.
</p>
<p>This shows that for λ �= 1, 3 the space S�λ does not depend on λ.
</p>
<p>(b) If λ = 1 we can delete the fourth equation since it is a trivial identity. We have
</p>
<p>then
</p>
<p>�λ=1 :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + 2y + z + t = &minus;1
</p>
<p>&minus;y &minus; 2z + t = 2
</p>
<p>y + t = 0
</p>
<p>.
</p>
<p>The Gauss&rsquo; method gives us
⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 0
</p>
<p>z = t &minus; 1
</p>
<p>y = &minus;t
</p>
<p>and this set of solutions can be written as
</p>
<p>{(x, y, z, t) &isin; R4 | (x, y, z, t) = (0,&minus;α,α &minus; 1,α), α &isin; R}.
</p>
<p>(c) If λ = 3 the non trivial part of the system turns out to be
</p>
<p>�λ=3 :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + 2y + z + t = &minus;1
</p>
<p>&minus;y &minus; 2z + t = 2
</p>
<p>&minus;y + t = 0
</p>
<p>and we write the solutions as
⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = &minus;3t
</p>
<p>z = &minus;1
</p>
<p>y = t
</p>
<p>or equivalently S�λ=3 = {(x, y, z, t) &isin; R
4 | (x, y, z, t) = (&minus;3α,α,&minus;1,α), α &isin; R}.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Space of Solutions for a General Linear System 89
</p>
<p>What we have discussed can be given in the form of the following theorem which
</p>
<p>provides general conditions under which a linear system is solvable.
</p>
<p>Theorem 6.3.6 (Rouch&eacute;&ndash;Capelli). The linear system� : AX = B is solvable if and
</p>
<p>only if rk(A) = rk(A, B). In such a case, denoting rk(A) = rk(A, B) = ρ and with
</p>
<p>n the number of unknowns in �, the following holds true:
</p>
<p>(a) the number of free unknowns is n &minus; ρ,
</p>
<p>(b) the n &minus; ρ free unknowns have to be selected in such a way that the remaining ρ
</p>
<p>unknowns correspond to linearly independent columns of A.
</p>
<p>Proof By noticing that the linear system � can be written as
</p>
<p>x1C1 + &middot; &middot; &middot; + xnCn = B
</p>
<p>with C1, . . . ,Cn the columns of A, we see that� is solvable if and only if B is a linear
</p>
<p>combination of these columns that is if and only if the linear span of the columns of
</p>
<p>A coincides with the linear span of the columns of (A, B). This condition is fulfilled
</p>
<p>if and only if rk(A) = rk(A, B).
</p>
<p>Suppose then that the system is solvable.
</p>
<p>(a) Let �&prime; : A&prime;X = B &prime; be the system obtained from (A, B) by reduction by rows.
</p>
<p>From the Remark 6.2.7 the system �&prime; has n &minus; rk(A&prime;) free unknowns. Since
</p>
<p>� &sim; �&prime; and rk(A) = rk(A&prime;) the claim follows.
</p>
<p>(b) Possibly with a swap of the columns in A = (C1, . . . ,Cn) (which amounts to
</p>
<p>renaming the unknown), the result that we aim to prove is the following:
</p>
<p>xρ+1, . . . , xn are free &hArr; C1, . . . ,Cρ are linearly independent.
</p>
<p>Let us at first suppose that C1, . . . ,Cρ are linearly independent, and set
</p>
<p>A = (C1, . . . ,Cρ). By a possible reduction and a swapping of some equations,
</p>
<p>with rk(A) = rk(A, B) = ρ, the matrix for the system can be written as
</p>
<p>(A&prime;, B &prime;) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 a13 . . . a1ρ &lowast; . . . &lowast; b1
0 a22 a23 . . . a2ρ &lowast; . . . &lowast; b2
0 0 a33 . . . a3ρ &lowast; . . . &lowast; b3
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 0 . . . aρρ &lowast; . . . &lowast; bρ
0 0 0 . . . 0 0 . . . 0 0
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>0 0 0 . . . 0 0 . . . 0 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The claim&mdash;that xρ+1, . . . , xn can be taken to be free&mdash;follows easily from the
</p>
<p>Gauss&rsquo; method.
</p>
<p>On the other hand, let us assume that xρ+1, . . . , xn are free unknowns for the
</p>
<p>linear system and let us also suppose that C1, . . . ,Cρ are linearly dependent. This</p>
<p/>
</div>
<div class="page"><p/>
<p>90 6 Systems of Linear Equations
</p>
<p>would result in the rank of A be less that ρ and there would exist a reduction of
</p>
<p>(A, B) for which the matrix of the linear system turns out to be
</p>
<p>(A&prime;, B &prime;) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 . . . a1ρ &lowast; . . . &lowast;
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>aρ&minus;1 1 . . . aρ&minus;1 ρ &lowast; . . . &lowast;
</p>
<p>0 . . . 0 &lowast; . . . &lowast;
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 . . . 0 &lowast; . . . &lowast;
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Since rk(A&prime;, B &prime;) = rk(A, B) = ρ there would then be a non zero row Ri in
</p>
<p>(A&prime;, B &prime;) with i &ge; ρ. The equation corresponding to such an Ri , not depending
</p>
<p>on the first ρ unknowns, would provide a relation among the xρ+1, . . . , xn , which
</p>
<p>would then be not free. �
</p>
<p>Remark 6.3.7 If the linear system � : AX = B, with n unknowns and m equations
</p>
<p>is solvable with rk(A) = ρ, then
</p>
<p>(i) � is equivalent to a linear system �&prime; with ρ equations arbitrarily chosen among
</p>
<p>the m equations in �, provided they are linearly independent.
</p>
<p>(ii) there is a bijection between the space S� and R
n&minus;ρ.
</p>
<p>Exercise 6.3.8 Let us solve the following linear system depending on a parameter
</p>
<p>λ &isin; R,
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λx + z = &minus;1
</p>
<p>x + (λ &minus; 1)y + 2z = 1
</p>
<p>x + (λ &minus; 1)y + 3z = 0
</p>
<p>.
</p>
<p>We reduce by rows the complete matrix corresponding to � as
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>λ 0 1
</p>
<p>1 λ &minus; 1 2
</p>
<p>1 λ &minus; 1 3
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>1
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>R2 &#13;&rarr; R2&minus;2R1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;3R1
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>λ 0 1
</p>
<p>1 &minus; 2λ λ &minus; 1 0
</p>
<p>1 &minus; 3λ λ &minus; 1 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>3
</p>
<p>3
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;R2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>λ 0 1
</p>
<p>1 &minus; 2λ λ &minus; 1 0
</p>
<p>&minus;λ 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>3
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠ = (A&prime;, B &prime;).
</p>
<p>Depending on the values of the parameter λ we have the following cases.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Space of Solutions for a General Linear System 91
</p>
<p>(a) If λ = 1, the matrix A&prime; is not reduced. We then write
</p>
<p>(A&prime;, B &prime;) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>&minus;1 0 0
</p>
<p>&minus;1 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>3
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠ &minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R3 &#13;&rarr; R3&minus;R2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>&minus;1 0 0
</p>
<p>0 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1
</p>
<p>3
</p>
<p>&minus;3
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The last row gives the equation 0 = &minus;3 and in this case the system has no
</p>
<p>solution.
</p>
<p>(b) If λ �= 1 the matrix A&prime; is reduced, so we have:
</p>
<p>&bull; If λ �= 0, then rk(A) = 3 = rk(A, B), so the linear system �λ=0 has a unique
</p>
<p>solution. With λ /&isin; {0, 1} the reduced system is
</p>
<p>�&prime; :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λx + z = &minus;1
</p>
<p>(1 &minus; 2λ)x + (λ &minus; 1)y = 3
</p>
<p>&minus;λx = 0
</p>
<p>and the Gauss&rsquo; method gives S�λ = (x, y, z) = (0, 3/(λ &minus; 1),&minus;1).
</p>
<p>&bull; If λ = 0 the system we have to solve is
</p>
<p>�&prime; :
</p>
<p>{
</p>
<p>z = &minus;1
</p>
<p>x &minus; y = 3
</p>
<p>whose solutions are given as
</p>
<p>S�λ=0 = {(x, y, z) &isin; R
3 | (x, y, z) = (α + 3,α,&minus;1)α &isin; R} .
</p>
<p>Exercise 6.3.9 Let us show that the following system of vectors,
</p>
<p>v1 = (1, 1, 0), v2 = (0, 1, 1), v3 = (1, 0, 1),
</p>
<p>is free and then write v = (1, 1, 1) as a linear combination of v1, v2, v3.
</p>
<p>We start by recalling that v1, v2, v3 are linearly independent if and only if the rank
</p>
<p>of the matrix whose columns are the vectors themselves is 3. We have the following
</p>
<p>reduction,
</p>
<p>(v1 v2 v3) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠ &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;1
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠ &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;1
</p>
<p>0 0 2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The number of non zero rows of the reduced matrix is 3 so the vectors v1, v2, v3
are linearly independent. Then they are a basis for R3, so the following relation,
</p>
<p>xv1 + yv2 + zv3 = v</p>
<p/>
</div>
<div class="page"><p/>
<p>92 6 Systems of Linear Equations
</p>
<p>is fullfilled by a unique triple (x, y, z) of coefficients for any v &isin; R3. Such a triple is
</p>
<p>the unique solution of the linear system whose complete matrix is
</p>
<p>(A, B) = (v1 v2 v3 v). For the case we are considering in this exercise we have
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>0 1 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>1
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Using for (A, B) the same reduction we used above for A we have
</p>
<p>(A, B) &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;1
</p>
<p>0 1 1
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 &minus;1
</p>
<p>0 0 2
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The linear system we have then to solve is
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + z = 1
</p>
<p>y &minus; z = 0
</p>
<p>2z = 1
</p>
<p>giving (x, y, x) = 1
2
(1, 1, 1). One can indeed directly compute that
</p>
<p>1
2
(1, 1, 0)+ 1
</p>
<p>2
(0, 1, 1)+ 1
</p>
<p>2
(1, 0, 1) = (1, 1, 1).
</p>
<p>Exercise 6.3.10 Let us consider the matrix
</p>
<p>Mλ =
</p>
<p>(
</p>
<p>λ 1
</p>
<p>1 λ
</p>
<p>)
</p>
<p>with λ &isin; R. We compute its inverse using the theory of linear systems.
</p>
<p>We can indeed write the problem in terms of the linear system
</p>
<p>(
</p>
<p>λ 1
</p>
<p>1 λ
</p>
<p>)(
</p>
<p>x y
</p>
<p>z t
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>,
</p>
<p>that is
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>λx + z = 1
</p>
<p>x + λz = 0
</p>
<p>λy + t = 0
</p>
<p>y + λt = 1
</p>
<p>.
</p>
<p>We reduce the complete matrix of the linear system as follows:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Space of Solutions for a General Linear System 93
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ 0 1 0
</p>
<p>1 0 λ 0
</p>
<p>0 λ 0 1
</p>
<p>0 1 0 λ
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>0
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>R2 &#13;&rarr; R2&minus;λR1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ 0 1 0
</p>
<p>1 &minus; λ2 0 0 0
</p>
<p>0 λ 0 1
</p>
<p>0 1 0 λ
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>&minus;λ
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>R4 &#13;&rarr; R4&minus;λR3
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ 0 1 0
</p>
<p>1 &minus; λ2 0 0 0
</p>
<p>0 λ 0 1
</p>
<p>0 1 &minus; λ2 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>&minus;λ
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= (A&prime;, B &prime;).
</p>
<p>The elementary transformations we used are well defined for any real value of λ.
</p>
<p>We start by noticing that if 1 &minus; λ2 = 0 that is λ = &plusmn;1, we have
</p>
<p>(A&prime;, B &prime;) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>&plusmn;1 0 1 0
</p>
<p>0 0 0 0
</p>
<p>0 &plusmn;1 0 1
</p>
<p>0 0 0 0
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>1
</p>
<p>∓1
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The second and the fourth rows of this matrix show that the corresponding linear
</p>
<p>system is incompatible. This means that when λ = &plusmn;1 the matrix Mλ is not invertible
</p>
<p>(as we would immediately see by computing its determinant).
</p>
<p>We assume next that 1 &minus; λ2 �= 0. In such a case we have rk(A) = rk(A, B) = 4,
</p>
<p>so there exists a unique solution for the linear system. We write it in the reduced
</p>
<p>form as
</p>
<p>�&prime; :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>λx + z = 1
</p>
<p>(1 &minus; λ2)x = &minus;λ
</p>
<p>λy + t = 0
</p>
<p>(1 &minus; λ2)y = 1
</p>
<p>.
</p>
<p>Its solution is then
⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>z = 1/(1 &minus; λ2)
</p>
<p>x = &minus;λ/(1 &minus; λ2)
</p>
<p>t = &minus;λ/(1 &minus; λ2)
</p>
<p>y = 1/(1 &minus; λ2)
</p>
<p>,
</p>
<p>that we write in matrix form as
</p>
<p>M&minus;1λ =
1
</p>
<p>(1&minus;λ2)
</p>
<p>(
</p>
<p>&minus;λ 1
</p>
<p>1 &minus;λ
</p>
<p>)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 6 Systems of Linear Equations
</p>
<p>6.4 Homogeneous Linear Systems
</p>
<p>We analyse now an interesting class of linear systems (for easy of notation we write
</p>
<p>0 = 0Rm ).
</p>
<p>Definition 6.4.1 A linear system � : AX = B is called homogeneous if B = 0.
</p>
<p>Remark 6.4.2 A linear system � : AX = 0 with A &isin; Rm,n is always solvable since
</p>
<p>the null n-tuple (the null vector in Rn) gives a solution for �, albeit a trivial
</p>
<p>one. This also follows form the Rouch&eacute;-Capelli theorem since one obviously has
</p>
<p>rk(A) = rk(A, 0). The same theorem allows one to conclude that such a trivial solu-
</p>
<p>tion is indeed the only solution for � if and only if n = ρ = rk(A).
</p>
<p>Theorem 6.4.3 Let � : AX = 0 be a homogeneous linear system with A &isin; Rm,n .
</p>
<p>Then S� is a vector subspace of R
n with dim S� = n &minus; rk(A).
</p>
<p>Proof From the Proposition 2.2.2 we have to show that if X1, X2 &isin; S� with λ1,
</p>
<p>λ2 &isin; R, then λ1 X1 + λ2 X2 is in S� . Since by hypothesis we have AX1 = 0 and
</p>
<p>AX2 = 0 we have also λ1(AX1)+ λ2(AX2) = 0. From the properties of the matrix
</p>
<p>calculus we have in turn λ1(AX1)+ λ2(AX2) = A(λ1 X1 + λ2 X2), thus giving
</p>
<p>λ1 X1 + λ2 X2 in S� . We conclude that S� is a vector subspace of R
n .
</p>
<p>With ρ = rk(A), from the Rouch&eacute;-Capelli theorem we know that � has n &minus; ρ
</p>
<p>free unknowns. This number coincides with the dimension of S� . To show this fact
</p>
<p>we determine a basis made up of n &minus; ρ elements. Let us assume for simplicity that
</p>
<p>the free unknowns are the last ones xρ+1, . . . , xn . Any solution of � can then be
</p>
<p>written as
</p>
<p>(&lowast;, . . . , &lowast;, xρ+1, . . . , xn)
</p>
<p>where the ρ symbols &lowast; stand for the values of x1, . . . , xρ corresponding to each possi-
</p>
<p>ble value of xρ+1, . . . , xn . We let now the (n &minus; ρ)-dimensional &lsquo;vector&rsquo; xρ+1, . . . , xn
range over all elements of the canonical basis of Rn&minus;ρ and write the corresponding
</p>
<p>elements in S� as
</p>
<p>v1 = (&lowast;, . . . , &lowast;, 1, 0, . . . , 0)
</p>
<p>v2 = (&lowast;, . . . , &lowast;, 0, 1, . . . , 0)
</p>
<p>...
</p>
<p>vn&minus;ρ = (&lowast;, . . . , &lowast;, 0, 0, . . . , 1).
</p>
<p>The rank of the matrix (v1, . . . , vn&minus;ρ) (that is the matrix whose rows are these vec-
</p>
<p>tors) is clearly equal to n &minus; ρ, since its last n &minus; ρ columns are linearly independent.
</p>
<p>This means that its rows, the vectors v1, . . . , vn&minus;ρ, are linearly independent. It is easy
</p>
<p>to see that such rows generate S� so they are a basis for it and dim(S�) = n &minus; ρ. �
</p>
<p>It is clear that the general reduction procedure allows one to solve any homoge-
</p>
<p>neous linear system �. Since the space S� is in this case a linear space, one can</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Homogeneous Linear Systems 95
</p>
<p>determine a basis for it. The proof of the previous theorem provides indeed an easy
</p>
<p>method to get such a basis for S� . Once the elements in S� are written in terms of
</p>
<p>the n &minus; ρ free unknowns a basis for S� is given by fixing for these unknowns the
</p>
<p>values corresponding to the elements of the canonical basis in Rn&minus;ρ.
</p>
<p>Exercise 6.4.4 Let us solve the following homogeneous linear system,
</p>
<p>� :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x1 &minus; 2x3 + x5 + x6 = 0
</p>
<p>x1 &minus; x2 &minus; x3 + x4 &minus; x5 + x6 = 0
</p>
<p>x1 &minus; x2 + 2x4 &minus; 2x5 + 2x6 = 0
</p>
<p>and let us determine a basis for its space of solutions. The corresponding A matrix is
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;2 0 1 1
</p>
<p>1 &minus;1 &minus;1 1 &minus;1 1
</p>
<p>1 &minus;1 0 2 &minus;2 2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We reduce it as follows
</p>
<p>A &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;2 0 1 1
</p>
<p>0 &minus;1 1 1 &minus;2 0
</p>
<p>0 &minus;1 2 2 &minus;3 1
</p>
<p>⎞
</p>
<p>⎠ &#13;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 &minus;2 0 1 1
</p>
<p>0 &minus;1 1 1 &minus;2 0
</p>
<p>0 0 1 1 &minus;1 1
</p>
<p>⎞
</p>
<p>⎠ = A&prime;.
</p>
<p>Thus rk(A) = rk(A&prime;) = 3. Since the first three rows in A&prime; (and then in A) are
</p>
<p>linearly independent we choose x4, x5, x6 to be the free unknowns. One clearly has
</p>
<p>� &sim; �&prime; : A&prime;X = 0 so we can solve
</p>
<p>�&prime; :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x1 &minus; 2x3 + x5 + x6 = 0
</p>
<p>x2 &minus; x3 &minus; x4 + 2x5 = 0
</p>
<p>x3 + x4 &minus; x5 + x6 = 0
</p>
<p>.
</p>
<p>By setting x4 = a , x5 = b and x6 = c we have
</p>
<p>S� = {(x1, ..., x6) = (&minus;2a + b &minus; 3c,&minus;b &minus; c,&minus;a + b &minus; c, a, b, c) | a, b, c &isin; R}.
</p>
<p>To determine a basis for S� we let (a, b, c)be the vectors (1, 0, 0), (0, 1, 0), (0, 0, 1)
</p>
<p>of the canonical basis in R3 since n &minus; ρ = 6 &minus; 3 = 3. With this choice we get the
</p>
<p>following basis
</p>
<p>v1 = (&minus;2, 0,&minus;1, 1, 0, 0)
</p>
<p>v2 = (1,&minus;1, 1, 0, 1, 0)
</p>
<p>v3 = (&minus;3,&minus;1,&minus;1, 0, 0, 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
</p>
<p>Linear Transformations
</p>
<p>Together with the theory of linear equations and matrices, the notion of linear
</p>
<p>transformations is crucial in both classical and quantum physics. In this chapter
</p>
<p>we introduce them and study their main properties.
</p>
<p>7.1 Linear Transformations and Matrices
</p>
<p>We have already seen that differently looking sets may have the same vector space
</p>
<p>structure. In this chapter we study mappings between vector spaces which are, in a
</p>
<p>proper sense, compatible with the vector space structure. The action of such maps
</p>
<p>will be represented by matrices.
</p>
<p>Example 7.1.1 Let A =
</p>
<p>(
</p>
<p>a b
</p>
<p>c d
</p>
<p>)
</p>
<p>&isin; R2,2. Let us define the map f : R2 &rarr; R2 by
</p>
<p>f (X) = AX
</p>
<p>where X = (x, y) is a (column) vector representing a generic element in R2 and AX
</p>
<p>denotes the usual row by column product, that is
</p>
<p>f
</p>
<p>(
</p>
<p>x
</p>
<p>y
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>a b
</p>
<p>c d
</p>
<p>) (
</p>
<p>x
</p>
<p>y
</p>
<p>)
</p>
<p>.
</p>
<p>With X = (x1, x2) and Y = (y1, y2) two elements in R
2, using the properties of
</p>
<p>the matrix calculus it is easy to show that
</p>
<p>f (X + Y ) = A(X + Y ) = AX + AY = f (X)+ f (Y )
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_7
</p>
<p>97</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_7&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_7&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>98 7 Linear Transformations
</p>
<p>as well as, with λ &isin; R, that
</p>
<p>f (λX) = A(λX) = λA(X) = λ f (X).
</p>
<p>This example is easily generalised to matrices of arbitrary dimensions.
</p>
<p>Exercise 7.1.2 Given A = (ai j ) &isin; R
m,n one considers the map f : Rn &rarr; Rm
</p>
<p>f (X) = AX,
</p>
<p>with X = t (x1, . . . , xn) and AX the usual row by column product. The above
</p>
<p>properties are easily generalised so this map satisfies the identities f (X + Y ) =
</p>
<p>f (X)+ f (Y ) for any X,Y &isin; Rn and f (λX) = λ f (X) for any X &isin; Rn , λ &isin; R.
</p>
<p>Example 7.1.3 Let A =
</p>
<p>(
</p>
<p>1 2 1
</p>
<p>1 &minus;1 0
</p>
<p>)
</p>
<p>&isin; R2,3. The associated map f : R3 &rarr; R2 is
</p>
<p>given by
</p>
<p>f ((x, y, z)) =
</p>
<p>(
</p>
<p>1 2 1
</p>
<p>1 &minus;1 0
</p>
<p>)
⎛
</p>
<p>⎝
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>(
</p>
<p>x + 2y + z
</p>
<p>x &minus; y
</p>
<p>)
</p>
<p>.
</p>
<p>The above lines motivate the following.
</p>
<p>Definition 7.1.4 Let V and W be two vector spaces over R. A map f : V &rarr; W
</p>
<p>is called linear if the following properties hold:
</p>
<p>(L1) f (X + Y ) = f (X)+ f (Y ) for all X,Y &isin; V ,
</p>
<p>(L2) f (λX) = λ f (X) for all X &isin; V, λ &isin; R .
</p>
<p>The proof of the following identities is immediate.
</p>
<p>Proposition 7.1.5 If f : V &rarr; W is a linear map then,
</p>
<p>(a) f (0V ) = 0W ,
</p>
<p>(b) f (&minus;v) = &minus; f (v) for any v &isin; V ,
</p>
<p>(c) f (a1v1 + &middot; &middot; &middot; + apvp) = a1 f (v1)+ &middot; &middot; &middot; + ap f (vp), for anyv1, . . . , vp &isin; V and
</p>
<p>a1, . . . , ap &isin; R.
</p>
<p>Proof (a) Since 0V = 0R0V the (L2) defining property gives
</p>
<p>f (0V ) = f (0R0V ) = 0R f (0V ) = 0W .
</p>
<p>(b) Since &minus;v = (&minus;1)v, again from (L2) we have
</p>
<p>f (&minus;v) = f ((&minus;1)v) = (&minus;1) f (v) = &minus; f (v).
</p>
<p>(c) This is proved by induction on p. If p = 2 the claim follows directly from (L1)
</p>
<p>and (L2) with</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Linear Transformations and Matrices 99
</p>
<p>f (a1v1 + a2v2) = f (a1v1)+ f (a2v2) = a1 f (v1)+ a2 f (v2).
</p>
<p>Let us assume it to be true for p &minus; 1. By setting w = a1v1 + &middot; &middot; &middot; + ap&minus;1vp&minus;1,
we have
</p>
<p>f (a1v1 + &middot; &middot; &middot; + apvp) = f (w + apvp) = f (w)+ f (apvp) = f (w)+ ap f (vp)
</p>
<p>(the first equality follows from (L1), the second from (L2)). From the induction
hypothesis, we have f (w) = a1 f (v1)+ &middot; &middot; &middot; + ap&minus;1 f (vp&minus;1), so
</p>
<p>f (a1v1 + &middot; &middot; &middot; + apvp) = f (w)+ ap f (vp) = a1 f (v1)+ &middot; &middot; &middot; + ap&minus;1 f (vp&minus;1)+ ap f (vp),
</p>
<p>which is the statement for p.
</p>
<p>⊓⊔
</p>
<p>Example 7.1.6 The Example 7.1.1 and the Exercise 7.1.2 show how one associates
</p>
<p>a linear map between Rn and Rm to a matrix A &isin; Rm,n . This construction can be
</p>
<p>generalised by using bases for vector spaces V and W .
</p>
<p>Let us consider a basis B = (v1, . . . , vn) for V and a basis C = (w1, . . . , wm)
</p>
<p>for W . Given the matrix A = (ai j ) &isin; R
m,n we define f : V &rarr; W as follows. For
</p>
<p>any v &isin; V we have uniquely v = x1v1 + &middot; &middot; &middot; + xnvn , that is v = (x1, . . . , xn)B. With
</p>
<p>X = t (x1, . . . , xn), we consider the vector AX &isin; R
m with AX = t (y1, . . . , ym)C . We
</p>
<p>write then
</p>
<p>f (v) = y1w1 + &middot; &middot; &middot; + ymwm
</p>
<p>which can be written as
</p>
<p>f ((x1, . . . , xn)B) =
</p>
<p>⎛
</p>
<p>⎜
⎝A
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>C
</p>
<p>.
</p>
<p>Exercise 7.1.7 Let us consider the matrix A =
</p>
<p>(
</p>
<p>1 2 1
</p>
<p>1 &minus;1 0
</p>
<p>)
</p>
<p>&isin; R2,3, with V = R[X ]2
</p>
<p>and W = R[X ]1. With respect to the bases B = (1, X, X
2) for V and C = (1, X) for
</p>
<p>W the map corresponding to A as in the previous example is
</p>
<p>f (a + bX + cX2) = (A t (a, b, c))C
</p>
<p>that is
</p>
<p>f (a + bX + cX2) = (a + 2b + c, a &minus; b)C = a + 2b + c + (a &minus; b)X.
</p>
<p>Proposition 7.1.8 The map f : V &rarr; W defined in the Example 7.1.6 is linear.
</p>
<p>Proof Let v, v&prime; &isin; V with v = (x1, . . . , xn)B and v
&prime; = (x &prime;1, . . . , x
</p>
<p>&prime;
n)B. From the
</p>
<p>Remark 2.4.16 we have</p>
<p/>
</div>
<div class="page"><p/>
<p>100 7 Linear Transformations
</p>
<p>v + v&prime; = (x1 + x
&prime;
1, . . . , xn + x
</p>
<p>&prime;
n)B
</p>
<p>so we get
</p>
<p>f (v + v&prime;) = (At(x1 + x
&prime;
1, . . . , xn + x
</p>
<p>&prime;
n))C
</p>
<p>= (At(x1, . . . , xn))C + (A
t(x &prime;1, . . . , x
</p>
<p>&prime;
n))C
</p>
<p>= f (v)+ f (v&prime;)
</p>
<p>(notice that the second equality follows from the Proposition 4.1.10). Along the same
</p>
<p>line one shows easily that for any λ &isin; R one has f (λv) = λ f (v). ⊓⊔
</p>
<p>The following definition (a rephrasing of Example 7.1.6) plays a central role in
</p>
<p>the theory of linear transformations.
</p>
<p>Definition 7.1.9 With V and W two vector spaces over R and basesB = (v1, . . . , vn)
</p>
<p>for V and C = (w1, . . . , wm) for W , consider a matrix A = (ai j ) &isin; R
m,n . The linear
</p>
<p>map
</p>
<p>f
C,B
A : V &rarr; W
</p>
<p>defined by
</p>
<p>V &ni; v = x1v1 + &middot; &middot; &middot; + xnvn �&rarr; f
B,C
A (v) = y1w1 + &middot; &middot; &middot; + ymwm &isin; W
</p>
<p>with
t (y1, . . . , ym) = A
</p>
<p>t (x1, . . . , xn),
</p>
<p>is the linear map corresponding to the matrix A with respect to the basis B e C.
</p>
<p>Remark 7.1.10 Denoting f
C,B
A = f , one immediately sees that the n columns in A
</p>
<p>provide the components with respect to C in W of the vectors f (v1), . . . , f (vn),
</p>
<p>with (v1, . . . , vn) the basis B for V . One has
</p>
<p>v1 = 1v1 + 0v2 + &middot; &middot; &middot; + 0vn = (1, 0, . . . , 0)B,
</p>
<p>thus giving
</p>
<p>f (v1) = (A
t (1, 0, . . . , 0))C =
</p>
<p>t (a11, . . . , am1)C
</p>
<p>= f (v1) = a11w1 + &middot; &middot; &middot; + am1wm .
</p>
<p>It is straightforward now to show that f (v j ) = (a1 j , . . . , amj )C for any index j .
</p>
<p>If A = (ai j ) &isin; R
m,n and f : Rn &rarr; Rm is the linear map defined by f (X) = AX ,
</p>
<p>then the columns of A give the images under f of the vectors (e1, . . . en) of the
</p>
<p>canonical basis En in R
n . This can be written as
</p>
<p>A =
(
</p>
<p>f (e1) f (e2) &middot; &middot; &middot; f (en)
)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Linear Transformations and Matrices 101
</p>
<p>Exercise 7.1.11 Let us consider the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 1 2
</p>
<p>1 1 0
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>with B = C = E3 the canonical basis in R
3, and the corresponding linear map
</p>
<p>f = f
E3,E3
A : R
</p>
<p>3 &rarr; R3. If (x, y, z) &isin; R3 then f ((x, y, z)) = A t (x, y, z). The
</p>
<p>action of f is then given by
</p>
<p>f ((x, y, z)) = (x + y &minus; z, y + 2z, x + y).
</p>
<p>Being B the canonical basis, it is also
</p>
<p>f (e1) = (1, 0, 1), f (e2) = (1, 1, 1), f (e3) = (&minus;1, 2, 0).
</p>
<p>We see that f (e1), f (e2), f (e3) are the columns of A. This is not an accident:
</p>
<p>as mentioned the columns of A are, in the general situation, the components of
</p>
<p>f (e1), f (e2), f (e3) with respect to a basis C&mdash;in this case the canonical one.
</p>
<p>The Proposition 7.1.8 shows that, given a matrix A, the map f
C,B
A is linear. Our
</p>
<p>aim is now to prove that for any linear map f : V &rarr; W there exists a matrix A such
</p>
<p>that f = f
B,C
A , with respect to two given bases B and C for V and W respectively.
</p>
<p>In order to determine such a matrix we use the Remark 7.1.10: given a matrix A
</p>
<p>the images under f
C,B
A of the elements in the basis B of V are given by the column
</p>
<p>elements in A. This suggests the following definition.
</p>
<p>Definition 7.1.12 Let B = (v1, . . . , vn) be a basis for the real vector space V and
</p>
<p>C = (w1, . . . , wm) a basis for the real vector space W . Let f : V &rarr; W be a linear
</p>
<p>map. The matrix associated to f with respect to the basis B and C, that we denote
</p>
<p>by M
C,B
f , is the element in R
</p>
<p>m,n whose columns are given by the components with
</p>
<p>respect to C of the images under f of the basis elements in B. That is, the matrix
</p>
<p>M
C,B
f = A = (ai j ) is given by
</p>
<p>f (v1) = a11w1 + &middot; &middot; &middot; + am1wm
</p>
<p>...
</p>
<p>f (vn) = a1nw1 + &middot; &middot; &middot; + amnwm,
</p>
<p>which can be equivalently written as
</p>
<p>M
C,B
f = ( f (v1), . . . , f (vn)).
</p>
<p>Such a definition inverts the one given in the Definition 7.1.9. This is the content
</p>
<p>of the following proposition, whose proof we omit.</p>
<p/>
</div>
<div class="page"><p/>
<p>102 7 Linear Transformations
</p>
<p>Proposition 7.1.13 Let V be a real vector space with basis B = (v1, . . . , vn) and
</p>
<p>W a real vector space with basis C = (w1, . . . , wm). The following results hold.
</p>
<p>(i) If f : V &rarr; W is a linear map, by setting A = M
C,B
f it holds that
</p>
<p>f
C,B
A = f.
</p>
<p>(ii) If A &isin; Rm,n , by setting f = f
C,B
A it holds that
</p>
<p>M
C,B
f = A.
</p>
<p>Proposition 7.1.14 Let V and W be two real vector spaces with (v1, . . . , vn) a basis
</p>
<p>for V . For any choice of {u1, . . . , un} of n elements in W there exists a unique linear
</p>
<p>map f : V &rarr; W such that f (v j ) = u j for any j = 1, . . . , n.
</p>
<p>Proof To define such a map one uses that any vector v &isin; V can be written uniquely
</p>
<p>as
</p>
<p>v = a1v1 + &middot; &middot; &middot; + anvn
</p>
<p>with respect to the basis (v1, . . . , vn). By setting
</p>
<p>f (v) = a1 f (v1)+ &middot; &middot; &middot; + an f (vn) = a1u1 + &middot; &middot; &middot; + anun
</p>
<p>we have a linear (by construction) map f that satisfies the required condition
</p>
<p>f (v j ) = u j for any j &isin; 1, . . . , n.
</p>
<p>Let us now suppose this map is not unique and that there exists a second linear
</p>
<p>map g : V &rarr; W with g(v j ) = u j . From the Proposition 7.1.5 we could then write
</p>
<p>g(v) = a1g(v1)+ &middot; &middot; &middot; + ang(vn) = a1u1 + &middot; &middot; &middot; + anun = f (v),
</p>
<p>thus getting g = f . ⊓⊔
</p>
<p>What we have discussed so far gives two equivalent ways to define a linear map
</p>
<p>between two vector spaces V and W .
</p>
<p>I. Once a basis B for V , a basis C for W and a matrix A = (ai j ) &isin; R
m,n are fixed,
</p>
<p>from the Proposition 7.1.13 we know that the linear map f
C,B
A is uniquely
</p>
<p>determined.
</p>
<p>II. Once a basis B = (v1, . . . , vn) for V and n vectors {u1, . . . , un} in W are fixed,
</p>
<p>we know from the Proposition 7.1.14 that there exists a unique linear map
</p>
<p>f : V &rarr; W with f (v j ) = u j for any j = 1, . . . , n.
</p>
<p>From now on, if V = Rn and B = E is its canonical basis we shall denote by
</p>
<p>f ((x1, . . . , xn))what we have previously denoted as f ((x1, . . . , xn)B). Analogously,
</p>
<p>with C = E the canonical basis for W = Rm we shall write (y1, . . . , ym) instead of
</p>
<p>(y1, . . . , ym)C .</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Linear Transformations and Matrices 103
</p>
<p>With such a notation, if f : Rn &rarr; Rm is the linear map which, with respect to
</p>
<p>the canonical basis for both vector spaces corresponds to the matrix A, its action is
</p>
<p>written as
</p>
<p>f ((x1, . . . , xn)) = A
t (x1, . . . , xn)
</p>
<p>or equivalently
</p>
<p>f ((x1, . . . , xn)) = (a11x1 + &middot; &middot; &middot; + a1n xn, . . . , am1x1 + &middot; &middot; &middot; + amn xn).
</p>
<p>Exercise 7.1.15 Let f0 : V &rarr; W be the null (zero) map, that is f0(v) = 0W for
</p>
<p>any v &isin; V . With B and C arbitrary bases for V and W respectively, it is clearly
</p>
<p>M
C,B
f0
</p>
<p>= 0Rm,n ,
</p>
<p>that is the null matrix.
</p>
<p>Exercise 7.1.16 If idV (v) = v is the identity map on V then, using any basis
</p>
<p>B = (v1, . . . , vn) for V , one has the following expression
</p>
<p>idV (v j ) = v j = (0, . . . , 0, 1︸︷︷︸
j
</p>
<p>, 0, . . . , 0)B
</p>
<p>for any j = 1, . . . , n. That is M
B,B
idV
</p>
<p>is the identity matrix In . Notice that M
C,B
idV
</p>
<p>	= In
if B 	= C.
</p>
<p>Exercise 7.1.17 Let us consider for R3 both the canonical basis E3 = (e1, e2, e3)
</p>
<p>and the basis B = (v1, v2, v3) with
</p>
<p>v1 = (0, 1, 1), v2 = (1, 0, 1), v3 = (1, 1, 0).
</p>
<p>A direct computation gives
</p>
<p>M
E3,B
id =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 1
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>⎞
</p>
<p>⎠ , M
E3,B
id =
</p>
<p>1
</p>
<p>2
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>&minus;1 1 1
</p>
<p>1 &minus;1 1
</p>
<p>1 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>and each of these matrices turns out to be the inverse of the other, that is
</p>
<p>M
E3,B
id M
</p>
<p>B,E3
id = In .</p>
<p/>
</div>
<div class="page"><p/>
<p>104 7 Linear Transformations
</p>
<p>7.2 Basic Notions on Maps
</p>
<p>Before we proceed we recall in a compact and direct way some of the basic notions
</p>
<p>concerning injectivity, surjectivity and bijectivity of mappings between sets.
</p>
<p>Definition 7.2.1 Let X and Y be two non empty sets and f : X &rarr; Y a map
</p>
<p>between them. The element f (x) in Y is called the image under f of the element
</p>
<p>x &isin; X . The set
</p>
<p>Im( f ) = {y &isin; Y | &exist; x &isin; X : y = f (x)}
</p>
<p>is called the image (or range) of f in Y . The set (that might be empty)
</p>
<p>f &minus;1(y) = {x &isin; X : f (x) = y}.
</p>
<p>defines the pre-image of the element y &isin; Y .
</p>
<p>Definition 7.2.2 Let X and Y be two non empty sets, with a map f : X &rarr; Y . One
</p>
<p>says that:
</p>
<p>(i) f is injective if, for any pair x1, x2 &isin; X with x1 	= x2, it is f (x1) 	= f (x2),
</p>
<p>(ii) f is surjective if Im( f ) = Y ,
</p>
<p>(iii) f is bijective if f is both injective and surjective.
</p>
<p>Definition 7.2.3 Let f : X &rarr; Y and g : Y &rarr; Z be two maps. The composition
</p>
<p>of g with f is the map
</p>
<p>g ◦ f : X &rarr; Z
</p>
<p>defined as (g ◦ f )(x) = g( f (x)) for any x &isin; X .
</p>
<p>Definition 7.2.4 A map f : X &rarr; Y is invertible if there exists a map g : Y &rarr; X
</p>
<p>such that g ◦ f = idX and f ◦ g = idY . In such a case the map g is called the
</p>
<p>inverse of f and denoted by f &minus;1. It is possible to prove that, if f is invertible, then
</p>
<p>f &minus;1 is unique.
</p>
<p>Proposition 7.2.5 A map f : X &rarr; Y is invertible if and only if it is bijective. In
</p>
<p>such a case the map f &minus;1 is invertible as well, with ( f &minus;1)&minus;1 = f .
</p>
<p>7.3 Kernel and Image of a Linear Map
</p>
<p>Injectivity and surjectivity of a linear map are measured by two vector subspaces
</p>
<p>that we now introduce and study.
</p>
<p>Definition 7.3.1 Consider a linear map f : V &rarr; W . The set
</p>
<p>V &supe; ker( f ) = {v &isin; V : f (v) = 0W }</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Kernel and Image of a Linear Map 105
</p>
<p>is called the kernel of f , while the set
</p>
<p>W &supe; Im( f ) = {w &isin; W : &exist; v &isin; V : w = f (v)}
</p>
<p>is called the image of f .
</p>
<p>Theorem 7.3.2 Given a linear map f : V &rarr; W , the set ker( f ) is a vector sub-
</p>
<p>space in V and Im( f ) is a vector subspace in W .
</p>
<p>Proof We recall the Proposition 2.2.2. Given v, v&prime; &isin; ker( f ) and λ,λ&prime; &isin; R we need to
</p>
<p>compute f (λv + λ&prime;v&prime;). Since f (v) = 0W = f (v
&prime;) by hypothesis, from the Proposi-
</p>
<p>tion 7.1.5 we have f (λv + λ&prime;v&prime;) = λ f (v)+ λ&prime; f (v&prime;) = 0W . This shows that ker( f )
</p>
<p>is a vector subspace in V .
</p>
<p>Analogously, let w,w&prime; &isin; Im( f ) and λ,λ&prime; &isin; R. From the hypothesis there exist
</p>
<p>v, v&prime; &isin; V such that w = f (v) and w&prime; = f (v&prime;); thus we can write λw + λ&prime;w&prime; =
</p>
<p>λ f (v)+ λ&prime; f (v&prime;) = f (λv + λ&prime;v&prime;) &isin; Im( f ) again from he Proposition 7.1.5. This
</p>
<p>shows that Im( f ) is a vector subspace in W . ⊓⊔
</p>
<p>Having proved that Im( f ) and ker( f ) are vector subspaces we look for a system
</p>
<p>of generators for them. Such a task is easier for the image of f as the following
</p>
<p>lemma shows.
</p>
<p>Lemma 7.3.3 With f : V &rarr; W a linear map, one has that
</p>
<p>Im( f ) = L( f (v1), . . . , f (vn)), where B = (v1, . . . , vn) is an arbitrary basis for
</p>
<p>V . The map f is indeed surjective if and only if f (v1), . . . , f (vn) generate W .
</p>
<p>Proof Letw&isin; Im( f ), that isw= f (v) for some v &isin; V . Being B a basis for V , one has
</p>
<p>v = a1v1 + &middot; &middot; &middot; + anvn and since f is linear, one has w = a1 f (v1)+ &middot; &middot; &middot; + an f (vn),
</p>
<p>thus givingw &isin; L( f (v1), . . . , f (vn)). We have then Im( f ) &sube; L( f (v1), . . . , f (vn)).
</p>
<p>The opposite inclusion is obvious since Im( f ) is a vector subspace in W and contains
</p>
<p>the vectors f (v1), . . . , f (vn).
</p>
<p>The last statement is the fact that f is surjective (Definition 7.2.2) if and only if
</p>
<p>Im( f ) = W . ⊓⊔
</p>
<p>Exercise 7.3.4 Let us consider the linear map f : R3 &rarr; R2 given by
</p>
<p>f ((x, y, z)) = (x + y &minus; z, x &minus; y + z).
</p>
<p>From the lemma above, the vector subspace Im( f ) is generated by the images
</p>
<p>under f of an arbitrary basis in R3. With the canonical basis E = (e1, e2, e3) we
</p>
<p>have Im( f ) = L( f (e1), f (e2), f (e3)), with
</p>
<p>f (e1) = (1, 1), f (e2) = (1,&minus;1), f (e3) = (&minus;1, 1).
</p>
<p>It is immediate to see that Im( f ) = R2, that is f is surjective.</p>
<p/>
</div>
<div class="page"><p/>
<p>106 7 Linear Transformations
</p>
<p>Lemma 7.3.5 Let f : V &rarr; W be a linear map between two real vector spaces.
</p>
<p>Then,
</p>
<p>(i) f is injective if and only if ker( f ) = {0V },
</p>
<p>(ii) if f is injective and (v1, . . . , vn) is a basis for V , the vectors f (v1), . . . , f (vn)
</p>
<p>are linearly independent.
</p>
<p>Proof (i) Let us assume that f is injective and v &isin; ker( f ), that is f (v) = 0W .
</p>
<p>From the Proposition 7.1.5 we know that f (0V ) = 0W . Since f is injective it
</p>
<p>must be v = 0V , that is ker( f ) = {0V }.
</p>
<p>Viceversa, let us assume that ker( f ) = 0V and let us consider two vectors v1, v2
such that f (v1) = f (v2). Since f is linear this reads 0W = f (v1)&minus; f (v2) =
</p>
<p>f (v1 &minus; v2), that is v1 &minus; v2 &isin; ker( f ) which, being the latter the null vector
</p>
<p>subspace, thus gives v1 = v2.
</p>
<p>(ii) In order to study the linear independence of the system of vectors
</p>
<p>{ f (v1), . . . , f (vn)} let us take scalars λ1, . . . ,λn &isin; R such that
</p>
<p>λ1 f (v1)+ &middot; &middot; &middot; + λn f (vn) = 0W . Being f linear, this gives f (λ1v1 + &middot; &middot; &middot; +
</p>
<p>λnvn) = 0W and then λ1v1 + &middot; &middot; &middot; + λnvn &isin; ker( f ). Since f is injective, from
</p>
<p>(i)we have ker( f ) = {0V } so it is λ1v1 + &middot; &middot; &middot; + λnvn = 0V . Being (v1, . . . , vn)
</p>
<p>a basis for V , we have that λ1 = &middot; &middot; &middot; = λn = 0R thus proving that also
</p>
<p>f (v1), . . . , f (vn) are linearly independent.
</p>
<p>⊓⊔
</p>
<p>Exercise 7.3.6 Let us consider the linear map f : R2 &rarr; R3 given by
</p>
<p>f ((x, y)) = (x + y, x &minus; y, 2x + 3y).
</p>
<p>The kernel of f is given by
</p>
<p>ker( f ) = {(x, y) &isin; R2 | f ((x, y)) = (x + y, x &minus; y, 2x + 3y) = (0, 0, 0)}
</p>
<p>so we have to solve the linear system
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x + y = 0
</p>
<p>x &minus; y = 0
</p>
<p>2x + 3y = 0
</p>
<p>.
</p>
<p>Its unique solution is (0, 0) so ker( f ) = {0R2} and we can conclude, from the
</p>
<p>lemma above, that f is injective. From the same lemma we also know that the
</p>
<p>images under f of a basis for R2 make a linearly independent set of vectors. If we
</p>
<p>take the canonical basis for R2 with e1 = (1, 0) and e2 = (0, 1), we have
</p>
<p>f (e1) = (1, 1, 2), f (e2) = (1,&minus;1, 3).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Isomorphisms 107
</p>
<p>7.4 Isomorphisms
</p>
<p>Definition 7.4.1 Let V and W be two real vector spaces. A bijective linear map
</p>
<p>f : V &rarr; W is called an isomorphism. Two vector spaces are said to be isomorphic
</p>
<p>if there exists an isomorphism between them. If f : V &rarr; W is an isomorphism
</p>
<p>we write V &sim;= W .
</p>
<p>Proposition 7.4.2 If the map f : V &rarr; W is an isomorphism, such is its inverse
</p>
<p>f &minus;1 : W &rarr; V .
</p>
<p>Proof From the Proposition 7.2.5 we have that f is invertible, with an invertible
</p>
<p>inverse map f &minus;1. We need to prove that f &minus;1 is linear. Let us consider two arbitrary
</p>
<p>vectors w1, w2 &isin; W with v1 = f
&minus;1(w1) and v2 = f
</p>
<p>&minus;1(w2) in V ; this is equivalent
</p>
<p>to w1 = f (v1) and w2 = f (v2). Let us consider also λ1,λ2 &isin; R. Since f is linear
</p>
<p>we can write
</p>
<p>λ1w1 + λ2w2 = f (λ1v1 + λ2v2).
</p>
<p>For the action of f &minus;1 is then
</p>
<p>f &minus;1(λ1w1 + λ2w2) = λ1v1 + λ2v2 = λ1 f
&minus;1(w1)+ λ2 f
</p>
<p>&minus;1(w2),
</p>
<p>which amounts to say that f &minus;1 is a linear map. ⊓⊔
</p>
<p>In order to characterise isomorphisms we first prove a preliminary result.
</p>
<p>Lemma 7.4.3 Let f : V &rarr; W be a linear map with (v1, . . . , vn) a basis for V .
</p>
<p>The map f is an isomorphism if and only if ( f (v1), . . . , f (vn)) is a basis for W .
</p>
<p>Proof If f is an isomorphism, it is both injective and surjective. From the
</p>
<p>Lemma 7.3.3 the system f (v1), . . . , f (vn) generates W , while from the Lemma 7.3.5
</p>
<p>such a system is linearly independent. This means that ( f (v1), . . . , f (vn)) is a basis
</p>
<p>for W .
</p>
<p>Let us now assume that the vectors ( f (v1), . . . , f (vn)) are a basis for W . From the
</p>
<p>Proposition 7.1.14 there exists a linear map g : W &rarr; V such that g( f (v j )) = v j
for any j = 1, . . . , n. This means that the linear maps g ◦ f and idV coincide on the
</p>
<p>basis (v1, . . . , vn) in V and then (again from Proposition 7.1.14) they coincide, that
</p>
<p>is g ◦ f = idV . Along the same lines it is easy to show that f ◦ g = idW , so we have
</p>
<p>g = f &minus;1; the map f is then invertible so it is an isomorphism. ⊓⊔
</p>
<p>Theorem 7.4.4 Let V and W be two real vector spaces. They are isomorphic if and
</p>
<p>only if dim(V ) = dim(W ).
</p>
<p>Proof Let us assume V and W to be isomorphic, that is there exists an isomor-
</p>
<p>phism f : V &rarr; W . From the previous lemma, if (v1, . . . , vn) is a basis for V , then
</p>
<p>( f (v1), . . . , f (vn)) is a basis for W and this gives dim(V ) = n = dim(W ).
</p>
<p>Let us now assume n = dim(V ) = dim(W ) and try to define an isomorphism
</p>
<p>f : V &rarr; W . By fixing a basisB = (v1, . . . , vn) for V and a basisC = (w1, . . . , wn)</p>
<p/>
</div>
<div class="page"><p/>
<p>108 7 Linear Transformations
</p>
<p>for W , we define the linear map f (v j ) = w j for any j . Such a linear map exists and it
</p>
<p>is unique from the Proposition 7.1.14. From the lemma above, f is an isomorphism
</p>
<p>since it maps the basis B to the basis C for W . ⊓⊔
</p>
<p>Corollary 7.4.5 If V is a real vector space with dim(V ) = n, then V &sim;= Rn . Any
</p>
<p>choice of a basis B for V induces the natural isomorphism
</p>
<p>α : V
&sim;=
</p>
<p>&minus;&rarr; Rn given by (x1, . . . , xn)B �&rarr; (x1, . . . , xn).
</p>
<p>Proof The first claim follows directly from the Theorem 7.4.4 above. Once the
</p>
<p>basis B = (v1, . . . , vn) is chosen the map α is defined as the linear map such that
</p>
<p>α(v j ) = e j for any j = 1, . . . , n. From the Lemma 7.4.3 such a map α is an iso-
</p>
<p>morphism. It is indeed immediate to check that the action of α on any vector in V is
</p>
<p>given by α : (x1, . . . , xn)B �&rarr; (x1, . . . , xn). ⊓⊔
</p>
<p>Exercise 7.4.6 Let V = R[X ]2 be the space of the polynomials whose degree is
</p>
<p>not higher than 2. As we know, V has dimension 3 and a basis for it is given by
</p>
<p>B = (1, X, X2). The isomorphismα : R[X ]2
&sim;=
</p>
<p>&minus;&rarr; R3 corresponding to such a basis
</p>
<p>reads
</p>
<p>a + bX + cX2 �&rarr; (a, b, c).
</p>
<p>It is simple to check whether a given system of polynomials is a basis for R[X ]2.
</p>
<p>As an example we consider
</p>
<p>p1(X) = 3X &minus; X
2, p2(X) = 1 + X, p3(X) = 2 + 3X
</p>
<p>2.
</p>
<p>By setting v1 = α(p1) = (0, 3,&minus;1), v2 = α(p2) = (1, 1, 0) and v3 = α(p3) =
</p>
<p>(2, 0, 3), it is clear that the rank of the matrix whose columns are the vectors
</p>
<p>v1, v2, v3 is 3, thus proving that (v1, v2, v3) is a basis for R
3. Since α is an iso-
</p>
<p>morphism, the inverse α&minus;1 : R3 &rarr; R[X ]2 is an isomorphism as well: the vectors
</p>
<p>α
&minus;1(v1), α
</p>
<p>&minus;1(v2), α
&minus;1(v2) provide a basis for R[X ]2 and coincide with the given
</p>
<p>polynomials p1(X), p2(X), p3(X).
</p>
<p>Theorem 7.4.4 shows that a linear isomorphism exists only if its domain has the
</p>
<p>same dimension of its image. A condition that characterises isomorphism can then
</p>
<p>be introduced only for vector spaces with the same dimensions. This is done in the
</p>
<p>following sections.
</p>
<p>7.5 Computing the Kernel of a Linear Map
</p>
<p>We have seen that isomorphisms can be defined only between spaces with the same
</p>
<p>dimension. Being not an isomorphism indeed means for a linear map to fail to be
</p>
<p>injective or surjective. In this section and the following one we characterise injectivity</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Computing the Kernel of a Linear Map 109
</p>
<p>and surjectivity of a linear map via the study of its kernel and its image. In particular,
</p>
<p>we shall describe procedures to exhibit bases for such spaces.
</p>
<p>Proposition 7.5.1 Let f : V &rarr; W be a linear map between real vector spaces,
</p>
<p>and dim(V ) = n. Fix a basis B for V and a basis C for W , with associated matrix
</p>
<p>A = M
C,B
f . By denoting� : AX = 0 the linear system associated to A, the following
</p>
<p>hold:
</p>
<p>(i) S� &sim;= ker( f ) via the isomorphism (x1, . . . , xn) �&rarr; (x1, . . . , xn)B,
</p>
<p>(ii) dim(ker( f )) = n &minus; rk(A),
</p>
<p>(iii) if (v1, . . . , vp) is a basis for S� , the vectors
(
</p>
<p>(v1)B, . . . , (vp)B
)
</p>
<p>are a basis for
</p>
<p>ker( f ).
</p>
<p>Proof (i) With the given hypothesis, from the definition of the kernel of a linear
</p>
<p>map we can write
</p>
<p>ker( f ) = {v &isin; V : f (v) = 0W }
</p>
<p>=
</p>
<p>⎧
</p>
<p>⎪
⎨
</p>
<p>⎪
⎩
</p>
<p>v = (x1, . . . , xn)B &isin; V :
</p>
<p>⎛
</p>
<p>⎜
⎝A
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>C
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>0
...
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>C
</p>
<p>⎫
</p>
<p>⎪
⎬
</p>
<p>⎪
⎭
</p>
<p>= {(x1, . . . , xn)B &isin; V : (x1, . . . , xn) &isin; S�}
</p>
<p>with S� denoting the space of solutions for �. As in Corollary 7.4.5 we can
</p>
<p>then write down the isomorphism S� &rarr; ker( f ) given by
</p>
<p>(x1, . . . , xn) �&rarr; (x1, . . . , xn)B.
</p>
<p>(ii) From the isomorphism of the previous point we then have
</p>
<p>dim(ker( f )) = dim(S�) = n &minus; rk(A)
</p>
<p>where the last equality follows from the Theorem 6.4.3.
</p>
<p>(iii) From the Lemma 7.4.3 we know that, under the isomorphism S� &rarr; ker( f ),
</p>
<p>a basis for S� is mapped into a basis for ker( f ).
</p>
<p>⊓⊔
</p>
<p>Exercise 7.5.2 Consider the linear map f : R3 &rarr; R3 defined by
</p>
<p>f ((x, y, z)B) = (x + y &minus; z, x &minus; y + z, 2x)E
</p>
<p>where B =
(
</p>
<p>(1, 1, 0), (0, 1, 1), (1, 0, 1)
)
</p>
<p>and E is the canonical basis for R3. We
</p>
<p>determine ker( f ) and compute a basis for it with respect to both B and E . Start by
</p>
<p>considering the matrix associated to the linear map f with the given basis,</p>
<p/>
</div>
<div class="page"><p/>
<p>110 7 Linear Transformations
</p>
<p>A = M
E,B
f =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>1 &minus;1 1
</p>
<p>2 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>To solve the linear system � : AX = 0 we reduce the matrix A by rows:
</p>
<p>A �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>2 0 0
</p>
<p>2 0 0
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>2 0 0
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>and the space S� of the solutions of � is then given by
</p>
<p>S� = {(0, a, a) : a &isin; R} = L((0, 1, 1)).
</p>
<p>This reads
</p>
<p>ker( f ) = {(0, a, a)B : a &isin; R},
</p>
<p>with a basis given by the vector (0, 1, 1)B . With the explicit expression of the elements
</p>
<p>of B,
</p>
<p>(0, 1, 1)B = (0, 1, 1)+ (1, 0, 1) = (1, 1, 2).
</p>
<p>This shows that the basis vector for ker( f ) given by (0, 1, 1) on the basis B is the
</p>
<p>same as the basis vector (1, 1, 2) with respect to the canonical basis E for R3.
</p>
<p>Exercise 7.5.3 With canonical bases E , consider the linear map f : R3 &rarr; R3
</p>
<p>given by
</p>
<p>f ((x, y, z)) = (x + y &minus; z, x &minus; y + z, 2x).
</p>
<p>To determine the space ker( f ), we observe that the matrix associated to f is
</p>
<p>the same matrix of the previous exercise, so the linear system � : AX = 0 has
</p>
<p>solutions S� = L((0, 1, 1)) = ker( f ), since E is the canonical basis.
</p>
<p>Since the kernel of a linear map is the preimage of the null vector in the image
</p>
<p>space, we can generalise the above procedure to compute the preimage of any element
</p>
<p>w &isin; W . We denote it as f &minus;1(w) = {v &isin; V : f (v) = w}, with ker( f ) = f &minus;1(0W ).
</p>
<p>Notice that we denote the preimage of a set under f by writing f &minus;1 also when f is
</p>
<p>not invertible.
</p>
<p>Proposition 7.5.4 Consider a real vector space V with basis B and a real vec-
</p>
<p>tor space W with basis C. Let f : V &rarr; W be a linear map with A = M
C,B
f its
</p>
<p>corresponding matrix. Given any w = (y1, . . . , ym)C &isin; W , it is
</p>
<p>f &minus;1(w) = {(x1, . . . , xn)B &isin; V : A
t (x1, . . . , xn) =
</p>
<p>t (y1, . . . , ym)}.
</p>
<p>Proof It is indeed true that, with v = (x1, . . . , xn)B, one has</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Computing the Kernel of a Linear Map 111
</p>
<p>f (v) = f ((x1, . . . , xn)B) = (A
t (x1, . . . , xn))C .
</p>
<p>The equality f (v) = w is the equality of components, given by
</p>
<p>A t (x1, . . . , xn) =
t (y1, . . . , ym), on the basis C. ⊓⊔
</p>
<p>Remark 7.5.5 This fact can be expressed via linear systems. Givenw &isin; W , its preim-
</p>
<p>age f &minus;1(w) is made of vectors in V whose components with respect to B solve the
</p>
<p>linear system AX = B, where B is the column of the components of w with respect
</p>
<p>to C.
</p>
<p>Exercise 7.5.6 Consider the linear map f : R3 &rarr; R3 given in the Exercise 7.5.2.
</p>
<p>We compute f &minus;1(w) forw = (1, 1, 1). We have then to solve the system� : AX = B,
</p>
<p>with B = t (1, 1, 1). We reduce the matrix (A, B) as follows
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 1
</p>
<p>1 &minus;1 1 1
</p>
<p>2 0 0 1
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 1
</p>
<p>2 0 0 2
</p>
<p>2 0 0 1
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 1
</p>
<p>1 0 0 1
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>This shows that the system � has no solution, that is w /&isin; Im( f ).
</p>
<p>Next, let us compute f &minus;1(u) for u = (2, 0, 2), so we have the linear system
</p>
<p>� : AX = B with B = t (2, 0, 2). Reducing by row, we have
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 2
</p>
<p>1 &minus;1 1 0
</p>
<p>2 0 0 2
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 2
</p>
<p>2 0 0 2
</p>
<p>2 0 0 2
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1 2
</p>
<p>2 0 0 2
</p>
<p>0 0 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The system � is then equivalent to
</p>
<p>{
</p>
<p>x = 1
</p>
<p>y = z + 1
</p>
<p>whose space of solutions is S� = {(1, a + 1, a) : a &isin; R}. We can then write
</p>
<p>f &minus;1(2, 0, 2) = {(1, a + 1, a)B : a &isin; R} = {(a + 1, a + 2, 2a + 1) : a &isin; R}.
</p>
<p>7.6 Computing the Image of a Linear Map
</p>
<p>We next turn to the study of the image of a linear map.
</p>
<p>Proposition 7.6.1 Let f : V &rarr; W be a linear map between real vector spaces,
</p>
<p>with dim(V ) = n and dim(W ) = m. Fix a basis B for V and a basis C for W , with
</p>
<p>associated matrix A = M
C,B
f and with C(A) its space of columns. The following
</p>
<p>results hold:
</p>
<p>(i) Im( f ) &sim;= C(A) via the isomorphism (y1, . . . , ym)C �&rarr; (y1, . . . , ym),</p>
<p/>
</div>
<div class="page"><p/>
<p>112 7 Linear Transformations
</p>
<p>(ii) dim(Im( f )) = rk(A),
</p>
<p>(iii) if (w1, . . . , wr ) is a basis for C(A), then ((w1)C, . . . , (wr )C) is a basis for
</p>
<p>Im( f ).
</p>
<p>Proof (i) With the given hypothesis, from the definition of the image of a linear
map we can write
</p>
<p>Im( f ) = {w &isin; W : &exist; v &isin; V : w = f (v)}
</p>
<p>=
</p>
<p>⎧
</p>
<p>⎪
⎪
⎨
</p>
<p>⎪
⎪
⎩
</p>
<p>w = (y1, . . . , ym)C &isin; W : &exist; (x1, . . . , xn)B &isin; V :
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>y1
.
.
.
</p>
<p>ym
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>C
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎜
⎝A
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
.
.
.
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>C
</p>
<p>⎫
</p>
<p>⎪
⎪
⎬
</p>
<p>⎪
⎪
⎭
</p>
<p>=
</p>
<p>⎧
</p>
<p>⎪
⎨
</p>
<p>⎪
⎩
</p>
<p>(y1, . . . , ym)C &isin; W : &exist; (x1, . . . , xn) &isin; R
n :
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>y1
.
.
.
</p>
<p>ym
</p>
<p>⎞
</p>
<p>⎟
⎠ = A
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
.
.
.
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠
</p>
<p>⎫
</p>
<p>⎪
⎬
</p>
<p>⎪
⎭
</p>
<p>.
</p>
<p>Representing the matrix A by its columns, that is A = (C1 &middot; &middot; &middot; Cn), we have
</p>
<p>A
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠ = x1C1 + &middot; &middot; &middot; + xnCn.
</p>
<p>We can therefore write
</p>
<p>Im( f ) =
</p>
<p>⎧
</p>
<p>⎪
⎨
</p>
<p>⎪
⎩
</p>
<p>(y1, . . . , ym)C &isin; W : &exist; (x1, . . . , xn) &isin; R
n :
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>y1
.
.
.
</p>
<p>ym
</p>
<p>⎞
</p>
<p>⎟
⎠ = x1C1 + &middot; &middot; &middot; xnCn
</p>
<p>⎫
</p>
<p>⎪
⎬
</p>
<p>⎪
⎭
</p>
<p>=
</p>
<p>⎧
</p>
<p>⎪
⎨
</p>
<p>⎪
⎩
</p>
<p>(y1, . . . , ym)C &isin; W :
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>y1
.
.
.
</p>
<p>ym
</p>
<p>⎞
</p>
<p>⎟
⎠ &isin; C(A)
</p>
<p>⎫
</p>
<p>⎪
⎬
</p>
<p>⎪
⎭
</p>
<p>.
</p>
<p>We have then the isomorphism C(A) &rarr; Im( f ) defined by
</p>
<p>(y1, . . . , ym) �&rarr; (y1, . . . , ym)C
</p>
<p>(compare this with the one in the Corollary 7.4.5).
</p>
<p>(ii) Being Im( f ) &sim;= C(A), it is dim(Im( f )) = dim(C(A)) = rk(A).
</p>
<p>(iii) The claim follows from (i) and the Lemma 7.4.3.
</p>
<p>⊓⊔
</p>
<p>Remark 7.6.2 To determine a basis for C(A) as in (iii) above, one can proceed as
</p>
<p>follows.
</p>
<p>(a) If the rank of A is known, one has to select n linearly independent columns:
</p>
<p>they will give a basis for C(A).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.6 Computing the Image of a Linear Map 113
</p>
<p>(b) If the rank of A is not known, by denoting A&prime; the matrix obtained from A by
</p>
<p>reduction by columns, a basis for C(A) is given by the r non zero columns of A&prime;.
</p>
<p>Exercise 7.6.3 Let f : R3 &rarr; R3 be the linear map with associated matrix
</p>
<p>A = M
C,E
f =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 2
</p>
<p>0 1 &minus;3
</p>
<p>2 &minus;1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>for the canonical basis E and basis C = (w1, w2, w3), with w1 = (1, 1, 0),
</p>
<p>w2 = (0, 1, 1), w3 = (1, 0, 1). We reduce A by columns
</p>
<p>A
</p>
<p>C2 �&rarr;C2 +C1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>C3 �&rarr;C3 &minus; 2C1
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 0
</p>
<p>0 1 &minus;3
</p>
<p>2 1 &minus;3
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C3 �&rarr;C3 + 3C1
</p>
<p>&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&minus;&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>2 1 0
</p>
<p>⎞
</p>
<p>⎠ = A&prime;.
</p>
<p>Being A&prime; reduced by columns, its non zero columns yield a basis for the space
</p>
<p>C(A). Thus, C(A) = C(A&prime;) = L((1, 0, 2), (0, 1, 1)). From the Proposition 7.6.1
</p>
<p>a basis for Im( f ) is given by the pair (u1, u2),
</p>
<p>u1 = (1, 0, 2)C = w1 + 2w3 = (3, 1, 2)
</p>
<p>u2 = (0, 1, 1)C = w2 + w3 = (1, 1, 2).
</p>
<p>Clearly, dim(Im( f )) = 2 = rk(A).
</p>
<p>From the previous results we have the following theorem.
</p>
<p>Theorem 7.6.4 Let f : V &rarr; W be a linear map. It holds that
</p>
<p>dim(ker( f ))+ dim(Im( f )) = dim(V ).
</p>
<p>Proof Let A be any matrix associated to f (that is irrespective of the bases chosen
</p>
<p>in V and W ). From the Proposition 7.5.1 one has dim(ker( f )) = dim(V )&minus; rk(A),
</p>
<p>while from the Proposition 7.6.1 one has dim(Im( f )) = rk(A). The claim follows.
</p>
<p>⊓⊔
</p>
<p>From this theorem, the next corollary follows easily.
</p>
<p>Corollary 7.6.5 Let f : V &rarr; W be a linear map, with dim(V ) = dim(W ). The
</p>
<p>following statements are equivalent.
</p>
<p>(i) f is injective,
</p>
<p>(ii) f is surjective,
</p>
<p>(iii) f is an isomorphism.
</p>
<p>Proof Clearly it is sufficient to prove the equivalence (i) &hArr; (ii). From the Lemma
</p>
<p>7.3.5 we know that f is injective if and only if dim(ker( f )) = 0. We also known
</p>
<p>that f is surjective if and only if dim(Im( f )) = dim(W ). Since dim(V ) = dim(W )
</p>
<p>by hypothesis, the statement thus follows from the Theorem 7.6.4. ⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>114 7 Linear Transformations
</p>
<p>7.7 Injectivity and Surjectivity Criteria
</p>
<p>In this section we study conditions for injectivity and surjectivity of a linear map
</p>
<p>through properties of its associated matrix.
</p>
<p>Proposition 7.7.1 (Injectivity criterion) Let f : V &rarr; W be a linear map. Then f
</p>
<p>is injective if and only if rk(A) = dim(V ) for any matrix A associated to f (that is,
</p>
<p>irrespective of the bases with respect to which the matrix A is given).
</p>
<p>Proof From (i) in the Lemma 7.3.5 we know that f is injective if and only if
</p>
<p>ker( f ) = {0V }, which means dim(ker( f )) = 0. From the Proposition 7.5.1 we have
</p>
<p>that dim(ker( f )) = dim(V )&minus; rk(A) for any matrix A associated to f . We then have
</p>
<p>that f is injective if and only if dim(V )&minus; rk(A) = 0. ⊓⊔
</p>
<p>Exercise 7.7.2 Let f : R[X ]2 &rarr; R
2,2 be the linear map associated to the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
⎜
⎝
</p>
<p>2 1 0
</p>
<p>&minus;1 0 1
</p>
<p>2 1 1
</p>
<p>1 0 0
</p>
<p>⎞
</p>
<p>⎟
⎟
⎠
</p>
<p>with respect to two given basis. SinceA is already reduced by column, rk(A) = 3,
</p>
<p>the number of its non zero columns. Being dim(R[X ]2) = 3 we have, from the
</p>
<p>Proposition 7.7.1, that f is injective.
</p>
<p>Proposition 7.7.3 (Surjectivity criterion) Let f : V &rarr; W be a linear map. The
</p>
<p>map f is surjective if and only if rk(A) = dim(W ) for any matrix associated to f
</p>
<p>(again irrespective of the bases with respect to which the matrix A is given).
</p>
<p>Proof This follows directly from the Proposition 7.6.1. ⊓⊔
</p>
<p>Exercise 7.7.4 Let f : R3 &rarr; R2 be the linear map given by
</p>
<p>f (x, y, z) = (x + y &minus; z, 2x &minus; y + 2z).
</p>
<p>With E the canonical basis in R3 and C the canonical basis in R2, we have
</p>
<p>A = M
C,E
f =
</p>
<p>(
</p>
<p>1 1 &minus;1
</p>
<p>2 &minus;1 2
</p>
<p>)
</p>
<p>:
</p>
<p>by reducing by rows,
</p>
<p>A �&rarr;
</p>
<p>(
</p>
<p>1 1 &minus;1
</p>
<p>3 0 1
</p>
<p>)
</p>
<p>= A&prime;.
</p>
<p>We know that rk(A) = rk(A&prime;) = 2, the number of non zero rows in A&prime;. Being
</p>
<p>dim(R2) = 2, the map f is surjective from the Proposition 7.7.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.7 Injectivity and Surjectivity Criteria 115
</p>
<p>We have seen in the Proposition 7.4.2 that if a linear map f is an isomorphism,
</p>
<p>then its domain and image have the same dimension. Injectivity and surjectivity of
</p>
<p>a linear map provide necessary conditions on the relative dimensions of the domain
</p>
<p>and the image of the map.
</p>
<p>Remark 7.7.5 Let f : V &rarr; W be a linear map. One has:
</p>
<p>(a) If f is injective, then dim(V ) &le; dim(W ). This claim easily follows from
</p>
<p>the Lemma 7.3.5, since the images under f of a basis for V gives linearly
</p>
<p>independent vectors in W .
</p>
<p>(b) If f is surjective, then dim(V ) &ge; dim(W ). This claim follows from the Lemma
</p>
<p>7.3.3, since the images under f of a basis for V generate (that is they linearly
</p>
<p>span) W .
</p>
<p>Remark 7.7.6 Let f : V &rarr; W be a linear map, with A its corresponding matrix
</p>
<p>with respect to any basis. One has:
</p>
<p>(a) With dim(V ) &lt; dim(W ), f is injective if and only if rk(A) is maximal;
</p>
<p>(b) With dim(V ) &gt; dim(W ), f is surjective if and only if rk(A) is maximal;
</p>
<p>(c) With dim(V ) = dim(W ), f is an isomorphism if and only if rk(A) is maximal.
</p>
<p>Exercise 7.7.7 The following linear maps are represented with respect to canonical
</p>
<p>bases.
</p>
<p>(1) Let the map f : R3 &rarr; R4 be defined by
</p>
<p>(x, y, z) �&rarr; (x &minus; y + 2z, y + z,&minus;x + z, 2x + y).
</p>
<p>To compute the rank of the corresponding matrix A with respect to the canonical
</p>
<p>basis, as usual we reduce it by rows. We have
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
⎜
⎝
</p>
<p>1 &minus;1 2
</p>
<p>0 1 1
</p>
<p>&minus;1 0 1
</p>
<p>2 1 0
</p>
<p>⎞
</p>
<p>⎟
⎟
⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
⎜
⎝
</p>
<p>1 &minus;1 2
</p>
<p>0 1 1
</p>
<p>0 0 1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎟
⎟
⎠
,
</p>
<p>and the rank of A is maximal, rk(A) = 3. Since dim(V ) &lt; dim(W ) we have that f
</p>
<p>is injective.
</p>
<p>(2) Let the map f : R4 &rarr; R3 be defined by
</p>
<p>(x, y, z, t) �&rarr; (x &minus; y + 2z + t, y + z + 3t, x &minus; y + 2z + 2t).
</p>
<p>We proceed as above and compute, via the following reduction,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 1 1 3
</p>
<p>1 &minus;1 2 2
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 2 1
</p>
<p>0 1 1 3
</p>
<p>0 0 0 1
</p>
<p>⎞
</p>
<p>⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>116 7 Linear Transformations
</p>
<p>that rk(A) = 3. Since rk(A) is maximal, with dim(V ) &gt; dim(W ), f turns out to be
</p>
<p>surjective.
</p>
<p>(3) Let f : R3 &rarr; R3 be represented as before by the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 1
</p>
<p>2 1 1
</p>
<p>1 &minus;1 2
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>which, by reduction, becomes
</p>
<p>A �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2 1
</p>
<p>0 &minus;3 &minus;1
</p>
<p>0 0 2
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>whose rank is clearly maximal. Thus f is an isomorphism since dim(V ) = dim(W ).
</p>
<p>7.8 Composition of Linear Maps
</p>
<p>We rephrase the general Definition 7.2.3 of composing maps.
</p>
<p>Definition 7.8.1 Let f : V &rarr; W and g : W &rarr; Z be two linear maps between
</p>
<p>real vector spaces. The composition between g and f is the map
</p>
<p>g ◦ f : X &rarr; Z
</p>
<p>defined as (g ◦ f )(v) = g( f (v)), for any v &isin; X .
</p>
<p>Proposition 7.8.2 If f : V &rarr; W and g : W &rarr; Z are two linear maps, the com-
</p>
<p>position map g ◦ f : V &rarr; Z is linear as well.
</p>
<p>Proof For any v, v&prime; &isin; V and λ,λ&prime; &isin; R, the linearity of both f and g allows one to
</p>
<p>write:
</p>
<p>(g ◦ f )(λv + λ&prime;v&prime;) = g( f (λv + λ&prime;v&prime;))
</p>
<p>= g(λ f (v)+ λ&prime; f (v&prime;))
</p>
<p>= λg( f (v))+ λ&prime;g( f (v&prime;))
</p>
<p>= λ(g ◦ f )(v)+ λ&prime;(g ◦ f )(v&prime;),
</p>
<p>showing the linearity of the composition map. ⊓⊔
</p>
<p>The following proposition, whose proof we omit, characterises the matrix corre-
</p>
<p>sponding to the composition of two linear maps.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Composition of Linear Maps 117
</p>
<p>Proposition 7.8.3 Let V, W, Z be real vector spaces with basis B, C,D respectively.
</p>
<p>Given linear maps f : V &rarr; W and g : W &rarr; Z, the corresponding matrices with
</p>
<p>respect to the given bases are related by
</p>
<p>M
D,B
g◦ f = M
</p>
<p>D,C
g &middot; M
</p>
<p>C,B
f .
</p>
<p>The following theorem characterises an isomorphism in terms of its corresponding
</p>
<p>matrix.
</p>
<p>Theorem 7.8.4 Let f : V &rarr; W be a linear map. The map f is an isomorphism
</p>
<p>if and only if, for any choice of the bases B for V and C for W , the corresponding
</p>
<p>matrix M
C,B
f with respect to the given bases is invertible, with
</p>
<p>M
B,C
</p>
<p>f &minus;1
=
</p>
<p>(
</p>
<p>M
C,B
f
</p>
<p>)&minus;1
</p>
<p>.
</p>
<p>Proof Let us assume that f is an isomorphism: we can then write dim(V ) = dim(W ),
</p>
<p>so M
C,B
f is a square matrix whose size is n &times; n (say). From the Proposition 7.4.2 we
</p>
<p>know that f &minus;1 exists as a linear map whose corresponding matrix, with the given
</p>
<p>bases, will be M
B,C
</p>
<p>f &minus;1
. From the Proposition 7.8.3 we can write
</p>
<p>M
B,C
</p>
<p>f &minus;1
&middot; M
</p>
<p>C,B
f = M
</p>
<p>B,B
</p>
<p>f &minus;1◦ f
= M
</p>
<p>B,B
idV
</p>
<p>= In &rArr; M
B,C
</p>
<p>f &minus;1
=
</p>
<p>(
</p>
<p>M
C,B
f
</p>
<p>)&minus;1
</p>
<p>.
</p>
<p>We set now A = M
C,B
f . By hypothesis A is a square invertible matrix, with
</p>
<p>inverse A&minus;1, so we can consider the linear map
</p>
<p>g = f
B,C
</p>
<p>A&minus;1
: W &rarr; V .
</p>
<p>In order to show that g is the inverse of f , consider the matrix corresponding to
</p>
<p>g ◦ f with respect to the basis B. From the Proposition 7.8.3,
</p>
<p>M
B,B
g◦ f = M
</p>
<p>B,C
g &middot; M
</p>
<p>C,B
f = A
</p>
<p>&minus;1 &middot; A = In.
</p>
<p>Since linear maps are in bijection with matrices, we have that g ◦ f = idV .
</p>
<p>Along the same lines we can show that f ◦ g = idW , thus proving g = f
&minus;1. ⊓⊔
</p>
<p>Exercise 7.8.5 Consider the linear map f : R3 &rarr; R3 defined by
</p>
<p>f ((x, y, z)) = (x &minus; y + z, 2y + z, z).
</p>
<p>With the canonical basis E for R3 the corresponding matrix is
</p>
<p>A = M
E,E
f =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;1 1
</p>
<p>0 2 1
</p>
<p>0 0 1
</p>
<p>⎞
</p>
<p>⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>118 7 Linear Transformations
</p>
<p>Since rk(A) = 3, f is an isomorphism, with f &minus;1 the linear map corresponding to
</p>
<p>A&minus;1. From the Proposition 5.3.3, we have
</p>
<p>A&minus;1 =
1
</p>
<p>det(A)
</p>
<p>(
</p>
<p>α j i
</p>
<p>)
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1/2 &minus;3/2
</p>
<p>0 1/2 &minus;1/2
</p>
<p>0 0 1
</p>
<p>⎞
</p>
<p>⎠ = M
E,E
</p>
<p>f &minus;1
.
</p>
<p>7.9 Change of Basis in a Vector Space
</p>
<p>In this section we study how to relate the components of the same vector in a vector
</p>
<p>space with respect to different bases. This problem has a natural counterpart in
</p>
<p>physics, where different bases for the same vector space represent different reference
</p>
<p>systems. Thus different observers measuring observables of the same physical system
</p>
<p>in a compatible way.
</p>
<p>Example 7.9.1 We start by considering the vector space R2 with two bases given by
</p>
<p>E =
(
</p>
<p>e1 = (1, 0), e2 = (0, 1)
)
</p>
<p>, B =
(
</p>
<p>b1 = (1, 2), b2 = (3, 4)
)
</p>
<p>.
</p>
<p>Any vector v &isin; R2 will then be written as
</p>
<p>v = (x1, x2)B = (y1, y2)E ,
</p>
<p>or, more explicitly,
</p>
<p>v = x1b1 + x2b2 = y1e1 + y2e2.
</p>
<p>By writing the components of the elements in B in the basis E , that is
</p>
<p>b1 = e1 + 2e2, b2 = 3e1 + 4e2,
</p>
<p>we have
</p>
<p>y1e1 + y2e2 = x1(e1 + 2e2)+ x2(3e1 + 4e2)
</p>
<p>= (x1 + 3x2)e1 + (2x1 + 4x2)e2.
</p>
<p>We have then obtained
</p>
<p>y1 = x1 + 3x2, y2 = 2x1 + 4x2.
</p>
<p>These expression can be written in matrix form
</p>
<p>(
</p>
<p>y1
y2
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>x1 + 3x2
2x1 + 4x2
</p>
<p>)
</p>
<p>&hArr;
</p>
<p>(
</p>
<p>y1
y2
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>1 3
</p>
<p>2 4
</p>
<p>) (
</p>
<p>x1
x2
</p>
<p>)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Change of Basis in a Vector Space 119
</p>
<p>Such a relation can be written as
</p>
<p>(
</p>
<p>y1
y2
</p>
<p>)
</p>
<p>= A
</p>
<p>(
</p>
<p>x1
x2
</p>
<p>)
</p>
<p>,
</p>
<p>where
</p>
<p>A =
</p>
<p>(
</p>
<p>1 3
</p>
<p>2 4
</p>
<p>)
</p>
<p>.
</p>
<p>Notice that the columns of A above are given by the components of the vectors
</p>
<p>in B with respect to the basis E . We have the following general result.
</p>
<p>Proposition 7.9.2 Let V be a real vector space with dim(V ) = n. Let B and C be
</p>
<p>two bases for V and denote by (x1, . . . , xn)B and (y1, . . . , yn)C the component of
</p>
<p>the same vector v with respect to them. It is
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>y1
...
</p>
<p>yn
</p>
<p>⎞
</p>
<p>⎟
⎠ = M
</p>
<p>C,B
idV
</p>
<p>⎛
</p>
<p>⎜
⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
⎠ .
</p>
<p>Such an expression will also be written as
</p>
<p>t (y1, . . . , yn) = M
C,B
idV
</p>
<p>&middot; t (x1, . . . , xn).
</p>
<p>Proof This is clear, by recalling the Definition 7.1.12 and the Proposition 7.1.13. ⊓⊔
</p>
<p>Definition 7.9.3 The matrix MC,B = M
C,B
idV
</p>
<p>is called the matrix of the change of
</p>
<p>basis from B to C. The columns of this matrix are given by the components with
</p>
<p>respect to C of the vectors in B.
</p>
<p>Exercise 7.9.4 Let B = (v1, v2, v3) and C = (w1, w2, w3) two different bases for
</p>
<p>R
3, with
</p>
<p>v1 = (0, 1,&minus;1), v2 = (1, 0,&minus;1), v3 = (2,&minus;2, 2),
</p>
<p>w1 = (0, 1, 1), w2 = (1, 0, 1), w3 = (1, 1, 0).
</p>
<p>We consider the vector v = (1,&minus;1, 1)B and we wish to determine its components
</p>
<p>with respect to C. The solution to the linear system
</p>
<p>v1 = a11w1 + a21w2 + a31w3
</p>
<p>v2 = a12w1 + a22w2 + a32w3
</p>
<p>v3 = a13w1 + a23w2 + a33w3
</p>
<p>give the entries for the matrix of the change of basis, which is found to be</p>
<p/>
</div>
<div class="page"><p/>
<p>120 7 Linear Transformations
</p>
<p>MC,B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 &minus;1 &minus;1
</p>
<p>&minus;1 0 3
</p>
<p>1 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We can then write
</p>
<p>v =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>y1
y2
y3
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎝MC,B
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>x1
x2
x3
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 &minus;1 &minus;1
</p>
<p>&minus;1 0 3
</p>
<p>1 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1
</p>
<p>&minus;1
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0
</p>
<p>2
</p>
<p>&minus;1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>C
</p>
<p>.
</p>
<p>Theorem 7.9.5 Let B and C be two bases for the vector space V over R. The matrix
</p>
<p>MC,B is invertible, with
(
</p>
<p>MC,B
)&minus;1
</p>
<p>= MB,C .
</p>
<p>Proof This easily follows by applying the Theorem 7.8.4 to MC,B = M
C,B
idV
</p>
<p>, since
</p>
<p>idV = id
&minus;1
V . ⊓⊔
</p>
<p>Theorem 7.9.6 Let A &isin; Rn,n be an invertible matrix. Denoting by v1, . . . , vn the
</p>
<p>column vectors in A and setting B = (v1, . . . , vn), it holds that:
</p>
<p>(i) B is a basis for Rn ,
</p>
<p>(ii) A = MB,E with E the canonical basis in Rn .
</p>
<p>Proof (i) From the Remark 7.7.6, we know that A has maximal rank, that is
</p>
<p>rk(A) = n. Being the column vectors in A, the system v1, . . . , vn is then free.
</p>
<p>A system of n linearly independent vectors in Rn is indeed a basis for Rn (see
</p>
<p>the Corollary 2.5.5 in Chap. 2).
</p>
<p>(ii) It directly follows from the Definition 7.9.3.
</p>
<p>⊓⊔
</p>
<p>Remark 7.9.7 From the Theorems 7.9.5 and 7.9.6 we have that the group GL(n,R)
</p>
<p>of invertible matrices of order n, is the same as (the group of) matrices providing
</p>
<p>change of basis in Rn .
</p>
<p>Exercise 7.9.8 The matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 &minus;1
</p>
<p>0 1 2
</p>
<p>0 0 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>is invertible since rk(A) = 3 (the matrix A is reduced by rows, so its rank is the
</p>
<p>number of non zero columns). The column vectors in A, that is
</p>
<p>v1 = (1, 0, 0), v2 = (1, 1, 0), v3 = (&minus;1, 2, 1),
</p>
<p>form a basis for R3. It is also clear that A = ME,B = M
E,B
id
</p>
<p>R3
, with B = (v1, v2, v3).</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Change of Basis in a Vector Space 121
</p>
<p>We next turn to study how M
C,B
f , the matrix associated to a linear map f : V &rarr; W
</p>
<p>with respect to the bases B for V and C in W , is transformed under a change of basis
</p>
<p>in V and W . In the following pages we shall denote by VB the vector space V referred
</p>
<p>to its basis B.
</p>
<p>Theorem 7.9.9 Let B and B&prime; be two bases for the real vector space V and C and
</p>
<p>C &prime; two bases for the real vector space W . With f : V &rarr; W a linear map, one has
</p>
<p>that
</p>
<p>M
C&prime;,B&prime;
</p>
<p>f = M
C&prime;,C &middot; M
</p>
<p>C,B
f &middot; M
</p>
<p>B,B&prime; .
</p>
<p>Proof The commutative diagram
</p>
<p>f
</p>
<p>VB &minus;&minus;&minus;&minus;&minus;&minus;&rarr; WC
</p>
<p>idV
</p>
<p>�
⏐
⏐
⏐
</p>
<p>⏐
⏐
⏐
� idW
</p>
<p>V &prime;
B
&minus;&minus;&minus;&minus;&minus;&minus;&rarr; W &prime;
</p>
<p>C
f
</p>
<p>shows the claim: going from V &prime;
B
</p>
<p>to W &prime;
C
</p>
<p>along the bottom line is equivalent to going
</p>
<p>around the diagram, that is
</p>
<p>f = idW ◦ f ◦ idV .
</p>
<p>Such a relation can be translated in a matrix form,
</p>
<p>M
C&prime;,B&prime;
</p>
<p>f = M
C&prime;,B&prime;
</p>
<p>idW ◦ f ◦idV
</p>
<p>and, by recalling the Proposition 7.8.3, we have
</p>
<p>M
C&prime;,B&prime;
</p>
<p>idW ◦ f ◦idV
= M
</p>
<p>C&prime;,C
idW
</p>
<p>&middot; M
C,B
f &middot; M
</p>
<p>B,B&prime;
</p>
<p>idV
,
</p>
<p>which proves the claim. ⊓⊔
</p>
<p>Exercise 7.9.10 Consider the linear map f : R2
B
</p>
<p>&rarr; R3
C
</p>
<p>whose corresponding
</p>
<p>matrix is
</p>
<p>A = M
C,B
f =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2
</p>
<p>&minus;1 0
</p>
<p>1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>with respect to B =
(
</p>
<p>(1, 1), (0, 1)
)
</p>
<p>and C =
(
</p>
<p>(1, 1, 0), (1, 0, 1), (0, 1, 1)
)
</p>
<p>. We deter-
</p>
<p>mine the matrix B = M
E3,E2
f , with E2 the canonical basis for R
</p>
<p>2 and E3 the canonical
</p>
<p>basis for R3. The commutative diagram above turns out to be</p>
<p/>
</div>
<div class="page"><p/>
<p>122 7 Linear Transformations
</p>
<p>f
C,B
A
</p>
<p>R
2
B &minus;&minus;&minus;&minus;&minus;&minus;&rarr; R
</p>
<p>3
C
</p>
<p>id
R2
</p>
<p>�
⏐
⏐
⏐
</p>
<p>⏐
⏐
⏐
� idR3
</p>
<p>R
2
E2 &minus;&minus;&minus;&minus;&minus;&minus;&rarr; R
</p>
<p>3
E3
</p>
<p>f
E3 ,E2
B
</p>
<p>,
</p>
<p>which reads
</p>
<p>B = M
E3,E2
f = M
</p>
<p>E3,C A MB,E2 .
</p>
<p>We have to compute the matrices ME3,C and MB,E2 . Clearly,
</p>
<p>ME3,C =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 0
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>and, from the Theorem 7.9.5, it is
</p>
<p>MB,E2 =
(
</p>
<p>ME2,B
)&minus;1
</p>
<p>=
</p>
<p>((
</p>
<p>1 0
</p>
<p>1 1
</p>
<p>))&minus;1
</p>
<p>=
</p>
<p>(
</p>
<p>1 0
</p>
<p>&minus;1 1
</p>
<p>)
</p>
<p>(the last equality follows from the Proposition 5.3.3). We have then
</p>
<p>B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 1 0
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 2
</p>
<p>&minus;1 0
</p>
<p>1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>(
</p>
<p>1 0
</p>
<p>&minus;1 1
</p>
<p>)
</p>
<p>=
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>&minus;2 2
</p>
<p>&minus;1 3
</p>
<p>&minus;1 1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We close this section by studying how to construct linear maps with specific
</p>
<p>properties.
</p>
<p>Exercise 7.9.11 We ask whether there is a linear map f : R3 &rarr; R3 which fulfils
</p>
<p>the conditions f (1, 0, 2) = 0 and Im( f ) = L((1, 1, 0), (2,&minus;1, 0)). Also, if such a
</p>
<p>map exists, is it unique?
</p>
<p>We start by setting v1 = (1, 0, 2), v2 = (1, 1, 0), v3 = (2,&minus;1, 0). Since a linear
</p>
<p>map is characterised by its action on the elements of a basis and v1 is required to
</p>
<p>be in the kernel of f , we complete v1 to a basis for R
3. By using the elements of
</p>
<p>the canonical basis E3, we may take the set (v1, e1, e2), which is indeed a basis: the
</p>
<p>matrix ⎛
</p>
<p>⎝
</p>
<p>1 0 2
</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>⎞
</p>
<p>⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Change of Basis in a Vector Space 123
</p>
<p>whose rows are given by (v1, e1, e2) has rank 3 (the matrix is already reduced by
</p>
<p>rows). So we can take the basis B = (v1, e1, e2) and define
</p>
<p>f (v1) = 0R3 ,
</p>
<p>f (e1) = v2,
</p>
<p>f (e2) = v3.
</p>
<p>Such a linear map satisfies the required conditions, since ker( f ) = {v1} and
</p>
<p>Im( f ) = L( f (v1), f (e1), f (e2)) = L(v2, v3).
</p>
<p>With respect to the bases E and B we have
</p>
<p>M
E,B
f =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 2
</p>
<p>0 1 &minus;1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>In order to understand whether the required conditions can be satisfied by a dif-
</p>
<p>ferent linear map f , we start by analysing whether the set (v1, v2, v3) itself provides
</p>
<p>a basis for R3. As usual, we reduce by rows the matrix associated to the vectors,
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 2
</p>
<p>1 1 0
</p>
<p>2 &minus;1 0
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 2
</p>
<p>1 1 0
</p>
<p>3 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Such a reduction gives rk(A) = 3, that is C = (v1, v2, v3) is a basis for R
3. Then,
</p>
<p>let g : R3 &rarr; R3 be defined by
</p>
<p>g(v1) = 0R3 ,
</p>
<p>g(v2) = v2,
</p>
<p>g(v3) = v3.
</p>
<p>Also the linear map g satisfies the conditions we set at the beginning and the
</p>
<p>matrix
</p>
<p>MC,Cg =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 0 0
</p>
<p>0 1 0
</p>
<p>0 0 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>represents its action by the basis C. It seems clear that f and g are different.
</p>
<p>In order to prove this claim, we shall see that their corresponding matrices with
</p>
<p>respect to the same pair of bases differ. We need then to find ME,Bg . We know that
</p>
<p>ME,Bg = M
E,C
g M
</p>
<p>C,B, with</p>
<p/>
</div>
<div class="page"><p/>
<p>124 7 Linear Transformations
</p>
<p>ME,Cg =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 2
</p>
<p>0 1 &minus;1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>since the column vectors in ME,Cg are given by g(v1), g(v2), g(v3). The columns of
</p>
<p>the matrix MC,B are indeed the components with respect to C of the vectors in B,
</p>
<p>that is
</p>
<p>v1 = v1,
</p>
<p>e1 =
1
</p>
<p>3
v2 +
</p>
<p>1
</p>
<p>3
v3,
</p>
<p>e2 =
2
</p>
<p>3
v2 &minus;
</p>
<p>1
</p>
<p>3
v3.
</p>
<p>Thus we have
</p>
<p>MC,B = 1
3
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>3 0 0
</p>
<p>0 1 2
</p>
<p>0 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>and in turn,
</p>
<p>ME,Bg = M
E,C
g M
</p>
<p>C,B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 2
</p>
<p>0 1 &minus;1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠ 1
3
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>3 0 0
</p>
<p>0 1 2
</p>
<p>0 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 1 0
</p>
<p>0 0 1
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>This shows that ME,Bg 	= M
E,B
f , so that g 	= f .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
</p>
<p>Dual Spaces
</p>
<p>8.1 The Dual of a Vector Space
</p>
<p>Let us consider two finite dimensional real vector spaces V and W , and denote by
</p>
<p>Lin(V &rarr; W ) the collection of all linear maps f : V &rarr; W . It is easy to show that
</p>
<p>Lin(V &rarr; W ) is itself a vector space over R. This is with respect to a sum ( f1 + f2)
</p>
<p>and a product by a scalar (λ f ), for any f1, f2, f &isin; Lin(V &rarr; W ) and λ &isin; R, defined
</p>
<p>pointwise, that is by
</p>
<p>( f1 + f2)(v) = f1(v) + f2(v)
</p>
<p>(λ f )(v) = λ f (v)
</p>
<p>for any v &isin; V . If B is a basis for V (of dimension n) and C a basis for W (of
</p>
<p>dimension m), the map Lin(V &rarr; W ) &rarr; Rm,n given by
</p>
<p>f �&rarr; M
C,B
</p>
<p>f
</p>
<p>is an isomorphism of real vector spaces and the following relations
</p>
<p>M
C,B
</p>
<p>f1+ f2
= M
</p>
<p>C,B
</p>
<p>f1
+ M
</p>
<p>C,B
</p>
<p>f2
</p>
<p>M
C,B
</p>
<p>λ f = λM
C,B
</p>
<p>f (8.1)
</p>
<p>hold (see the Proposition 4.1.4). It is then clear that dim(Lin(V &rarr; W )) = mn.
</p>
<p>In particular, the vector space of linear maps from a vector space V to R, that is
</p>
<p>the set of linear forms on V , deserves a name of its own.
</p>
<p>Definition 8.1.1 Given a finite dimensional vector space V , the space of linear maps
</p>
<p>Lin(V &rarr; R) is called the dual space to V and is denoted by V &lowast; = Lin(V &rarr; R).
</p>
<p>The next result follows from the general discussion above.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_8
</p>
<p>125</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_8&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_8&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>126 8 Dual Spaces
</p>
<p>Proposition 8.1.2 Given a finite dimensional real vector space V , its dual space V &lowast;
</p>
<p>is a real vector space with dim(V &lowast;) = dim(V ).
</p>
<p>Let B = (b1, . . . , bn) be a basis for V . We define elements {ϕi }i=1,...,n in V
&lowast; by
</p>
<p>ϕi (b j ) = δi j with
</p>
<p>{
</p>
<p>1 if i = j
</p>
<p>0 if i �= j .
(8.2)
</p>
<p>With V &ni; v = x1b1 + . . . + xnbn , we have for the components that xi = ϕi (v). If
</p>
<p>f &isin; V &lowast; we write
</p>
<p>f (v) = f (b1) x1 + . . . + f (bn) xn
</p>
<p>= f (b1)ϕ1(v) + . . . + f (bn)ϕn(v)
</p>
<p>=
(
</p>
<p>f (b1)ϕ1 + . . . + f (bn)ϕn
)
</p>
<p>(v).
</p>
<p>This shows that the action of f upon the vector v is the same as the action on v of the
</p>
<p>linear map f = f (b1)ϕ1 + . . . + f (bn)ϕn , that is we have that V
&lowast; = L(ϕ1, . . . ,ϕn).
</p>
<p>It is indeed immediate to prove that, with respect to the linear structure in V &lowast;, the
</p>
<p>linear maps ϕi are linearly independent, so they provide a basis for V
&lowast;. We have then
</p>
<p>sketched the proof of the following proposition.
</p>
<p>Proposition 8.1.3 Given a basis B for a n-dimensional real vector space V , the
</p>
<p>elements ϕi defined in (8.2) provide a basis for V
&lowast;. Such a basis, denoted B&lowast;, is
</p>
<p>called the dual basis to B.
</p>
<p>We can also write
</p>
<p>f (v) = ( f (b1) . . . f (bn))
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
. (8.3)
</p>
<p>Referring to the Definition 7.1.12 (and implicitly fixing a basis for W &sim;= R), the
</p>
<p>relation (8.3) provides us the single row matrix MBf = ( f (b1) . . . f (bn)) associated
</p>
<p>to f with respect to the basis B for V . Its entries are the image under f of the basis
</p>
<p>elements in B. The proof of the proposition above shows that such entries are the
</p>
<p>components of f &isin; V &lowast; with respect to the dual basis B&lowast;.
</p>
<p>Let B&prime; be another basis for V , with elements {b&prime;i }i=1,...,n . With
</p>
<p>v = x1b1 + . . . + xnbn = x
&prime;
1b
</p>
<p>&prime;
1 + . . . + x
</p>
<p>&prime;
nb
</p>
<p>&prime;
n
</p>
<p>we can write, following the Definition 7.9.3,
</p>
<p>x &prime;k =
</p>
<p>n
&sum;
</p>
<p>s=1
</p>
<p>(MB
&prime;,B)ks xs, bi =
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>(MB
&prime;,B) j i b
</p>
<p>&prime;
j</p>
<p/>
</div>
<div class="page"><p/>
<p>8.1 The Dual of a Vector Space 127
</p>
<p>or, in a matrix notation,
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x &prime;1
...
</p>
<p>x &prime;n
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
= MB
</p>
<p>&prime;,B
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
, (b1 . . . bn) M
</p>
<p>B,B&prime; = (b&prime;1 . . . b
&prime;
n). (8.4)
</p>
<p>From the Theorem 7.9.9 we have the matrix associated to f with respect to B&prime;
</p>
<p>MB
&prime;
</p>
<p>f = M
B
</p>
<p>f M
B,B&prime;,
</p>
<p>which we write as
</p>
<p>( f (b1) . . . f (bn)) M
B,B&prime; = ( f (b&prime;1) . . . f (b
</p>
<p>&prime;
n)). (8.5)
</p>
<p>Since the entries of MB
&prime;
</p>
<p>f provide the components of the element f &isin; V
&lowast; with respect
</p>
<p>to the basis B
&prime;&lowast;, a comparison between (8.4) and (8.5) shows that, under a change
</p>
<p>of basis B �&rarr; B&prime; for V and the corresponding change of the dual basis in V &lowast;, the
</p>
<p>components of a vector in V &lowast; are transformed under a map which is the inverse of
</p>
<p>the map that transforms the components of a vector in V .
</p>
<p>The above is usually referred to by saying that the transformation law for vectors
</p>
<p>in V &lowast; is contravariant with respect to the covariant one for vectors in V . In Sect. 13.3
</p>
<p>we shall describe these facts with an important physical example, the study of the
</p>
<p>electromagnetic field.
</p>
<p>If we express f &isin; V &lowast; with respect to the dual bases B&lowast; and B&prime;&lowast; as
</p>
<p>f (v) =
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>f (bi )ϕn =
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>f (v&prime;k)ϕ
&prime;
k
</p>
<p>and consider the rule for the change of basis, we have
</p>
<p>n
&sum;
</p>
<p>k,i=1
</p>
<p>(MB
&prime;,B)ki f (b
</p>
<p>&prime;
k)ϕi =
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>f (v&prime;k)ϕ
&prime;
k .
</p>
<p>Since this must be valid for any f &isin; V &lowast;, we can write the transformation law
</p>
<p>B&lowast; �&rarr; B
&prime;&lowast;:
</p>
<p>ϕ&prime;k =
</p>
<p>n
&sum;
</p>
<p>i=1
</p>
<p>(MB
&prime;,B)kiϕi that is
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>ϕ&prime;1
...
</p>
<p>ϕ&prime;n
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
= MB
</p>
<p>&prime;,B
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>ϕ1
...
</p>
<p>ϕn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎠
.
</p>
<p>It is straightforward to extend to the complex case, mutatis mutandis, all the
</p>
<p>results of the present chapter given above. In particular, one has the following natural
</p>
<p>definition.</p>
<p/>
</div>
<div class="page"><p/>
<p>128 8 Dual Spaces
</p>
<p>Definition 8.1.4 Let V be a finite dimensional complex vector space. The set
</p>
<p>V &lowast; = Lin(V &rarr; C) is called the dual space to V .
</p>
<p>Indeed, the space V &lowast; is a complex vector space, with dim(V &lowast;) = dim(V ), and a
</p>
<p>natural extension of (8.2) to the complex case allows one to introduce a dual basis
</p>
<p>B&lowast; for any basis B of V .
</p>
<p>Also, we could consider linear maps between finite dimensional complex vector
</p>
<p>spaces. In the next section we shall explicitly consider linear transformations of the
</p>
<p>complex vector space Cn .
</p>
<p>8.2 The Dirac&rsquo;s Bra-Ket Formalism
</p>
<p>Referring to Sect. 3.4 let us denote by H n = (Cn, &middot;) the canonical hermitian vector
</p>
<p>space. Following Dirac (and by now a standard practice in textbooks on quantum
</p>
<p>mechanics), the hermitian product is denoted as
</p>
<p>〈 | 〉 : Cn &times; Cn &rarr; C&prime;, 〈z|w〉 = z̄1w1 + &middot; &middot; &middot; + z̄nwn,
</p>
<p>for any z = (z1, . . . , zn),w = (w1, . . . , wn) &isin; C
n . Thus its properties (see the Propo-
</p>
<p>sition 3.4.2) are written as follows. For any z, w, v &isin; Cn and a, b &isin; C,
</p>
<p>(i) 〈w|z〉 = 〈z|w〉,
</p>
<p>(ii) 〈az + bw|v〉 = ā〈z|v〉 + b̄〈w|v〉 while 〈v|az + bw〉 = a〈v|z〉 + b〈v|w〉,
</p>
<p>(iii) 〈z|z〉 &ge; 0,
</p>
<p>(iv) 〈z|z〉 = 0 &hArr; z = (0, . . . , 0) &isin; Cn .
</p>
<p>Since the hermitian product is bilinear (for the sum), for any fixed w &isin; H n , the
</p>
<p>mapping
</p>
<p>fw : v �&rarr; 〈w|z〉
</p>
<p>provides indeed a linear map from Cn to C, that is fw is an element of the dual space
</p>
<p>(Cn)&lowast;. Given a hermitian basis B = {e1, . . . , en} for H
n , with w = (w1, . . . , wn)B
</p>
<p>and z = (z1, . . . , zn)B, one has
</p>
<p>fw(z) = w̄1z1 + . . . + w̄nzn.
</p>
<p>The corresponding dual basis B&lowast; = {ε1, . . . , εn} for (C
n)&lowast; is defined in analogy to
</p>
<p>(8.2) for the real case by taking εi (e j ) = δi j . In terms of the hermitian product, these
</p>
<p>linear maps can be defined as εi (z) = 〈ei |z〉. Then, to any w = (w1, . . . , wn)B we
</p>
<p>can associate an element fw = w̄1ε1 + . . . + w̄nεn in (C
n)&lowast;, whose action on Cn can
</p>
<p>be written as
</p>
<p>fw(v) = 〈w|v〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 The Dirac&rsquo;s Bra-Ket Formalism 129
</p>
<p>Thus, via the hermitian product, to any vector w &isin; Cn one associates a unique dual
</p>
<p>element fw &isin; (C
n)&lowast;; viceversa, to any element f &isin; (Cn)&lowast; one associates a unique
</p>
<p>element w &isin; Cn in such a way that f = fw:
</p>
<p>w = w1e1 + . . . + wnen &harr; fw = w̄1ε1 + . . . + w̄nεn.
</p>
<p>Remark 8.2.1 Notice that this bijection between Cn and (Cn)&lowast; is anti-linear (for the
</p>
<p>product by complex numbers), since we have to complex conjugate the components
</p>
<p>of the vectors in order to satisfy the defining requirement of the hermitian product
</p>
<p>in H n , that is
</p>
<p>fλw = λ̄ fw, for λ &isin; C, w &isin; C
n.
</p>
<p>For the canonical euclidean space En one could proceed in a similar manner and
</p>
<p>in such a case the bijection between En and its dual (En)&lowast; given by the euclidean
</p>
<p>product is linear.
</p>
<p>Given the bijection above, Dirac&rsquo;s idea was to split the hermitian product bracket.
</p>
<p>Any element w &isin; H n provides a ket element |w〉 and a bra element 〈w| &isin; (Cn)&lowast;.
</p>
<p>A basis for H n is then written as made of elements |e j 〉 while the bra elements 〈e j |
</p>
<p>form the dual basis for (Cn)&lowast;, with
</p>
<p>w = w1e1 + . . . + wnen &harr; |w〉 = w1|e1〉 + . . . + wn|en〉,
</p>
<p>fw = w̄1ε1 + . . . + w̄nεn &harr; 〈w| = w̄1〈e1| + . . . + w̄n〈en| .
</p>
<p>The action of a bra element on a ket element is just given as a bra-ket juxtapposition,
</p>
<p>with
</p>
<p>fw(z) = 〈w|z〉 &isin; C.
</p>
<p>We are now indeed allowed to define a ket-bra juxtaposition, that is we have elements
</p>
<p>T = |z〉〈w|. The action of such a T from the left upon a |u〉, is then defined as
</p>
<p>T : |u〉 �&rarr; |z〉〈w|u〉.
</p>
<p>Since 〈w|u〉 is a complex number, we see that for this action the element T maps a
</p>
<p>ket vector linearly into a ket vector, so T is a linear map from H n to H n .
</p>
<p>Definition 8.2.2 With z, w &isin; H n , the ket-bra element T = |z〉〈w| is the linear oper-
</p>
<p>ator whose action is defined as v �&rarr; T (v) = 〈w|v〉z = (w &middot; v)z.
</p>
<p>It is then natural to consider linear combination of the form T =
&sum;n
</p>
<p>k,s=1 Tks |ek〉〈es |
</p>
<p>with Tks &isin; C the entries of a matrix T &isin; C
n,n so to compute
</p>
<p>T |e j 〉 =
</p>
<p>n
&sum;
</p>
<p>k,s=1
</p>
<p>Tks |ek〉〈es |e j 〉 =
</p>
<p>n
&sum;
</p>
<p>k=1
</p>
<p>Tk j |ek〉
</p>
<p>Tk j = 〈ek |T (e j )〉. (8.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>130 8 Dual Spaces
</p>
<p>In order to relate this formalism to the one we have already developed in this
</p>
<p>chapter, consider a linear map φ : H n &rarr; H n and its associated matrix M
B,B
</p>
<p>φ = (aks)
</p>
<p>with respect to a given hermitian basis B = (e1, . . . , en). From the Propositions
</p>
<p>7.1.13 and 7.1.14 it is easy to show that one has
</p>
<p>ak j = ek &middot; (A(e j )) = 〈ek |A(e j )〉. (8.7)
</p>
<p>The analogy between (8.6) and (8.7) shows that, for a fixed basis of H n , the action
</p>
<p>of a linear map φ with associated matrix A = M
B,B
</p>
<p>φ = (aks) is equivalently written
</p>
<p>as the action of the operator
</p>
<p>TA(= Tφ) =
</p>
<p>n
&sum;
</p>
<p>k,s=1
</p>
<p>aks |ek〉〈es |
</p>
<p>in the Dirac&rsquo;s notation. The association A &rarr; TA is indeed an isomorphism of (com-
</p>
<p>plex) vector space of dimension n2.
</p>
<p>Next, let φ,ψ be two linear maps on H n with associated matrices A, B with
</p>
<p>respect to the hermitian basis B. They correspond to the operators that we write as
</p>
<p>TA =
&sum;n
</p>
<p>r,s=1 ars |er 〉〈es | and TB =
&sum;n
</p>
<p>j,k=1 b jk |e j 〉〈ek |. With a natural juxtaposition
</p>
<p>we write the composition of the linear maps as
</p>
<p>φ ◦ ψ =
</p>
<p>n
&sum;
</p>
<p>r,s=1
</p>
<p>n
&sum;
</p>
<p>j,k=1
</p>
<p>arsb jk |er 〉〈es |e j 〉〈ek |
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>r,k=1
</p>
<p>(
</p>
<p>n
&sum;
</p>
<p>j=1
</p>
<p>ar j b jk)|er 〉〈ek | .
</p>
<p>We see that the matrix associated, via the isomorphism A &rarr; TA above, to the com-
</p>
<p>position φ ◦ ψ has entries (r, k) given by
&sum;n
</p>
<p>j=1 ar j b jk , thus coinciding with the row
</p>
<p>by column product between the matrices A and B associated to φ and ψ, that is
</p>
<p>TAB = TATB .
</p>
<p>Thus, the Proposition 7.8.3 for composition of matrices associated to linear maps is
</p>
<p>valid when we represent linear maps on H n using the Dirac&rsquo;s notation.
</p>
<p>All of this section has clearly a real version and could be repeated for the (real)
</p>
<p>euclidean space En with its linear maps and associated real matrices T &isin; Rn,n .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
</p>
<p>Endomorphisms and Diagonalization
</p>
<p>Both in classical and quantum physics, and in several branches of mathematics, it
</p>
<p>is hard to overestimate the role that the notion of diagonal action of a linear map
</p>
<p>has. The aim of this chapter is to introduce this topic which will be crucial in all the
</p>
<p>following chapters.
</p>
<p>9.1 Endomorphisms
</p>
<p>Definition 9.1.1 Let V be a real vector space. A linear map φ : V &rarr; V is called
</p>
<p>an endomorphism of V . The set of all endomorphisms of V is denoted End(V ). Non
</p>
<p>invertible endomorphisms are also called singular or degenerate.
</p>
<p>As seen in Sect. 8.1, the set End(V ) is a real vector space with dim(End(V )) = n2
</p>
<p>if dim(V ) = n.
</p>
<p>The question we address now is whether there exists a class of bases of the vector
</p>
<p>space V , with respect to which a matrix M
B,B
φ has a particular (diagonal, say) form.
</p>
<p>We start with a definition.
</p>
<p>Definition 9.1.2 The matrices A, B &isin; Rn,n are called similar if there exists a real
</p>
<p>vector space V and an endomorphism φ &isin; End(V ) such that A = M
B,B
φ and
</p>
<p>B = M
C,C
φ , where B and C are bases for V . We denote similar matrices by A &sim; B.
</p>
<p>Similarity between matrices can be described in a purely algebraic way.
</p>
<p>Proposition 9.1.3 The matrices A, B &isin; Rn,n are similar if and only if there exists
</p>
<p>an invertible matrix P &isin; GL(n), such that P&minus;1 AP = B.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_9
</p>
<p>131</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_9&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_9&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>132 9 Endomorphisms and Diagonalization
</p>
<p>Proof Let us assume A &sim; B: we then have a real vector space V , bases B and C for
</p>
<p>it and an endomorphism φ &isin; End(V ) such that A = M
B,B
φ e B = M
</p>
<p>C,C
φ . From the
</p>
<p>Theorem 7.9.9 we have
</p>
<p>B = MC,B A MB,C .
</p>
<p>Since the matrix MC,B is invertible, with (MC,B)&minus;1 = MB,C , the claim follows with
</p>
<p>P = MB,C .
</p>
<p>Next, let us assume there exists a matrix P &isin; GL(n) such that P&minus;1 AP = B.
</p>
<p>From the Theorem 7.9.6 and the Remark 7.9.7 we know that the invertible matrix
</p>
<p>P gives a change of basis in Rn: there exists a basis C for Rn (the columns of P),
</p>
<p>with P = ME,C and P&minus;1 = MC,E . Let φ = f
E,E
A be the endomorphism in R
</p>
<p>n
</p>
<p>corresponding to the matrix A with respect to the canonical bases, A = M
E,E
φ . We
</p>
<p>then have
</p>
<p>B = P&minus;1 A P
</p>
<p>= MC,E M
E,E
φ M
</p>
<p>E,C
</p>
<p>= M
C,C
φ .
</p>
<p>This shows that B corresponds to the endomorphism φ with respect to the different
</p>
<p>basis C, that is A and B are similar. ⊓⊔
</p>
<p>Remark 9.1.4 The similarity we have introduced is an equivalence relation in Rn,n ,
</p>
<p>since it is
</p>
<p>(a) reflexive, that is A &sim; A since A = In AIn ,
</p>
<p>(b) symmetric, that is A &sim; B &rArr; B &sim; A since
</p>
<p>P&minus;1 A P = B &rArr; P B P&minus;1 = A,
</p>
<p>(c) transitive, that is A &sim; B and B &sim; C imply A &sim; C , since P&minus;1 A P = B and
</p>
<p>Q&minus;1 B Q = C clearly imply Q&minus;1 P&minus;1 A P Q = (P Q)&minus;1 A(P Q) = C .
</p>
<p>If A &isin; Rn,n , we denote its equivalence class by similarity as [A] = {B &isin; Rn,n :
</p>
<p>B &sim; A}.
</p>
<p>Proposition 9.1.5 Let matrices A, B &isin; Rn,n be similar. Then
</p>
<p>det(B) = det(A) and tr(B) = tr(A).
</p>
<p>Proof From Proposition 9.1.3, we know there exists an invertible matrix P &isin; GL(n),
</p>
<p>such that P&minus;1 AP = B. From the Binet Theorem 5.1.16 and the Proposition 4.5.2 we
</p>
<p>can write
</p>
<p>det(B) = det(P&minus;1 AP)
</p>
<p>= det(P&minus;1) det(A) det(P) = det(P&minus;1) det(P) det(A)
</p>
<p>= det(A)
</p>
<p>and tr(B) = tr(P&minus;1 AP) = tr(P P&minus;1 A) = tr(A). ⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>9.1 Endomorphisms 133
</p>
<p>A natural question is whether, for a given A, the equivalence class [A] contains a
</p>
<p>diagonal element (equivalently, whether A is similar to a diagonal matrix).
</p>
<p>Definition 9.1.6 A matrix A &isin; Rn,n is called diagonalisable if it is similar to a
</p>
<p>diagonal (� say) matrix, that is if there is a diagonal matrix � in the equivalence
</p>
<p>class [A].
</p>
<p>Such a definition has a counterpart in terms of endomorphisms.
</p>
<p>Definition 9.1.7 An endomorphism φ &isin; End(V ) is called simple if there exists a
</p>
<p>basis B for V such that the matrix M
B,B
φ is diagonalisable.
</p>
<p>We expect that for an endomorphism to be simple is an intrinsic property which
</p>
<p>does not depend on the basis with respect to which its corresponding matrix is given.
</p>
<p>The following proposition confirms this point.
</p>
<p>Proposition 9.1.8 Let V be a real vector space, with φ &isin; End(V ). The following
</p>
<p>are equivalent:
</p>
<p>(i) φ is simple, there is a basis B for V such that M
B,B
φ is diagonalisable,
</p>
<p>(ii) there exists a basis C for V such that M
C,C
φ is diagonal,
</p>
<p>(iii) given any basis D for V , the matrix M
D,D
φ is diagonalisable.
</p>
<p>Proof (i) &rArr; (ii): Since M
B,B
φ is similar to a diagonal matrix �, from the proof
</p>
<p>of the Proposition 9.1.3 we know that there is a basis C with respect to which
</p>
<p>� = M
C,C
φ is diagonal.
</p>
<p>(ii) &rArr; (iii): Let C be a basis of V such that M
C,C
φ = � is diagonal. For any basis D
</p>
<p>we have then M
D,D
φ &sim; �, thus M
</p>
<p>D,D
φ is diagonalisable.
</p>
<p>(iii) &rArr; (i): obvious.
</p>
<p>⊓⊔
</p>
<p>9.2 Eigenvalues and Eigenvectors
</p>
<p>Remark 9.2.1 Let φ : V &rarr; V be a simple endomorphism, with � = M
C,C
φ a diag-
</p>
<p>onal matrix associated to φ. It is then
</p>
<p>� =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ1 0 0 &middot; &middot; &middot; 0
</p>
<p>0 λ2 0 &middot; &middot; &middot; 0
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 0 &middot; &middot; &middot; λn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>for scalars λ j &isin; R, with j = 1, . . . , n. By setting C = (v1, . . . , vn), we write then
</p>
<p>φ(v j ) = λ jv j .</p>
<p/>
</div>
<div class="page"><p/>
<p>134 9 Endomorphisms and Diagonalization
</p>
<p>The vectors of the basis C and the scalars λ j plays a prominent role in the analysis
</p>
<p>of endomorphisms. This motivates the following definition.
</p>
<p>Definition 9.2.2 Let φ &isin; End(V ) with V a real vector space. If there exists a non
</p>
<p>zero vector v &isin; V and a scalar λ &isin; R, such that
</p>
<p>φ(v) = λv,
</p>
<p>then λ is called an eigenvalue of φ and v is called an eigenvector of φ associated to
</p>
<p>λ. The spectrum of an endomorphism is the collection of its eigenvalues.
</p>
<p>Remark 9.2.3 Let φ &isin; End(V ) and C be a basis of V . With the definition above,
</p>
<p>the content of the Remark 9.2.1 can be rephrased as follow:
</p>
<p>(a) M
C,C
φ is diagonal if and only if C is a basis of eigenvectors for φ,
</p>
<p>(b) φ is simple if and only if V has a basis of eigenvectors for φ (from the Defini-
</p>
<p>tion 9.1.7).
</p>
<p>Notice that each eigenvector v for an endomorphism φ is uniquely associated to
</p>
<p>an eigenvalue λ of φ. On the other hand, more than one eigenvector can be associated
</p>
<p>to a given eigenvalue λ. It is indeed easy to see that, if v is associated to λ, also αv,
</p>
<p>withα &isin; R, is associated to the same λ since φ(αv) = αφ(v) = α(λv) = λ(αv).
</p>
<p>Proposition 9.2.4 If V is a real vector space , and φ &isin; End(V ), the set
</p>
<p>Vλ = {v &isin; V : φ(v) = λv}
</p>
<p>is a vector subspace in V .
</p>
<p>Proof We explicitly check that Vλ is closed under linear combinations. With v1, v2 &isin;
</p>
<p>Vλ and a1, a2 &isin; R, we can write
</p>
<p>φ(a1v1 + a2v2) = a1φ(v1)+ a2φ(v2) = a1λv1 + a2λv2 = λ(a1v1 + a2v2),
</p>
<p>showing that Vλ is a vector subspace of V ⊓⊔
</p>
<p>Definition 9.2.5 If λ &isin; R is an eigenvalue of φ &isin; End(V ), the space Vλ is called
</p>
<p>the eigenspace corresponding to λ.
</p>
<p>Remark 9.2.6 It is easy to see that ifλ &isin; R is not an eigenvalue for the endomorphism
</p>
<p>φ, then the set Vλ = {v &isin; V |φ(v) = λv} contains only the zero vector. It is indeed
</p>
<p>clear that, if Vλ contains the zero vector only, then λ is not an eigenvalue for φ. We
</p>
<p>have that λ &isin; R is an eigenvalue for φ if and only if dim(Vλ) &ge; 1.
</p>
<p>Exercise 9.2.7 Let φ &isin; End(R2) be defined by φ((x, y)) = (y, x). Is λ = 2 an
</p>
<p>eigenvalue for φ? The corresponding set V2 would then be
</p>
<p>V2 = {v &isin; R
2 : φ(v) = 2v} = {(x, y) &isin; R2 : (y, x) = 2(x, y)},</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Eigenvalues and Eigenvectors 135
</p>
<p>that is, V2 would be given by the solutions of the system
</p>
<p>{
</p>
<p>y = 2x
</p>
<p>x = 2y
&rArr;
</p>
<p>{
</p>
<p>y = 2x
</p>
<p>x = 4x
&rArr;
</p>
<p>{
</p>
<p>x = 0
</p>
<p>y = 0
.
</p>
<p>Since V2 = {(0, 0)}, we conclude that λ = 2 is not an eigenvalue for φ.
</p>
<p>Exercise 9.2.8 The endomorphism φ &isin; End(R2) given by φ((x, y)) = (2x, 3y) is
</p>
<p>simple since the corresponding matrix with respect to the canonical basis E = (e1, e2)
</p>
<p>is diagonal,
</p>
<p>M
E,E
φ =
</p>
<p>(
</p>
<p>2 0
</p>
<p>0 3
</p>
<p>)
</p>
<p>.
</p>
<p>Its eigenvalues are λ1 = 2 (with eigenvector e1) and λ2 = 3 (with eigenvector e2).
</p>
<p>The corresponding eigenspaces are then V2 = L(e1) and V3 = L(e2).
</p>
<p>Exercise 9.2.9 We consider again the endomorphismφ((x, y)) = (y, x) in R2 given
</p>
<p>in the Exercise 9.2.7. We wonder whether it is simple. We start by noticing that its
</p>
<p>corresponding matrix with respect to the canonical basis is the following,
</p>
<p>M
E,E
φ =
</p>
<p>(
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>)
</p>
<p>,
</p>
<p>which is not diagonal. We look then for a basis (if it exists) with respect to which
</p>
<p>the matrix corresponding to φ is diagonal. By recalling the Remark 9.2.3 we look
</p>
<p>for a basis of R2 made up of eigenvectors for φ. In order for v = (a, b) to be an
</p>
<p>eigenvector for φ, there must exist a real scalar λ such that φ((a, b)) = λ(a, b),
</p>
<p>{
</p>
<p>b = λa
</p>
<p>a = λb
.
</p>
<p>It follows that the eigenvalues, if they exist, must fulfill the condition λ2 = 1. For
</p>
<p>λ = 1 the corresponding eigenspace is
</p>
<p>V1 = {(x, y) &isin; R
2 : φ((x, y)) = (x, y)} = {(x, x) &isin; R2} = L((1, 1)).
</p>
<p>And for λ = &minus;1 the corresponding eigenspace is
</p>
<p>V&minus;1 = {(x, y) &isin; R
2 : φ((x, y)) = &minus;(x, y)} = {(x,&minus;x) &isin; R2} = L((1,&minus;1)).
</p>
<p>Since the vectors (1, 1), (1,&minus;1) form a basis B for R2 with respect to which the
</p>
<p>matrix of φ is
</p>
<p>M
B,B
φ =
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
</p>
<p>,</p>
<p/>
</div>
<div class="page"><p/>
<p>136 9 Endomorphisms and Diagonalization
</p>
<p>we conclude that φ is simple. We expect M
B,B
φ &sim; M
</p>
<p>E,E
φ , since they are associated
</p>
<p>to the same endomorphism; the algebraic proof of this claim is easy. By defining
</p>
<p>P = ME,B =
</p>
<p>(
</p>
<p>1 1
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>the matrix of the change of basis, we compute explicitly,
</p>
<p>(
</p>
<p>1 1
</p>
<p>1 &minus;1
</p>
<p>)&minus;1 (
</p>
<p>0 1
</p>
<p>1 0
</p>
<p>) (
</p>
<p>1 1
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>=
</p>
<p>(
</p>
<p>1 0
</p>
<p>0 &minus;1
</p>
<p>)
</p>
<p>,
</p>
<p>that is P&minus;1 M
E,E
φ P = M
</p>
<p>B,B
φ (see the Proposition 9.1.3).
</p>
<p>Not any endomorphism is simple as the following exercise shows.
</p>
<p>Exercise 9.2.10 The endomorphism in R2 defined as φ((x, y)) = (&minus;y, x) is not
</p>
<p>simple. For v = (a, b) to be an eigenvector, φ((a, b)) = λ(a, b) it would be equiv-
</p>
<p>alent to (&minus;b, a) = λ(a, b), leading to λ2 = &minus;1. The only solution in R is then
</p>
<p>a = b = 0, showing that φ is not simple.
</p>
<p>Proposition 9.2.11 Let V be a real vector space with φ &isin; End(V ). If λ1,λ2
are distinct eigenvalues, any two corresponding eigenvectors, 0 	= v1 &isin; Vλ1 and
</p>
<p>0 	= v2 &isin; Vλ2 , are linearly independent. Also, the sum Vλ1 + Vλ2 is direct.
</p>
<p>Proof Let us assume that v2 = αv1, with R &ni; α 	= 0. By applying the linear map
</p>
<p>φ to both members, we have φ(v2) = αφ(v1). Since v1 and v2 are eigenvectors with
</p>
<p>eigenvalues λ1 and λ2,
</p>
<p>φ(v1) = λ1 v1
</p>
<p>φ(v2) = λ2 v2
</p>
<p>and the relation φ(v2) = αφ(v1), using v2 = αv1 become
</p>
<p>λ2v2 = α(λ1v1) = λ1(αv1) = λ1v2,
</p>
<p>that is
</p>
<p>(λ2 &minus; λ1)v2 = 0V .
</p>
<p>Since λ2 	= λ1, this would lead to the contradiction v2 = 0V . We therefore conclude
</p>
<p>that v1 and v2 are linearly independent.
</p>
<p>For the last claim we use the Proposition 2.2.13 and show that Vλ1 &cap; Vλ2 = {0V }.
</p>
<p>If v &isin; Vλ1 &cap; Vλ2 , we could write both φ(v) = λ1v (since v &isin; Vλ1 ) and φ(v) = λ2v
</p>
<p>(since v &isin; Vλ2 ): it would then be λ1v = λ2v, that is (λ1 &minus; λ2)v = 0V . From the
</p>
<p>hypothesis λ1 	= λ2, we would get v = 0V . ⊓⊔
</p>
<p>The following proposition is proven along the same lines.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Eigenvalues and Eigenvectors 137
</p>
<p>Proposition 9.2.12 Let V be a real vector space, with φ &isin; End(V ). Let λ1, . . . ,
</p>
<p>λs &isin; R be distinct eigenvalues of φwith 0V 	= v j &isin; Vλ j , j = 1, . . . , s corresponding
</p>
<p>eigenvectors. The set {v1, . . . , vs} is free, and the sum Vλ1 + &middot; &middot; &middot; + Vλs is direct.
</p>
<p>Corollary 9.2.13 Ifφ is an endomorphism of the real vector space V , with dim(V ) =
</p>
<p>n, then φ has at most n distinct eigenvalues.
</p>
<p>Proof If φ had s &gt; n distinct eigenvalues, there would exist a set v1, . . . , vs of non
</p>
<p>zero corresponding eigenvectors. From the proposition above, such a system should
</p>
<p>be free, thus contradicting the fact that the dimension of V is n. ⊓⊔
</p>
<p>Remark 9.2.14 Let φ and ψ be two commuting endomorphisms, that is they are
</p>
<p>such that φ(ψ(w)) = ψ(φ(w)) for any v &isin; V . If v &isin; Vλ is an eigenvector for φ
</p>
<p>corresponding to λ, it follows that
</p>
<p>φ(ψ(v)) = ψ(φ(v)) = λψ(v).
</p>
<p>Thus the endomorphism ψ maps any eigenspace Vλ of φ into itself, and analogously
</p>
<p>φ preserves any eigenspace V &prime;λ&prime; of ψ.
</p>
<p>Finding the eigenspaces of an endomorphism amounts to compute suitable ker-
</p>
<p>nels. Let f : V &rarr; W be a linear map between real vector spaces with bases B and
</p>
<p>C. We recall (see Proposition 7.5.1) that if A = M
C,B
f and � : AX = 0 is the linear
</p>
<p>system associated to A, the map S� &rarr; ker( f ) given by
</p>
<p>(x1, . . . , xn) &#13;&rarr; (x1, . . . , xn)B
</p>
<p>is an isomorphism of vector spaces.
</p>
<p>Lemma 9.2.15 If V is a real vector space with basis B, let φ &isin; End(V ) and λ &isin; R.
</p>
<p>Then
</p>
<p>Vλ = ker(φ &minus; λ idV ) &sim;= S�λ ,
</p>
<p>where S�λ is the space of the solutions of the linear homogeneous system
</p>
<p>S�λ :
(
</p>
<p>M
B,B
φ &minus; λIn
</p>
<p>)
</p>
<p>X = 0.
</p>
<p>Proof From the Definition 9.2.4 we write
</p>
<p>Vλ = {v &isin; V : φ(v) = λv}
</p>
<p>= {v &isin; V : φ(v)&minus; λv = 0V }
</p>
<p>= ker(φ&minus; λ idV ).
</p>
<p>Such a kernel is isomorphic (as recalled above) to the space of solutions of the linear
</p>
<p>system given by the matrix M
B,B
φ&minus;λidV
</p>
<p>, where B is an arbitrary basis of V . We conclude
</p>
<p>by noticing that M
B,B
φ&minus;λidV
</p>
<p>= M
B,B
φ &minus; λIn . ⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>138 9 Endomorphisms and Diagonalization
</p>
<p>Proposition 9.2.16 Let φ &isin; End(V ) be an endomorphism of the real vector space
</p>
<p>V , with dim(V ) = n, and let λ &isin; R. The following are equivalent:
</p>
<p>(i) λ is an eigenvalue for φ,
</p>
<p>(ii) dim(Vλ) &ge; 1,
</p>
<p>(iii) det(M
B,B
φ &minus; λIn) = 0 for any basis B in V .
</p>
<p>Proof (i) &hArr; (ii) is the content of the Remark 9.2.6;
</p>
<p>(ii) &hArr; (iii). Let B be an arbitrary basis of V , and consider the linear system
</p>
<p>S�λ :
(
</p>
<p>M
B,B
φ &minus; λIn
</p>
<p>)
</p>
<p>X = 0. We have
</p>
<p>dim(Vλ) = dim(S�λ)
</p>
<p>= n &minus; rk
(
</p>
<p>M
B,B
φ &minus; λIn
</p>
<p>)
</p>
<p>;
</p>
<p>the first and the second equality follow from Definition 6.2.1 and Theorem 6.4.3
</p>
<p>respectively. From Proposition 5.3.1 we finally write
</p>
<p>dim(Vλ) &ge; 1 &hArr; rk
(
</p>
<p>M
B,B
φ &minus; λIn
</p>
<p>)
</p>
<p>&lt; n &hArr; det
(
</p>
<p>M
B,B
φ &minus; λIn
</p>
<p>)
</p>
<p>= 0,
</p>
<p>which concludes the proof. ⊓⊔
</p>
<p>This proposition shows that the computation of an eigenspace reduces to finding
</p>
<p>the kernel of a linear map, a computation which has been described in the Proposi-
</p>
<p>tion 7.5.1.
</p>
<p>9.3 The Characteristic Polynomial of an Endomorphism
</p>
<p>In this section we describe how to compute the eigenvalues of an endomorphism.
</p>
<p>These will be the roots of a canonical polynomial associate with the endomorphism.
</p>
<p>Definition 9.3.1 Given a square matrix A &isin; Rn,n , the expression
</p>
<p>pA(T ) = det(A &minus; T In)
</p>
<p>is a polynomial of order n in T with real coefficients. Such a polynomial is called
</p>
<p>the characteristic polynomial of the matrix A.
</p>
<p>Exercise 9.3.2 If A =
</p>
<p>(
</p>
<p>a11 a12
a21 a22
</p>
<p>)
</p>
<p>is a square 2 &times; 2 matrix, then
</p>
<p>pA(T ) =
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>a11 &minus; T a12
a21 a22 &minus; T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= T 2 &minus; (a11 + a22) T + (a11a22 &minus; a12a21)
</p>
<p>= T 2 &minus; (tr(A)) T + (det(A)).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 The Characteristic Polynomial of an Endomorphism 139
</p>
<p>If λ1 and λ2 are the zeros (the roots) of the polynomial pA(T ), with elementary
</p>
<p>algebra we write
</p>
<p>pA(T ) = T
2 &minus; (λ1 + λ2) T + λ1λ2
</p>
<p>thus obtaining
</p>
<p>λ1 + λ2 = a11 + a22 = tr(A), λ1λ2 = (a11a22 &minus; a12a21) = det(A).
</p>
<p>Proposition 9.3.3 Let V be a real vector space with dim(V ) = n, and let
</p>
<p>φ &isin; End(V ). For any choice of bases B and C in V , with corresponding matri-
</p>
<p>ces A = M
B,B
φ and B = M
</p>
<p>C,C
φ , it is
</p>
<p>pA(T ) = pB(T ).
</p>
<p>Proof We know that B = P&minus;1 AP , with P = MB,C the matrix of change of basis.
</p>
<p>So we write
</p>
<p>B &minus; T In = P
&minus;1 AP &minus; P&minus;1(T In)P = P
</p>
<p>&minus;1(A &minus; T In)P.
</p>
<p>From the Binet Theorem 5.1.16 we have then
</p>
<p>det(B &minus; T In) = det(P
&minus;1(A &minus; T In)P) = det(P
</p>
<p>&minus;1) det(A &minus; T In) det(P)
</p>
<p>= det(A &minus; T In),
</p>
<p>which yields a proof of the claim, since det(P&minus;1) det(P) = det(In) = 1. ⊓⊔
</p>
<p>Given a matrix A &isin; Rn,n , an explicit computation of det(A &minus; T In) shows that
</p>
<p>pA(T ) = (&minus;1)
nT n + (&minus;1)n&minus;1tr(A) T n&minus;1 + &middot; &middot; &middot; + det(A).
</p>
<p>The case n = 2 is the Exercise 9.3.2.
</p>
<p>Given φ &isin; End(V ), the Proposition 9.3.3 shows that the characteristic polyno-
</p>
<p>mial of the matrix associated to φ does not depend on the given basis of V .
</p>
<p>Definition 9.3.4 For any matrix A associated to the endomorphism φ &isin; End(V ),
</p>
<p>the polynomial pφ(T ) = pA(T ) is called the characteristic polynomial of φ.
</p>
<p>From the Proposition 9.2.16 and the Definition 9.3.4 we have the following result.
</p>
<p>Corollary 9.3.5 The eigenvalues of the endomorphism φ &isin; End(V ) (the spectrum
</p>
<p>of φ) are the real roots of the characteristic polynomial pφ(T ).
</p>
<p>Exercise 9.3.6 Let φ &isin; End(R2) be associated to the matrix
</p>
<p>M
E,E
φ =
</p>
<p>(
</p>
<p>0 1
</p>
<p>&minus;1 0
</p>
<p>)
</p>
<p>.
</p>
<p>Since pφ(T ) = T
2 + 1, the endomorphism has no (real) eigenvalues.</p>
<p/>
</div>
<div class="page"><p/>
<p>140 9 Endomorphisms and Diagonalization
</p>
<p>Definition 9.3.7 Let p(X) be a polynomial with real coefficients, and letα be one of
</p>
<p>its real root. From the fundamental theorem of algebra (see the Proposition A.5.7) we
</p>
<p>know that then (X &minus; α) is a divisor for p(X), and that we have the decomposition
</p>
<p>p(X) = (X &minus; α)m(α) &middot; q(X)
</p>
<p>where q(X) is not divisible by (X &minus; α) and 1 &le; m(α) is an integer depending on
</p>
<p>α. Such an integer is called the multiplicity of α.
</p>
<p>Exercise 9.3.8 Let p(X) = (X &minus; 2)(X &minus; 3)(X2 + 1). Its real roots are 2 (with mul-
</p>
<p>tiplicity m(2) = 1, since (X &minus; 3)(X2 + 1) cannot be divided by 2) and 3 (with multi-
</p>
<p>plicity m(3) = 1). Clearly the polynomial p(X) has also two imaginary roots, given
</p>
<p>by &plusmn;i.
</p>
<p>Proposition 9.3.9 Let V be a real vector space with φ &isin; End(V ). If λ is an eigen-
</p>
<p>value for φ with multiplicity m(λ) and eigenspace Vλ, it holds that
</p>
<p>1 &le; dim(Vλ) &le; m(λ).
</p>
<p>Proof Let r = dim(Vλ) and C be a basis of Vλ. We complete C to a basis B for V . We
</p>
<p>then have B = (v1, . . . , vr , vr+1, . . . , vn), where the first elements v1, . . . , vr &isin; Vλ
are eigenvectors for λ. The matrix M
</p>
<p>B,B
φ has the following block form,
</p>
<p>A = M
B,B
φ =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ 0 . . . 0 a1,r+1 . . . a1,n
0 λ . . . 0 a2,r+1 . . . a2,n
...
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 . . . λ ar,r+1 . . . ar,n
0 0 . . . 0 ar+1,r+1 . . . ar+1,n
0 0 . . . 0 ar+2,r+1 . . . ar+2,n
...
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 0 . . . 0 an,r+1 . . . an,n
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>If det(A &minus; T In) is computed by the Laplace theorem (with respect to the first row,
</p>
<p>say), we have
</p>
<p>pφ(T ) = det(A &minus; T In) = (λ &minus; T )
r g(T ),
</p>
<p>where g(T ) is the characteristic polynomial of the lower diagonal (n &minus; r)&times; (n &minus; r)
</p>
<p>square block of A. We can then conclude that r &le; m(λ). ⊓⊔
</p>
<p>Definition 9.3.10 The integer dim(Vλ) is called the geometric multiplicity of the
</p>
<p>eigenvalue λ, while m(λ) is called the algebraic multiplicity of the eigenvalue λ.
</p>
<p>Remark 9.3.11 Let φ &isin; End(V ).
</p>
<p>(a) If λ = 0 is an eigenvalue for φ, the corresponding eigenspace V0 is ker(φ).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 The Characteristic Polynomial of an Endomorphism 141
</p>
<p>(b) If λ 	= 0 is an eigenvalue for φ, then Vλ &sube; Im(φ):
</p>
<p>let us indeed consider 0V 	= v &isin; Vλ with φ(v) = λv. Since λ 	= 0, we divide
</p>
<p>by λ and write
</p>
<p>v = λ&minus;1φ(v) = φ(λ&minus;1v) &isin; Im(φ).
</p>
<p>(c) If λ1 	= λ2 	= &middot; &middot; &middot; 	= λs are distinct non zero eigenvalues for φ, from the Propo-
</p>
<p>sition 9.2.12 we have the direct sum of corresponding eigenspaces and
</p>
<p>Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs &sube; Im(φ).
</p>
<p>Exercise 9.3.12 Let φ &isin; End(R4) be given by
</p>
<p>φ((x, y, z, t)) = (2x + 4y, x + 2y,&minus;z &minus; 2t, z + t).
</p>
<p>The corresponding matrix with respect to the canonical basis E4 is
</p>
<p>A = M
E,E
φ =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>2 4 0 0
</p>
<p>1 2 0 0
</p>
<p>0 0 &minus;1 &minus;2
</p>
<p>0 0 1 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Its characteristic polynomial reads
</p>
<p>pφ(T ) = pA(T ) = det(A &minus; T I4)
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2 &minus; T 4 0 0
</p>
<p>1 2 &minus; T 0 0
</p>
<p>0 0 &minus;1 &minus; T &minus;2
</p>
<p>0 0 1 1 &minus; T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2 &minus; T 4
</p>
<p>1 2 &minus; T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>&minus;1 &minus; T &minus;2
</p>
<p>1 1 &minus; T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= T (T &minus; 4)(T 2 + 1).
</p>
<p>The eigenvalues (the real roots of such a polynomial) of φ are λ = 0, 4. It is easy to
</p>
<p>compute that
</p>
<p>V0 = ker(φ) = L((&minus;2, 1, 0, 0)),
</p>
<p>V4 = ker(φ&minus; 4I4) = L((2, 1, 0, 0)).
</p>
<p>This shows that V4 is the only eigenspace corresponding to a non zero eigenvalue
</p>
<p>for φ.</p>
<p/>
</div>
<div class="page"><p/>
<p>142 9 Endomorphisms and Diagonalization
</p>
<p>From the Theorem 7.6.4 we know that dim Im(φ) = 4 &minus; dim ker(φ) = 3, with a
</p>
<p>basis of the image of φ given by 3 linearly independent columns in A. It is immediate
</p>
<p>to notice that the second column is a multiple of the first one, so we have
</p>
<p>Im(φ) = L((2, 1, 0, 0), (0, 0,&minus;1, 1), (0, 0,&minus;2, 1)).
</p>
<p>It is evident that V4 &sub; Im(φ), as shown in general in the Remark 9.3.11.
</p>
<p>Exercise 9.3.13 We consider the endomorphism in R4 given by
</p>
<p>φ((x, y, z, t)) = (2x + 4y, x + 2y,&minus;z, z + t),
</p>
<p>whose corresponding matrix with respect to the canonical basis E4 is
</p>
<p>A = M
E,E
φ =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>2 4 0 0
</p>
<p>1 2 0 0
</p>
<p>0 0 &minus;1 0
</p>
<p>0 0 1 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The characteristic polynomial reads
</p>
<p>pφ(T ) = pA(T ) = det(A &minus; T I4)
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>2 &minus; T 4 0 0
</p>
<p>1 2 &minus; T 0 0
</p>
<p>0 0 &minus;1 &minus; T 0
</p>
<p>0 0 1 1 &minus; T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= T (T &minus; 4)(T + 1)(T &minus; 1).
</p>
<p>The eigenvalues are given by λ = 0, 4,&minus;1, 1. The corresponding eigenspaces are
</p>
<p>V0 = ker(φ) = L((&minus;2, 1, 0, 0)),
</p>
<p>V4 = ker(φ&minus; 4I4) = L((2, 1, 0, 0)),
</p>
<p>V&minus;1 = ker(φ+ I4) = L((0, 0,&minus;2, 1)),
</p>
<p>V1 = ker(φ&minus; I4) = L((0, 0, 0, 1)),
</p>
<p>with
</p>
<p>Im(φ) = V&minus;1 &oplus; V1 &oplus; V4.
</p>
<p>The characteristic polynomial pφ(T ) of an endomorphism over a real vector space
</p>
<p>has real coefficients. If λ1, . . . ,λs are its non zero real distinct roots (that is, the
</p>
<p>eigenvalues of φ), we can write
</p>
<p>pφ(T ) = (T &minus; λ1)
m1 &middot; &middot; &middot; (T &minus; λp)
</p>
<p>ms &middot; q(T ),</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 The Characteristic Polynomial of an Endomorphism 143
</p>
<p>where m j , j = 1, . . . , s are the algebraic multiplicities and q(T ) has no real roots.
</p>
<p>We have then
</p>
<p>deg(pφ(T )) &ge; m1 + &middot; &middot; &middot; + ms .
</p>
<p>This proves the following proposition.
</p>
<p>Proposition 9.3.14 Let V be a real vector space with dim(V ) = n, and let φ &isin;
</p>
<p>End(V ). By denoting λ1, . . . ,λs the distinct eigenvalues of φ with corresponding
</p>
<p>algebraic multiplicities m1, . . . ,ms , one has
</p>
<p>m1 + &middot; &middot; &middot; + ms &le; n,
</p>
<p>with the equality holding if and only if every root in pφ(T ) is real. ⊓⊔
</p>
<p>9.4 Diagonalisation of an Endomorphism
</p>
<p>In this section we describe conditions under which an endomorphism is simple. As we
</p>
<p>have seen, this problem is equivalent to study conditions under which a square matrix
</p>
<p>is diagonalisable. The first theorem we prove characterises simple endomorphims.
</p>
<p>Theorem 9.4.1 Let V be a real n-dimensional vector space, with φ &isin; End(V ).
</p>
<p>If λ1, . . . ,λs are the different roots of pφ(T ) with multiplicities m1, . . . ,ms , the
</p>
<p>following claims are equivalent:
</p>
<p>(a) φ is a simple endomorphism,
</p>
<p>(b) V has a basis of eigenvectors for φ,
</p>
<p>(c) λi &isin; R for any i = 1, . . . , s, with V = Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs ,
</p>
<p>(d) λi &isin; R and mi = dim(Vλi ) for any i = 1, . . . , s.
</p>
<p>When φ is simple, each basis of V of eigenvectors for φ contains mi eigenvectors for
</p>
<p>each distinct eigenvalues λi , for i = 1, . . . , s.
</p>
<p>Proof &bull; (a) &hArr; (b): this has been shown in the Remark 9.2.3.
</p>
<p>&bull; (b) &rArr; (c): let B = (v1, . . . , vn) be a basis of V of eigenvectors for φ. Any vector
</p>
<p>vi belongs to one of the eigenspaces, so we can write
</p>
<p>V = L(v1, . . . , vn) &sube; Vλ1 + &middot; &middot; &middot; + Vλs ,
</p>
<p>while the opposite inclusion is obvious. Since the sum of eigenspaces corre-
</p>
<p>sponding to distinct eigenvalues is direct (see the Proposition 9.2.12), we have
</p>
<p>V = Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs .
</p>
<p>&bull; (c) &rArr; (b): let Bi be a basis of Vλi for any i . Since V is the direct sum of all the
</p>
<p>eigenspaces Vλi , the set B = B1 &cup; . . . &cup; Bs is a basis of V made by eigenvectors
</p>
<p>for φ.</p>
<p/>
</div>
<div class="page"><p/>
<p>144 9 Endomorphisms and Diagonalization
</p>
<p>&bull; (c) &rArr; (d): from the Grassmann Theorem 2.5.8, we have
</p>
<p>n = dim(V ) = dim(Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs )
</p>
<p>= dim(Vλ1)+ &middot; &middot; &middot; + dim(Vλs )
</p>
<p>&le; m1 + &middot; &middot; &middot; + ms
</p>
<p>&le; n,
</p>
<p>where the inequalities follow from the Propositions 9.3.9 and 9.3.14. We can then
</p>
<p>conclude that dim(Vλi ) = m(λi ) for any i .
</p>
<p>&bull; (d) &rArr; (c): from the hypothesis mi = dim(Vλi ) for any i = 1, . . . , s, and the
</p>
<p>Proposition 9.3.14 we have
</p>
<p>n = m1 + &middot; &middot; &middot; + ms = dim(Vλ1) + &middot; &middot; &middot; + dim(Vλs ).
</p>
<p>We have then n = dim(Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs ) and this equality amounts to prove the
</p>
<p>claim, since Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs has dimension n and therefore coincides with V . ⊓⊔
</p>
<p>Corollary 9.4.2 If λi &isin; R and m(λi ) = 1 for any i = 1, . . . , n, then is φ simple.
</p>
<p>Proof It is immediate, by recalling the Proposition 9.3.9 and (d) in the Theorem
</p>
<p>9.4.1. ⊓⊔
</p>
<p>Exercise 9.4.3 Let φ be the endomorphism in R2 whose corresponding matrix with
</p>
<p>respect to the canonical basis is the matrix
</p>
<p>A =
</p>
<p>(
</p>
<p>1 1
</p>
<p>0 1
</p>
<p>)
</p>
<p>.
</p>
<p>It is pA(T ) = (1 &minus; T )
2: such a polynomial has only one root λ = 1 with alge-
</p>
<p>braic multiplicity m = 2. It is indeed easy to compute that V1 = L((1, 0)), so the
</p>
<p>geometric multiplicity is 1. This proves that the matrix A is not diagonalisable, the
</p>
<p>corresponding endomorphism is not simple.
</p>
<p>Proposition 9.4.4 Let φ &isin; End(V ) be a simple endomorphism and C be a basis of
</p>
<p>V such that � = M
C,C
φ . Then,
</p>
<p>(a) the eigenvalues λ1, . . . ,λs for φ, counted with their multiplicities m(λ1), . . . ,
</p>
<p>m(λs), are the diagonal elements for �;
</p>
<p>(b) the diagonal matrix � is uniquely determined up to permutations of the eigen-
</p>
<p>values (such a permutation corresponds to a permutation in the ordering of the
</p>
<p>basis elements in C).
</p>
<p>Proof (a) From the Remark 9.2.1 we know that the diagonal elements in � =
</p>
<p>M
C,C
φ &isin; R
</p>
<p>n,n are given by the eigenvalues λ1, . . . ,λs : each eigenvalue λi must
</p>
<p>be counted as many times as the geometric multiplicity of the eigenvector vi ,</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Diagonalisation of an Endomorphism 145
</p>
<p>since C is a basis of eigenvectors. From the claim (d) in the Theorem 9.4.1, the
</p>
<p>geometric multiplicity of each eigenvalue coincides with its algebraic multiplic-
</p>
<p>ity.
</p>
<p>(a) This is obvious. ⊓⊔
</p>
<p>Proposition 9.4.5 Let φ be a simple endomorphism on V , with B an arbitrary basis
</p>
<p>of V . By setting A = M
B,B
φ , let P be a matrix such that
</p>
<p>P&minus;1 A P = �.
</p>
<p>Then the columns in P are the components, with respect to B, of a basis of V made
</p>
<p>by eigenvectors for φ.
</p>
<p>Proof Let C be a basis of V such that � = M
C,C
φ . From the Remark 9.2.3 the basis C
</p>
<p>is made by eigenvectors for φ. The claim follows by setting P = MB,C , that is the
</p>
<p>matrix of the change of basis. ⊓⊔
</p>
<p>Definition 9.4.6 Given a matrix A &isin; Rn,n , its diagonalisation consists of determin-
</p>
<p>ing, (if they exist) a diagonal matrix � &sim; A and an invertible matrix P &isin; GL(n)
</p>
<p>such that P&minus;1 A P = �.
</p>
<p>The following remark gives a resum&eacute; of the steps needed for the diagonalisation
</p>
<p>of a given matrix.
</p>
<p>Remark 9.4.7 (An algorithm for the diagonalisation) Let A &isin; Rn,n be a square
</p>
<p>matrix. In order to diagonalise it:
</p>
<p>(1) Write the characteristic polynomial pA(T ) of A and find its roots λ1, . . . ,λs
with the corresponding algebraic multiplicities m1, . . . ,ms .
</p>
<p>(2) If one of the roots λi /&isin; R, then A is not diagonalisable.
</p>
<p>(3) If λi &isin; R for any i = 1, . . . , s, compute the geometric multiplicities
</p>
<p>dim(Vλi ) = n &minus; rk(A &minus; λi In).
</p>
<p>If there is an eigenvalue λi such that mi 	= dim(Vλi ), then A is not diagonalis-
</p>
<p>able.
</p>
<p>(4) if λi &isin; R and m(λ)i = dim(Vλi ) for any i = 1, . . . , s, then A is diagonalisable.
</p>
<p>In such a case, A is similar to a diagonal matrix �: the eigenvalues λi , counted
</p>
<p>with their multiplicities, give the diagonal elements for �.
</p>
<p>(5) it is � = M
B,B
φ , where B is a basis of V given by eigenvectors for the endomor-
</p>
<p>phism corresponding to the matrix A. By defining P = ME,B, it is� = P&minus;1 AP .
</p>
<p>Since V is the direct sum of the eigenspaces for A (see Theorem 9.4.1), it follows
</p>
<p>that B = B1 &cup; &middot; &middot; &middot; &cup; Bs , with Bi a basis of Vλi for any i = 1, . . . , s. (The spaces
</p>
<p>Vλi can be obtained explicitly as in the Lemma 9.2.15.)</p>
<p/>
</div>
<div class="page"><p/>
<p>146 9 Endomorphisms and Diagonalization
</p>
<p>Exercise 9.4.8 We study whether the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>3 1 1
</p>
<p>1 0 2
</p>
<p>1 2 0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>is diagonalisable. Its characteristic polynomial is
</p>
<p>pA(T ) = det(A &minus; T I3)
</p>
<p>=
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>3 &minus; T 1 1
</p>
<p>1 &minus;T 2
</p>
<p>1 2 &minus;T
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>∣
</p>
<p>= &minus;T 3 + 3T 2 + 6T &minus; 8 = (T &minus; 1)(T &minus; 4)(T + 2).
</p>
<p>Its eigenvalues are found to be λ1 = 1,λ2 = 4,λ3 = &minus;2. Since each root of the
</p>
<p>characteristic polynomial has algebraic multiplicity m = 1, from the Corollary 9.4.2
</p>
<p>the matrix A is diagonalisable, and indeed similar to
</p>
<p>� =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 0
</p>
<p>0 4 0
</p>
<p>0 0 &minus;2
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We compute a basis B for R3 of eigenvectors for A. We know that V1 = ker(A &minus; I3),
</p>
<p>so V1 is the space of the solutions of the homogeneous linear system (A &minus; I3)X = 0
</p>
<p>associated to the matrix
</p>
<p>A &minus; I3 =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2 1 1
</p>
<p>1 &minus;1 2
</p>
<p>1 2 &minus;1
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>which is reduced to
⎛
</p>
<p>⎝
</p>
<p>2 1 1
</p>
<p>3 0 3
</p>
<p>0 0 0
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>The solution of such a linear system are given by (x, y, z) = (x,&minus;x,&minus;x), thus
</p>
<p>V1 = L((&minus;1, 1, 1)). Along the same lines we compute
</p>
<p>V4 = ker(A &minus; 4I3) = L((2, 1, 1)),
</p>
<p>V&minus;2 = ker(A + 2I3) = L((0,&minus;1, 1)).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Diagonalisation of an Endomorphism 147
</p>
<p>We have then B = ((&minus;1, 1, 1), (2, 1, 1), (0,&minus;1, 1)) and
</p>
<p>P = ME,B =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>&minus;1 2 0
</p>
<p>1 1 &minus;1
</p>
<p>1 1 1
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>It is easy to compute that P&minus;1 A P = �.
</p>
<p>Proposition 9.4.9 Let A &isin; Rn,n be diagonalisable, with eigenvalues λ1, . . . ,λs and
</p>
<p>corresponding multiplicities m1, . . . ,ms . Then
</p>
<p>det(A) = λ
m1
1 &middot; λ
</p>
<p>m2
2 &middot; &middot; &middot; &middot; &middot; λ
</p>
<p>ms
s ,
</p>
<p>tr(A) = m1λ1 + m2λ2 + &middot; &middot; &middot; + msλs .
</p>
<p>Proof Since A is diagonalisable, there exists an invertible n-dimensional matrix P
</p>
<p>such that � = P&minus;1 A P . The matrix � is diagonal, and its diagonal elements are
</p>
<p>(see the Proposition 9.4.4) the eigenvalues of A counted with their multiplicities.
</p>
<p>Then, from the Proposition 9.1.5 on has,
</p>
<p>det(A) = det(P&minus;1 AP) = det(�) = λ
m1
1 &middot; λ
</p>
<p>m2
2 &middot; &middot; &middot; &middot; &middot; λ
</p>
<p>ms
s
</p>
<p>and
</p>
<p>tr(A) = tr(P&minus;1 AP) = tr(�) = m1λ1 + m2λ2 + &middot; &middot; &middot; + msλs .
</p>
<p>⊓⊔
</p>
<p>9.5 The Jordan Normal Form
</p>
<p>In this section we briefly describe the notion of Jordan normal form of a matrix. As we
</p>
<p>have described before in this chapter, a square matrix is not necessarily diagonalis-
</p>
<p>able, that is it is not necessarily similar to a diagonal matrix. It is nonetheless possible
</p>
<p>to prove that any square matrix is similar to a triangular matrix J which is not far
</p>
<p>from being diagonal. Such a matrix J is diagonal if and only if A is diagonalisable;
</p>
<p>if not it has a &lsquo;standard&rsquo; block structure.
</p>
<p>An example of a so called Jordan block is the non diagonalisable matrix A in
</p>
<p>Exercise 9.4.3. We denote it by
</p>
<p>J2(1) =
</p>
<p>(
</p>
<p>1 1
</p>
<p>0 1
</p>
<p>)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>148 9 Endomorphisms and Diagonalization
</p>
<p>A Jordan block of order k is a k-dimensional upper triangular square matrix of the
</p>
<p>form
</p>
<p>Jk(λ) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>λ 1 0 &middot; &middot; &middot; 0
</p>
<p>0 λ 1 &middot; &middot; &middot; 0
...
...
...
</p>
<p>...
...
...
... &middot; &middot; &middot; 1
</p>
<p>0 0 0 &middot; &middot; &middot; λ
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>where the diagonal terms are given by a scalar λ &isin; R, the (Jk(λ)) j, j+1 entries are
</p>
<p>1 and the remaining entries are zero. It is immediate to show that the characteristic
</p>
<p>polynomial of such a matrix is given by
</p>
<p>pJk (λ)(T ) = (T &minus; λ)
k,
</p>
<p>and the parameter λ is the unique eigenvalue with algebraic multiplicity mλ = k.
</p>
<p>The corresponding eigenspace is
</p>
<p>Vλ = ker(Jk(λ) &minus; λ In) = L((1, 0, . . . , 0)),
</p>
<p>with geometric multiplicity dim(Vλ) = 1. Thus, if k &gt; 1, a Jordan block is not diag-
</p>
<p>onalisable.
</p>
<p>A matrix J is said to be in (canonical or normal) Jordan form if it has a block
</p>
<p>diagonal form
</p>
<p>J =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>Jk1(λ1) 0 . . . 0
</p>
<p>0 Jk2(λ2) . . . 0
...
</p>
<p>...
. . .
</p>
<p>...
</p>
<p>0 0 . . . Jks (λs)
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>,
</p>
<p>where each Jk j (λ j ) is a Jordan block of order k j and eigenvalue λ j , for j = 1, . . . , s.
</p>
<p>Notice that nothing prevents from having the same eigenvalue in different Jordan
</p>
<p>blocks, that is λ j = λl even with k j 	= kl . Since each Jordan block Jk j (λ j ) provides
</p>
<p>a one dimensional eigenspace for λ j , the geometric multiplicity of λ j coincides with
</p>
<p>the number of Jordan blocks with eigenvalue λ j . The algebraic multiplicity of λ j
coincides indeed with the sum of the orders of the Jordan blocks having the same
</p>
<p>eigenvalue λ j .
</p>
<p>Theorem 9.5.1 (Jordan) Let A &isin; Rn,n such that its characteristic polynomial has
</p>
<p>only real roots (such roots are all the eigenvalues for A). Then,
</p>
<p>(i) the matrix A is similar to a Jordan matrix,
</p>
<p>(ii) two Jordan matrices J and J &prime; are similar if and only if one is mapped into the
</p>
<p>other under a block permutation.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 The Jordan Normal Form 149
</p>
<p>We omit a complete proof of this theorem, and we limit ourselves to briefly
</p>
<p>introduce the notion of generalised eigenvector of a matrix A. We recall that, when
</p>
<p>A is not diagonalisable, the set of eigenvectors for A is not enough for a basis of Rn .
</p>
<p>The columns of the invertible matrix P that realises the similarity between A and
</p>
<p>the Jordan form J (such that P&minus;1 AP = J ) are the components with respect to the
</p>
<p>canonical basis En of the so called generalised eigenvectors for A.
</p>
<p>Given an eigenvalue λ for A with algebraic multiplicity mλ &ge; 1, a corresponding
</p>
<p>generalised eigenvector is a non zero vector v that solves the linear homogeneous
</p>
<p>system
</p>
<p>(A &minus; λ In)
mv = 0Rn .
</p>
<p>It is possible to show that such a system has m solutions v j (with v j = 1, . . . ,m)
</p>
<p>which can be obtained by recursion,
</p>
<p>(A &minus; λ In)v1 = 0Rn ,
</p>
<p>(A &minus; λ In)vk = vk&minus;1, k = 2, . . .m.
</p>
<p>The elements v j span the generalised eigenspace Vλ for A corresponding to the
</p>
<p>eigenvalue λ. The generalised eigenvectors satisfy the condition
</p>
<p>(A &minus; λ In)
kvk = 0Rn for any k = 1, 2, . . .m.
</p>
<p>Since the characteristic polynomial of A has in general complex roots, we end by
</p>
<p>noticing that a more natural version of the Jordan theorem is valid on C.
</p>
<p>Exercise 9.5.2 We consider the matrix
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>5 4 2 1
</p>
<p>0 1 &minus;1 &minus;1
</p>
<p>&minus;1 &minus;1 3 0
</p>
<p>1 1 &minus;1 2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Its characteristic polynomial is computed to be pA(T ) = (T &minus; 1)(T &minus; 2)(T &minus; 4)
2,
</p>
<p>so its eigenvalues are λ = 1, 2, 4, 4. Since the algebraic multiplicity of the eigen-
</p>
<p>values λ = 1 and λ = 2 is 1, their geometric multiplicity is also 1. An explicit
</p>
<p>computation shows that
</p>
<p>dim(ker(A &minus; 4 I4)) = 1.
</p>
<p>We have then that A is not diagonalisable, and that the eigenvalue λ = 4 corresponds
</p>
<p>to a Jordan block. A canonical form for the matrix A is then given by
</p>
<p>J =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 0 0 0
</p>
<p>0 2 0 0
</p>
<p>0 0 4 1
</p>
<p>0 0 0 4
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>150 9 Endomorphisms and Diagonalization
</p>
<p>Exercise 9.5.3 The matrices
</p>
<p>J =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>3 1 0 0
</p>
<p>0 3 0 0
</p>
<p>0 0 3 0
</p>
<p>0 0 0 3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, J &prime; =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>3 1 0 0
</p>
<p>0 3 0 0
</p>
<p>0 0 3 1
</p>
<p>0 0 0 3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>have the same characteristic polynomial, the same determinant, and the same trace.
</p>
<p>They are however not similar, since they are in Jordan form, and there is no block
</p>
<p>permutation under which J is mapped into J &prime;.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
</p>
<p>Spectral Theorems on Euclidean Spaces
</p>
<p>In Chap. 7 we studied the operation of changing a basis for a real vector space. In
</p>
<p>particular, in the Theorem 7.9.6 and the Remark 7.9.7 there, we showed that any
</p>
<p>matrix giving a change of basis for the vector space Rn is an invertible n &times; n matrix,
and noticed that any n &times; n invertible yields a change of basis for Rn .
</p>
<p>In this chapter we shall consider the endomorphisms of the euclidean space
</p>
<p>En = (Rn, &middot;), where the symbol &middot; denotes the euclidean scalar product, that we
have described in Chap. 3.
</p>
<p>10.1 Orthogonal Matrices and Isometries
</p>
<p>As we noticed, the natural notion of basis for a euclidean space is that of orthonormal
</p>
<p>one. This restricts the focus to matrices which gives a change of basis between
</p>
<p>orthonormal bases for En .
</p>
<p>Definition 10.1.1 A square matrix A &isin; Rn,n is called orthogonal if its columns
form an orthonormal basis B for En . In such a case A = ME,B, that is A is the
matrix giving the change of basis from the canonical basis E to the basis B.
</p>
<p>It follow from this definition that an orthogonal matrix is invertible.
</p>
<p>Exercise 10.1.2 The identity matrix In is clearly orthogonal for each E
n . Since the
</p>
<p>vectors
</p>
<p>v1 = 1&radic;
2
(1, 1), v2 = 1&radic;
</p>
<p>2
(1,&minus;1)
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_10
</p>
<p>151</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_10&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_10&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>152 10 Spectral Theorems on Euclidean Spaces
</p>
<p>form an orthonormal basis for E2, the matrix
</p>
<p>A = 1&radic;
2
</p>
<p>(
1 1
</p>
<p>1 &minus;1
</p>
<p>)
</p>
<p>is orthogonal.
</p>
<p>Proposition 10.1.3 A matrix A is orthogonal if and only if
</p>
<p>tA A = In,
</p>
<p>that is if and only if A&minus;1 = tA.
</p>
<p>Proof With (v1, . . . , vn) a system of vectors in E
n , we denote by A = (v1 &middot; &middot; &middot; vn)
</p>
<p>the matrix with columns given by the given vectors, and by
</p>
<p>tA =
</p>
<p>⎛
⎜⎝
</p>
<p>tv1
...
</p>
<p>tvn
</p>
<p>⎞
⎟⎠
</p>
<p>its transpose. We have the following equivalences. The matrix A is orthogonal (by
</p>
<p>definition) if and only if (v1, . . . , vn) is an orthonormal basis for E
n , that is if and
</p>
<p>only if vi &middot; v j = δi j for any i, j . Recalling the representation of the row by column
product of matrices, one has vi &middot; v j = δi j if and only if (tAA)i j = δi j for any i, j ,
which amounts to say that tAA = In . �
</p>
<p>Exercise 10.1.4 For the matrix A considered in the Exercise 10.1.2 one has easily
</p>
<p>compute that A = tA and A2 = I2.
</p>
<p>Exercise 10.1.5 The matrix
</p>
<p>A =
(
</p>
<p>1 0
</p>
<p>1 1
</p>
<p>)
</p>
<p>is not orthogonal, since
</p>
<p>tAA =
(
</p>
<p>1 1
</p>
<p>0 1
</p>
<p>) (
1 0
</p>
<p>1 1
</p>
<p>)
=
</p>
<p>(
2 1
</p>
<p>1 1
</p>
<p>)
�= I2.
</p>
<p>Proposition 10.1.6 If A is orthogonal, then det(A) = &plusmn;1.
</p>
<p>Proof This statement easily follows from the Binet Theorem 5.1.16: with tAA = In ,
one has
</p>
<p>det(tA) det(A) = det(In) = 1,
</p>
<p>and the Corollary 5.1.12, that is det(tA) = det(A), which then implies
(det(A))2 = 1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Orthogonal Matrices and Isometries 153
</p>
<p>Remark 10.1.7 The converse to this statement does not hold. The matrix A from the
</p>
<p>Exercise 10.1.5 is not orthogonal, while det(A) = 1.
</p>
<p>Definition 10.1.8 An orthogonal matrix A with det(A) = 1 is called special orthog-
onal.
</p>
<p>Proposition 10.1.9 The set O(n) of orthogonal matrices in Rn,n is a group, with
</p>
<p>respect to the usual matrix product. Its subset SO(n) = {A &isin; O(n) : det(A) = 1}
is a subgroup of O(n) with respect to the same product.
</p>
<p>Proof We prove that O(n) is stable under the matrix product, has an identity element,
</p>
<p>and the inverse of an orthogonal matrix is orthogonal as well.
</p>
<p>&bull; The identity matrix In is orthogonal, as we already mentioned.
&bull; If A and B are orthogonal, then we can write
</p>
<p>t (AB)AB = tB tAAB
= tB In B
= tB B = In,
</p>
<p>that is, AB is orthogonal.
</p>
<p>&bull; If A is orthogonal, tAA = In , then
</p>
<p>t (A&minus;1)A&minus;1 = (A tA)&minus;1 = In,
</p>
<p>that proves that A&minus;1 is orthogonal.
</p>
<p>From the Binet theorem it easily follows that the set of special orthogonal matrices
</p>
<p>is stable under the product, and the inverse of a special orthogonal matrix is special
</p>
<p>orthogonal. �
</p>
<p>Definition 10.1.10 The group O(n) is called the orthogonal group of order n, its
</p>
<p>subset SO(n) is called the special orthogonal group of order n.
</p>
<p>We know from the Definition 10.1.1 that a matrix is orthogonal if and only if
</p>
<p>it is the matrix of the change of basis between the canonical basis E (which is
</p>
<p>orthonormal) and a second orthonormal basis B. A matrix A is then orthogonal if
</p>
<p>and only if A&minus;1 = tA (Proposition 10.1.3).
The next theorem shows that we do not need the canonical basis. If one defines
</p>
<p>a matrix A to be orthogonal by the condition A&minus;1 = tA, then A is the matrix for
a change between two orthonormal bases and viceversa, any matrix A giving the
</p>
<p>change between orthonormal bases satisfies the condition A&minus;1 = tA.
</p>
<p>Theorem 10.1.11 Let C be an orthonormal basis for the euclidean vector space En ,
</p>
<p>with B another (arbitrary) basis for it. The matrix MC,B of the change of basis from
</p>
<p>C to B is orthogonal if and only if also the basis B is orthonormal.</p>
<p/>
</div>
<div class="page"><p/>
<p>154 10 Spectral Theorems on Euclidean Spaces
</p>
<p>Proof We start by noticing that, since C is an orthonormal basis, the matrix ME,C
</p>
<p>giving the change of basis between the canonical basis E and C is orthogonal by the
</p>
<p>Definition 10.1.1. It follows that, being O(n) a group, the inverse MC,E = (ME,C)&minus;1
is orthogonal. With B an arbitrary basis, from the Theorem 7.9.9 we can write
</p>
<p>MC,B = MC,E ME,E ME,B
</p>
<p>= MC,E In ME,B = MC,E ME,B.
</p>
<p>Firstly, let us assume B to be orthonormal. We have then that ME,B is orthogonal;
</p>
<p>thus MC,B is orthogonal since it is the product of orthogonal matrices.
</p>
<p>Next, let us assume that MC,B is orthogonal; from the chain relations displayed
</p>
<p>above we have
</p>
<p>ME,B = (MC,E)&minus;1 MC,B = ME,C MC,B.
</p>
<p>This matrix ME,B is then orthogonal (being the product of orthogonal matrices), and
</p>
<p>therefore B is an orthonormal basis. �
</p>
<p>We pass to endomorphisms corresponding to orthogonal matrices. We start by
</p>
<p>recalling, from the Definition 3.1.4, that a scalar product has a &lsquo;canonical&rsquo; form
</p>
<p>when it is given with respect to orthonormal bases.
</p>
<p>Remark 10.1.12 Let C be an orthonormal basis for the euclidean space En . If
</p>
<p>v,w &isin; En are given by v = (x1, . . . , xn)C and w = (y1, . . . , yn)C , one has that
v &middot; w = x1 y1 + &middot; &middot; &middot; + xn yn . By denoting X and Y the one-column matrices whose
entries are the components of v,w with respect to C, that is
</p>
<p>X =
</p>
<p>⎛
⎜⎝
</p>
<p>x1
...
</p>
<p>xn
</p>
<p>⎞
⎟⎠ , Y =
</p>
<p>⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>yn
</p>
<p>⎞
⎟⎠ ,
</p>
<p>we can write
</p>
<p>v &middot; w = x1 y1 + &middot; &middot; &middot; + xn yn =
(
x1 . . . xn
</p>
<p>)
⎛
⎜⎝
</p>
<p>y1
...
</p>
<p>yn
</p>
<p>⎞
⎟⎠ = tXY.
</p>
<p>Theorem 10.1.13 Let φ &isin; End(En), with E the canonical basis of En . The follow-
ing statements are equivalent:
</p>
<p>(i) The matrix A = ME,Eφ is orthogonal.
(ii) It holds that φ(v) &middot; φ(w) = v &middot; w for any v,w &isin; En .
</p>
<p>(iii) IfB = (b1, . . . , bn) is an orthonormal basis for En , then the setB&prime; = (φ(b1), . . . ,
φ(bn)) is such.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Orthogonal Matrices and Isometries 155
</p>
<p>Proof (i) &rArr; (ii): by denoting X = tv and Y = tw we can write
</p>
<p>v &middot; w = tXY, φ(v) &middot; φ(w) = t (AX)(AY ) = tX (tAA)Y,
</p>
<p>and since A is orthogonal, tAA = In , we conclude that φ(v) &middot; φ(w) = v &middot; w for
any v,w &isin; En .
</p>
<p>(ii) &rArr; (iii): let A = MC,Cφ be the matrix of the endomorphismφ with respect to the
basis C. We start by proving that A is invertible. By adopting the notation used
</p>
<p>above, we can represent the condition φ(v) &middot; φ(w) = v &middot; w as t (AX)(AY ) =
tXY for any X,Y &isin; En . It follows that tAA = In , that is A is orthogonal, and
then invertible. This means (see Theorem 7.8.4) that φ is an isomorphism, so it
</p>
<p>maps a basis for En into a basis for En . If B is an orthonormal basis, then we
</p>
<p>can write
</p>
<p>φ(bi ) &middot; φ(b j ) = bi &middot; b j = δi j
</p>
<p>which proves that B&prime; is an orthonormal basis.
</p>
<p>(iii) &rArr; (i): since E , the canonical basis for En , is orthonormal, then (φ(e1), . . . ,
φ(en)) is orthonormal. Recall the Remark 7.1.10: the components with respect
</p>
<p>to E of the elements φ(ei ) are the column vectors of the matrix M
E,E
</p>
<p>φ , thus M
E,E
</p>
<p>φ
</p>
<p>is orthogonal. �
</p>
<p>We have seen that, if the action of φ &isin; End(En) is represented with respect to the
canonical basis by an orthogonal matrix, then φ is an isomorphism and preserves the
</p>
<p>scalar product, that is, for any v,w &isin; En one has that,
</p>
<p>v &middot; w = φ(v) &middot; φ(w).
</p>
<p>The next result is therefore evident.
</p>
<p>Corollary 10.1.14 If φ &isin; End(En) is an endomorphism of the euclidean space En
whose corresponding matrix with respect to the canonical basis is orthogonal then
</p>
<p>φ preserves the norms, that is, for any v &isin; En one has
</p>
<p>‖φ(v)‖ = ‖v‖.
</p>
<p>This is the reason why such an endomorphism is also called an isometry.
</p>
<p>The analysis we developed so far allows us to introduce the following definition,
</p>
<p>which will be more extensively scrutinised when dealing with rotations maps.
</p>
<p>Definition 10.1.15 If φ &isin; End(En) takes the orthonormal basis B = (b1, . . . , bn)
to the orthonormal basis B&prime; = (b&prime;1 = φ(b1), . . . , b&prime;n = φ(bn)) in En , we say that B
and B&prime; have the same orientation if the matrix representing the endomorphism φ is
</p>
<p>special orthogonal.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 10 Spectral Theorems on Euclidean Spaces
</p>
<p>Remark 10.1.16 It is evident that this definition provides an equivalence relation
</p>
<p>within the collection of all orthonormal bases for En . The corresponding quotient
</p>
<p>can be labelled by the values of the determinant of the orthogonal map giving the
</p>
<p>change of basis, that is det φ = {&plusmn;1}. This is usually referred to by saying that the
euclidean space En has two orientations.
</p>
<p>10.2 Self-adjoint Endomorphisms
</p>
<p>We need to introduce an important class of endomorphisms.
</p>
<p>Definition 10.2.1 An endomorphismφ of the euclidean vector space En is called
</p>
<p>self-adjoint if
</p>
<p>φ(v) &middot; w = v &middot; φ(w) &forall; v,w &isin; E .
</p>
<p>From the Proposition 9.2.11 we know that eigenvectors corresponding to dis-
</p>
<p>tinct eigenvalues are linearly independent. When dealing with self-adjoint endomor-
</p>
<p>phisms, a stronger property holds.
</p>
<p>Proposition 10.2.2 Let φ be a self-adjoint endomorphism of En , with λ1,λ2 &isin; R
different eigenvalues for it. Any two corresponding eigenvectors, 0 �= v1 &isin; Vλ1 and
0 �= v2 &isin; Vλ2 , are orthogonal.
</p>
<p>Proof Since φ is self-adjoint, one has φ(v1) &middot; v2 = v1 &middot; φ(v2) while, v1 and v2 being
eigenvectors, one has φ(vi ) = λivi for i = 1, 2. We can then write
</p>
<p>(λ1v1) &middot; v2 = v1 &middot; (λ2v2)
</p>
<p>which reads
</p>
<p>λ1(v1 &middot; v2) = λ2(v1 &middot; v2) &rArr; (λ2 &minus; λ1)(v1 &middot; v2) = 0.
</p>
<p>The assumption that the eigenvalues are different allows one to conclude that
</p>
<p>v1 &middot; v2 = 0, that is v1 is orthogonal to v2. �
</p>
<p>The self-adjointness of an endomorphism can be characterised in terms of proper-
</p>
<p>ties of the matrices representing its action on En . We recall from the Definition 4.1.21
</p>
<p>that a matrix A = (ai j ) &isin; Rn,n is called symmetric if tA = A, that is if one has
ai j = a j i , for any i, j .
</p>
<p>Theorem 10.2.3 Let φ &isin; End(En) and B an orthonormal basis for En . The endo-
morphism φ is self-adjoint if and only if M
</p>
<p>B,B
</p>
<p>φ is symmetric.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Self-adjoint Endomorphisms 157
</p>
<p>Proof Using the usual notation, we set A = (ai j ) = MB,Bφ and X,Y be the columns
giving the components with respect to B of the vectors v,w in En . From the
</p>
<p>Remark 10.1.12 we write
</p>
<p>φ(v) &middot; w = t (AX)Y = (tX tA)Y = tX tAY
and v &middot; φ(w) = tX (AY ) = tX AY.
</p>
<p>Let us assume A to be symmetric. From the relations above we conclude that φ(v) &middot;
w = v &middot; φ(w) for any v,w &isin; En , that is φ is self-adjoint.
</p>
<p>If we assume φ to be self-adjoint, then we can equate
</p>
<p>tX tAY = tX AY
</p>
<p>for any X,Y in Rn . If we let X and Y to range on the elements of the canonical basis
</p>
<p>E = (e1, . . . , en) in Rn , such a condition is just the fact that ai j = a j i for any i, j ,
that is A is symmetric. �
</p>
<p>Exercise 10.2.4 The following matrix is symmetric:
</p>
<p>A =
(
</p>
<p>2 &minus;1
&minus;1 3
</p>
<p>)
.
</p>
<p>Then the endomorphism φ &isin; End(E2) corresponding to A with respect to the
canonical basis is self-adjoint. This can also be shown by a direct calculation:
</p>
<p>φ((x, y)) = (2x &minus; y,&minus;x + 3y); then
</p>
<p>(a, b) &middot; φ((x, y)) = a(2x &minus; y)+ b(&minus;x + 3y)
= (2a &minus; b)x + (&minus;a + 3b)y
= φ((a, b)) &middot; (x, y).
</p>
<p>Exercise 10.2.5 The following matrix is not symmetric
</p>
<p>B =
(
</p>
<p>1 1
</p>
<p>&minus;1 0
</p>
<p>)
.
</p>
<p>The corresponding (with respect to the canonical basis) endomorphismφ &isin; End(E2)
is indeed not self-adjoint since for instance,
</p>
<p>φ(e1) &middot; e2 = (1,&minus;1) &middot; (0, 1) = &minus;1,
e1 &middot; φ(e2) = (1, 0) &middot; (1, 0) = 1.
</p>
<p>An important family of self-adjoint endomorphisms is illustrated in the following
</p>
<p>exercise.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 10 Spectral Theorems on Euclidean Spaces
</p>
<p>Exercise 10.2.6 We know from Sect. 8.2 that, if B = (e1, . . . , en) is an orthonormal
basis for En , then the action of an endomorphism φ whose associated matrix is
</p>
<p>� = MB,Bφ can be written with the Dirac&rsquo;s notation as
</p>
<p>φ =
n&sum;
</p>
<p>a,b=1
</p>
<p>�ab|ea〉〈eb|,
</p>
<p>with �ab = 〈ea|φ(eb)〉. Then, the endomorphismφ is self-adjoint if and only if
�ab = �ba . Consider vectors u = (u1, . . . , un)B, v = (v1, . . . , vn)B in En , and
define the operator L = |u〉〈v|. We have
</p>
<p>〈ea|Leb〉 = 〈ea|u〉〈v|eb〉 = uavb,
〈eb|Lea〉 = 〈eb|u〉〈v|ea〉 = ubva,
</p>
<p>so we conclude that the operator L = |u〉〈v| is self-adjoint if and only if u = v.
Exercise 10.2.7 Let φ be a self-adjoint endomorphism of the euclidean space En ,
</p>
<p>and let the basis B = (e1, . . . , en)made of orthonormal eigenvectors for φ with corre-
sponding eigenvalues (λ1, . . . ,λn) (not necessarily all distinct). A direct computation
</p>
<p>shows that, in the Dirac&rsquo;s notation, the action of φ can be written as
</p>
<p>φ = λ1|e1〉〈e1| + &middot; &middot; &middot; + λn|en〉〈en|,
</p>
<p>so that, for any v &isin; En , one writes
</p>
<p>φ(v) = λ1|e1〉〈e1|v〉 + &middot; &middot; &middot; + λn|en〉〈en|v〉.
</p>
<p>10.3 Orthogonal Projections
</p>
<p>As we saw in Chap. 3, given any vector subspace W &sub; En , with orthogonal com-
plement W&perp; we have a direct sum decomposition En = W &oplus; W&perp;, so for any vector
v &isin; En we have (see the Proposition 3.2.5) a unique decomposition v = vW + vW&perp; .
This suggests the following definition.
</p>
<p>Definition 10.3.1 Given the (canonical) euclidean space En with W &sub; En a vector
subspace and the orthogonal sum decomposition v = vW + vW&perp; , the map
</p>
<p>PW : En &rarr; En, v �&rarr; uW
</p>
<p>is linear, and it is called the orthogonal projection onto the subspace W . The dimen-
</p>
<p>sion of W is called the rank of the orthogonal projection PW .
</p>
<p>If W &sub; En it is easy to see that Im(PW ) = W while ker(PW ) = W&perp;. Moreover,
since PW acts as an identity operator on its range W , one also has P
</p>
<p>2
W = PW . If</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Orthogonal Projections 159
</p>
<p>u, v are vectors in En , with orthogonal sum decomposition u = uW + uW&perp; and v =
vW + vW&perp; , we can explicitly compute
</p>
<p>PW (u) &middot; v = uW &middot; (vW + vW&perp;)
= uW &middot; vW and
</p>
<p>u &middot; PW (v) = (uW + uW&perp;) &middot; vW
= uW &middot; vW .
</p>
<p>This shows that orthogonal projectors are self-adjoint endomorphisms. To which
</p>
<p>extent can one reverse these computations, that is can one characterise, within all
</p>
<p>self-adjoint endomorphisms, the collection of orthogonal projectors? This is the
</p>
<p>content of the next proposition.
</p>
<p>Proposition 10.3.2 Given the euclidean vector space En , an endomorphism
</p>
<p>φ &isin; End(En) is an orthogonal projection if and only if it is self-adjoint and sat-
isfies the condition φ2 = φ.
</p>
<p>Proof We have already shown that the conditions are necessary for an endomorphism
</p>
<p>to be an orthogonal projection in En . Let us now assume that φ is a self-adjoint
</p>
<p>endomorphism fulfilling φ2 = φ. For any choice of u, v &isin; En we have
</p>
<p>((1 &minus; φ)(u)) &middot; φ(v) = u &middot; φ(v) &minus; φ(u) &middot; φ(v)
= u &middot; φ(v) &minus; u &middot; φ2(v)
= u &middot; φ(v) &minus; u &middot; φ(v) = 0
</p>
<p>with the second line coming from the self-adjointness of φ and the third line from
</p>
<p>the condition φ2 = φ. This shows that the vector subspace Im(1 &minus; φ) is orthogo-
nal to the vector subspace Im(φ). We can then decompose any vector y &isin; En as
an orthogonal sum y = yIm(1&minus;φ) + yImφ + ξ, where ξ is an element in the vector
subspace orthogonal to the sum Im(1 &minus; φ)&oplus; Im(φ). For any u &isin; En and any such
vector ξ we have
</p>
<p>φ(u) &middot; ξ = 0, ((1 &minus; φ)(u)) &middot; ξ = 0.
</p>
<p>These conditions give that u &middot; ξ = 0 for any u &isin; En , so we can conclude that ξ = 0.
Thus we have the orthogonal vector space decomposition
</p>
<p>En = Im(1 &minus; φ) &oplus; Im(φ).
</p>
<p>We show next that ker(φ) = Im(1 &minus; φ). If u &isin; Im(1 &minus; φ), we have u = (1 &minus; φ)v
with v &isin; En , thus φ(u) = φ(1 &minus; φ)v = 0, that is Im(1 &minus; φ) &sube; ker(φ). Conversely,
if u &isin; ker(φ), then φ(u) &middot; v = 0 for any v &isin; En , and u &middot; φ(v) = 0, since φ is self-
adjoint, which gives ker(φ) &sube; (Im(φ))&perp; and ker(φ) &sube; Im(1 &minus; φ), from the decom-
position of En above.</p>
<p/>
</div>
<div class="page"><p/>
<p>160 10 Spectral Theorems on Euclidean Spaces
</p>
<p>If w &isin; Im(φ), then w = φ(x) for a given x &isin; En , thus φ(w) = φ2(x) = φ(x) = w.
We have shown that we can identify φ = PIm(φ). This concludes the proof. �
</p>
<p>Exercise 10.3.3 Consider the three dimensional euclidean space E3 with canon-
</p>
<p>ical basis and take W = L((1, 1, 1)). Its orthogonal subspace is given by the
vectors (x, y, z) whose components solve the linear equation � : x + y + z = 0,
so we get S� = W&perp; = L((1,&minus;1, 0), (1, 0,&minus;1)). The vectors of the canonical
basis when expressed with respect to the vectors u1 = (1, 1, 1) spanning W and
u2 = (1,&minus;1, 0), u3 = (1, 0,&minus;1) spanning W&perp;, are written as
</p>
<p>e1 =
1
</p>
<p>3
(u1 + u2 + u3),
</p>
<p>e2 =
1
</p>
<p>3
(u1 &minus; 2u2 + u3),
</p>
<p>e3 =
1
</p>
<p>3
(u1 + u2 &minus; 2u3).
</p>
<p>Therefore,
</p>
<p>PW (e1) =
1
</p>
<p>3
u1, PW (e2) =
</p>
<p>1
</p>
<p>3
u1, PW (e3) =
</p>
<p>1
</p>
<p>3
u1,
</p>
<p>and
</p>
<p>PW&perp;(e1) =
1
</p>
<p>3
(u2 + u3), PW&perp;(e2) =
</p>
<p>1
</p>
<p>3
(&minus;2u2 + u3), PW&perp;(e3) =
</p>
<p>1
</p>
<p>3
(u2 &minus; 2u3).
</p>
<p>Remark 10.3.4 Given an orthogonal space decomposition En = W &oplus; W&perp;, the union
of the basis BW and BW&perp; of W and W
</p>
<p>&perp;, is a basis B for W . It is easy to see that the
</p>
<p>matrix associated to the orthogonal projection operator PW with respect to such a
</p>
<p>basis B has a block diagonal structure
</p>
<p>M
B,B
</p>
<p>PW
=
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>1 &middot; &middot; &middot; 0 0 &middot; &middot; &middot; 0
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 &middot; &middot; &middot; 1 0 &middot; &middot; &middot; 0
0 &middot; &middot; &middot; 0 0 &middot; &middot; &middot; 0
...
</p>
<p>...
...
</p>
<p>...
</p>
<p>0 &middot; &middot; &middot; 0 0 &middot; &middot; &middot; 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>,
</p>
<p>where the order of the diagonal identity block is the dimension of W = Im(PW ). This
makes it evident that an orthogonal projection operator is diagonalisable: its spectrum
</p>
<p>contains the real eigenvalue λ = 1 with multiplicity equal to mλ=1 = dim(W ) and
the real eigenvalue λ = 0 with multiplicity equal to mλ=0 = dim(W&perp;).
</p>
<p>It is clear that the rank of PW (the dimension of W ) is given by the trace tr(M
B,B
</p>
<p>PW
)
</p>
<p>irrespectively of the basis chosen to represent the projection (see the Proposi-</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Orthogonal Projections 161
</p>
<p>tion 9.1.5) since as usual, for a change of basis with matrix MB,C , one has that
</p>
<p>M
C,C
</p>
<p>PW
= MC,B MB,BPW M
</p>
<p>B,C , with MB,C = (MC,B)&minus;1.
</p>
<p>Exercise 10.3.5 The matrix
</p>
<p>M =
(
</p>
<p>a
&radic;
</p>
<p>a &minus; a2&radic;
a &minus; a2 1 &minus; a
</p>
<p>)
</p>
<p>is symmetric and satisfies M2 = M for any a &isin; (0, 1]. With respect to an orthonormal
basis (e1, e2) for E
</p>
<p>2, it is then associated to an orthogonal projection with rank given
</p>
<p>by tr(M) = 1. In order to determine its range, we diagonalise M . Its characteristic
polynomial is
</p>
<p>pM(T ) = det(M &minus; T I2) = T 2 &minus; T
</p>
<p>and the eigenvalues are then λ = 0 and λ = 1. Since they are both simple, the
matrix M is diagonalisable. The eigenspace Vλ=1 corresponding to the range of the
</p>
<p>orthogonal projection is one dimensional and given as the solution (x, y) of the
</p>
<p>system (
a &minus; 1
</p>
<p>&radic;
a &minus; a2&radic;
</p>
<p>a &minus; a2 &minus;a
</p>
<p>) (
x
</p>
<p>y
</p>
<p>)
=
</p>
<p>(
0
</p>
<p>0
</p>
<p>)
,
</p>
<p>that is (x,
</p>
<p>&radic;
1&minus;a
</p>
<p>a
x) with x &isin; R. This means that the range of the projection is given
</p>
<p>by L((1,
</p>
<p>&radic;
1&minus;a
</p>
<p>a
)).
</p>
<p>We leave as an exercise to show that M is the most general rank 1 orthogonal
</p>
<p>projection in E2.
</p>
<p>Exercise 10.3.6 We know from Exercise 10.2.6 that the operator L = |u〉〈u| is self-
adjoint. We compute
</p>
<p>L2 = |u〉〈u|u〉〈u| = ‖u‖2 L .
</p>
<p>Thus such an operator L is an orthogonal projection if and only if ‖u‖ = 1. It is then
the rank one orthogonal projection L = |u〉〈u| = PL(u).
</p>
<p>Let us assume that W1 and W2 are two orthogonal subspaces (to be definite we
</p>
<p>take W2 &sube; W&perp;1 ). By using for instance the Remark 10.3.4 it is not difficult to show
that PW1 PW2 = PW2 PW1 = 0. As a consequence,
</p>
<p>(PW1 + PW2)(PW1 + PW2) = P2W1 + P
2
W2
</p>
<p>+ PW1 PW2 + PW2 PW1 = (PW1 + PW2).
</p>
<p>Since the sum of two self-adjoint endomorphisms is self-adjoint, we can conclude
</p>
<p>(from Proposition 10.3.2) that the sum PW1 + PW2 is an orthogonal projector, with
PW1 + PW2 = PW1&oplus;W2 . This means that with two orthogonal subspaces, the sum of
the corresponding orthogonal projectors is the orthogonal projection onto the direct
</p>
<p>sum of the given subspaces.</p>
<p/>
</div>
<div class="page"><p/>
<p>162 10 Spectral Theorems on Euclidean Spaces
</p>
<p>These results can be extended. If the euclidean space has a finer orthogonal
</p>
<p>decomposition, that is there are mutually orthogonal subspaces {Wa}a=1,...,k with
En = W1 &oplus; &middot; &middot; &middot; &oplus; Wk , then we have a corresponding set of orthogonal projectors
PWa . We omit the proof of the following proposition, which we shall use later on in
</p>
<p>the chapter.
</p>
<p>Proposition 10.3.7 If En = W1 &oplus; &middot; &middot; &middot; &oplus; Wk with mutually orthogonal subspaces
Wa , a = 1, . . . , k, then the following hold:
</p>
<p>(a) For any a, b = 1, . . . , k, one has
</p>
<p>PWa PWb = δab PWa .
</p>
<p>(b) If W̃ = Wa1 &oplus; &middot; &middot; &middot; &oplus; Was is the vector subspace given by the direct sum of the
orthogonal subspaces {Wa j } with a j any subset of (1, . . . , k) without repeti-
tion, then the sum P̃ = PWa1 + . . .+ PWas is the orthogonal projection operator
P̃ = PW̃ .
</p>
<p>(c) For any v &isin; En , one has
</p>
<p>v = (PW1 + &middot; &middot; &middot; + PWk )(v).
</p>
<p>Notice that point (c) shows that the identity operator acting on En can be decom-
</p>
<p>posed as the sum of all the orthogonal projectors corresponding to any orthogonal
</p>
<p>subspace decomposition of En .
</p>
<p>Remark 10.3.8 All we have described for the euclidean space En can be natu-
</p>
<p>rally extended to the hermitian space (Cn, &middot;) introduced in Sect. 3.4. If for example
(e1, . . . , en) gives a hermitian orthonormal basis for H
</p>
<p>n , the orthogonal projection
</p>
<p>onto Wa = L(ea) can be written in the Dirac&rsquo;s notation (see the Exercise 10.3.6) as
</p>
<p>PWa = |ea〉〈ea|,
</p>
<p>while the orthogonal projection onto W̃ = Wa1 &oplus; &middot; &middot; &middot; &oplus; Was (point b) of the Propo-
sition 10.3.7) as
</p>
<p>PW̃ = |ea1〉〈ea1 | + &middot; &middot; &middot; + |eas 〉〈eas |.
</p>
<p>The decomposition of the identity operator can be now written as
</p>
<p>idH n = |e1〉〈e1| + &middot; &middot; &middot; + |en〉〈en|.
</p>
<p>Thus, any vector v &isin; H n can be decomposed as
</p>
<p>v = |v〉 = |e1〉〈e1|v〉 + &middot; &middot; &middot; + |en〉〈en|v〉.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Diagonalization of Self-adjoint Endomorphisms 163
</p>
<p>10.4 The Diagonalization of Self-adjoint Endomorphisms
</p>
<p>The following theorem is a central result for the diagonalization of real symmetric
</p>
<p>matrices.
</p>
<p>Theorem 10.4.1 Let A &isin; Rn,n be symmetric, tA = A. Then, any root of its charac-
teristic polynomial pA(T ) is real.
</p>
<p>Proof Let us assume λ to be a root of pA(T ). Since pA(T ) has real coefficients,
</p>
<p>its roots are in general complex (see the fundamental theorem of algebra, Theo-
</p>
<p>rem A.5.7). We therefore think of A as the matrix associate to an endomorphism
</p>
<p>φ : Cn &minus;&rarr; Cn,
</p>
<p>with M
E,E
</p>
<p>φ = A with respect to the canonical basis E for Cn as a complex vector
space. Let v be a non zero eigenvector for φ, that is
</p>
<p>φ(v) = λv.
</p>
<p>By denoting with X the column of the components of v = (x1, . . . , xn) with respect
to E , we write
</p>
<p>tX = t(x1, . . . , xn), AX = λX.
</p>
<p>Under complex conjugation, with Ā = A since A has real entries, we get
</p>
<p>tX̄ = t (x̄1, . . . , x̄n), AX̄ = λ̄X̄ .
</p>
<p>From these relations we can write the scalar tX̄ AX in the following two ways,
</p>
<p>tX̄ AX = tX̄(AX) = tX̄(λX) = λ (tX̄ X)
and tX̄ AX = (tX̄ A)X = t (AX̄)X = t (λ̄X̄)X = λ̄ (tX̄ X).
</p>
<p>By equating them, we have
</p>
<p>(λ &minus; λ̄) (tX̄ X) = 0.
</p>
<p>The quantity tX̄ X = x̄1x1 + x̄2x2 + &middot; &middot; &middot; + x̄n xn is a positive real number, since
v �= 0Cn ; we can then conclude λ = λ̄, that is λ &isin; R. �
</p>
<p>Example 10.4.2 The aim of this example is threefold, namely
</p>
<p>&bull; it provides an ad hoc proof of the Theorem 10.4.1 for symmetric 2 &times; 2 matrices;
&bull; it provides a direct proof for the Proposition 10.2.2 for symmetric 2 &times; 2 matrices;
&bull; it shows that, if φ is a self-adjoint endomorphism in E2, then E2 has an orthonormal
</p>
<p>basis made up of eigenvectors for φ. This result anticipates the general result which
</p>
<p>will be proven in the Theorem 10.4.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>164 10 Spectral Theorems on Euclidean Spaces
</p>
<p>We consider then a symmetric matrix A &isin; R2,2,
</p>
<p>A =
(
</p>
<p>a11 a12
a12 a22
</p>
<p>)
.
</p>
<p>Its characteristic polynomial pA(T ) = det(A &minus; T I2) is then
</p>
<p>pA(T ) = T 2 &minus; (a11 + a22)T + a11a22 &minus; a212.
</p>
<p>The discriminant of this degree 2 characteristic polynomial pA(T ) is not negative:
</p>
<p>� = (a11 + a22)2 &minus; 4(a11a22 &minus; a212)
= (a11 &minus; a22)2 + 4a212 &ge; 0
</p>
<p>being the sum of two square terms; therefore the roots λ1,λ2 of pA(T ) are both real.
</p>
<p>We prove next that A is diagonalisable, and that the matrix P giving the change of
</p>
<p>basis is orthogonal. We consider the endomorphism φ corresponding to A = ME,Eφ ,
for the canonical basis E for E2, and compute the eigenspaces Vλ1 and Vλ2 .
</p>
<p>&bull; If � = 0, then a11 = a22 and a12 = 0. The matrix A is already diagonal, so we
may take P = I2. There is only one eigenvalue λ1 = a11 = a22. Its algebraic mul-
tiplicity is 2 and its geometric multiplicity is 2, with corresponding eigenspace
</p>
<p>Vλ1 = E2.
&bull; If � &gt; 0 the characteristic polynomial has two simple roots λ1 �= λ2 with corre-
</p>
<p>sponding one dimensional orthogonal (from the Proposition 10.2.2) eigenspaces
</p>
<p>Vλ1 and Vλ2 . The change of basis matrix P , whose columns are the normalised
</p>
<p>eigenvectors
v1
</p>
<p>‖v1‖
and
</p>
<p>v2
</p>
<p>‖v2‖
,
</p>
<p>is then orthogonal by construction. We notice that P can be always chosen to be
</p>
<p>an element in SO(2), since a permutation of its columns changes the sign of its
</p>
<p>determinant, and is compatible with the permutation of the eigenvalue λ1,λ2 in
</p>
<p>the diagonal matrix.
</p>
<p>In order to explicitly compute the matrix P we see that the eigenspace Vλi for any
</p>
<p>i = 1, 2 is given by the solutions of the linear homogeneous system associated to the
matrix
</p>
<p>A &minus; λi I2 =
(
</p>
<p>a11 &minus; λi a12
a12 a22 &minus; λi
</p>
<p>)
.
</p>
<p>Since we already know that dim(Vλi ) = 1, such a linear system is equivalent to a
single linear equation. We can write
</p>
<p>Vλi = {(x, y) : (a11 &minus; λi )x + a12 y = 0}
= L((&minus;a12, a11 &minus; λi )) = L(vi ),</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Diagonalization of Self-adjoint Endomorphisms 165
</p>
<p>where we set
</p>
<p>v1 = (&minus;a12, a11 &minus; λ1), v2 = (&minus;a12, a11 &minus; λ2).
</p>
<p>For the scalar product,
</p>
<p>v1 &middot; v2 = a212 + a
2
11 &minus; (λ1 + λ2)a11 + λ1λ2 = 0
</p>
<p>since one has
</p>
<p>λ1 + λ2 = a11 + a22, λ1λ2 = a11a22 &minus; a212.
</p>
<p>Exercise 10.4.3 We consider again the symmetric matrix
</p>
<p>A =
(
</p>
<p>2 &minus;1
&minus;1 3
</p>
<p>)
</p>
<p>from the Exercise 10.2.4. Its characteristic polynomial is
</p>
<p>pA(T ) = det(A &minus; T I2) = pA(T ) = T 2 &minus; 5T + 5,
</p>
<p>with roots
</p>
<p>λ&plusmn; =
1
</p>
<p>2
(5 &plusmn;
</p>
<p>&radic;
5).
</p>
<p>The corresponding eigenspaces V&plusmn; are the solutions of the homogeneous linear
</p>
<p>systems associated to the matrices
</p>
<p>A &minus; λ&plusmn; I2 =
1
</p>
<p>2
</p>
<p>(
(&minus;1 ∓
</p>
<p>&radic;
5) &minus;2
</p>
<p>&minus;2 (1 &plusmn;
&radic;
</p>
<p>5)
</p>
<p>)
.
</p>
<p>one has dim(V&plusmn;) = 1, so each system is equivalent to a single linear equation, that
is
</p>
<p>V&plusmn; = L((&minus;2, 1 &plusmn;
&radic;
</p>
<p>5) = L(v&plusmn;),
</p>
<p>where
</p>
<p>v+ = (&minus;2, 1 +
&radic;
</p>
<p>5), v&minus; = (&minus;2, 1 &minus;
&radic;
</p>
<p>5),
</p>
<p>and one computes that
</p>
<p>v+ &middot; v&minus; = 4 &minus; 4 = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>166 10 Spectral Theorems on Euclidean Spaces
</p>
<p>that is the eigenspaces are orthogonal. The elements
</p>
<p>u1 =
v+
</p>
<p>‖v+‖
and u2 =
</p>
<p>v&minus;
</p>
<p>‖v&minus;‖
</p>
<p>form an orthonormal basis for E2 of eigenvectors for the endomorphism φA.
</p>
<p>We present now the fundamental result of this chapter, that is the spectral theorem
</p>
<p>for self-adjoint endomorphisms and for symmetric matrices. Towards this, it is worth
</p>
<p>mentioning that the whole theory, presented in this chapter for the euclidean space
</p>
<p>En , can be naturally formulated for any finite dimensional real vector space equipped
</p>
<p>with a scalar product (see Chap. 3).
</p>
<p>Definition 10.4.4 Let φ : V &rarr; V be an endomorphism of the real vector space
V , and let Ṽ &sub; V be a vector subspace in V . If the image of Ṽ for φ is a subset of the
same Ṽ (that is, φ(Ṽ ) &sube; Ṽ ), there is a well defined endomorphism φṼ : Ṽ &rarr; Ṽ
given by
</p>
<p>φṼ (v) = φ(v), for all v &isin; Ṽ
</p>
<p>(clearly a linear map). The endomorphism φṼ acts in the same way as the endomor-
</p>
<p>phism φ, but on a restricted domain. This is why φṼ is called the restriction to Ṽ
</p>
<p>of φ.
</p>
<p>Proposition 10.4.5 (Spectral theorem for endomorphisms) Let (V, &middot;) be a real vec-
tor space equipped with a scalar product, and let φ &isin; End(V ). The endomorphism
φ is self-adjoint if and only if V has an orthonormal basis of eigenvectors for φ.
</p>
<p>Proof Let us assume the orthonormal basis C for V is made of eigenvectors for φ. This
</p>
<p>implies that M
C,C
</p>
<p>φ is diagonal and therefore symmetric. From the Theorem 10.2.3 we
</p>
<p>conclude that φ is self-adjoint.
</p>
<p>The proof of the converse is by induction on n = dim(V ). For n = 2 the state-
ment is true, as we explicitly proved in the Example 10.4.2. Let us then assume it
</p>
<p>to be true for any (n &minus; 1)-dimensional vector space. Then, let us consider a real
n-dimensional vector space (V, &middot;) equipped with a scalar product, and let φ be a
self-adjoint endomorphism on V . With B an orthonormal basis for V (remember
</p>
<p>from the Theorem 3.3.9 that such a basis always exists V finite dimensional), the
</p>
<p>matrix A = MB,Bφ is symmetric (from the Theorem 10.2.3) and thus any root of the
characteristic polynomial pA(T ) is real. Denote by λ one such an eigenvalue for φ,
</p>
<p>with v1 a corresponding eigenvector that we can assume of norm 1.
</p>
<p>Then, let us consider the orthogonal complement to the vector line spanned
</p>
<p>by v1,
</p>
<p>Ṽ = (L(v1))&perp; .
</p>
<p>In order to meaningfully define the restriction to Ṽ of φ, we have to verify that for
</p>
<p>any v &isin; Ṽ one has φ(v) &isin; Ṽ , that is, we have to prove the implication
</p>
<p>v &middot; v1 = 0 &rArr; φ(v) &middot; v1 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 The Diagonalization of Self-adjoint Endomorphisms 167
</p>
<p>By recalling that φ is self-adjoint and φ(v1) = λv1 we can write
</p>
<p>φ(v) &middot; v1 = v &middot; φ(v1) = v &middot; (λv1)
= λ (v &middot; v1) = 0.
</p>
<p>This proves that φ can be restricted to a φṼ : Ṽ &rarr; Ṽ , clearly self-adjoint. Since
dim(Ṽ ) = n &minus; 1, by the inductive assumption there exist n &minus; 1 elements (v2, . . . , vn)
of eigenvectors for φṼ making up an orthonormal basis for Ṽ . Since φṼ is a restriction
</p>
<p>of φ, the elements (v2, . . . , vn) are eigenvectors for φ as well, and orthogonal to v1
as they all belong to Ṽ . Then the elements (v1, v2, . . . , vn) are orthonormal and
</p>
<p>eigenvectors for φ. Being n = dim(V ), they are an orthonormal basis for V . �
</p>
<p>10.5 The Diagonalization of Symmetric Matrices
</p>
<p>There is a counterpart of Proposition 10.4.5 for symmetric matrices.
</p>
<p>Proposition 10.5.1 (Spectral theorem for symmetric matrices) Let A &isin; Rn,n be sym-
metric. There exists an orthogonal matrix P such that tP AP is diagonal. This result is
</p>
<p>often referred to by saying that symmetric matrices are orthogonally diagonalisable.
</p>
<p>Proof Let us consider the endomorphism φ = f E,EA : En &rarr; En , which is self-
adjoint since A is symmetric and E is the canonical basis (see the Theorem 10.2.3).
</p>
<p>From the Proposition 10.4.5, the space En has an orthonormal basis C of eigenvectors
</p>
<p>for φ, so the matrix M
C,C
</p>
<p>φ is diagonal. From Theorem 7.9.9 we can write
</p>
<p>M
C,C
</p>
<p>φ = M
C,E M
</p>
<p>E,E
</p>
<p>φ M
E,C .
</p>
<p>Since M
E,E
</p>
<p>φ = A, by setting P = MC,E we have that P&minus;1 AP is diagonal. The
columns of the matrix P are given by the components with respect to E of the
</p>
<p>elements in C, so P is orthogonal since C is orthonormal. �
</p>
<p>Remark 10.5.2 The orthogonal matrix P can always be chosen in SO(n), since, as
</p>
<p>already mentioned, the sign of its determinant changes under a permutation of two
</p>
<p>columns.
</p>
<p>Exercise 10.5.3 Consider φ &isin; End(R4) given by
</p>
<p>φ((x, y, z, t)) = (x + y, x + y,&minus;z + t, z &minus; t).</p>
<p/>
</div>
<div class="page"><p/>
<p>168 10 Spectral Theorems on Euclidean Spaces
</p>
<p>Its corresponding matrix with respect to the canonical basis E in R4 is given by
</p>
<p>A = ME,Eφ =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 1 0 0
</p>
<p>1 1 0 0
</p>
<p>0 0 &minus;1 1
0 0 1 &minus;1
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Being A symmetric and E orthonormal, than φ is self-adjoint. Its characteristic poly-
</p>
<p>nomial is
</p>
<p>pφ(T ) = pA(T ) = det(A &minus; T I4)
</p>
<p>=
∣∣∣∣
1 &minus; T 1
</p>
<p>1 1 &minus; T
</p>
<p>∣∣∣∣
∣∣∣∣
&minus;1 &minus; T 1
</p>
<p>1 &minus;1 &minus; T
</p>
<p>∣∣∣∣
= T 2(T &minus; 2)(T + 2).
</p>
<p>The eigenvalues are then λ1 = 0 with (algebraic) multiplicity m(0) = 2, λ2 = &minus;2
with m(&minus;2) = 1 and λ2 = 2 with m(2) = 1. The corresponding eigenspaces are
computed to be
</p>
<p>V0 = ker(φ) = L((1,&minus;1, 0, 0), (0, 0, 1, 1)),
V&minus;2 = ker(φ &minus; 2I4) = L((1, 1, 0, 0)),
V2 = ker(φ &minus; I4) = L((0, 0, 1,&minus;1))
</p>
<p>and as we expect, these three eigenspaces are mutually orthogonal, with the two
</p>
<p>basis vectors spanning V0 orthogonal as well. In order to write the matrix P which
</p>
<p>diagonalises A one just needs to normalise such a system of four basis eigenvectors.
</p>
<p>We have
</p>
<p>P =
1
&radic;
</p>
<p>2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 1 0
</p>
<p>&minus;1 0 1 0
0 1 0 1
</p>
<p>0 1 0 &minus;1
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>tP AP =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 0
</p>
<p>0 0 0 0
</p>
<p>0 0 &minus;2 0
0 0 0 2
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>where we have chosen an ordering for the eigenvalues that gives det(P) = 1.
</p>
<p>Corollary 10.5.4 Let φ &isin; End(En). If the endomorphism φ is self-adjoint then it is
simple.
</p>
<p>Proof The proof is immediate. From the Proposition 10.4.5 we know that the self-
</p>
<p>adjointness of φ implies that En has an orthonormal basis of eigenvectors for φ. From
</p>
<p>the Remark 9.2.3 we conclude that φ is simple. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Diagonalization of Symmetric Matrices 169
</p>
<p>Exercise 10.5.5 The converse of the previous corollary does not hold in general.
</p>
<p>Consider for example the endomorphismφ in E2 whose matrix with respect to the
</p>
<p>canonical basis E is
</p>
<p>A =
(
</p>
<p>1 1
</p>
<p>0 &minus;1
</p>
<p>)
.
</p>
<p>An easy calculation gives for the eigenvalues λ1 = 1 e λ2 = &minus;1 and φ is (see the
Corollary 9.4.2) therefore simple. But φ is not self-adjoint, since
</p>
<p>φ(e1) &middot; e2 = (1, 0) &middot; (0, 1) = 0,
e1 &middot; φ(e2) = (1, 0) &middot; (1,&minus;1) = 1,
</p>
<p>or simply because A is not symmetric. The eigenspaces are given by
</p>
<p>V1 = L((1, 0)), V&minus;1 = L((1,&minus;2)),
</p>
<p>and they are not orthogonal. As a further remark, notice that the diagonalising matrix
</p>
<p>P =
(
</p>
<p>1 1
</p>
<p>0 &minus;2
</p>
<p>)
</p>
<p>is not orthogonal.
</p>
<p>What we have shown in the previous exercise is a general property characterising
</p>
<p>self-adjoint endomorphisms within the class of simple endomorphisms, as the next
</p>
<p>theorem shows.
</p>
<p>Theorem 10.5.6 Let φ &isin; End(En) be simple, with Vλ1 , . . . , Vλs the corresponding
eigenspaces. Then φ is self-adjoint if and only if Vλi &perp; Vλ j for any i �= j .
</p>
<p>Proof That the eigenspaces corresponding to distinct eigenvalues are orthogonal for
</p>
<p>a self-adjoint endomorphism comes directly from the Proposition 10.2.2.
</p>
<p>Conversely, let us assume that φ is simple, so that En = Vλ1 &oplus; &middot; &middot; &middot; &oplus; Vλs . The
union of the bases given by applying the Gram-Schmidt orthogonalisation proce-
</p>
<p>dure to an arbitrary basis for each Vλ j , yield an orthonormal basis for E
n , which is
</p>
<p>clearly made of eigenvectors for φ. The statement then follows from the
</p>
<p>Proposition 10.4.5. �
</p>
<p>Exercise 10.5.7 The aim of this exercise is to define (if possible) a self-adjoint
</p>
<p>endomorphismφ : E3 &rarr; E3 such that ker(φ) = L((1, 2, 1)) and λ1 = 1, λ2 = 2
are eigenvalues of φ.
</p>
<p>Since ker(φ) �= {(0, 0, 0)}, then λ3 = 0 is the third eigenvalue for φ, with
ker(φ) = V0. Thus φ is simple since it has three distinct eigenvalues, with E3 =
V1 &oplus; V2 &oplus; V0. In order for φ to be self-adjoint, we have to impose that Vλi &perp; Vλ j ,
for all i �= j . In particular, one has
</p>
<p>(ker(φ))&perp; = (V0)&perp; = V1 &oplus; V2.</p>
<p/>
</div>
<div class="page"><p/>
<p>170 10 Spectral Theorems on Euclidean Spaces
</p>
<p>We compute
</p>
<p>(ker(φ))&perp; = (L((1, 2, 1))&perp;
</p>
<p>= {(α,β,&minus;α &minus; 2β) : α,β &isin; R}
= L((1, 0,&minus;1), (a, b, c))
</p>
<p>where we impose that (a, b, c) belongs to L((1, 2, 1))&perp; and is orthogonal to
</p>
<p>(1, 0,&minus;1). By setting {
(1, 2, 1) &middot; (a, b, c) = 0
(1, 0,&minus;1) &middot; (a, b, c) = 0 ,
</p>
<p>we have (a, b, c) = (1,&minus;1, 1), so we select
</p>
<p>V1 = L((1, 0,&minus;1)), V2 = L((1,&minus;1, 1)).
</p>
<p>Having a simple φ with mutually orthogonal eigenspaces, the endomorphism φ self-
</p>
<p>adjoint. To get a matrix representing φ we can choose the basis in E3
</p>
<p>B = ((1, 0,&minus;1), (1,&minus;1, 1), (1, 2, 1)),
</p>
<p>thus obtaining
</p>
<p>M
B,B
</p>
<p>φ =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 2 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ .
</p>
<p>By defining e1 = (1, 0,&minus;1), e2 = (1,&minus;1, 1) we can write, in the Dirac&rsquo;s notation,
</p>
<p>φ = |e1〉〈e1| + 2|e2〉〈e2|.
</p>
<p>Exercise 10.5.8 This exercise defines a simple, but not self-adjoint, endomor-
</p>
<p>phismφ : E3 &rarr; E3 such that ker(φ) = L((1,&minus;1, 1)) and Im(φ) = (ker(φ))&perp;.
We know that φ has the eigenvalue λ1 = 0 with V0 = ker(φ). For φ to be simple,
</p>
<p>the algebraic multiplicity of the eigenvalueλ1 must be 1, and there have to be two
</p>
<p>additional eigenvalues λ2 and λ3 with either λ2 = λ3 or λ2 �= λ3. If λ2 = λ3, one
has then
</p>
<p>Vλ2 = Im( f ) = (ker( f ))&perp; = (Vλ1)&perp;.
</p>
<p>In such a case, φ would be a simple endomorphism with mutually orthogonal
</p>
<p>eigenspaces for distinct eigenvalues. This would imply φ to be self-adjoint. Thus
</p>
<p>to satisfy the conditions we require for φ we need λ2 �= λ3. In such a case, one has
Vλ2 &oplus; Vλ3 = Im(φ) and also clearly Vλi &perp; V0 for i = 2, 3. In order for φ to be sim-</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Diagonalization of Symmetric Matrices 171
</p>
<p>ple but not self-adjoint, we select the eigenspaces Vλ2 and Vλ3 to be not mutually
</p>
<p>orthogonal subspaces in Im( f ). Since
</p>
<p>Im( f ) = (L((1,&minus;1, 1)))&perp;
</p>
<p>= {(x, y, z) : x &minus; y + z = 0}
= L((1, 1, 0), (0, 1, 1))
</p>
<p>we can choose
</p>
<p>Vλ2 = L((1, 1, 0)), Vλ3 = L((0, 1, 1)).
</p>
<p>If we set B =
(
(1,&minus;1, 1), (1, 1, 0), (0, 1, 1)
</p>
<p>)
(clearly not an orthonormal basis for
</p>
<p>E3), we have
</p>
<p>M
B,B
</p>
<p>φ =
</p>
<p>⎛
⎝
</p>
<p>0 0 0
</p>
<p>0 λ2 0
</p>
<p>0 0 λ3
</p>
<p>⎞
⎠ .
</p>
<p>Exercise 10.5.9 Consider the endomorphism φ : E3 &rarr; E3 whose corresponding
matrix with respect to the basis B =
</p>
<p>(
v1 = (1, 1, 0), v2 = (1,&minus;1, 0), v3 = (0, 0,&minus;1)
</p>
<p>)
is
</p>
<p>M
B,B
</p>
<p>φ =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 2 0
</p>
<p>0 0 3
</p>
<p>⎞
⎠ .
</p>
<p>With E as usual the canonical basis for E3, in this exercise we would like to determine:
</p>
<p>(1) an orthonormal basis C for E3 given by eigenvectors for φ,
</p>
<p>(2) the orthogonal matrix ME,C ,
</p>
<p>(3) the matrix MC,E ,
</p>
<p>(4) the matrix M
E,E
</p>
<p>φ ,
</p>
<p>(5) the eigenvalues of φ with their corresponding multiplicities.
</p>
<p>(1) We start by noticing that, since M
B,B
</p>
<p>φ is diagonal, the basis B is given by eigen-
</p>
<p>vectors of φ, as the action of φ on the basis vectors in B can be clearly written
</p>
<p>as φ(v1) = v1, φ(v2) = 2v2, φ(v3) = 3v3. The basis B is indeed orthogonal, but
not orthonormal, and for an orthonormal basis C of eigenvectors for φ we just
</p>
<p>need to normalize, that is to consider
</p>
<p>u1 =
v1
</p>
<p>‖v1‖
, u2 =
</p>
<p>v2
</p>
<p>‖v2‖
, u3 =
</p>
<p>v3
</p>
<p>‖v3‖
</p>
<p>just obtaining C = ( 1&radic;
2
(1, 1, 0), 1&radic;
</p>
<p>2
(1,&minus;1, 0), (0, 0,&minus;1)). While the existence
</p>
<p>of such a basis C implies that φ is self-adjoint, the self-adjointness of φ could
</p>
<p>not be derived from the matrix M
B,B
</p>
<p>φ , which is symmetric with respect to a basis
</p>
<p>B which is not orthonormal.</p>
<p/>
</div>
<div class="page"><p/>
<p>172 10 Spectral Theorems on Euclidean Spaces
</p>
<p>(2) From its definition, the columns of ME,C are given by the components with
</p>
<p>respect to E of the vectors in C. We then have
</p>
<p>ME,C =
1
&radic;
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1 1 0
</p>
<p>1 &minus;1 0
0 0 &minus;
</p>
<p>&radic;
2
</p>
<p>⎞
⎠ .
</p>
<p>(3) We know that MC,E =
(
ME,C
</p>
<p>)&minus;1
. Since the matrix above is orthogonal, we
</p>
<p>have
</p>
<p>MC,E = t
(
ME,C
</p>
<p>)
=
</p>
<p>1
&radic;
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1 1 0
</p>
<p>1 &minus;1 0
0 0 &minus;
</p>
<p>&radic;
2
</p>
<p>⎞
⎠ .
</p>
<p>(4) From the Theorem 7.9.9 we have
</p>
<p>M
E,E
</p>
<p>φ = M
E,C M
</p>
<p>C,C
</p>
<p>φ M
C,E .
</p>
<p>Since M
C,C
</p>
<p>φ = M
B,B
</p>
<p>φ , the matrix M
E,E
</p>
<p>φ can be now directly computed.
</p>
<p>(5) Clearly, from M
B,B
</p>
<p>φ the eigenvalues for φ are all simple and given by λ = 1, 2, 3.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
</p>
<p>Rotations
</p>
<p>The notion of rotation appears naturally in physics, and is geometrically formulated in
</p>
<p>terms of a euclidean structure as a suitable linear map on a real vector space. The aim
</p>
<p>of this chapter is to analyse the main properties of rotations using the spectral theory
</p>
<p>previously developed, as well as to recover known results from classical mechanics,
</p>
<p>using the geometric language we are describing.
</p>
<p>11.1 Skew-Adjoint Endomorphisms
</p>
<p>In analogy to the Definition 10.2.1 of a self-adjoint endomorphism, we have the
</p>
<p>following.
</p>
<p>Definition 11.1.1 An endomorphism φ of the euclidean vector space En is called
</p>
<p>skew-adjoint if
</p>
<p>φ(v) &middot; w = &minus; v &middot; φ(w), for all v,w &isin; En.
</p>
<p>From the Definition 4.1.7 we call a matrix A = (ai j ) &isin; Rn,n skew-symmetric (or anti-
symmetric) if tA = &minus;A, that is if ai j = &minus;a j i , for any i, j . Notice that the skew-
symmetry condition for A clearly implies for its diagonal elements that ai i = 0. The
following result is an analogous of the Theorem 10.2.3 and can be established in a
</p>
<p>similar manner.
</p>
<p>Theorem 11.1.2 Let φ &isin; End(En) and B an orthonormal basis for En . The endo-
morphism φ is skew-adjoint if and only if M
</p>
<p>B,B
φ is skew-symmetric.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_11
</p>
<p>173</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_11&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_11&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>174 11 Rotations
</p>
<p>Proposition 11.1.3 Let φ &isin; End(En) be skew-adjoint. It holds that
</p>
<p>(a) the euclidean vector space En has an orthogonal decomposition
</p>
<p>En = Im(φ) &oplus; ker(φ),
</p>
<p>(b) the rank of φ is even.
</p>
<p>Proof (a) Let u &isin; En and v &isin; ker(φ). We can write
</p>
<p>0 = u &middot; φ(v) = &minus;φ(u) &middot; v.
</p>
<p>Since this is valid for any u &isin; En , the element φ(u) ranges over the whole space
Im(φ), so we have that ker(φ) = (Im(φ)&perp;.
</p>
<p>(b) From t M
B,B
φ = &minus;M
</p>
<p>B,B
φ , it follows det(M
</p>
<p>B,B
φ ) = (&minus;1)n det(M
</p>
<p>B,B
φ ). Thus a
</p>
<p>skew-adjoint endomorphism on an odd dimensional euclidean space is singu-
</p>
<p>lar (that is it is not invertible). From the orthogonal decomposition for En of
</p>
<p>point (a) we conclude that the restriction φ̃Im(φ) : Im(φ) &rarr; Im(φ̃) is regular
(that is it is invertible). Since such a restriction is skew-adjoint, we have that
</p>
<p>dim(Im(φ)) = dim(Im(φ̃)) = rk(φ) is even. �
</p>
<p>A skew-adjoint endomorphismφ on En can have only the zero as (real) eigenvalue,
</p>
<p>so it is not diagonalisable. Indeed, if λ is an eigenvalue for φ, that is φ(v) = λv for
v �= 0En &isin; En , from the skew-symmetry condition we have that 0 = v &middot; φ(v) = λ v &middot; v,
which implies λ = 0. Also, since its characteristic polynomial has non real roots, it
does not have a Jordan form (see Theorem 9.5.1).
</p>
<p>Although not diagonalisable, a skew-adjoint endomorphism has nonetheless a
</p>
<p>canonical form.
</p>
<p>Proposition 11.1.4 Given a skew-adjoint invertible endomorphismφ : E2p &rarr; E2p,
there exists an orthonormal basis B for E2p with respect to which the representing
</p>
<p>matrix for φ is of the form,
</p>
<p>M
B,B
φ =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0 μ1
&minus;μ1 0
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>0 μp
&minus;μp 0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>with μ j &isin; R for j = 1, . . . , p.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Skew-Adjoint Endomorphisms 175
</p>
<p>Proof The map S = φ2 = φ ◦ φ is a self-adjoint endomorphism on E2p, so there
exists an orthonormal basis of eigenvectors for S. Given S(w j ) = λ jw j withλ j &isin; R,
each eigenvalue λ j has even multiplicity, since the identity
</p>
<p>S(φ(wi )) = φ(S(wi )) = λiφ(wi )
</p>
<p>shows that wi and φ(wi ) are eigenvectors of S with the same eigenvalue λi . We label
</p>
<p>then the spectrum of S by (λ1, . . . ,λk) and the basis C =
(
w1,φ(w1), . . . , wk ,φ(wk)
</p>
<p>)
.
</p>
<p>We also have
</p>
<p>λi = wi &middot; S(wi ) = wi &middot; φ2(wi ) = &minus;φ(wi ) &middot; φ(wi ) = &minus;‖φ(wi )‖2
</p>
<p>and, since we tookφ to be invertible, we haveλi &lt; 0. Define the setB = (e1, . . . , e2p)
of vectors as
</p>
<p>e2 j&minus;1 = w j , e2 j =
1&radic;
|λ j |
</p>
<p>φ(w j )
</p>
<p>for j = 1, . . . , p. A direct computation shows that e j &middot; ek = δ jk with j, k = 1, . . . , 2p
and
</p>
<p>φ(e2 j&minus;1) =
&radic;
|λ j | e2 j , φ(e2 j ) = &minus;
</p>
<p>&radic;
|λ j | e2 j&minus;1.
</p>
<p>Thus B is an orthonormal basis with respect to which the matrix representing the
</p>
<p>endomorphism φ has the form above, with μ j =
&radic;
|λ j |. �
</p>
<p>Corollary 11.1.5 If φ is a skew-adjoint endomorphism on En , then there exists an
</p>
<p>orthonormal basis B for En with respect to which the associated matrix of φ has the
</p>
<p>form
</p>
<p>M
B,B
φ =
</p>
<p>⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
</p>
<p>0 μ1
&minus;μ1 0
</p>
<p>. . .
</p>
<p>. . .
</p>
<p>0 μp
&minus;μp 0
</p>
<p>0
</p>
<p>. . .
</p>
<p>0
</p>
<p>⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
</p>
<p>,
</p>
<p>with μ j &isin; R, j = 1, &middot; &middot; &middot; , p, and 2p &le; n.
</p>
<p>The study of antisymmetric matrices makes it natural to introduce the notion of
</p>
<p>Lie algebra.</p>
<p/>
</div>
<div class="page"><p/>
<p>176 11 Rotations
</p>
<p>Definition 11.1.6 Given A, B &isin; Rn,n , one defines the map [ , ] : Rn,n &times; Rn,n &rarr; Rn,n ,
</p>
<p>[A, B] = AB &minus; B A
</p>
<p>as the commutator of A and B. Using the properties of the matrix product is it easy
</p>
<p>to prove that the following hold, for any A, B,C &isin; Rn,n and any α &isin; R:
</p>
<p>(1) [A, B] = &minus;[B, A], [αA, B] = α[A, B], [A + B,C] = [A,C] + [B,C], that
is the commutator is bilinear and antisymmetric,
</p>
<p>(2) [AB,C] = A[B,C] + [A,C]B,
(3) [A, [B,C]] + [B, [C, A]] + [C, [A, B]] = 0; this is called the Jacoby identity.
</p>
<p>Definition 11.1.7 If W &sube; Rn,n is a vector subspace such that the commutator maps
W &times; W into W , we say that W is a (matrix) Lie algebra. Its rank is the dimension
of W as a vector space.
</p>
<p>Excercise 11.1.8 The collection of all antisymmetric matrices WA &sub; Rn,n is a
matrix Lie algebra since, if tA = &minus;A and tB = &minus;B it is
</p>
<p>t ([A, B]) = t B tA &minus; tA tB = B A &minus; AB.
</p>
<p>As a Lie algebra, it is denoted so(n) and one easily computed its dimension to be
</p>
<p>n(n &minus; 1)/2. As we shall see, this Lie algebra has a deep relation with the orthogonal
group SO(n).
</p>
<p>Remark 11.1.9 It is worth noticing that the vector space WS &sub; Rn,n of symmetric
matrices is not a matrix Lie algebra, since the commutator of two symmetric matrices
</p>
<p>is an antisymmetric matrix.
</p>
<p>Excercise 11.1.10 It is clear that the matrices
</p>
<p>L1 =
</p>
<p>⎛
⎝
</p>
<p>0 0 0
</p>
<p>0 0 &minus;1
0 1 0
</p>
<p>⎞
⎠ , L2 =
</p>
<p>⎛
⎝
</p>
<p>0 0 1
</p>
<p>0 0 0
</p>
<p>&minus;1 0 0
</p>
<p>⎞
⎠ , L3 =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;1 0
1 0 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠
</p>
<p>provide a basis for the three dimensional real vector space of antisymmetric matrices
</p>
<p>WA &sub; R3,3. As the matrix Lie algebra so(3), one computes the commutators:
</p>
<p>[L1, L2] = L3, [L2, L3] = L1, [L3, L1] = L2.
</p>
<p>Excercise 11.1.11 We consider the most general skew-adjoint endomorphismφ on
</p>
<p>E3. With respect to the canonical orthonormal basis E = (e1, e2, e3) it has associated
matrix of the form
</p>
<p>M
E,E
φ =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;γ β
γ 0 &minus;α
&minus;β α 0
</p>
<p>⎞
⎠ = αL1 + βL2 + γL3.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.1 Skew-Adjoint Endomorphisms 177
</p>
<p>with α,β, γ &isin; R. Any vector (x, y, z) in its kernel is a solution of the system
⎛
⎝
</p>
<p>0 &minus;γ β
γ 0 &minus;α
&minus;β α 0
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
</p>
<p>0
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
⎠ .
</p>
<p>It is easy to show that the kernel is one-dimensional with ker(φ) = L((α,β, γ)).
Since φ is defined on a three dimensional space and has a one-dimensional kernel,
</p>
<p>from the Proposition 11.1.4 the spectrum of the map S = φ2 is made of the sim-
ple eigenvalue λ0 = 0 and a multiplicity 2 eigenvalue λ &lt; 0, which is such that
2λ = tr(ME,ES ) with
</p>
<p>M
E,E
S =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;γ β
γ 0 &minus;α
&minus;β α 0
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>0 &minus;γ β
γ 0 &minus;α
&minus;β α 0
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
&minus;γ2 &minus; β2 αβ αγ
</p>
<p>αβ &minus;γ2 &minus; α2 βγ
αγ βγ &minus;β2 &minus; α2
</p>
<p>⎞
⎠ ;
</p>
<p>thus λ = &minus;(α2 + β2 + γ2). For the corresponding eigenspace Vλ &ni; (x, y, x) one
has ⎛
</p>
<p>⎝
α2 αβ αγ
</p>
<p>αβ β2 βγ
</p>
<p>αγ βγ γ2
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
</p>
<p>0
</p>
<p>0
</p>
<p>0
</p>
<p>⎞
⎠ &hArr;
</p>
<p>⎧
⎨
⎩
α(αx + βy + γz) = 0
β(αx + βy + γz) = 0
γ(αx + βy + γz) = 0
</p>
<p>.
</p>
<p>Such a linear system is equivalent to the single equation (αx + βy + γz) = 0, which
shows that ker(S) is orthogonal to Im(S). To be definite, assume α �= 0, and fix as
basis for Vλ
</p>
<p>w1 = (&minus;γ, 0,α),
w2 = φ(w1) = (&minus;αβ,α2 + γ2,&minus;βγ),
</p>
<p>with w1 &middot; φ(w1) = 0. With the appropriate normalization, we define
</p>
<p>u1 =
w1
</p>
<p>‖w1‖
,
</p>
<p>u2 =
φ(w1)
</p>
<p>‖φ(w1)‖
,
</p>
<p>u3 =
1&radic;
</p>
<p>α2 + β2 + γ2
(α,β, γ)
</p>
<p>and verify that C = (u1, u2, u3) is an orthonormal basis for E3. With MC,E the
orthogonal matrix of change of bases (see the Theorem 7.9.9), this leads to
</p>
<p>MC,E M
E,E
φ M
</p>
<p>E,C = MC,Cφ =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ρ 0
ρ 0 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ , ρ = |λ| = α2 + β2 + γ2,
</p>
<p>an example indeed of Corollary 11.1.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>178 11 Rotations
</p>
<p>11.2 The Exponential of a Matrix
</p>
<p>In Sect. 10.1 we studied the properties of the orthogonal group O(n) in En . Before
</p>
<p>studying the spectral properties of orthogonal matrices we recall some general results.
</p>
<p>Definition 11.2.1 Given a matrix A &isin; Rn,n , its exponential is the matrix eA defined
by
</p>
<p>eA =
&infin;&sum;
</p>
<p>k=0
</p>
<p>1
</p>
<p>k!
Ak
</p>
<p>where the sum is defined component-wise, that is (eA) jl =
&sum;&infin;
</p>
<p>k=0
1
k! (A
</p>
<p>k) jl .
</p>
<p>We omit the proof that such a limit exists (that is each series converges) for every
</p>
<p>matrix A, and we omit as well the proof of the following proposition, which lists
</p>
<p>several properties of the exponential maps on matrices.
</p>
<p>Proposition 11.2.2 Given matrices A, B &isin; Rn,n and an invertible matrix P &isin; GL(n),
the following identities hold:
</p>
<p>(a) eA &isin; GL(n), that is the matrix eA is invertible, with (eA)&minus;1 = e&minus;A and
det(eA) = etr A,
</p>
<p>(b) if A = diag(a11, . . . , ann), then eA = diag(ea11 , . . . , eann ),
(c) eP AP
</p>
<p>&minus;1 = PeA P&minus;1,
(d) if AB = B A, that is [A, B] = 0, then eAeB = eBeA = eA+B ,
(e) it is e
</p>
<p>tA = t (eA),
(f) if W &sub; Rn,n is a matrix Lie algebra, the elements eM with M &isin; W form a group
</p>
<p>with respect to the matrix product.
</p>
<p>Excercise 11.2.3 Let as determine the exponential eQ of the symmetric matrix
</p>
<p>Q =
(
</p>
<p>0 a
</p>
<p>a 0
</p>
<p>)
, a &isin; R.
</p>
<p>We can proceed in two ways. On the one hand, it is easy to see that
</p>
<p>Q2k =
(
</p>
<p>a2k 0
</p>
<p>0 a2k
</p>
<p>)
, Q2k+1 =
</p>
<p>(
0 a2k+1
</p>
<p>a2k+1 0
</p>
<p>)
.
</p>
<p>Thus, by using the definition we compute
</p>
<p>eQ =
</p>
<p>⎛
⎜⎝
</p>
<p>&sum;&infin;
k=0
</p>
<p>a2k
</p>
<p>(2k)!
&sum;&infin;
</p>
<p>k=0
a2k+1
</p>
<p>(2k+1)!
</p>
<p>&sum;&infin;
k=0
</p>
<p>a2k+1
</p>
<p>(2k+1)!
&sum;&infin;
</p>
<p>k=0
a2k
</p>
<p>(2k)!
</p>
<p>⎞
⎟⎠ =
</p>
<p>(
cosh a sinh a
</p>
<p>sinh a cosh a
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The Exponential of a Matrix 179
</p>
<p>Alternatively, we can use the identities (c) and (b) in the previous proposition, once
</p>
<p>Q has been diagonalised. It is easy to compute the eigenvalues of Q to be λ&plusmn; = &plusmn;a,
</p>
<p>with diagonalising orthogonal matrix P = 1&radic;
2
</p>
<p>(
1 1
</p>
<p>&minus;1 1
</p>
<p>)
. That is, P�Q P
</p>
<p>&minus;1 = Q with
with �Q = diag(&minus;a, a),
</p>
<p>1
2
</p>
<p>(
1 1
</p>
<p>&minus;1 1
</p>
<p>) (
&minus;a 0
0 a
</p>
<p>) (
1 &minus;1
1 1
</p>
<p>)
=
</p>
<p>(
0 a
</p>
<p>a 0
</p>
<p>)
.
</p>
<p>We then compute
</p>
<p>eQ = eP�Q P&minus;1 = Pe�Q T &minus;1
</p>
<p>= 1
2
</p>
<p>(
1 1
</p>
<p>&minus;1 1
</p>
<p>) (
e&minus;a 0
0 ea
</p>
<p>) (
1 &minus;1
1 1
</p>
<p>)
=
</p>
<p>(
cosh a sinh a
</p>
<p>sinh a cosh a
</p>
<p>)
.
</p>
<p>Notice that det(eQ) = cosh2 a &minus; sinh2 a = 1 = etr Q .
</p>
<p>Excercise 11.2.4 Let us determine the exponential eM of the anti-symmetric matrix
</p>
<p>M =
(
</p>
<p>0 a
</p>
<p>&minus;a 0
</p>
<p>)
, a &isin; R.
</p>
<p>Since M is not diagonalisable, we explicitly compute eM as we did in the previous
</p>
<p>exercise, finding
</p>
<p>M2k = (&minus;1)k
(
</p>
<p>a2k 0
</p>
<p>0 a2k
</p>
<p>)
, M2k+1 = (&minus;1)k
</p>
<p>(
0 a2k+1
</p>
<p>&minus;a2k+1 0
</p>
<p>)
.
</p>
<p>By putting together all terms, one finds
</p>
<p>eM =
</p>
<p>⎛
⎜⎝
</p>
<p>&sum;&infin;
k=0(&minus;1)k a
</p>
<p>2k
</p>
<p>(2k)!
&sum;&infin;
</p>
<p>k=0(&minus;1)k a
2k+1
</p>
<p>(2k+1)!
</p>
<p>&minus;
&sum;&infin;
</p>
<p>k=0(&minus;1)k a
2k+1
</p>
<p>(2k+1)!
&sum;&infin;
</p>
<p>k=0(&minus;1)k a
2k
</p>
<p>(2k)!
</p>
<p>⎞
⎟⎠ =
</p>
<p>(
cos a sin a
</p>
<p>&minus; sin a cos a
</p>
<p>)
.
</p>
<p>We see that if M is a 2 &times; 2 anti-symmetric matrix, the matrix eM is special
orthogonal. This is an example for the point ( f ) in the Proposition 11.2.2.
</p>
<p>Excercise 11.2.5 In order to further explore the relations between anti-symmetric
</p>
<p>matrices and special orthogonal matrices, consider the matrix
</p>
<p>M =
</p>
<p>⎛
⎝
</p>
<p>0 a 0
</p>
<p>&minus;a 0 0
0 0 0
</p>
<p>⎞
⎠ a &isin; R.</p>
<p/>
</div>
<div class="page"><p/>
<p>180 11 Rotations
</p>
<p>In parallel with the computations from the previous exercise, it is immediate to see
</p>
<p>that
</p>
<p>eM =
</p>
<p>⎛
⎝
</p>
<p>cos a sin a 0
</p>
<p>&minus; sin a cos a 0
0 0 1
</p>
<p>⎞
⎠ .
</p>
<p>This hints to the conclusion that eM &isin; SO(3) if M &isin; R3,3 is anti-symmetric.
</p>
<p>The following proposition generalises the results of the exercises above, and
</p>
<p>provides a further example for the claim ( f ) from the Proposition 11.2.2, since the
</p>
<p>set WA &sub; Rn,n of antisymmetric matrices is the (matrix) Lie algebraso(n), as shown
in the exercise 11.1.8.
</p>
<p>Proposition 11.2.6 If M &isin; Rn,n is anti-symmetric, then eM is special orthogonal.
The restriction of the exponential map to the Lie algebra so(n) of anti-symmetric
</p>
<p>matrices is surjective onto SO(n).
</p>
<p>Proof We focus on the first claim which follows from point (a) of Proposition 11.2.2.
</p>
<p>If M &isin; Rn,n is anti-symmetric, tM = &minus;M and tr(M) = 0. Thus t (eM) = etM =
e&minus;M = (eM)&minus;1 and det(eM) = etr(M) = e0 = 1. �
</p>
<p>Remark 11.2.7 As the Exercise 11.2.5 directly shows, the restriction of the expo-
</p>
<p>nential map to the Lie algebra so(n) of anti-symmetric matrices is not injective into
</p>
<p>SO(n).
</p>
<p>In the Example 11.3.1 below, we shall sees explicitly that the exponential map,
</p>
<p>when restricted to 2-dimensional anti-symmetric matrices, is indeed surjective onto
</p>
<p>the group SO(2).
</p>
<p>11.3 Rotations in Two Dimensions
</p>
<p>We study now spectral properties of orthogonal matrices. We start with the orthogonal
</p>
<p>group O(2).
</p>
<p>Example 11.3.1 Let A =
(
</p>
<p>a11 a12
a21 a22
</p>
<p>)
&isin; R2,2. The condition tAA = A tA = I2 is
</p>
<p>equivalent to the conditions for its entries given by
</p>
<p>a211 + a212 = 1
a221 + a222 = 1
a11a21 + a12a22 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Rotations in Two Dimensions 181
</p>
<p>To solve these equations, let us assume a11 �= 0 (the case a22 �= 0 is analogous). We
have then a21 = &minus;(a12a22)/a11 from the third equation while, from the others, we
have
</p>
<p>a222
</p>
<p>(
a212
</p>
<p>a211
+ 1
</p>
<p>)
= 1 &rArr; a222 = a211.
</p>
<p>There are two possibilities.
</p>
<p>&bull; If a11 = a22, it follows that a12 + a21 = 0, so the matrix A can be written as
</p>
<p>A+ =
(
</p>
<p>a b
</p>
<p>&minus;b a
</p>
<p>)
with a2 + b2 = 1,
</p>
<p>and det(A+) = a2 + b2 = 1. One can write a = cosϕ, b = sinϕ, for ϕ &isin; R, so
to get
</p>
<p>A+ =
(
</p>
<p>cosϕ sinϕ
</p>
<p>&minus; sinϕ cosϕ
</p>
<p>)
.
</p>
<p>&bull; If a11 = &minus;a22, it follows that a12 = a21, so the matrix A can be written as
</p>
<p>A&minus; =
(
</p>
<p>a b
</p>
<p>b &minus;a
</p>
<p>)
with a2 + b2 = 1,
</p>
<p>and we can write
</p>
<p>A&minus; =
(
</p>
<p>cosϕ sinϕ
</p>
<p>sinϕ &minus; cosϕ
</p>
<p>)
</p>
<p>with det(A&minus;) = &minus;a2 + b2 = &minus;1.
</p>
<p>Finally, it is easy to see that a11 = 0 would imply a22 = 0 and a212 = a221 = 1. These
four cases correspond to ϕ = &plusmn;π
</p>
<p>2
for A+ or A&minus;, according to wether a12 = &minus;a21 or
</p>
<p>a12 = a21 respectively.
</p>
<p>We see that A+ makes up the special orthogonal group SO(2), while A&minus; the orthog-
onal transformations in E2 which in physics are usually called improper rotations.
</p>
<p>Given the 2π-periodicity of the trigonometric functions, we see that any element in
</p>
<p>the special orthogonal group SO(2) corresponds bijectively to an angle ϕ &isin; [0, 2π).
On the other hand, any improper orthogonal transformation can be factorised as
</p>
<p>the product of a SO(2) matrix times the matrix Q = diag(1,&minus;1),
(
</p>
<p>cosϕ sinϕ
</p>
<p>sinϕ &minus; cosϕ
</p>
<p>)
=
</p>
<p>(
1 0
</p>
<p>0 &minus;1
</p>
<p>)(
cosϕ sinϕ
</p>
<p>&minus; sinϕ cosϕ
</p>
<p>)
.
</p>
<p>Thus, an improper orthogonal transformation &lsquo;reverses&rsquo; one of the axis of any given
</p>
<p>orthogonal basis for E2 and so changes its orientation.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 11 Rotations
</p>
<p>Remark 11.3.2 Being O(2) a group, the product of two improper orthogonal trans-
</p>
<p>formations is a special orthogonal transformation. We indeed compute
</p>
<p>(
cosϕ sinϕ
</p>
<p>sinϕ &minus; cosϕ
</p>
<p>) (
cosϕ&prime; sinϕ&prime;
</p>
<p>sinϕ&prime; &minus; cosϕ&prime;
)
=
</p>
<p>(
cos(ϕ&prime; &minus; ϕ) sin(ϕ&prime; &minus; ϕ)
&minus; sin(ϕ&prime; &minus; ϕ) cos(ϕ&prime; &minus; ϕ)
</p>
<p>)
&isin; SO(2).
</p>
<p>Proposition 11.3.3 A matrix A &isin; SO(2) is diagonalisable if and only if A = &plusmn;I2.
An orthogonal matrix A with det(A) = &minus;1 is diagonalisable, with spectrum given
by λ = &plusmn;1.
</p>
<p>Proof From the previous example we have:
</p>
<p>(a) The eigenvalues λ for a special orthogonal matrix are given by the solutions of
</p>
<p>the equation
</p>
<p>pA+(T ) = (cosϕ&minus; T )2 + sin2 ϕ = T 2 &minus; 2(cosϕ) T + 1 = 0,
</p>
<p>which are λ&plusmn; = cosϕ &plusmn;
&radic;
</p>
<p>cos2 ϕ &minus; 1. This shows that A+ is diagonalisable
if and only if cos2 = 1, that is A+ = &plusmn;I2.
</p>
<p>(b) Improper orthogonal matrices A&minus; turn to be diagonalisable since they are sym-
metric. The eigenvalue equation is
</p>
<p>pA&minus; = (T + cosϕ)(T &minus; cosϕ)&minus; sin2 ϕ = T 2 &minus; 1 = 0,
</p>
<p>giving λ&plusmn; = &plusmn;1. �
</p>
<p>11.4 Rotations in Three Dimensions
</p>
<p>We move to the analysis of rotations in three dimensional spaces.
</p>
<p>Excercise 11.4.1 From the Exercise 11.1.11 we know that the anti-symmetric matri-
</p>
<p>ces in R3,3 form a three dimensional vector space, thus any anti-symmetric matrix
</p>
<p>M is labelled by a triple (α,β, γ) of real parameters. The vector a = (α,β, γ) is the
generator, with respect the canonical basis E of E3, of the kernel of the endomorphism
</p>
<p>φ associated to M with respect to the basis E , M = ME,Eφ .
Moreover, from the same exercise we know that there exists an orthogonal
</p>
<p>matrix P which reduces M to its canonical form (see Corollary 11.1.5), that is
</p>
<p>M = P aM P&minus;1 with
</p>
<p>aM =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ρ 0
ρ 0 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ , and ρ2 = α2 + β2 + γ2,</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Rotations in Three Dimensions 183
</p>
<p>with respect to an orthonormal basis C for E3 such that P = ME,C , the matrix of
change of basis. From the Exercise 11.2.5 it is
</p>
<p>SO(3) &ni; eaM =
</p>
<p>⎛
⎝
</p>
<p>cos ρ &minus; sin ρ 0
sin ρ cos ρ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ , (11.1)
</p>
<p>and, if R = eM , from the Proposition 11.2.2 one has R = PeaM P&minus;1.
The only real eigenvalue of the orthogonal transformation eaM is then λ = 1, cor-
</p>
<p>responding to the 1-dimensional eigenspace spanned by the vector a = (α,β, γ). The
vector lineL(a) is therefore left unchanged by the isometryφ of E3 corresponding
</p>
<p>to the matrix R, that is such that M
E,E
φ = R.
</p>
<p>From the Proposition 11.2.6 we know that given R &isin; SO(3), there exists an anti-
symmetric matrix M &isin; R3,3 such that R = eM . The previous exercise gives then the
proof of the following theorem.
</p>
<p>Theorem 11.4.2 For any matrix R &isin; SO(3) with R �= I3 there exists an orthonor-
mal basis B on E3 with respect to which the matrix R has the form (11.1).
</p>
<p>This theorem, that is associated with the name of Euler, can also be stated as
</p>
<p>follow:
</p>
<p>Theorem 11.4.3 Any special orthogonal matrix R &isin; SO(3) has the eigenvalue +1.
</p>
<p>Those isometries φ &isin; End(E3)whose representing matrices ME,Eφ with respect to
an orthonormal basis E are special orthogonal are also called 3-dimensional rotation
</p>
<p>endomorphisms or rotations tout court. With a language used for the euclidean affine
</p>
<p>spaces (Chap. 15), we then have:
</p>
<p>&bull; For each rotation R of E3 there exists a unique vector line (a direction) which is
left unchanged by the action of the rotation. Such a vector line is called the rotation
</p>
<p>axis.
</p>
<p>&bull; The width of the rotation around the rotation axis is given by an angle ρ obtained
from (11.1), and implicitly given by
</p>
<p>1 + 2 cos ρ = tr
</p>
<p>⎛
⎝
</p>
<p>cos ρ &minus; sin ρ 0
sin ρ cos ρ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ = tr P&minus;1 R P = trR, (11.2)
</p>
<p>from the cyclic property of the trace.
</p>
<p>Excercise 11.4.4 Consider the rotation of E3 whose matrix E = (e1, e2.e3) is
</p>
<p>R =
</p>
<p>⎛
⎝
</p>
<p>cosα sinα 0
</p>
<p>&minus; sinα cosα 0
0 0 1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 cosβ sin β
</p>
<p>0 &minus; sin β cosβ
</p>
<p>⎞
⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>184 11 Rotations
</p>
<p>with respect to the canonical basis. Such a matrix is the product R = R1 R2 of two
special orthogonal matrices. The matrix R1 is a rotation by an angle α with rota-
</p>
<p>tion axis the vector lineL(e1) and angular width α, while R2 is a rotation by the
</p>
<p>angle β with rotation axisL(e3). We wish to determine the rotation axis for R with
</p>
<p>corresponding angle. A direct calculation yields
</p>
<p>R =
</p>
<p>⎛
⎝
</p>
<p>cosα sinα cosβ sinα sin β
</p>
<p>&minus; sinα cosα cosβ cosα sin β
0 &minus; sin β cosβ
</p>
<p>⎞
⎠ .
</p>
<p>Since R �= I3 for α �= 0 and β �= 0, the rotation axis is given by the eigenspace
corresponding to the eigenvalue λ = 1. This eigenspace is found to be spanned by
the vector v with
</p>
<p>v =
(
</p>
<p>sinα(1 &minus; cosβ), (cosα&minus; 1)(1 &minus; cosβ), sin β(1 &minus; cosα)
)
</p>
<p>if α �= 0, β �= 0,
v = (1, 0, 0) if α = 0,
v = (0, 0, 1) if β = 0.
</p>
<p>The rotation angleρ can be obtained (implicitly) from the Eq. (11.2) as
</p>
<p>1 + 2 cos ρ = tr(R) = cosα+ cosβ + cosα cosβ.
</p>
<p>Excercise 11.4.5 Since the special orthogonal group SO(n) is non abelian for n &gt; 2,
</p>
<p>for the special orthogonal matrix given by R&prime; = R2 R1 one has R&prime; �= R. The matrix
R&prime; can be written as
</p>
<p>R&prime; =
</p>
<p>⎛
⎝
</p>
<p>cosα sinα 0
</p>
<p>&minus; sinα cosβ cosα cosβ sin β
sinα sin β &minus; sin β cosα cosβ
</p>
<p>⎞
⎠ .
</p>
<p>One now computes that while the rotation angle is the same as in the previous exercise,
</p>
<p>the rotation axis is spanned by the vector v&prime; with
</p>
<p>v&prime; =
(
</p>
<p>sinα sin β, (1 &minus; cosα) sin β, (1 &minus; cosα)(1 + cosβ)
)
</p>
<p>if α �= 0, β �= 0,
v&prime; = (1, 0, 0) if α = 0,
v&prime; = (0, 0, 1) if β = 0.
</p>
<p>Excercise 11.4.6 Consider the matrix R&prime;&prime; = Q1 Q2 given by
</p>
<p>R&prime;&prime; =
</p>
<p>⎛
⎝
</p>
<p>cosα sinα 0
</p>
<p>sinα &minus; cosα 0
0 0 1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 cosβ&prime; sin β&prime;
</p>
<p>0 sin β&prime; &minus; cosβ&prime;
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Rotations in Three Dimensions 185
</p>
<p>Now neither Q1 nor Q2 are (proper) rotation matrix: both Q1 and Q2 are in O(3), but
</p>
<p>det(Q1) = det(Q2) = &minus;1 (see the Example 11.3.1, where O(2) has been described,
and the Remark 11.3.2). The matrix R&prime;&prime; is nonetheless special orthogonal since O(3)
is a group and det(R&prime;&prime;) = 1.
</p>
<p>One finds that the rotation axis is the vector line spanned by the vector v&prime;&prime; with
</p>
<p>v&prime;&prime; =
(
</p>
<p>sinα sin β&prime;, (1 &minus; cosα) sin β&prime;, (1 &minus; cosα)(1 &minus; cosβ&prime;)
)
</p>
<p>if α �= 0, β&prime; �= 0,
v&prime;&prime; = (1, 0, 0) if α = 0,
v&prime;&prime; = (0, 0, 1) if β&prime; = 0.
</p>
<p>One way to establish this result without doing explicit computation, is to observe that
</p>
<p>R&prime;&prime; is obtained from R&prime; in Exercise 11.4.5 under a transposition and the identification
β&prime; = π &minus; β.
Excercise 11.4.7 As an easy application of the Theorem 10.1.13 we know that, if
</p>
<p>B = (u1, u2, u3) and C = (v1, v2, v3) are orthonormal bases in E3, then the orthog-
onal endomorphism φ mapping vk �&rarr; uk is represented by a matrix whose entry �ab
is given by the scalar product ub &middot; va
</p>
<p>M
C,C
φ = � =
</p>
<p>(
�ab = ub &middot; va
</p>
<p>)
a,b=1,2,3
</p>
<p>.
</p>
<p>It is easy indeed to see that the matrix element (t��)ks is given by
</p>
<p>3&sum;
</p>
<p>a=1
�ak�as =
</p>
<p>3&sum;
</p>
<p>a=1
(ua &middot; vk)(ua &middot; vs) = vk &middot; vs = δks
</p>
<p>thus proving that � is orthogonal. Notice that M
B,B
φ = t� = �&minus;1.
</p>
<p>Excercise 11.4.8 Let E = (e1, e2, e3) be an orthonormal basis for E3. We compute
the rotation matrix corresponding to the change of basisE &rarr; B withB = (u1, u2, u3)
for any given basis B with the same orientation (see the Definition 10.1.15) of E .
</p>
<p>Firstly, consider a vector u of norm 1. Since such a vector defines a point on a
</p>
<p>sphere of radius 1 in the three dimensional physical space S, which can be identified
</p>
<p>by a latitude and a longitude, its components with respect to E are determined by
</p>
<p>two angles. With respect to Figure 11.1 we write them as
</p>
<p>u = (sinϕ sin θ,&minus; cosϕ sin θ, cos θ)
</p>
<p>with θ &isin; (0,π) and ϕ &isin; [0, 2π). Then, to complete u to an orthonormal basis for E3
with u&prime;3 = u, one finds,
</p>
<p>u&prime;1 = uN = (cosϕ, sinϕ, 0),
u&prime;2 = (&minus; sinϕ cos θ, cosϕ cos θ, sin θ).</p>
<p/>
</div>
<div class="page"><p/>
<p>186 11 Rotations
</p>
<p>Fig. 11.1 The Euler angles
</p>
<p>The rotation matrix (with respect to the basis E) of the transformation
</p>
<p>E &rarr; (u&prime;1, u&prime;2, u&prime;3) is given by
</p>
<p>R&prime;(θ,ϕ) =
</p>
<p>⎛
⎝
</p>
<p>cosϕ &minus; sinϕ cos θ sinϕ sin θ
sinϕ cosϕ cos θ &minus; cosϕ sin θ
</p>
<p>0 sin θ cos θ
</p>
<p>⎞
⎠ .
</p>
<p>Since the choice of u&prime;1, u
&prime;
2 is unique up to a rotation around the orthogonal vector u,
</p>
<p>we see that the most general SO(3) rotation matrix mapping e3 &rarr; u is given by
</p>
<p>R(θ,ϕ,ψ) = R(θ,ϕ)
</p>
<p>⎛
⎝
</p>
<p>cosψ &minus; sinψ 0
sinψ cosψ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>=
</p>
<p>⎛
⎝
</p>
<p>cosϕ cosψ &minus; sinϕ cos θ sinψ &minus; cosϕ sinψ &minus; sinϕ cos θ cosψ sinϕ cos θ
sinϕ cosψ + cosϕ cos θ sinψ &minus; sinϕ sinψ + cosϕ cos θ cosψ &minus; cosϕ sin θ
</p>
<p>sin θ sinψ sin θ cosψ cos θ
</p>
<p>⎞
⎠
</p>
<p>with ψ &isin; [0, 2π). This shows that the proper 3-dimensional rotations, that is the
group SO(3), can be parametrised by 3 angles. Such angles are usually called Euler
</p>
<p>angles, and clearly there exist several (consistent and equivalent) different choices
</p>
<p>for them.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Rotations in Three Dimensions 187
</p>
<p>Our result depends on the assumption that sin θ �= 0, which means that u1 �= &plusmn;e3
(this corresponds to the case when u1 is the north-south pole direction). The most gen-
</p>
<p>eral rotation matrix representing an orthogonal transformation withe1 &rarr; u1 = &plusmn;e3
is given by
</p>
<p>R(ψ) =
</p>
<p>⎛
⎝
</p>
<p>0 cosψ ∓ sinψ
0 sinψ &plusmn; cosψ
&plusmn;1 0 0
</p>
<p>⎞
⎠ .
</p>
<p>We finally remark that the rotation matrix R(θ,ϕ,ψ) can be written as the product
</p>
<p>R(θ,ϕ,ψ) =
</p>
<p>⎛
⎝
</p>
<p>cosϕ &minus; sinϕ 0
sinϕ cosϕ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 cos θ &minus; sin θ
0 sin θ cos θ
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>cosψ &minus; sinψ 0
sinψ cosψ 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ .
</p>
<p>This identity shows that we can write
</p>
<p>R(θ,β,ψ) = eϕL3 eθL1 eψL3 .
</p>
<p>where L1 and L3 are the matrices in Exercise 11.1.10. These matrices are the &lsquo;gen-
</p>
<p>erators&rsquo; of the rotations around the first and third axis, respectively.
</p>
<p>In applications to the dynamics of a rigid body, with reference to the Figure 11.1,
</p>
<p>the angle ϕ parametrises the motion of precession of the axis u3 around the axis e3,
</p>
<p>the angle θ the motion of nutation of the axis u3 and the angle ϕ the intrinsic rotation
</p>
<p>around the axis u3. The unit vector uN indicates the line of nodes, the intersection of
</p>
<p>the plane (e1e2) with the plane (u1u2).
</p>
<p>We close this section by listing the most interesting properties of orthogonal
</p>
<p>endomorphisms in En with n &gt; 0. Endomorphisms φ whose representing matrix
</p>
<p>M
E,E
φ are special orthogonal, with respect to an orthonormal basis E for E
</p>
<p>n , are called
</p>
<p>rotations. From the Proposition 11.2.6 we know that there exists an anti-symmetric
</p>
<p>matrix M such that M
E,E
φ = eM . When rk(M) = 2k, the matrix eM depends on k
</p>
<p>angular variables.
</p>
<p>From the Corollary 11.1.5 and a direct generalisation of the computations above,
</p>
<p>one can conclude that for each n-dimensional rotation:
</p>
<p>&bull; There exists a vector subspace V &sub; En which is left unchanged by the action of
the rotation, with dim(V ) = n &minus; rk(M).
</p>
<p>&bull; Since rk(M) is even, we have that, if n is odd, then V is odd dimensional as well,
and at least one dimensional. If n is even and the matrix M is invertible, then V is
</p>
<p>the null space.</p>
<p/>
</div>
<div class="page"><p/>
<p>188 11 Rotations
</p>
<p>11.5 The Lie Algebra so(3)
</p>
<p>We have a closer look at the Lie algebraso(3) introduced in the Exercise 11.1.10. As
</p>
<p>mentioned, it is three dimensional and generated by the three matrices
</p>
<p>L1 =
</p>
<p>⎛
⎝
</p>
<p>0 0 0
</p>
<p>0 0 &minus;1
0 1 0
</p>
<p>⎞
⎠ , L2 =
</p>
<p>⎛
⎝
</p>
<p>0 0 1
</p>
<p>0 0 0
</p>
<p>&minus;1 0 0
</p>
<p>⎞
⎠ , L3 =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;1 0
1 0 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ ,
</p>
<p>which are closed under matrix commutator.
</p>
<p>Consider the three dimensional euclidean totally antisymmetric Levi-Civita sym-
</p>
<p>bol εa1a2a3 with indices a j = 1, 2, 3 and defined by
</p>
<p>εa1a2a3 =
</p>
<p>⎧
⎨
⎩
+1 if (a1, a2, a3) is an even permutation of (1, 2, 3)
&minus;1 if (a1, a2, a3) is an odd permutation of (1, 2, 3) .
0 if any two indices are equal
</p>
<p>One has the identity
&sum;3
</p>
<p>a=1 εabcǫaks = (δbkδcs &minus; δbsδck).
</p>
<p>Excercise 11.5.1 Using the Levi-Civita symbol, it is easy to see that the generators
</p>
<p>La have components given by
</p>
<p>(La)mn = εman,
</p>
<p>while their commutators are written as
</p>
<p>[Lm, Ln] =
3&sum;
</p>
<p>a=1
εmna La .
</p>
<p>There is an important subtlety when identifying 3 &times; 3 antisymmetric matrices
with three dimensional vectors. The most general antisymmetric matrix in indeed
</p>
<p>characterised by three scalars,
</p>
<p>A =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;v3 v2
v3 0 &minus;v1
&minus;v2 v1 0
</p>
<p>⎞
⎠ =
</p>
<p>3&sum;
</p>
<p>a=1
va La
</p>
<p>For the time being, this only defines a triple of numbers (v1, sv2, v3) in E
3.
</p>
<p>Whether this triple provides the components of a vector in the three dimensional
</p>
<p>euclidean space, will depend on how it transforms under an orthonormal transforma-
</p>
<p>tion. Now, we may think of A as the matrix, with respect to the canonical orthonormal
</p>
<p>basis E of a skew-adjoint endomorphismφ on E3: A = ME,Eφ . When changing basis
to an orthonormal basis B with matrix of change of basis R = ME,B &isin; O(3), the
matrix A is transformed to</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Lie Algebra so(3) 189
</p>
<p>A&prime; = R AR&minus;1 = R A tR,
</p>
<p>since R is orthogonal and thus R&minus;1 = tR. Since A&prime; is antisymmetric as well, it
can be written as A&prime; =
</p>
<p>&sum;3
a,b=1 v
</p>
<p>&prime;
a La for some (v
</p>
<p>&prime;
1, v
</p>
<p>&prime;
2, v
</p>
<p>&prime;
3). In order to establish the
</p>
<p>transformation rule from (v1, v2, v3) to (v
&prime;
1, v
</p>
<p>&prime;
2, v
</p>
<p>&prime;
3), we need an additional result on
</p>
<p>orthogonal matrices.
</p>
<p>Excercise 11.5.2 Using the expression in Sect. 5.3 for the inverse of an invertible
</p>
<p>matrix, the orthogonality condition for a matrix R &isin; O(3), that is
Rab = (tR)ba = (R&minus;1)ba , can be written as
</p>
<p>Rab =
1
</p>
<p>det R
(&minus;1)a+b det(R̂ab),
</p>
<p>where R̂ab is the 2 dimensional matrix obtained by deleting the row a and the column
</p>
<p>b in the 3 dimensional matrix R. (Then det(R̂ab is the minor of the element Rab, see
</p>
<p>the Definition 5.1.7.) In terms of the Levi-Civita symbol this identity transform to
</p>
<p>3&sum;
</p>
<p>j=1
εmjn R jq =
</p>
<p>1
</p>
<p>det R
</p>
<p>3&sum;
</p>
<p>a,b=1
Rmaεaqb Rnb, (11.3)
</p>
<p>or, being t R orthogonal as well, with det R = det t R,
</p>
<p>3&sum;
</p>
<p>j=1
εmjn Rq j =
</p>
<p>1
</p>
<p>det R
</p>
<p>3&sum;
</p>
<p>a,b=1
Ramεaqb Rbn. (11.4)
</p>
<p>Going back to A =
&sum;3
</p>
<p>a=1 va La and A
&prime; =
</p>
<p>&sum;3
a,b=1 v
</p>
<p>&prime;
a La , we have for their com-
</p>
<p>ponents:
</p>
<p>Amn =
3&sum;
</p>
<p>j=1
v j εmjn and A
</p>
<p>&prime;
mn =
</p>
<p>3&sum;
</p>
<p>j=1
v&prime;j εmjn.
</p>
<p>We then compute, using the relation (11.3),
</p>
<p>A&prime;mn = (R A tR)mn =
3&sum;
</p>
<p>a,b=1
Rma Aab Rnb
</p>
<p>=
3&sum;
</p>
<p>j=1
</p>
<p>3&sum;
</p>
<p>a,b=1
v j εajb Rma Rnb
</p>
<p>= (det R)
3&sum;
</p>
<p>j=1
</p>
<p>3&sum;
</p>
<p>c=1
Rcjv j εmcn = (det R)
</p>
<p>3&sum;
</p>
<p>c=1
(Rv)c εmcn,</p>
<p/>
</div>
<div class="page"><p/>
<p>190 11 Rotations
</p>
<p>that is,
</p>
<p>v&prime;j = (det R) (Rv) j = (det R)
3&sum;
</p>
<p>c=1
Rcjv j .
</p>
<p>This shows that, under an orthogonal transformation between different bases of
</p>
<p>E3, the components of an antisymmetric matrix transforms as the components of a
</p>
<p>vector only if the orientation is preserved, that is only if the transformation is special
</p>
<p>orthogonal.
</p>
<p>Using a terminology from physics, elements in E3 whose components with respect
</p>
<p>to orthonormal basis transform as the general theory (see the Proposition 7.9.2) pre-
</p>
<p>scribes are called polar vectors (or vectors tout court), while elements in E3 whose
</p>
<p>components transform as the components of an antisymmetric matrix are called axial
</p>
<p>(or pseudo) vectors.
</p>
<p>An example of an axial vector is given by the vector product in E3 of two (polar)
</p>
<p>vector, that we recall from the Chap. 1. To be definite, let us start with the canonical
</p>
<p>orthonormal basisE . Ifv = (v1, v2, v3) andw = (w1, w2, w3), the Proposition 1.3.15
define the vector product of v and w as,
</p>
<p>τ (v,w) = v &and; w = (v2w3 &minus; v3w2, v3w1 &minus; v1w3, v1w2 &minus; v2w1).
</p>
<p>Using the Levi-Civita symbol, the components are written as
</p>
<p>(v &and; w)a =
3&sum;
</p>
<p>b,c=1
εabc vb wc.
</p>
<p>If R = ME,B &isin; O(3) is the change of basis to a new orthonormal basis B for E3, on
one hand we have (v &and; w)&prime;q = (R(v &and; w))q while the relation (11.4) yields,
</p>
<p>(v&prime; &and; w&prime;)q =
3&sum;
</p>
<p>k, j=1
εqk jv
</p>
<p>&prime;
kw
</p>
<p>&prime;
j =
</p>
<p>3&sum;
</p>
<p>k, j,b,s=1
εqk j Rkb R jsvbws
</p>
<p>= (det R)
3&sum;
</p>
<p>a,b,s=1
Rqaεabs vbws = (det R)(v &and; w)&prime;q .
</p>
<p>This shows that the components of a vector product transforms as an axial vector
</p>
<p>under an orthogonal transformation between different bases of E3. In a similar man-
</p>
<p>ner one shows that the vector product of an axial vector with a polar vector, is a polar
</p>
<p>vector.
</p>
<p>Excercise 11.5.3 For example, the change of basis from B to B&prime; = (b&prime;1 = &minus;b1,
b&prime;2 = &minus;b2, b&prime;3 = &minus;b3) is clearly represented by the matrix MB,B
</p>
<p>&prime; = MB&prime;,B = &minus;I3
which is orthogonal but not special orthogonal. It is immediate to see that we have</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 The Lie Algebra so(3) 191
</p>
<p>v = (&minus;v1,&minus;v2,&minus;v3)B&prime; and w = (&minus;w1,&minus;w2,&minus;w3)B&prime; , but v &and; w = (v2w3 &minus; v3w2,
v3w1 &minus; v1w3, v1w2 &minus; v2w1)B&prime; .
</p>
<p>From the Example 1.3.17 we see that, since the physical observables position, veloc-
</p>
<p>ity, acceleration and force are described by polar vectors, both momenta and angular
</p>
<p>momenta for the dynamics of a point mass are axial vectors.
</p>
<p>Excercise 11.5.4 We recall from Sect. 1.4 the action of the operator rot on a vector
</p>
<p>field A(x),
</p>
<p>rot A = &nabla; &and; A =
3&sum;
</p>
<p>i, j,k=1
(εi jk&part; j Ak) ei
</p>
<p>with respect to an orthonormal basis E = (e1, e2, e3) of E3 which represents the
physical space S. This identity shows that, if A is a polar vector (field) then rot A is
</p>
<p>an axial vector (field).
</p>
<p>Example 11.5.5 The (Lorentz) force F acting on a point electric charge q whose
</p>
<p>motion is given by x(t), in the presence of an electric field E(x) and a magnetic field
</p>
<p>B(x) is written as
</p>
<p>F = q(E + ẋ &and; B).
</p>
<p>We conclude that E is a polar vector field, while B is an axial vector field. Indeed,
</p>
<p>the correct way to describe B is with an antisymmetric 3 &times; 3 matrix.
</p>
<p>11.6 The Angular Velocity
</p>
<p>When dealing with rotations in physics, an important notion is that of angular veloc-
</p>
<p>ity. This and several related notions can be analysed in terms of the spectral prop-
</p>
<p>erties of orthogonal matrices that we have illustrated above. It is worth recalling
</p>
<p>from Chap. 1 that euclidean vector spaces with orthonormal bases are the natural
</p>
<p>framework for the notion of cartesian orthogonal coordinate systems for the physical
</p>
<p>space S (inertial reference frames).
</p>
<p>Example 11.6.1 Consider the motion x(t) in E3 of a point mass such that its distance
</p>
<p>‖x(t)‖ from the origin of the coordinate system is fixed. We then consider a fixed
orthonormal basis E = (e1, e2, e3), and a orthonormal basis E &prime; = (e&prime;1(t), e&prime;2(t), e&prime;3(t))
which rotates with respect to E in such a way that the components of x(t) along E &prime;
</p>
<p>do not depend on time &mdash; the point mass is at rest with respect to E &prime;. We can write
the position vector x(t) as
</p>
<p>x(t) =
3&sum;
</p>
<p>a=1
xa(t)ea =
</p>
<p>3&sum;
</p>
<p>k=1
x &prime;ke
</p>
<p>&prime;
k(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>192 11 Rotations
</p>
<p>Since E &prime; depends on time, the change of the basis is given by a time-dependent
orthogonal matrix ME,E
</p>
<p>&prime;(t) = R(t) &isin; SO(3) as
</p>
<p>xk(t) =
3&sum;
</p>
<p>j=1
Rk j (t)x
</p>
<p>&prime;
j .
</p>
<p>By differentiating with respect to time t (recall that the dot means time derivative),
</p>
<p>with ẋ &prime;j = 0, the above relation gives,
</p>
<p>ẋk =
3&sum;
</p>
<p>a=1
Ṙa x
</p>
<p>&prime;
a =
</p>
<p>3&sum;
</p>
<p>a,b=1
Ṙka(R
</p>
<p>&minus;1)abxb =
3&sum;
</p>
<p>a,b=1
Ṙka(
</p>
<p>tR)abxb.
</p>
<p>From the relation R(t) tR(t) = I3 it follows that, by differentiating with respect to t ,
</p>
<p>Ṙ tR + R tṘ = 0 &rArr;
Ṙ tR = &minus;R (tṘ) = &minus; t (Ṙ tR)
</p>
<p>We see that the matrix Ṙ t R is antisymmetric, so from the Exercise 11.1.11 there
</p>
<p>exist real scalars (ω1(t),ω2(t),ω3(t)) such that
</p>
<p>Ṙ tR =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3(t) ω2(t)
ω3(t) 0 &minus;ω1(t)
&minus;ω2(t) ω1(t) 0
</p>
<p>⎞
⎠ . (11.5)
</p>
<p>A comparison with the Example 1.3.17 than shows that the expression for the velocity,
</p>
<p>⎛
⎝
</p>
<p>ẋ1
ẋ2
ẋ3
</p>
<p>⎞
⎠ = Ṙ tR
</p>
<p>⎛
⎝
</p>
<p>x1
x2
x3
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
</p>
<p>0 &minus;ω3(t) ω2(t)
ω3(t) 0 &minus;ω1(t)
&minus;ω2(t) ω1(t) 0
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>x1
x2
x3
</p>
<p>⎞
⎠ ,
</p>
<p>can be written as
</p>
<p>ẋ(t) = ω(t) &and; x(t). (11.6)
</p>
<p>The triple ω(t) =
(
ω1(t),ω2(t),ω3(t)
</p>
<p>)
is the angular velocity vector of the motion
</p>
<p>described by the rotation R(t).
</p>
<p>As we shall see in the Exercise 11.7.1, this relation also describes the rotation of
</p>
<p>a rigid body with a fixed point.
</p>
<p>Excercise 11.6.2 The velocity corresponding to the motion in E3 given by (here
</p>
<p>r &gt; 0)
</p>
<p>x(t) =
(
r cosα(t), r sinα(t), 0
</p>
<p>)
</p>
<p>with respect to an orthonormal basis E is</p>
<p/>
</div>
<div class="page"><p/>
<p>11.6 The Angular Velocity 193
</p>
<p>ẋ(t) = α̇
(
&minus; r sinα(t), r cosα(t), 0
</p>
<p>)
= ω(t) &and; x(t)
</p>
<p>with ω(t) = (0, 0, α̇).
</p>
<p>From the Sect. 11.5, we know that the angular velocity is an axial vector, so we
</p>
<p>write
</p>
<p>ωa(t) �&rarr; ω&prime;b(t) = (det P)
3&sum;
</p>
<p>a=1
Pabωa(t).
</p>
<p>for the transformation of the components under a change of basis in E3 given by
</p>
<p>an orthogonal matrix P &isin; O(3). Notice that the relation (11.6) shows that the vector
ẋ(t), although expressed via an axial vector, is a polar vector, since the vector product
</p>
<p>between an axial vector and a polar vector yields a polar vector. This is consistent
</p>
<p>with the formulation of ẋ(t) as the physical velocity of a point mass.
</p>
<p>A different perspective on these notions and examples, allows one to study how
</p>
<p>the dynamics of a point mass is described with respect to different reference systems,
</p>
<p>in physicists&rsquo; parlance.
</p>
<p>Example 11.6.3 We describe the motion of a point mass with respect to an orthonor-
</p>
<p>mal basis E = (e1, e2, e3) and with respect to an orthonormal basis E &prime;(t) = (e&prime;1(t),
e&prime;2(t), e
</p>
<p>&prime;
3(t)) that rotates with respect to E . So we write
</p>
<p>x(t) =
3&sum;
</p>
<p>a=1
xa(t) ea =
</p>
<p>3&sum;
</p>
<p>k=1
x &prime;k(t) e
</p>
<p>&prime;
k(t).
</p>
<p>Considering the time derivative of both sides, we have
</p>
<p>ẋ(t) =
3&sum;
</p>
<p>a=1
ẋa(t) ea =
</p>
<p>3&sum;
</p>
<p>k=1
ẋ &prime;k(t) e
</p>
<p>&prime;
k(t) +
</p>
<p>3&sum;
</p>
<p>k=1
x &prime;k ė
</p>
<p>&prime;
k(t).
</p>
<p>Using the results of the Example 11.6.1, the second term can be written by means of
</p>
<p>an angular velocity ω(t) and thus we have
</p>
<p>ẋ(t) = ẋ&prime;(t) + ω(t) &and; x&prime;(t),
</p>
<p>where v = ẋ is the velocity of the point mass with respect to E , while v&prime; = ẋ&prime; is the
velocity of the point mass with respect to E &prime;(t).
</p>
<p>With one step further along the same line, by taking a second time derivative
</p>
<p>results in
</p>
<p>ẍ(t) = ẍ&prime;(t) + ω(t) &and; ẋ&prime;(t) + ω(t) &and;
(
ẋ&prime;(t) + ω(t) &and; x&prime;(t)
</p>
<p>)
+ ω̇(t) &and; x&prime;(t)
</p>
<p>= ẍ&prime;(t) + 2ω(t) &and; ẋ&prime;(t) + ω(t) &and;
(
ω(t) &and; x&prime;(t)
</p>
<p>)
+ ω̇(t) &and; x&prime;(t).</p>
<p/>
</div>
<div class="page"><p/>
<p>194 11 Rotations
</p>
<p>Using the language of physics, the term ẋ&prime;(t) is the acceleration of the point mass
with respect to the &lsquo;observer&rsquo; at rest E , while ẋ&prime;(t) gives its acceleration with respect
to the moving &lsquo;observer&rsquo; E &prime;(t).
</p>
<p>With the rotation of E &prime;(t) with respect to E given in terms of the angular veloc-
ityω(t), the term
</p>
<p>aC = 2ω(t) &and; ẋ&prime;(t)
</p>
<p>is called the Coriolis acceleration, the term
</p>
<p>aR = ω(t) &and;
(
ω(t) &and; x&prime;(t)
</p>
<p>)
</p>
<p>is the radial (that is parallel to x&prime;(t)) acceleration, while the term
</p>
<p>aT = ω̇(t) &and; x&prime;(t)
</p>
<p>is the tangential (that is orthogonal to x&prime;(t)) one, and depending on the variation of
the angular velocity.
</p>
<p>11.7 Rigid Bodies and Inertia Matrix
</p>
<p>Example 11.7.1 Consider a system of point masses {m( j)} j=1,...,N whose mutual
distances in E3 is constant, so that it can be considered as an example of a rigid
</p>
<p>body. The dynamics of each point mass is described by vectors x( j)(t).
</p>
<p>If we do not consider rigid translations, each motion x( j)(t) is a rotation with
</p>
<p>the same angular velocityω(t) around a fixed point. If we assume, with no loss of
</p>
<p>generality, that the fixed point coincides with the centre of mass of the system, and
</p>
<p>we set it to be the origin of E3, then the total angular momentum of the system (the
</p>
<p>natural generalization of the angular momentum defined for a single point mass in
</p>
<p>the Example 1.3.17) is given by (using (11.6))
</p>
<p>L(t) =
N&sum;
</p>
<p>j=1
m( j)x( j)(t) &and; ẋ( j)(t) =
</p>
<p>N&sum;
</p>
<p>j=1
m( j)x( j)(t) &and;
</p>
<p>(
ω(t) &and; x( j)(t)
</p>
<p>)
.
</p>
<p>With an orthonormal basis E = (e1, e2, e3) for E3, so that x( j) = (x( j)1, x( j)2, x( j)3)
and using the definition of vector product in terms of the Levi-Civita symbol, it is
</p>
<p>straightforward to compute that L = (L1, L2, L3) is given by
</p>
<p>Lk =
3&sum;
</p>
<p>s=1
</p>
<p>{ N&sum;
</p>
<p>i=1
m( j)(‖x( j)‖2δks &minus; x( j)k x( j)s)
</p>
<p>}
ωs .</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Rigid Bodies and Inertia Matrix 195
</p>
<p>(In order to lighten notations, we drop for this example the explicit t dependence on
</p>
<p>the maps.) This expression can be written as
</p>
<p>Lk =
3&sum;
</p>
<p>s=1
Iks ωs
</p>
<p>where the quantities
</p>
<p>Iks =
N&sum;
</p>
<p>j=1
m( j)
</p>
<p>(
‖x( j)‖2δks &minus; x( j)k x( j)s
</p>
<p>)
</p>
<p>are the entries of the so called inertia matrix I (or inertia tensor) of the rigid body
</p>
<p>under analysis.
</p>
<p>It is evident that the inertia matrix is symmetric, so from the Proposition 10.5.1,
</p>
<p>there exists an orthonormal basis for E3 of eigenvectors for it. Moreover, if λ is an
</p>
<p>eigenvalue with eigenvector u, we have
</p>
<p>λ‖u‖2 =
3&sum;
</p>
<p>k,s=1
Iksukus =
</p>
<p>N&sum;
</p>
<p>j=1
m( j)
</p>
<p>(
‖u‖2‖x( j)‖2 &minus; (u &middot; x( j))2
</p>
<p>)
&ge; 0
</p>
<p>where the last relation comes from the Schwarz inequality of Proposition 3.1.8.
</p>
<p>This means that I has no negative eigenvalues. If (u1, u2, u3) is the orthonormal
</p>
<p>basis for which the inertia matrix is diagonal, and (λ1,λ2,λ3) are the corresponding
</p>
<p>eigenvalues, the vector lines L(ua) are the so called principal axes of inertia for the
</p>
<p>rigid body, while the eigenvalues are the moments of inertia.
</p>
<p>We give some basic examples for the inertia matrix of a rigid body.
</p>
<p>Excercise 11.7.2 Consider a rigid body given by two point masses with
</p>
<p>m(1) = αm(2) = αm with α &gt; 0, whose position is given in E3 by the vectors
x(1) = (0, 0, r) and x(2) = (0, 0,&minus;αr) with r &gt; 0. The corresponding inertia matrix
is found to be
</p>
<p>I = α(1 + α)mr2
⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>0 0 0
</p>
<p>⎞
⎠ .
</p>
<p>The principal axes of inertia coincide with the vector lines spanned by the orthonormal
</p>
<p>basis E . The rigid body has two non zero momenta of inertia; the third momentum
</p>
<p>of inertia is zero since the rigid body is one dimensional.
</p>
<p>Consider a rigid body given by three equal masses m( j) = m and
</p>
<p>x(1) = (r, 0, 0), x(2) =
1
</p>
<p>2
(&minus;r,
</p>
<p>&radic;
3r, 0), x(3) =
</p>
<p>1
</p>
<p>2
(&minus;r,&minus;
</p>
<p>&radic;
3r, 0)</p>
<p/>
</div>
<div class="page"><p/>
<p>196 11 Rotations
</p>
<p>with r &gt; 0, with respect to an orthonormal basis E in E3. The inertia matrix is
</p>
<p>computed to be
</p>
<p>I =
3mr2
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>0 0 2
</p>
<p>⎞
⎠ ,
</p>
<p>so the basis elements E provide the inertia principal axes.
</p>
<p>Finally, consider a rigid body in E3 consisting of four point masses with m( j) = m
and
</p>
<p>x(1) = (r, 0, 0), x(2) = (&minus;r, 0, 0), x(3) = (0, r, 0), x(4) = (0,&minus;r, 0)
</p>
<p>with r &gt; 0. The inertia matrix is already diagonal with respect to E whose basis
</p>
<p>elements give the principal axes of inertia for the rigid body, while the momenta of
</p>
<p>inertia is
</p>
<p>I = 2mr2
⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 1 0
</p>
<p>0 0 2
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
</p>
<p>Spectral Theorems on Hermitian Spaces
</p>
<p>In this chapter we shall extend to the complex case some of the notions and results
</p>
<p>of Chap. 10 on euclidean spaces, with emphasis on spectral theorems for a natural
</p>
<p>class of endomorphisms.
</p>
<p>12.1 The Adjoint Endomorphism
</p>
<p>Consider the vector space Cn and its dual space Cn&lowast;, as defined in Sect. 8.1. The
duality between Cn and Cn&lowast; allows one to define, for any endomorphism φ of Cn ,
its adjoint.
</p>
<p>Definition 12.1.1 Given φ : Cn &rarr; Cn , the map φ&dagger; : ω &isin; Cn&lowast; �&rarr; φ&dagger;(ω) &isin; Cn&lowast;
defined by
</p>
<p>(φ&dagger;(ω))(v) = ω(φ(v)) (12.1)
</p>
<p>for any ω &isin; Cn&lowast; and any v &isin; Cn is called the adjoint to φ.
</p>
<p>Remark 12.1.2 From the linearity of φ and ω it follows that φ&dagger; is linear, so φ&dagger; &isin;
End(Cn&lowast;).
</p>
<p>Example 12.1.3 Let B = (b1, b2) be a basis for C2, with B&lowast; = (β1,β2) its dual basis
for C2&lowast;. If φ is the endomorphism given by
</p>
<p>φ : b1 �&rarr; kb1 + b2
φ : b2 �&rarr; b2,
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_12
</p>
<p>197</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_12&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_12&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>198 12 Spectral Theorems on Hermitian Spaces
</p>
<p>with k &isin; C, we see from the definition of adjoint that
</p>
<p>φ&dagger;(β1) : b1 �&rarr; β1(φ(b1)) = k
φ&dagger;(β1) : b2 �&rarr; β1(φ(b2)) = 0
φ&dagger;(β2) : b1 �&rarr; β2(φ(b1)) = 1
φ&dagger;(β2) : b2 �&rarr; β2(φ(b2)) = 1.
</p>
<p>The (linear) action of the adjoint map φ&dagger; to φ is then
</p>
<p>φ&dagger; : β1 �&rarr; kβ1
φ&dagger; : β2 �&rarr; β1 + β2.
</p>
<p>Consider now the canonical hermitian space H n = (Cn, &middot;), that is the vector space
C
</p>
<p>n with the canonical hermitian product (see Sect. 3.4). As described in Sect. 8.2,
</p>
<p>the hermitian product allows one to identify Cn&lowast; with Cn . Under such identification,
the defining relation for φ&dagger; can be written as
</p>
<p>(φ&dagger;u) &middot; v = u &middot; (φ v) or equivalently 〈φ&dagger;(u) | v〉 = 〈u|φ(v)〉
</p>
<p>for any u, v &isin; Cn , so that φ&dagger; is an endomorphism of H n = (Cn, &middot;).
</p>
<p>Definition 12.1.4 Given a matrix A = (ai j ) &isin; Cn,n , its adjoint A&dagger; &isin; Cn,n is the
matrix whose entries are given by (A&dagger;)ab = aba .
</p>
<p>Thus, adjoining a matrix is the composition of two compatible involutions, the
</p>
<p>transposition and the complex conjugation.
</p>
<p>Exercise 12.1.5 Clearly
</p>
<p>A =
(
</p>
<p>1 α
</p>
<p>0 β
</p>
<p>)
</p>
<p>&rArr; A&dagger; =
(
</p>
<p>1 0
</p>
<p>ᾱ β̄
</p>
<p>)
</p>
<p>.
</p>
<p>Exercise 12.1.6 By using the matrix calculus we described in the previous chapters,
</p>
<p>it comes as no surprise that the following relations hold.
</p>
<p>(A&dagger;)&dagger; = A,
(AB)&dagger; = B&dagger; A&dagger;,
(A + αB)&dagger; = (A&dagger; + ᾱB&dagger;)
</p>
<p>for any A, B &isin; Cn,n and α &isin; C. The second line indeed parallels the Remark 8.2.1.
If we have two endomorphisms φ,ψ &isin; End(H n), one has
</p>
<p>〈(φψ)&dagger;(u)|v〉 = 〈u|φψ(v)〉 = 〈φ&dagger;(u)|ψ(v)〉 = 〈ψ&dagger;φ&dagger;(u)|v〉,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 The Adjoint Endomorphism 199
</p>
<p>for any u, v &isin; H n . With α &isin; C, it is also
</p>
<p>〈(φ+ αψ)&dagger;u|v〉 = 〈u|(φ+ αψ)v〉 = 〈φ(u)|v〉 + α〈ψ(u)|v〉 = 〈(φ&dagger; + ᾱψ&dagger;)(u)|v〉.
</p>
<p>Again using the properties of the hermitian product together with the definition
</p>
<p>of adjoint, it is
</p>
<p>〈(φ&dagger;)&dagger;u|v〉 = 〈u|φ&dagger;(v)〉 = 〈φ(u)|v〉
</p>
<p>The above lines establish the following identities
</p>
<p>(φ&dagger;)&dagger; = φ,
(φψ)&dagger; = ψ&dagger;φ&dagger;,
(φ+ αψ)&dagger; = φ&dagger; + ᾱψ&dagger;
</p>
<p>which are the operator counterpart of the matrix identities described above.
</p>
<p>Definition 12.1.7 An endomorphism φ on H n is called
</p>
<p>(a) self-adjoint, or hermitian, if
</p>
<p>φ = φ&dagger;,
</p>
<p>that is if 〈φ(u)|v〉 = 〈u|φ(v)〉 for any u, v &isin; H n ,
(b) unitary, if
</p>
<p>φφ&dagger; = φ&dagger;φ = In,
</p>
<p>that is if 〈φ(u)|φ(v)〉 = 〈u|v〉 for any u, v &isin; H n ,
(c) normal, if φφ&dagger; = φ&dagger;φ.
</p>
<p>In parallel to these, a matrix A &isin; Cn,n is called
(a) self-adjoint, or hermitian, if A&dagger; = A,
(b) unitary, if AA&dagger; = A&dagger; A = In ,
(c) normal, if AA&dagger; = A&dagger; A.
</p>
<p>Remark 12.1.8 Clearly the condition of unitarity for φ is equivalent to the condition
</p>
<p>φ&dagger; = φ&minus;1. Also, both unitary and self-adjoint endomorphisms are normal. From the
Remark 12.1.6 it follows that for any endomorphism ψ, the compositions ψψ&dagger; and
</p>
<p>ψ&dagger;ψ are self-adjoint.
</p>
<p>Remark 12.1.9 The notion of adjoint of an endomorphism can be introduced also
</p>
<p>on euclidean spaces En , where it is identified, at a matrix level, by the transposition.
</p>
<p>Then, it is clear that the notion of self-adjointness in H n generalises that in En , since
</p>
<p>if A = tA in En , then A = A&dagger; in H n , while orthogonal matrices in En are unitary
matrices in H n with real entries.
</p>
<p>The following theorem is the natural generalisation for hermitian spaces of a
</p>
<p>similar result for euclidean spaces. Its proof, that we omit, mimics indeed that of the
</p>
<p>Theorem 10.1.11.</p>
<p/>
</div>
<div class="page"><p/>
<p>200 12 Spectral Theorems on Hermitian Spaces
</p>
<p>Theorem 12.1.10 Let C be an orthonormal basis for the hermitian vector space H n
</p>
<p>and let B be any other basis. The matrix MC,B of the change of basis from C to B is
</p>
<p>unitary if and only if B is orthonormal.
</p>
<p>The following proposition, gives an ex-post motivation for the definitions above.
</p>
<p>Proposition 12.1.11 If E is the canonical basis for H n , with φ &isin; End(H n), it holds
that
</p>
<p>M
E,E
</p>
<p>φ&dagger;
= (ME,Eφ )
</p>
<p>&dagger;
</p>
<p>Proof Let E = (e1, . . . , en) be the canonical basis for H n . If ME,Eφ &isin; Cn,n is the
matrix that represents the action of φ on H n with respect to the basis E , its entries
</p>
<p>are given (see 8.7) by
</p>
<p>(M
E,E
</p>
<p>φ )ab = 〈ea|φ(eb)〉.
</p>
<p>By denoting φab = (ME,Eφ )ab, the action of φ is given by φ(ea) =
&sum;n
</p>
<p>b=1 φbaeb, so
we can compute
</p>
<p>(M
E,E
</p>
<p>φ&dagger;
)ab = 〈ea|φ&dagger;(eb)〉 = 〈φ(ea)|eb〉 =
</p>
<p>n
&sum;
</p>
<p>c=1
〈φcaec|eb〉 = φba
</p>
<p>As an application of this proposition, the next proposition also generalises to
</p>
<p>hermitian spaces analogous results proven in Chap. 10 for euclidean spaces.
</p>
<p>Proposition 12.1.12 The endomorphism φ on H n is self-adjoint (resp. unitary, resp.
</p>
<p>normal) if and only if there exists an orthonormal basis B for H n with respect to
</p>
<p>which the matrix M
B,B
</p>
<p>φ is self-adjoint (resp. unitary, resp. normal).
</p>
<p>Exercise 12.1.13 Consider upper triangular matrices in C2,2,
</p>
<p>M =
(
</p>
<p>a b
</p>
<p>0 c
</p>
<p>)
</p>
<p>&rArr; M&dagger; =
(
</p>
<p>ā 0
</p>
<p>b̄ c̄
</p>
<p>)
</p>
<p>.
</p>
<p>One explicitly computes
</p>
<p>M M&dagger; =
(
</p>
<p>aā + bb̄ bc̄
cb̄ bb̄ + cc̄
</p>
<p>)
</p>
<p>, M&dagger; M =
(
</p>
<p>aā bā
</p>
<p>ab̄ bb̄ + cc̄
</p>
<p>)
</p>
<p>,
</p>
<p>and the matrix M is normal, M M&dagger; = M&dagger; M , if and only if bb̄ = 0 &hArr; b = 0. Thus
an upper triangular matrix in 2-dimension is normal if and only if it is diagonal. In
</p>
<p>such a case, the matrix is self-adjoint if the diagonal entries are real, and unitary if
</p>
<p>the diagonal entries have norm 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.1 The Adjoint Endomorphism 201
</p>
<p>Exercise 12.1.14 We consider the following family of matrices in C2,2,
</p>
<p>M =
(
</p>
<p>a b
</p>
<p>c 0
</p>
<p>)
</p>
<p>, M&dagger; =
(
</p>
<p>ā c̄
</p>
<p>b̄ 0
</p>
<p>)
</p>
<p>.
</p>
<p>It is
</p>
<p>M M&dagger; =
(
</p>
<p>aā + bb̄ ac̄
cā +cc̄
</p>
<p>)
</p>
<p>, M&dagger; M =
(
</p>
<p>aā + cc̄ bā
ab̄ +bb̄
</p>
<p>)
</p>
<p>.
</p>
<p>The conditions for which M is normal are
</p>
<p>bb̄ = cc̄, ac̄ = bā.
</p>
<p>These are solved by b = Reiβ, c = Reiγ, A = |A|eiα with 2α = (β + γ) mod 2π,
where R &gt; 0 and |A| &gt; 0 are arbitrary moduli for complex numbers.
</p>
<p>Exercise 12.1.15 With the Dirac&rsquo;s notation as in (8.6), an endomorphism φ and its
</p>
<p>adjoint are written as
</p>
<p>φ =
n
</p>
<p>&sum;
</p>
<p>a,b=1
φab |ea〉〈eb| and φ&dagger; =
</p>
<p>n
&sum;
</p>
<p>a,b=1
φba |ea〉〈eb|
</p>
<p>with φab = 〈ea|φ(eb)〉 = (ME,Eφ )ab with respect to the orthonormal basis
E = (e1, . . . , en).
</p>
<p>With u = (u1, &middot; &middot; &middot; , un) and v = (v1, . . . , vn) vectors in H n we have the endo-
morphism P = |u〉〈v|. If we decompose the identity endomorphism (see the point
(c) from the Proposition 10.3.7) as
</p>
<p>id =
n
</p>
<p>&sum;
</p>
<p>s=1
|es〉〈es |
</p>
<p>we can write
</p>
<p>P = |u〉〈v| =
n
</p>
<p>&sum;
</p>
<p>ab=1
|ea〉〈ea|u〉〈v|eb〉〈eb| =
</p>
<p>n
&sum;
</p>
<p>ab=1
Pab |ea〉〈eb|
</p>
<p>with Pab = uavb = 〈ea|P(eb)〉. Clearly then
</p>
<p>P&dagger; = |v〉〈u|.</p>
<p/>
</div>
<div class="page"><p/>
<p>202 12 Spectral Theorems on Hermitian Spaces
</p>
<p>Example 12.1.16 Let φ an endomorphism H n with matrix M
E,E
</p>
<p>φ with respect to the
</p>
<p>canonical orthonormal basis, thus (M
E,E
</p>
<p>φ )ab = 〈ea|φ(eb)〉. If B = (b1, . . . , bn) is a
second orthonormal basis for H n , we have two decompositions
</p>
<p>id =
n
</p>
<p>&sum;
</p>
<p>k=1
|ek〉〈ek | =
</p>
<p>n
&sum;
</p>
<p>s=1
|es〉〈es |.
</p>
<p>Thus, by inserting these two expressions of the identity operators, we have
</p>
<p>〈ea|φ(eb)〉 =
n
</p>
<p>&sum;
</p>
<p>k,s=1
〈ea|bk〉〈bk |φ(bs)〉〈bs |eb〉,
</p>
<p>giving in components,
</p>
<p>(M
E,E
</p>
<p>φ )ab =
n
</p>
<p>&sum;
</p>
<p>k,s=1
〈ea|bk〉(MB,Bφ )ks〈bs |eb〉.
</p>
<p>The matrix of the change of basis from E to B has entries 〈ea|bk〉 = (ME,B)ak , with
its inverse matrix entries given by (MB,E)sb = 〈bs |eb〉. From the previous examples
we see that
</p>
<p>(MB,E&dagger;)ak = (MB,E)ka = 〈bk |ea〉 = 〈ea|bk〉 = (ME,B)ak
</p>
<p>thus finding that the change of basis is given by a unitary matrix.
</p>
<p>Proposition 12.1.17 For any endomorphism φ in H n , there is an orthogonal vector
</p>
<p>space decomposition
</p>
<p>H n = Im(φ) &oplus; ker(φ&dagger;)
</p>
<p>Proof If u is any vector in H n , the vector φ(u) cover over all of Im(φ), so the
</p>
<p>condition 〈φ(u)|w〉 = 0 characterises the elements w &isin; (Im(φ))&perp;. It is now easy to
compute
</p>
<p>0 = 〈φ(u)|w〉 = 〈u|φ&dagger;(w)〉.
</p>
<p>Since u is arbitrary and the hermitian product is not degenerate, we have
</p>
<p>ker(φ&dagger;) = (Im(φ))&perp;. �</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Spectral Theory for Normal Endomorphisms 203
</p>
<p>12.2 Spectral Theory for Normal Endomorphisms
</p>
<p>We prove a few results for normal endomorphisms which will be useful for spectral
</p>
<p>theorems.
</p>
<p>Proposition 12.2.1 Let φ be a normal endomorphism of H n .
</p>
<p>(a) With u &isin; H n , we can write
</p>
<p>‖φ(u)‖2 = 〈φ(u)|φ(u)〉 = 〈u|φ&dagger;φ(u)〉 = 〈u|φφ&dagger;(u)〉 = 〈φ&dagger;(u)|φ&dagger;(u)〉 = ‖φ&dagger;(u)‖2.
</p>
<p>Since the order of these computations can be reversed, we have the following
</p>
<p>characterisation.
</p>
<p>φφ&dagger; = φ&dagger;φ &hArr; ‖φ(u)‖ = ‖φ&dagger;(u)‖ for all u &isin; H n.
</p>
<p>(b) From this it also follows that ker(φ) = ker(φ&dagger;). So from the Proposition 12.1.17,
we have the following orthogonal decomposition,
</p>
<p>H n = Im(φ) &oplus; ker(φ).
</p>
<p>(c) Clearly (φ&minus; λI ) is a normal endomorphism if φ is such. This gives
ker(φ&minus; λI ) = ker(φ&dagger; &minus; λ̄I ), meaning that if λ is an eigenvalue of a normal
endomorphism φ, then λ̄ is an eigenvalue for φ&dagger;, with the same eigenspaces.
</p>
<p>(d) Let λ,&micro; be two distinct eigenvalues for φ, with φ(v) = λv and φ(w) = &micro;w.
Then we have
</p>
<p>(λ&minus; &micro;)〈v|w〉 = 〈λ̄v|w〉 &minus; 〈v|&micro;w〉 = 〈φ&dagger;(v)|w〉 &minus; 〈v|φ(w)〉 = 0.
</p>
<p>We can conclude that the eigenspaces corresponding to distinct eigenvalues for
</p>
<p>a normal endomorphism are mutually orthogonal. ⊓⊔
</p>
<p>We are ready to characterise a normal operator in terms of its spectral properties.
</p>
<p>The proof of the following result generalises to hermitian spaces the proof of the
</p>
<p>Theorem 10.4.5 on the diagonalization of symmetric endomorphisms on euclidean
</p>
<p>spaces.
</p>
<p>Theorem 12.2.2 An endomorphism φ of H n is normal if and only there exists an
</p>
<p>orthonormal basis for H n made of eigenvectors for φ.
</p>
<p>Proof If B = (b1, . . . , bn) is an orthonormal basis of eigenvectors for φ, with corre-
sponding eigenvalues (λ1, . . . ,λn), we can write
</p>
<p>φ =
n
</p>
<p>&sum;
</p>
<p>a=1
λa |ba〉〈ba| and φ&dagger; =
</p>
<p>n
&sum;
</p>
<p>a=1
λa |ba〉〈ba|</p>
<p/>
</div>
<div class="page"><p/>
<p>204 12 Spectral Theorems on Hermitian Spaces
</p>
<p>which directly yields (see the Exercise 12.1.15)
</p>
<p>φφ&dagger; =
n
</p>
<p>&sum;
</p>
<p>a=1
|λa|2 |ba〉〈ba| = φ&dagger;φ.
</p>
<p>The converse, the less trivial part of the statement, is proven once again by induc-
</p>
<p>tion.
</p>
<p>Consider first a normal operator φ on the two dimensional hermitian space H 2.
</p>
<p>With respect to any basis, the characteristic polynomial pφ(T ) has two complex
</p>
<p>roots, from the fundamental theorem of algebra. A normal endomorphism of H 2
</p>
<p>with only the zero eigenvalue, would be the null endomorphism. So we can assume
</p>
<p>there is a root λ �= 0, with v a (normalised) eigenvectors, that is φ(v) = λv with
‖v‖ = 1. If C = (v,w) is an orthonormal basis for H 2 that completes v, we have,
from point (c) above,
</p>
<p>〈φ(w)|v〉 = 〈w|φ&dagger;(v)〉 = 〈w|v〉λ̄ = 0.
</p>
<p>Being λ �= 0, this shows that φ(w) is orthogonal to L(v), so that there must exists a
scalar&micro;, such that φ(w) = &micro;w. In turn this shows that if φ is a normal endomorphism
of H 2, then H 2 has an orthonormal basis of eigenvectors for φ.
</p>
<p>Inductively, let us assume that the statement is valid when the dimension of the
</p>
<p>hermitian space is n &minus; 1. The n-dimensional case is treated analogously to what done
above. If φ is a normal endomorphism of H n , its characteristic polynomial pφ(T )
</p>
<p>has at least a non zero complex root, λ say, with v a corresponding normalised
</p>
<p>eigenvector: φ(v) = λv, with ‖v‖ = 1. (Again, a normal endomorphism of H n with
only the zero eigenvalue is the null endomorphism.) We have H n = Vλ &oplus; V &perp;λ and v
can be completed to an orthonormal basis C = (v,w1, . . . , wn) for H n . If w &isin; V &perp;λ
we compute as above
</p>
<p>〈φ(w)|v〉 = 〈w|φ&dagger;(v)〉 = 〈w|v〉λ̄ = 0.
</p>
<p>This shows that φ maps V &perp;λ to itself, while also φ
&dagger; maps V &perp;λ to itself since,
</p>
<p>〈φ&dagger;(w)|v〉 = 〈w|φ(v)〉 = 〈w|v〉λ = 0.
</p>
<p>The restriction of φ to V &perp;λ is then a normal operator on a (n &minus; 1) dimensional
hermitian space, and by assumption there exists an orthonormal basis (u1, . . . , un&minus;1)
for V &perp;λ made of eigenvectors forφ. The basisE = (v, u1, . . . , un&minus;1) is an orthonormal
basis for H n of eigenvectors for φ. �
</p>
<p>Remark 12.2.3 Since the field of real numbers is not algebraically closed (and the
</p>
<p>fundamental theorem of algebra is valid on C), it is worth stressing that an analogue
</p>
<p>of this theorem for normal endomorphisms on euclidean spaces does not hold. A
</p>
<p>matrix A &isin; Rn,n such that (tA) A = A (tA), needs not be diagonalisable. An example</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Spectral Theory for Normal Endomorphisms 205
</p>
<p>is given by an antisymmetric (skew-adjoint, see Sect. 11.1) matrix A, which clearly
</p>
<p>commutes with tA, being nonetheless not diagonalisable.
</p>
<p>We showed in the Remark 12.1.8 that self-adjoint and unitary endomorphisms are
</p>
<p>normal. Within the set of normal endomorphisms, they can be characterised in terms
</p>
<p>of their spectrum.
</p>
<p>If λ is an eigenvalue of a self-adjoint endomorphism φ, with φ(v) = λv, then
</p>
<p>λv = φ(v) = φ&dagger;(v) = λ̄v
</p>
<p>and thus one hasλ = λ̄. Ifλ is an eigenvalue for a unitary operatorφ, withφ(v) = λv,
then
</p>
<p>‖v‖2 = ‖φ(v)‖2 = |λ|2‖v‖2,
</p>
<p>which gives |λ| = 1. It is easy to show also the converse of these claims, so to have
the following.
</p>
<p>Theorem 12.2.4 A normal operator on H n is self-adjoint if and only if its eigenval-
</p>
<p>ues are real. A normal operator on H n is unitary if and only if its eigenvalues have
</p>
<p>modulus 1.
</p>
<p>As a corollary, by merging the previous two theorems, we have a characterisation
</p>
<p>of self-adjoint and unitary operators in terms of their spectral properties, as follows.
</p>
<p>Corollary 12.2.5 An endomorphism φ on H n is self-adjoint if and only if its spec-
</p>
<p>trum is real and there exists an orthonormal basis for H n of eigenvectors for φ. An
</p>
<p>endomorphism φ on H n is unitary if and only if its spectrum is a subset of the unit
</p>
<p>circle in C, and there exists an orthonormal basis for H n of eigenvectors for φ.
</p>
<p>Exercise 12.2.6 Consider the hermitian space H 2, with E = (e1, e2) its canonical
orthonormal basis, and the endomorphism φ represented with respect to E by
</p>
<p>M
E,E
</p>
<p>φ =
(
</p>
<p>0 a
</p>
<p>&minus;a 0
</p>
<p>)
</p>
<p>with a &isin; R.
</p>
<p>This endomorphism is not diagonalisable over R, since it is antisymmetric (see
</p>
<p>Sect. 11.1) and the Remark 12.2.3. Being normal with respect to the hermitian struc-
</p>
<p>ture in H 2, there exists an orthonormal basis for H 2 of eigenvectors for φ. The
</p>
<p>eigenvalue equation is pφ(T ) = T 2 + a2 = 0, so the eigenvalues are λ&plusmn; = &plusmn;ia,
with normalised eigenvectors u&plusmn; given by
</p>
<p>λ&plusmn; = &plusmn;i a u&plusmn; = 1&radic;
2
(1,&plusmn;i)E ,
</p>
<p>while the unitary conjugation that diagonalises the matrix M
E,E
</p>
<p>φ is given by
</p>
<p>1
2
</p>
<p>(
</p>
<p>1 &minus;i
1 i
</p>
<p>) (
</p>
<p>0 a
</p>
<p>&minus;a 0
</p>
<p>) (
</p>
<p>1 1
</p>
<p>i &minus;i
</p>
<p>)
</p>
<p>=
(
</p>
<p>i a 0
</p>
<p>0 &minus;i a
</p>
<p>)
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>206 12 Spectral Theorems on Hermitian Spaces
</p>
<p>The comparison of this with the content of the Example 12.1.16 follows by writing
</p>
<p>the matrix giving the change of basis from E to B = (u+, u&minus;) as
</p>
<p>MB,E = 1
2
</p>
<p>(
</p>
<p>1 &minus;i
1 i
</p>
<p>)
</p>
<p>=
(
</p>
<p>〈u+|e1〉 〈u+|e2〉
〈u&minus;|e1〉 〈u&minus;|e2〉
</p>
<p>)
</p>
<p>.
</p>
<p>We next study a family of normal endomorphisms, which will be useful when con-
</p>
<p>sidering the properties of unitary matrices. The following definition comes naturally
</p>
<p>from the Definition 12.1.7.
</p>
<p>Definition 12.2.7 An endomorphism φ in H n is named skew-adjoint if
</p>
<p>〈u|φ(v)〉 + 〈φ&dagger;(u)|v〉 = 0 for any u, v &isin; H n . A matrix A &isin; Cn,n is named skew-
adjoint if A&dagger; = &minus;A.
</p>
<p>We list some important results on skew-adjoint endomorphisms and matrices.
</p>
<p>(a) It is clear that an endomorphism φ on H n is skew-adjoint if and only if there
</p>
<p>exists an orthonormal basis E for H n with respect to which the matrix M
E,E
</p>
<p>φ is
</p>
<p>skew-adjoint.
</p>
<p>(b) Skew-adjoint endomorphisms are normal. We know from the Proposition 12.2.1
</p>
<p>point (c), that if λ is an eigenvalue for the endomorphism φ, then λ̄ is an eigen-
</p>
<p>value for φ&dagger;. This means that if λ is an eigenvalue For a skew-adjoint endomor-
</p>
<p>phism φ, then λ̄ = &minus;λ, so any eigenvalue for a skew-adjoint endomorphism is
either purely imaginary or zero.
</p>
<p>(c) There exists an orthonormal basis E = (e1, . . . , en) of eigenvectors for φ such
that
</p>
<p>φ =
n
</p>
<p>&sum;
</p>
<p>a=1
iλa |ea〉〈ea| with λa &isin; R.
</p>
<p>(d) The real vector space of skew-adjoint matrices A = &minus;A&dagger; &isin; Cn,n is a matrix
Lie algebra (see the Definition 11.1.6), that is the commutator of skew-adjoint
</p>
<p>matrices is a skew-adjoint matrix; it is denoted u(n) and it has dimension n.
</p>
<p>Remark 12.2.8 In parallel with the Remark 11.1.9, self-adjoint matrices do not make
</p>
<p>up a Lie algebra since the commutator of two self-adjoint matrices is a skew-adjoint
</p>
<p>matrix.
</p>
<p>Exercise 12.2.9 On the hermitian space H 3 we consider the endomorphismφwhose
</p>
<p>representing matrix is, with respect to the canonical basis E , given by
</p>
<p>M
E,E
</p>
<p>φ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>0 i a
</p>
<p>i 0 0
</p>
<p>&minus;a 0 0
</p>
<p>⎞
</p>
<p>⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Spectral Theory for Normal Endomorphisms 207
</p>
<p>with a a real parameter. Since (M
E,E
</p>
<p>φ )
&dagger; = &minus;ME,Eφ , then φ is skew-adjoint (and thus
</p>
<p>normal). Its characteristic equation
</p>
<p>pφ(T ) = &minus;T (1 + a2 + T 2) = 0
</p>
<p>has solutions λ = 0 and λ&plusmn; = &plusmn;i
&radic;
</p>
<p>1 + a2. Explicit calculations show that the
eigenspaces are given by ker(φ) = Vλ=0 = L(u0) and Vλ&plusmn; = L(u&plusmn;) with
</p>
<p>u0 = 1&radic;
1+a2 (0, ia, 1),
</p>
<p>u&plusmn; = 1&radic;
2(1+a2)
</p>
<p>(
&radic;
</p>
<p>1 + a2,&plusmn;1,&plusmn;ia).
</p>
<p>It is immediate to see that the set B = (u0, u&plusmn;) gives an orthonormal basis for
H 3.
</p>
<p>Exercise 12.2.10 We close this section by studying an endomorphism which is not
</p>
<p>normal, and indeed diagonalisable with an eigenvector basis which is not orthonor-
</p>
<p>mal. In H 2 with respect to E = (e1, e2), consider the endomorphism whose repre-
senting matrix is
</p>
<p>M =
(
</p>
<p>0 1
</p>
<p>a 0
</p>
<p>)
</p>
<p>with a &isin; R. Than M is normal if and only if a = 1. The characteristic equation is
</p>
<p>pM(T ) = T 2 &minus; a = 0
</p>
<p>so its spectral decomposition is given by
</p>
<p>λ&plusmn; = &plusmn;
&radic;
</p>
<p>a, Vλ&plusmn; = L(u&plusmn;) with u&plusmn; = (1,&plusmn;
&radic;
</p>
<p>a)E .
</p>
<p>Being 〈u+|u&minus;〉 = 1 &minus; a, the eigenvectors are orthogonal if and only if M is normal.
</p>
<p>12.3 The Unitary Group
</p>
<p>If A, B &isin; Cn,n are two unitary matrices, A&dagger; A = In and B&dagger; B = In (see the Definition
12.1.7), one has (AB)&dagger; AB = B&dagger; A&dagger; AB = In . Furthermore, det(A&dagger;) = det(A), so
from det(AA&dagger;) = 1 we have | det(A)| = 1. Clearly, the identity matrix In is unitary
and these leads to the following definition.
</p>
<p>Definition 12.3.1 The collection of n &times; n unitary matrices is a group, called the
unitary group of order n and denoted U(n). The subset SU(n) = {A &isin; U(n) :
det(A) = 1} is a subgroup of U(n), called the special unitary group of order n.</p>
<p/>
</div>
<div class="page"><p/>
<p>208 12 Spectral Theorems on Hermitian Spaces
</p>
<p>Remark 12.3.2 With the the natural inclusion of real matrices as complex matrices
</p>
<p>whose entries are invariant under complex conjugation, it is clear that O(n) is a
</p>
<p>subgroup of U(n) and SO(n) is a subgroup of SU(n).
</p>
<p>Now, the exponential of a matrix as in the Definition 11.2.1 can be extended to
</p>
<p>complex matrices. Thus, for a matrix A &isin; Cn,n , its exponential is defined by by the
expansion,
</p>
<p>eA =
&infin;
&sum;
</p>
<p>k=0
</p>
<p>1
k! A
</p>
<p>k .
</p>
<p>Then, all properties in the Proposition 11.2.2 have a counterpart for complex matrices,
</p>
<p>with point (e) there now reading eA
&dagger; = (eA)&dagger;.
</p>
<p>Theorem 12.3.3 Let M,U &isin; Cn,n . One has the following results.
(a) If M&dagger; = &minus;M, then eM &isin; U(n). If M&dagger; = &minus;M and tr(M) = 0, then eM &isin; SU(n).
(b) Conversely, if UU &dagger; = In , there exists a skew-adjoint matrix M = &minus;M&dagger; such that
</p>
<p>U = eM . If U is a special unitary matrix, there exists a skew-adjoint traceless
matrix, M = &minus;M&dagger; with tr(M) = 0, such that U = eM .
</p>
<p>Proof Let M be a skew-adjoint matrix. From the previous section we know that there
</p>
<p>exists a unitary matrix V such that M = V�M V &dagger;, with �M = diag(iρ1, . . . , iρn) for
ρa &isin; R. We can then write
</p>
<p>eM = eV�M V &dagger; = V e�M V &dagger;
</p>
<p>with e�M = diag(eiρ1 , . . . , eiρn ). This means that e�M is a unitary matrix, and we
can conclude that the starting matrix eM is unitary. If tr(M) = 0, then eM is a special
unitary matrix.
</p>
<p>Alternatively, the result can be shown as follows. If M = &minus;M&dagger;, then
</p>
<p>(eM)&dagger; = eM&dagger; = e&minus;M = (eM)&minus;1.
</p>
<p>This concludes the proof of point (a).
</p>
<p>Consider then a unitary matrix U . Since U is normal, there exists a unitary matrix
</p>
<p>V such that U = V�U V &dagger; with �U = diag(eiϕ1 , . . . , eiϕn ), where eiϕk are the mod-
ulus 1 eigenvalues of U . Clearly, the matrix �U can be written as
</p>
<p>�U = eδU
</p>
<p>with δU = diag(iϕ1, . . . , iϕn) = &minus;(δU )&dagger;. This means that
</p>
<p>U = V eδU V &dagger; = eV δU V &dagger;
</p>
<p>with (V δU V
&dagger;)&dagger; = &minus;(V δU V &dagger;). If U &isin; SU(n), then one has tr(V δU V &dagger;) = 0. This
</p>
<p>establishes point (b) and concludes the proof. ⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 The Unitary Group 209
</p>
<p>Exercise 12.3.4 Consider the matrix, with a, b &isin; R,
</p>
<p>A =
(
</p>
<p>b a
</p>
<p>a b
</p>
<p>)
</p>
<p>= A&dagger;.
</p>
<p>Its eigenvalues λ are given by the solutions of the characteristic equation
</p>
<p>pA(T ) = (b &minus; T )2 &minus; a2 = (b &minus; T &minus; a)(b &minus; T + a) = 0.
</p>
<p>Its spectral decomposition turns out to be
</p>
<p>λ&plusmn; = b &plusmn; a, Vλ&plusmn; = L((1,&plusmn;1)).
</p>
<p>To exponentiate the skew-adjoint matrix iA we can follow two ways.
</p>
<p>&bull; By normalising the eigenvectors, we have the conjugation with its diagonal form
A = V�AV &dagger;,
</p>
<p>(
</p>
<p>b a
</p>
<p>a b
</p>
<p>)
</p>
<p>= 1&radic;
2
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;1 1
</p>
<p>) (
</p>
<p>b &minus; a 0
0 b + a
</p>
<p>)
</p>
<p>1&radic;
2
</p>
<p>(
</p>
<p>1 &minus;1
1 1
</p>
<p>)
</p>
<p>so we have
</p>
<p>eiA = eiV�A V &dagger; = V ei�A V &dagger; = 1
2
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;1 1
</p>
<p>) (
</p>
<p>ei(b&minus;a) 0
0 ei(b+a)
</p>
<p>) (
</p>
<p>1 &minus;1
1 1
</p>
<p>)
</p>
<p>= 1
2
</p>
<p>(
</p>
<p>ei(b&minus;a) + ei(a+b) &minus;ei(b&minus;a) + ei(a+b)
&minus;ei(b&minus;a) + ei(a+b) ei(b&minus;a) + ei(a+b)
</p>
<p>)
</p>
<p>=
(
</p>
<p>eib cos a ieib sin a
</p>
<p>ieib sin a eib cos a
</p>
<p>)
</p>
<p>.
</p>
<p>Notice that det(eiA) = e2ib = eitr(A).
&bull; By setting
</p>
<p>A = Ã + B̃ =
(
</p>
<p>0 a
</p>
<p>a 0
</p>
<p>)
</p>
<p>+
(
</p>
<p>b 0
</p>
<p>0 b
</p>
<p>)
</p>
<p>we see that A is the sum of two commuting matrices, since B̃ = bI2. So we can
write
</p>
<p>eiA = ei( Ã+B̃) = ei ÃeiB̃ .
</p>
<p>Since B̃ is diagonal, eiB̃ = diag(eib, ei ib). Computing as in the Exercise 11.2.4
we have
</p>
<p>Ã2k =
(
</p>
<p>(&minus;1)ka2k 0
0 (&minus;1)ka2k
</p>
<p>)
</p>
<p>, Ã2k+1 =
(
</p>
<p>0 (&minus;1)k ia2k+1
(&minus;1)k ia2k+1 0
</p>
<p>)
</p>
<p>so that
</p>
<p>ei Ã =
(
</p>
<p>cos a i sin a
</p>
<p>i sin a cos a
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>210 12 Spectral Theorems on Hermitian Spaces
</p>
<p>and
</p>
<p>eiA =
(
</p>
<p>cos a i sin a
</p>
<p>i sin a cos a
</p>
<p>) (
</p>
<p>eib 0
</p>
<p>0 eib
</p>
<p>)
</p>
<p>=
(
</p>
<p>eib cos a ieib sin a
</p>
<p>ieib sin a eib cos a
</p>
<p>)
</p>
<p>.
</p>
<p>Exercise 12.3.5 In this exercise we describe how to reverse the construction of the
</p>
<p>previous one. That is, given the unitary matrix
</p>
<p>U = 1&radic;
2
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;1 1
</p>
<p>)
</p>
<p>,
</p>
<p>we determine the self-adjoint matrix A = A&dagger; such that U = eiA. Via the usual tech-
niques it is easy to show that the spectral decomposition of U is given by
</p>
<p>λ&plusmn; =
a &plusmn; i
</p>
<p>&radic;
1 + a2
</p>
<p>, with Vλ&plusmn; = L((1,&plusmn;i)).
</p>
<p>Notice that |λ&plusmn;| = 1 so we can write λ&plusmn; = eiϕ&plusmn; and, by normalising the eigenvectors
for U ,
</p>
<p>U = V�U V &dagger; = 12
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;i i
</p>
<p>) (
</p>
<p>eiϕ&minus; 0
</p>
<p>0 eiϕ+
</p>
<p>) (
</p>
<p>1 i
</p>
<p>1 &minus;i
</p>
<p>)
</p>
<p>,
</p>
<p>with V &dagger;V = I2. Since �U = eiδU with δU = δ&dagger;U = diag(ϕ&minus;,ϕ+), we write
</p>
<p>U = V eiδU V &dagger; = eiV δU V &dagger; = eiA
</p>
<p>where A = A&dagger; with
</p>
<p>A = V δU V &dagger; = 12
</p>
<p>(
</p>
<p>1 1
</p>
<p>&minus;i i
</p>
<p>) (
</p>
<p>eiϕ&minus; 0
</p>
<p>0 eiϕ+
</p>
<p>) (
</p>
<p>1 i
</p>
<p>1 &minus;i
</p>
<p>)
</p>
<p>= 1
2
</p>
<p>(
</p>
<p>ϕ&minus; + ϕ+ i(ϕ&minus; &minus; ϕ+)
i(ϕ&minus; &minus; ϕ+) ϕ&minus; + ϕ+
</p>
<p>)
</p>
<p>.
</p>
<p>Notice that the matrix A is not uniquely determined by U , since the angular
</p>
<p>variables ϕ&plusmn; are defined up to 2π periodicity by
</p>
<p>cosϕ&plusmn; =
a
</p>
<p>&radic;
1 + a2
</p>
<p>, sinϕ&plusmn; = &plusmn;
1
</p>
<p>&radic;
1 + a2
</p>
<p>.
</p>
<p>We close this section by considering one parameter groups of unitary matrices.
</p>
<p>We start with a self-adjoint matrix A = A&dagger; &isin; Cn,n , and define the matrix
</p>
<p>Us = eis A, for s &isin; R.
</p>
<p>From the properties of the exponential of a matrix, it is easy to show that, for any
</p>
<p>real s, s &prime;, the following identities hold.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 The Unitary Group 211
</p>
<p>(i) (Us)
&dagger;Us = In ,
</p>
<p>that is Us is unitary,
</p>
<p>(ii) U0 = In ,
(iii) (Us)
</p>
<p>&dagger; = U&minus;s ,
(iv) Us+s &prime; = UsUs &prime; = Us &prime;Us ,
</p>
<p>thus in particular, these unitary matrices commute for different values of the
</p>
<p>parameter.
</p>
<p>The map R &rarr; U(n) given by s �&rarr; Us is, according to the definition in the
Appendix A.4, a group homomorphism between (R,+) and U(n) (with group mul-
tiplication), that is between the abelian group R with respect to the sum and the non
</p>
<p>abelian group U(n) with respect to the matrix product. This leads to the following
</p>
<p>definition.
</p>
<p>Definition 12.3.6 If Us is a family (labelled by a real parameter s) of elements in
</p>
<p>U(n) such that, for any value of s &isin; R, the above identities ii)&minus; iv) are fulfilled,
then Us is called a one parameter group of unitary matrices of order n.
</p>
<p>For any self-adjoint matrix A, we have a one parameter group of unitary matrices
</p>
<p>given by Us = eis A. The matrix A is usually called the infinitesimal generator of the
one parameter group.
</p>
<p>Proposition 12.3.7 For any A = A&dagger; &isin; Cn,n , the elements Us = eis A give a one
parameter group of unitary matrices in H n . Conversely, if Us is a one parameter
</p>
<p>group of unitary matrices in H n , there exists a self-adjoint matrix A = A&dagger; such that
Us = eis A.
</p>
<p>Proof Let Us &isin; U(n) be a one parameter group of unitary matrices. For each value
s &isin; R the matrix Us can be diagonalised, and since Us commutes with any Us &prime; , it
follows that there exists an orthonormal basis B for H n of common eigenvectors for
</p>
<p>any Us . So there is a unitary matrix V (providing the change of basis from B to the
</p>
<p>canonical base E) such that
</p>
<p>Us = V {diag(eiϕ1(s),...,iϕn(s))}V &dagger;
</p>
<p>where eiϕk (s) are the eigenvalues of Us . From the condition UsUs &prime; = Us+s &prime; it follows
that the dependence of the eigenvalues on the parameter s is linear, and from U0 = In
we know that ϕk(s = 0) = 0. We can eventually write
</p>
<p>Us = V {diag(eisϕ1,...,isϕn )}V &dagger; = V eis δV &dagger; = eis V δV
&dagger;
</p>
<p>where δ = diag(ϕ1, . . . ,ϕn) is a self-adjoint matrix. We then set A = V δV &dagger; = A&dagger;
to be the infinitesimal generator of the given one parameter group of unitary matrices.
</p>
<p>⊓⊔</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
</p>
<p>Quadratic Forms
</p>
<p>13.1 Quadratic Forms on Real Vector Spaces
</p>
<p>In Sect. 3.1 the notion of scalar product on a finite dimensional real vector space has
</p>
<p>been introducedas a bilinear symmetric map &middot; : V &times; V &rarr; R with additional prop-
erties. Such additional properties are that v &middot; v &ge; 0 for v &isin; V , with
v &middot; v = 0 &hArr; v = 0V . This is referred to as positive definiteness.
</p>
<p>We start by introducing the more general notion of quadratic form.
</p>
<p>Definition 13.1.1 Let V be a finite dimensional real vector space. A quadratic form
</p>
<p>on V is a map
</p>
<p>Q : V &times; V &minus;&rarr; R (v,w) �&rarr; Q(v,w)
</p>
<p>that fulfils the following properties. For any v,w, v1, v2 &isin; V and a1, a2 &isin; R it holds
that:
</p>
<p>(Q1) Q(v,w) = Q(w, v),
(Q2) Q((a1v1 + a2v2), w) = a1Q(v1, w)+ a2Q(v2, w).
</p>
<p>When a quadratic form is positive definite, that is for any v &isin; V the additional
conditions
</p>
<p>(E1) Q(v, v) &ge; 0;
(E2) Q(v, v) = 0 &hArr; v = 0V .
</p>
<p>are satisfied, then Q is a scalar product, and we say that V is an euclidean space.
</p>
<p>With respect to a basis B = (u1, . . . , un) for V , the conditions Q1 and Q2 are
clearly satisfied if and only if there exists a symmetric matrix F = (Fab) &isin; Rn,n such
that
</p>
<p>Q(v,w) = Q
(
(v1, . . . , vn)B, (w1, . . . , wn)B
</p>
<p>)
=
</p>
<p>n&sum;
</p>
<p>a,b=1
</p>
<p>Fab vawb.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_13
</p>
<p>213</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_13&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_13&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>214 13 Quadratic Forms
</p>
<p>This expression can be also written as
</p>
<p>Q(v,w) =
(
v1 &middot; &middot; &middot; vn
</p>
<p>)
⎛
⎜⎝
</p>
<p>F11 &middot; &middot; &middot; F1n
...
</p>
<p>...
</p>
<p>Fn1 &middot; &middot; &middot; Fnn
</p>
<p>⎞
⎟⎠
</p>
<p>⎛
⎜⎝
w1
...
</p>
<p>wn
</p>
<p>⎞
⎟⎠
</p>
<p>Not surprisingly, the matrix representing the action of the quadratic form Q depends
</p>
<p>on the basis considered in V . Under a change of basis B &rarr; B&prime; with B&prime; = (u&prime;1, . . . , u&prime;n)
and corresponding matrix M B
</p>
<p>&prime;,B, as we know, the components of the vectors v,w
</p>
<p>are transformed as ⎛
⎜⎝
v&prime;1
...
</p>
<p>v&prime;n
</p>
<p>⎞
⎟⎠ = M B
</p>
<p>&prime;,B
</p>
<p>⎛
⎜⎝
v1
...
</p>
<p>vn
</p>
<p>⎞
⎟⎠
</p>
<p>and analogously for w. So we write the action of the quadratic form Q as
</p>
<p>Q(v,w) =
(
v&prime;1 &middot; &middot; &middot; v&prime;n
</p>
<p>) (
tM B
</p>
<p>&prime;,B F M B
&prime;,B
</p>
<p>)
⎛
⎜⎝
w&prime;1
...
</p>
<p>w&prime;n
</p>
<p>⎞
⎟⎠ .
</p>
<p>If we write the dependence on the basis as Q &rarr; FB, we have then shown the
following result.
</p>
<p>Proposition 13.1.2 Given a quadratic form Q on the finite dimensional real vector
</p>
<p>space V , with FB and FB
&prime;
</p>
<p>the matrices representing Q on V with respect to the
</p>
<p>bases B and B&prime;, it holds that
</p>
<p>FB
&prime; = tM B&prime;,B FB M B&prime;,B.
</p>
<p>Corollary 13.1.3 Since the matrix FB associated with the quadratic form Q on V
</p>
<p>for the basis B is symmetric, it is evident from the Proposition4.1.20 that the matrix
</p>
<p>FB
&prime;
associated with Q with respect to any other basis B&prime; is symmetric as well.
</p>
<p>The Proposition 13.1.2 is the counterpart of the Proposition 7.9.9 which related
</p>
<p>the matrices of a linear maps in different bases. This transformation is not the same as
</p>
<p>the one for the matrix of an endomorphism as described at the beginning of Chap. 9.
</p>
<p>To parallel the definition there, one is led to the following definition.
</p>
<p>Definition 13.1.4 The symmetric matrices A,B &isin; Rn,n are called quadratically
equivalent (or simply equivalent) if there exists a matrix P &isin; GL(n), such that
B = tPAP. Analogously, the quadratic forms Q and Q&prime; defined on a real finite dimen-
sional vector space V are called equivalent if their representing matrices are (quadrat-
</p>
<p>ically) equivalent.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Quadratic Forms on Real Vector Spaces 215
</p>
<p>Exercise 13.1.5 Let us consider the symmetric matrices
</p>
<p>A =
(
</p>
<p>1 0
</p>
<p>0 2
</p>
<p>)
, B =
</p>
<p>(
1 0
</p>
<p>0 3
</p>
<p>)
.
</p>
<p>They are not similar, since for example det(A) = 2 �= det(B) = 3 (recall that if two
matrices are similar, then their determinants must coincide, from the Binet Theo-
</p>
<p>rem 5.1.16). They are indeed quadratically equivalent: the matrix
</p>
<p>P =
</p>
<p>(
1 0
</p>
<p>0
</p>
<p>&radic;
3
2
</p>
<p>)
</p>
<p>gives tPAP = B.
</p>
<p>In parallel with the Remark 9.1.4 concerning similarity of matrices, it is easy to
</p>
<p>show that the quadratic equivalence is an equivalence relation within the collection
</p>
<p>of symmetric matrices in Rn,n. It is then natural to look for a canonical representative
</p>
<p>in any equivalence class.
</p>
<p>Proposition 13.1.6 Any quadratic form Q is equivalent to a diagonal quadratic
</p>
<p>form, that is one whose representing matrix is diagonal.
</p>
<p>Proof This is just a consequence of the fact that symmetric matrices are orthogonally
</p>
<p>diagonalisable. From the Proposition 10.5.1 we know that for any symmetric matrix
</p>
<p>A &isin; Rn,n there exists a matrix P which is orthogonal, that is P&minus;1 = tP, such that
</p>
<p>tPAP = �A
</p>
<p>where �A is a diagonal matrix whose entries are the eigenvalues of A. �
</p>
<p>Without any further requirements on the quadratic form, the matrix �A may have
</p>
<p>a number μ of positive eigenvalues, a number ν of negative eigenvalues, and also
</p>
<p>the zero eigenvalue with multiplicity m0 = mλ=0. We can order the eigenvalues as
follows
</p>
<p>�A = diag (λp1 , &middot; &middot; &middot; , λpμ , λn1 , &middot; &middot; &middot; , λnν , 0, &middot; &middot; &middot; , 0)
</p>
<p>As in the Exercise 13.1.5, we know that the diagonal matrix
</p>
<p>Q = diag ( 1&radic;
λp1
</p>
<p>, . . . , 1&radic;
λpμ
</p>
<p>, 1&radic;
|λn1 |
</p>
<p>, . . . , 1&radic;
|λnν |
</p>
<p>, 1, . . . , 1)
</p>
<p>is such that
</p>
<p>tQ�AQ = diag (1, . . . , 1,&minus;1, . . . ,&minus;1, 0, . . . , 0) = DA</p>
<p/>
</div>
<div class="page"><p/>
<p>216 13 Quadratic Forms
</p>
<p>with the expected multiplicities μ for +1, ν for &minus;1 and m0 for 0. Since we are
considering only transformations between real basis, these multiplicities are constant
</p>
<p>in each equivalence class of symmetric matrices.
</p>
<p>For quadratic forms, this means that any quadratic form Q on V is equivalent
</p>
<p>to a diagonal one whose diagonal matrix has a number of μ times +1, a number
of ν times &minus;1 and a number of m0 = dim(V )&minus; μ&minus; ν times 0. The multiplicities
μ and ν depend only on the equivalence class. Equivalently, for a quadratic form
</p>
<p>Q on V , there is a basis for V with respect to which the matrix representing Q is
</p>
<p>diagonal, with diagonal entries given +1 repeated μ times, &minus;1 repeated ν times and
m0 multiplicity of 0.
</p>
<p>Definition 13.1.7 Given a symmetric matrix A on Rn,n, we call DA its canonical form
</p>
<p>(or reduced form). If Q is a quadratic form on Rn whose matrix FB is canonical,
</p>
<p>then one has
</p>
<p>Q(v,w) = vp1wp1 + &middot; &middot; &middot; + vpμwpμ &minus; (vn1wn1 + &middot; &middot; &middot; + vnνwnν )
</p>
<p>with v = (vp1 , . . . , vpμ , vn1 , . . . , vnν , ṽ1, . . . , ṽm0) and analogously for w. This is the
canonical form for the quadratic formQ. The triple sign(Q) = (μ, ν,m0) is called the
signature of the quadratic formQ. In particular, the quadratic formQ is called positive
</p>
<p>definite if sign(Q) = (μ = n, 0, 0), and negative definite if sign(Q) = (0, ν = n, 0).
</p>
<p>Exercise 13.1.8 On V = R3 consider the quadratic form
</p>
<p>Q(v,w) = v1w2 + v2w1 + v1w3 + v3w1 + v2w3 + v3w2
</p>
<p>where v = (v1, v2, v3)B and w = (w1, w2, w3)B with respect to a given basis
(u1, u2, u3). Its action is represented by the matrix
</p>
<p>FB =
</p>
<p>⎛
⎝
</p>
<p>0 1 1
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>⎞
⎠
</p>
<p>To diagonalise it, we compute its eigenvalues from the characteristic polynomial,
</p>
<p>pFB(T ) = &minus;T 3 + 3T + 2 = (2 &minus; T )(1 + T )2.
</p>
<p>The eigenvalue λ = 2 is simple, with eigenspace Vλ=2 = L((1, 1, 1)), while
the eigenvalue λ = &minus;1 has multiplicity mλ=&minus;1 = 2, with corresponding eigenspace
Vλ=&minus;1 = L((1,&minus;1, 0), (1, 1,&minus;2)). If we define
</p>
<p>P = M B&prime;,B = 1&radic;
6
</p>
<p>⎛
⎝
</p>
<p>&radic;
2
</p>
<p>&radic;
3 1&radic;
</p>
<p>2 &minus;
&radic;
</p>
<p>3 1&radic;
2 0 &minus;2
</p>
<p>⎞
⎠
</p>
<p>we see that</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Quadratic Forms on Real Vector Spaces 217
</p>
<p>tPFBP =
</p>
<p>⎛
⎝
</p>
<p>2 0 0
</p>
<p>0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠ = �A = FB
</p>
<p>&prime;
</p>
<p>with respect to the basis B&prime; = (u&prime;1, u&prime;2, u&prime;3) of eigenvectors given explicitly by
</p>
<p>u&prime;1 = 1&radic;3 (u1 + u2 + u3),
</p>
<p>u&prime;2 = 1&radic;2 (u1 &minus; u2),
</p>
<p>u&prime;3 = 1&radic;6 (u1 + u2 &minus; 2u3).
</p>
<p>With respect to the basis B&prime; the quadratic form is written as
</p>
<p>Q(v,w) = 2v&prime;1w
&prime;
1 &minus; (v
</p>
<p>&prime;
2w
</p>
<p>&prime;
2 + v
</p>
<p>&prime;
3w
</p>
<p>&prime;
3).
</p>
<p>Motivated by the Exercise 13.1.5, with the matrix
</p>
<p>Q =
</p>
<p>⎛
⎝
</p>
<p>1&radic;
2
</p>
<p>0 0
</p>
<p>0 1 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>we have that
</p>
<p>tQFB
&prime;
Q =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠ = FB&prime;&prime;
</p>
<p>on the basis B&prime;&prime; = (u&prime;&prime;1 =
1&radic;
2
u&prime;1, u
</p>
<p>&prime;&prime;
2 = u&prime;2, u&prime;&prime;3 = u&prime;3). With respect to B&prime;&prime; the quadratic
</p>
<p>form is
</p>
<p>Q(v,w) = v&prime;&prime;1w
&prime;&prime;
1 &minus; v
</p>
<p>&prime;&prime;
2w
</p>
<p>&prime;&prime;
2 &minus; v
</p>
<p>&prime;&prime;
3w
</p>
<p>&prime;&prime;
3 ,
</p>
<p>in terms of the components ofv,w in the basisB&prime;&prime;. Its signature is sign(Q) = (1, 2, 0).
</p>
<p>Exercise 13.1.9 On the vector space R4 with canonical basis E , consider the
</p>
<p>quadratic form
</p>
<p>Q(v,w) = u1w1 + u2w2 + u1w2 + u2w1 + u3w4 + u4w3 &minus; u3w3 &minus; u4w4,
</p>
<p>for any two vectors v,w in R4. Its representing matrix is
</p>
<p>FE =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 1 0 0
</p>
<p>1 1 0 0
</p>
<p>0 0 &minus;1 1
0 0 1 &minus;1
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>which has been already studied in the Exercise 10.5.3. We can then immediately write</p>
<p/>
</div>
<div class="page"><p/>
<p>218 13 Quadratic Forms
</p>
<p>P = 1&radic;
2
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 1 0
</p>
<p>&minus;1 0 1 0
0 1 0 1
</p>
<p>0 1 0 &minus;1
</p>
<p>⎞
⎟⎟⎠ ,
</p>
<p>tPFEP =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 0
</p>
<p>0 0 0 0
</p>
<p>0 0 &minus;2 0
0 0 0 2
</p>
<p>⎞
⎟⎟⎠ = F
</p>
<p>E
&prime;
,
</p>
<p>with the basis E &prime; = (e&prime;1, e&prime;2, e&prime;3, e&prime;4) given by
</p>
<p>e&prime;1 = 1&radic;2 (e1 &minus; e2),
</p>
<p>e&prime;2 = 1&radic;2 (e3 + e4),
</p>
<p>e&prime;3 = 1&radic;2 (e1 + e2),
</p>
<p>e&prime;4 = 1&radic;2 (e3 &minus; e4).
</p>
<p>With respect to the basis E &prime;&prime; = (e&prime;&prime;1 = e&prime;1, e&prime;&prime;2 = e&prime;2, e&prime;&prime;3 =
1&radic;
2
e&prime;3, e
</p>
<p>&prime;&prime;
4 =
</p>
<p>1&radic;
2
e&prime;4) it is
</p>
<p>clear that the matrix representing the action of Q is FE
&prime;&prime; = diag(0, 0,&minus;1, 1), so that
</p>
<p>the canonical form of the quadratic form Q reads
</p>
<p>Q(v,w) = &minus;v&prime;&prime;3w
&prime;&prime;
3 + v
</p>
<p>&prime;&prime;
4w
</p>
<p>&prime;&prime;
4
</p>
<p>with v = (v&prime;&prime;1 , v&prime;&prime;2 , v&prime;&prime;3 , v&prime;&prime;4 )E &prime;&prime; and analogously forw. It signature is sign(Q) = (1, 1, 2)
</p>
<p>Remark 13.1.10 Once the dimension n of the real vector space V is fixed, the collec-
</p>
<p>tion of inequivalent quadratic forms, that is the quotient of the symmetric matrices by
</p>
<p>the quadratic equivalence relation of the Definition 13.1.7, is labelled by the possible
</p>
<p>signatures of the quadratic forms, or equivalently by the signatures of the symmetric
</p>
<p>matrices, written as sign(Q) = (μ, ν, n &minus; μ&minus; ν).
</p>
<p>Finally, we state the conditions for a quadratic form to provides a scalar product
</p>
<p>for a finite dimensional real vector space V . Since we have discussed the topics at
</p>
<p>length, we omit the proof of the following proposition.
</p>
<p>Proposition 13.1.11 A quadratic form Q on a finite dimensional real vector space
</p>
<p>V provides a scalar product if and only if it is positive definite. In such a case we
</p>
<p>denote the scalar product by
</p>
<p>v &middot; w = Q(v,w).
</p>
<p>Exercise 13.1.12 With respect to the canonical basis E on R2 we consider the
</p>
<p>quadratic form
</p>
<p>Q(v,w) = av1w1 + v1w2 + v2w1, with a &isin; R,
</p>
<p>for v = (v1, v2) and w = (w1, w2). The matrix representing Q is given by
</p>
<p>FE =
(
</p>
<p>a 1
</p>
<p>1 0
</p>
<p>)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Quadratic Forms on Real Vector Spaces 219
</p>
<p>and its characteristic polynomial, pFE (T ) = T 2 &minus; aT &minus; 1, gives eigenvalues
</p>
<p>λ&plusmn; = 12 (a &plusmn;
&radic;
</p>
<p>a2 + 4).
</p>
<p>Since for any real value of a there is one positive eigenvalue and one negative
</p>
<p>eigenvalue, we conclude that the signature of the quadratic form is
</p>
<p>sign(Q) = (1, 1, 0).
</p>
<p>Exercise 13.1.13 Consider, from the Exercise 11.1.11, the three dimensional vector
</p>
<p>space V of antisymmetric matrices in R3,3. If we set
</p>
<p>Q(L,L&prime;) = &minus; 1
2
</p>
<p>tr (LL&prime;)
</p>
<p>with L,L&prime; &isin; V , it is immediate to verify that Q is a quadratic form. Also, the basis
elements La given in the Exercise 11.1.11 are orthonormal,
</p>
<p>Q(La,Lb) = δab.
</p>
<p>Then, the space of real antisymmetric 3 &times; 3 matrices is an euclidean space for
this scalar product.
</p>
<p>Exercise 13.1.14 On R2 again with the canonical basis, we consider the quadratic
</p>
<p>form
</p>
<p>Q(v,w) = v1w1 + v2w2 + a(v1w2 + v2w1), with a &isin; R,
</p>
<p>whose representing matrix is
</p>
<p>FE =
(
</p>
<p>1 a
</p>
<p>a 1
</p>
<p>)
.
</p>
<p>Its characteristic polynomial is pFE = (1 &minus; T )2 &minus; a2 = (1 &minus; T &minus; a)(1 &minus; T + a),
with eigenvalues
</p>
<p>λ&plusmn; = 1 &plusmn; a .
</p>
<p>We have the following cases:
</p>
<p>&bull; for a &gt; 1, it is sign(Q) = (1, 1, 0);
&bull; for a = &plusmn;1, it is sign(Q) = (1, 0, 1);
&bull; for a &lt; &minus;1, it is sign(Q) = (1, 1, 0);
&bull; for &minus;1 &lt; a &lt; 1, it is sign(Q) = (2, 0, 0).
</p>
<p>In this last case, the quadratic form endows R2 with a scalar product. The
</p>
<p>eigenspaces are
</p>
<p>λ&minus; = (1 &minus; a), Vλ&minus; = L((1,&minus;1)),
λ+ = (1 + a), Vλ+ = L((1, 1)),</p>
<p/>
</div>
<div class="page"><p/>
<p>220 13 Quadratic Forms
</p>
<p>so we can define the matrix
</p>
<p>M E
&prime;,E = 1&radic;
</p>
<p>2
</p>
<p>(
1 1
</p>
<p>&minus;1 1
</p>
<p>)
</p>
<p>which gives
</p>
<p>tM E
&prime;,EFEM E
</p>
<p>&prime;,E =
(
</p>
<p>1 &minus; a 0
0 1 + a
</p>
<p>)
.
</p>
<p>With respect to the basisE &prime; = 1&radic;
2
(e&prime;1 = (e1 &minus; e2), e&prime;2 =
</p>
<p>1&radic;
2
(e1 + e2)) the quadratic
</p>
<p>form is
</p>
<p>Q(v,w) = (1 &minus; a)v&prime;1w
&prime;
1 + (1 + a)v
</p>
<p>&prime;
2w
</p>
<p>&prime;
2.
</p>
<p>We obtain the canonical form for Q if we consider the basis E &prime;&prime; given by
</p>
<p>e&prime;&prime;1 =
1
</p>
<p>&radic;
1 &minus; a
</p>
<p>e&prime;1, e
&prime;&prime;
2 =
</p>
<p>1
&radic;
</p>
<p>1 + a
e&prime;2.
</p>
<p>The basis E &prime;&prime; is orthonormal with respect to the scalar product defined by Q.
</p>
<p>Exercise 13.1.15 This exercise puts the results of the previous one in a more general
</p>
<p>context.
</p>
<p>(a) From Exercise 13.1.14 we know that the symmetric matrix
</p>
<p>S =
(
</p>
<p>1 a
</p>
<p>a 1
</p>
<p>)
,
</p>
<p>with a &isin; R, is quadratically equivalent to the diagonal matrix
</p>
<p>S &prime; =
(
</p>
<p>1 &minus; a 0
0 1 + a
</p>
<p>)
.
</p>
<p>Let us consider S and S &prime; as matrices in C2,2 with real entries (recall that R is a
</p>
<p>subfield of C). We can then write
</p>
<p>(
(1 &minus; a)&minus;1/2 0
</p>
<p>0 (1 + a)&minus;1/2
) (
</p>
<p>1 &minus; a 0
0 1 + a
</p>
<p>) (
(1 &minus; a)&minus;1/2 0
</p>
<p>0 (1 + a)&minus;1/2
)
</p>
<p>=
(
</p>
<p>1 0
</p>
<p>0 1
</p>
<p>)
</p>
<p>for any a &isin; R. This means that, by complexifying the entries of the real sym-
metric matrix S, there exists a transformation
</p>
<p>S �&rarr; tPSP = I2
</p>
<p>with P &isin; GL(n,C) (the group of invertible n &times; n complex matrices), which
transforms S to I2.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 Quadratic Forms on Real Vector Spaces 221
</p>
<p>(b) From the Exercise 13.1.8 we know that the symmetric matrix
</p>
<p>S =
</p>
<p>⎛
⎝
</p>
<p>0 1 1
</p>
<p>1 0 1
</p>
<p>1 1 0
</p>
<p>⎞
⎠
</p>
<p>is quadratically equivalent to
</p>
<p>S &prime; =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠ .
</p>
<p>By again considering them as complex matrices, we can write
</p>
<p>I3 =
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 i 0
</p>
<p>0 0 i
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 &minus;1 0
0 0 &minus;1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 0 0
</p>
<p>0 i 0
</p>
<p>0 0 i
</p>
<p>⎞
⎠ .
</p>
<p>Thus, S is quadratically equivalent to I3 via an invertible matrix P &isin; Cn,n.
</p>
<p>If A is a symmetric matrix with real entries, from the Proposition 13.1.6 we know
</p>
<p>that it is quadratically equivalent to
</p>
<p>�A = diag (λp1 , &middot; &middot; &middot; , λpμ , λn1 , &middot; &middot; &middot; , λnν , 0, &middot; &middot; &middot; , 0),
</p>
<p>with λpj &gt; 0 and λnj &lt; 0. Given the invertible matrix
</p>
<p>P = diag ( 1&radic;
λp1
</p>
<p>, . . . , 1&radic;
λpμ
</p>
<p>, i&radic;
|λn1 |
</p>
<p>, . . . , i&radic;
|λnν |
</p>
<p>, 1, . . . , 1)
</p>
<p>in Cn,n, one finds that
</p>
<p>tP�AP = diag (1, . . . , 1, 1, . . . , 1, 0, . . . , 0) = D̃A,
</p>
<p>where the number of non zero terms +1 is given by the rank of A.
If we now define that two symmetric matrices A,B &isin; Cn,n are quadratically equiv-
</p>
<p>alent if there exists a matrix P &isin; GL(n,C) such that B = tPAP, we can conclude that
any real symmetric matrix A is quadratically equivalent to a diagonal matrix D̃A as
</p>
<p>above.
</p>
<p>The diagonal matrix D̃A above gives a canonical form for A with respect to
</p>
<p>quadratic equivalence after complexification. Notice that, since (iIn)A(iIn) = &minus;A,
we have that A is quadratically equivalent to &minus;A. This means that a notion of
complex signature does not carry much information since it cannot measure the
</p>
<p>signs of the eingenvalues of A, but only its rank. If A = tA = Ā, then we set
sign(A) = (rk(A), dim ker(A)).</p>
<p/>
</div>
<div class="page"><p/>
<p>222 13 Quadratic Forms
</p>
<p>We conclude by observing that what we have sketched above gives the main
</p>
<p>properties of a real quadratic form on a complex finite dimensional vector space,
</p>
<p>whose definition is as follows.
</p>
<p>Definition 13.1.16 A real quadratic forms on a complex finite dimensional vector
</p>
<p>spaces is a map
</p>
<p>S : Cn &times; Cn &minus;&rarr; C, (v,w) �&rarr; S(v,w)
</p>
<p>such that, for any v,w, v1, v2 &isin; Cn and a1, a2 &isin; C it holds that:
</p>
<p>(S1) S(v,w) = S(w, v),
(S2) S(v,w) &isin; R if and only if v = v̄ and w = w̄,
(S3) S((a1v1 + a2v2), w) = a1S(v1, w)+ a2S(v2, w).
</p>
<p>It is clear that S is a real quadratic form on Cn if and only if there exists a real
</p>
<p>basis B for Cn, that is a basis which is invariant under complex conjugation, with
</p>
<p>respect to which the matrix SB &isin; Cn,n representing S is symmetric with real entries.
In order to have a more elaborate notion of signature for a bilinear form on complex
</p>
<p>vector spaces, one needs the notion of hermitian form as explained in the next section.
</p>
<p>13.2 Quadratic Forms on Complex Vector Spaces
</p>
<p>It is straightforward to generalise to Cn the main results of the theory of quadratic
</p>
<p>forms on Rn. The following definition comes naturally after Sects. 3.4 and 8.2.
</p>
<p>Definition 13.2.1 Let V be a finite dimensional complex vector space. A hermitian
</p>
<p>form on V is a map
</p>
<p>H : V &times; V &minus;&rarr; C, (v,w) �&rarr; H(v,w)
</p>
<p>that fulfils the following properties. For any v,w, v1, v2 &isin; V and a1, a2 &isin; C it holds
that:
</p>
<p>(H1) H(v,w) = H(w, v),
(H2) H((a1v1 + a2v2), w) = a1H(v1, w)+ a2Q(v2, w).
</p>
<p>When a hermitian form is positive definite, that is for any v &isin; V the additional
conditions
</p>
<p>(E1) H(v, v) &ge; 0;
(E2) H(v, v) = 0 &hArr; v = 0V .
</p>
<p>are satisfied, then H is a hermitian product, and we say that V is a hermitian space.
</p>
<p>We list the properties of hermitian forms in parallel with those of the real case.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Quadratic Forms on Complex Vector Spaces 223
</p>
<p>(a) With respect to any given basis B = (u1, . . . , un) of V , the conditions H1 and
H2 are satisfied if and only if there exists a selfadjoint matrix H = (Hab) &isin; Cn,n,
H = H &dagger;, such that
</p>
<p>H(v,w) =
n&sum;
</p>
<p>a,b=1
</p>
<p>Hab vawb.
</p>
<p>If we denote by HB the dependence on the basis of V for the matrix giving the
</p>
<p>action of H, under a change of bases B &rarr; B&prime; we have
</p>
<p>HB
&prime; = (M B&prime;,B)&dagger; HB M B&prime;,B = (HB&prime;)&dagger;. (13.1)
</p>
<p>(b) Two selfadjoint matrices A,B &isin; Cn,n are defined to be equivalent if there exists
an invertible matrix P such that B = P&dagger;AP. This is an equivalence relation within
the set of selfadjoint matrices. Analogously, two hermitian forms H and H&prime; on
</p>
<p>C
n are defined to be equivalent if their representing matrices are equivalent.
</p>
<p>(c) From the spectral theory for selfadjoint matrices it is clear that any hermitian
</p>
<p>form H is equivalent to a hermitian form whose representing matrix is diagonal.
</p>
<p>Referring to the relation (13.1), there exists a unitary matrix U = M B&prime;,B of the
change of basis from B to B&prime; such that HB
</p>
<p>&prime; = diag(λ1, . . . , λn), with λj &isin; R
giving the spectrum of HB.
</p>
<p>(d) The matrix HB
&prime;
is further reduced to its canonical form via the same conjugation
</p>
<p>operation described for the real case after the Proposition 13.1.6.
</p>
<p>Since, as in real case, no conjugation as in (13.1) can alter the signs of the eigen-
</p>
<p>values of a given selfadjoint matrix, the notion of signature is meaningful for
</p>
<p>hermitian forms. Such a signature characterises equivalence classes of selfad-
</p>
<p>joint matrices (and then of hermitian forms) via the equivalence relation we are
</p>
<p>considering.
</p>
<p>(e) A hermitian form H equips Cn with a hermitian product if and only if it is positive
</p>
<p>definite.
</p>
<p>Exercise 13.2.2 On C2 we consider the basis B = (u1, u2) and the hermitian form
</p>
<p>H(v,w) = a(v1w1 + v2w2)+ i b(v1w2 &minus; v2w1), with a, b &isin; R
</p>
<p>for v = (v1, v2)B and w = (w1, w2)B. The hermitian form is represented by the
matrix
</p>
<p>HB =
(
</p>
<p>a i b
</p>
<p>&minus;i b a
</p>
<p>)
= (HB)&dagger;.
</p>
<p>The spectral resolution of this matrix gives
</p>
<p>λ&plusmn; = a &plusmn; b, Vλ&plusmn; = L(u&plusmn;)
</p>
<p>with normalised eigenvectors</p>
<p/>
</div>
<div class="page"><p/>
<p>224 13 Quadratic Forms
</p>
<p>u&plusmn; = 1&radic;
2
(&plusmn;i, 1)B,
</p>
<p>and with respect to the basis B&prime; = (b&prime;1 = u+, b&prime;2 = u&minus;) one finds
</p>
<p>HB
&prime; =
</p>
<p>(
a + b 0
a &minus; b 0
</p>
<p>)
.
</p>
<p>We reduce the hermitian form H to its canonical form by defining a basis
</p>
<p>B&prime;&prime; = ( 1&radic;|a+b| b
&prime;
1,
</p>
<p>1&radic;
|a&minus;b| b
</p>
<p>&prime;
2)
</p>
<p>so to have
</p>
<p>M B
&prime;&prime; =
</p>
<p>(
a+b
|a+b| 0
</p>
<p>0 a&minus;b|a&minus;b|
</p>
<p>)
.
</p>
<p>We see that the signature of H depends on the relative moduli of a and b. It endows
</p>
<p>C
2 with a hermitian product if and only if |a| &gt; |b|, with B&prime;&prime; giving an orthonormal
</p>
<p>basis for it.
</p>
<p>13.3 The Minkowski Spacetime
</p>
<p>We now describe the quadratic form used for a geometrical description of the elec-
</p>
<p>tromagnetism and for the special theory of relativity.
</p>
<p>Let V be a four dimensional real vector space equipped with a quadratic form Q
</p>
<p>with signature sign(Q) = (3, 1, 0). From the theory we have developed in Sect. 13.1
there exists a (canonical) basis E = (e0, e1, e2, e3) with respect to which the action
of Q is given by1
</p>
<p>Q(v,w) = &minus;v0w0 + v1w1 + v2w2 + v3w3
</p>
<p>with v = (v0, v1, v2, v3) and w = (w0, w1, w2, w3).
Definition 13.3.1 The equivalence class of quadratic forms on R4 characterised by
</p>
<p>the signature (3, 1, 0) is said to provide R4 a Minkowski quadratic form, that we
</p>
<p>denote by η. The datum (R4, η) is called the Minkowski spacetime, using the name
</p>
<p>from physics. We shall denote it by M 4 and with a slight abuse of terminology, we
</p>
<p>shall also denote the action of η as a scalar product
</p>
<p>v &middot; w = η(v,w)
</p>
<p>and refer to it as the (Minkowski) scalar product in M 4.
</p>
<p>1The reason why we denote the first element by e0 and the corresponding component of a vector v
</p>
<p>by v0 comes from physics, since such components is identified with the time coordinate of an event.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 The Minkowski Spacetime 225
</p>
<p>Definition 13.3.2 We list the natural generalisations to M 4 of well known definitions
</p>
<p>in En.
</p>
<p>(a) For any v &isin; (R4, η), the quantity ‖v‖2 = v &middot; v is the square of the (Minkowski)
norm of v &isin; R4;
</p>
<p>the vector v is called space-like if ‖v‖2 &gt; 0,
</p>
<p>the vector v is called light-like if ‖v‖ = 0,
</p>
<p>the vector v is called time-like if ‖v‖2 &lt; 0.
(b) Two vectors v,w &isin; M 4 are orthogonal if v &middot; w = 0; thus a light-like vector is
</p>
<p>orthogonal to itself.
</p>
<p>(c) A basis B for R4 is orthonormal if the action of η with respect to B is diagonal,
</p>
<p>that is if and only if the matrix ηB has the form
</p>
<p>ηB =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&minus;1 0 0 0
0 1 0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>We simply denote ημν = (ηB)μν with B orthonormal.
(d) A matrix A &isin; R4,4 is a Lorentz matrix if its columns yield an orthonormal basis
</p>
<p>for M 4.
</p>
<p>We omit the proof of the following results, which generalise to M 4 analogous
</p>
<p>results valid in En.
</p>
<p>Proposition 13.3.3 Let B = (e0, e1, e2, e3) be an orthonormal basis for M 4, with
A &isin; R4,4 and φ &isin; End(M 4).
</p>
<p>(a) The matrix A is a Lorentz matrix if and only if tA η A = η.
(b) It holds that φ(v) &middot; φ(w) = v &middot; w for any v,w &isin; M 4 if and only if M B,Bφ is a
</p>
<p>Lorentz matrix.
</p>
<p>(c) The system B&prime; = (φ(e0), . . . , φ(e3)) is an orthonormal basis for M 4 if and only
if for any v,w &isin; M 4 one has φ(v) &middot; φ(w) = v &middot; w, that is if and only if
</p>
<p>φ(eμ) &middot; φ(eν) = eμ &middot; eν = ημν .
</p>
<p>As an immediate consequence of such proposition, one proves that, if u &isin; M 4 is a
space-like vector, there exists an orthonormal basis B&prime; for M 4 with respect to which
</p>
<p>u = (0, u&prime;1, u&prime;2, u&prime;3)B&prime; . Analogously, if u is a time-like vector, there exists a basis B&prime;&prime;
with respect to which u = (u&prime;&prime;0, 0, 0, 0)B&prime;&prime; .
</p>
<p>Indeed it is straightforward to prove that the set of Lorentz matrices form a group,
</p>
<p>for matrix multiplication, denoted by O(3, 1) and called the Lorentz group. If the
</p>
<p>endomorphism φ is represented, with respect to an orthonormal basis for M 4 by a</p>
<p/>
</div>
<div class="page"><p/>
<p>226 13 Quadratic Forms
</p>
<p>Lorentz matrix, then φ is said to be a Lorentz transformation. This means that the
</p>
<p>set of Lorentz transformations is a group isomorphic to the Lorentz group.
</p>
<p>Example 13.3.4 In the special theory of relativity the position of a point mass at a
</p>
<p>given time t is represented with a vector x = (x0 = ct, x1, x2, x3)B in M 4 with respect
to an orthonormal basis B, with (x1, x2, x3) giving the so called spatial components
</p>
<p>of x and c denoting the speed of light. Such a vector x is also called an event. The
</p>
<p>linear map ⎛
⎜⎜⎝
</p>
<p>x&prime;0
x&prime;1
x&prime;2
x&prime;3
</p>
<p>⎞
⎟⎟⎠ =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>γ &minus;βγ 0 0
&minus;βγ γ 0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>⎞
⎟⎟⎠
</p>
<p>⎛
⎜⎜⎝
</p>
<p>x0
x1
x2
x3
</p>
<p>⎞
⎟⎟⎠
</p>
<p>with
</p>
<p>β = v/c and γ = (1 &minus; β2)&minus;1/2,
</p>
<p>yields the components of the vector x with respect to an orthonormal basis B&prime; corre-
</p>
<p>sponding to an inertial reference system, (an inertial observer) which is moving with
</p>
<p>constant spatial velocity v along the direction e1. Notice that, being c a limit value
</p>
<p>for the velocity, we have |β| &lt; 1 and then γ &ge; 1. It is easy to see that this map is a
Lorentz transformation, and that the matrix gives the change of basis M B
</p>
<p>&prime;,B in M 4.
</p>
<p>From the identity tA η A = η one gets det A = &plusmn;1 for a Lorentz matrix A. The
set of Lorentz matrices whose determinant is positive is the (sub)group SO(3, 1) of
</p>
<p>proper Lorentz matrices.
</p>
<p>If Aμν denotes the entries of a Lorentz matrix A, then from the same identity we
</p>
<p>can write that
</p>
<p>&minus;A200 +
3&sum;
</p>
<p>k=1
</p>
<p>A2k0 = &minus;1 and &minus; A
2
00 +
</p>
<p>3&sum;
</p>
<p>k=1
</p>
<p>A20k = &minus;1,
</p>
<p>thus proving that A200 &ge; 1. Lorentz matrices with A00 &gt; 1 are called ortochronous.
We omit the proof that the set of ortochronous Lorentz matrices form a group as well.
</p>
<p>Proper and ortochronous Lorentz matrices form therefore a group, that we denote
</p>
<p>by
</p>
<p>SO(3, 1)&uarr; = {A &isin; O(3, 1) : det A = 1, A00 &gt; 1}.
</p>
<p>Notice that the Lorentz matrix given in Example 13.3.4 is proper and ortochronous.
</p>
<p>Given the physical interpretation of the components of a vector in M 4 mentioned
</p>
<p>before, it is natural to call the endomorphisms represented by the Lorentz matrices
</p>
<p>P =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
</p>
<p>0 &minus;1 0 0
0 0 &minus;1 0
0 0 0 &minus;1
</p>
<p>⎞
⎟⎟⎠ , T =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>&minus;1 0 0 0
0 1 0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>⎞
⎟⎟⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 The Minkowski Spacetime 227
</p>
<p>the (spatial) parity and the time reversal. The matrix P is improper and ortochronous,
</p>
<p>while T is improper and antichronous.
</p>
<p>We can generalise the final remark from Example 11.3.1 to the Lorentz group case.
</p>
<p>If A is an improper ortochronous Lorentz matrix, then it is given by the product PA&prime;
</p>
<p>with A&prime; &isin; SO(3, 1)&uarr;. If A is an improper antichronous Lorentz matrix, then it is given
by the product TA&prime; with A&prime; &isin; SO(3, 1)&uarr;. If A is the product PTA&prime; with A&prime; &isin; SO(3, 1)&uarr;,
it is called a proper antichronous Lorentz matrix.
</p>
<p>Let us describe the structure of the group SO(3, 1)&uarr; in more details. Firstly, notice
</p>
<p>that if R &isin; SO(3) then all matrices of the form
</p>
<p>AR =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>1 0 0 0
</p>
<p>0
</p>
<p>0 R
</p>
<p>0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>are elements in SO(3, 1)&uarr;. The set of such matrices A is clearly isomorphic to the
</p>
<p>group SO(3), so we can refer to SO(3) as the subgroup of spatial rotations within
</p>
<p>the Lorentz group.
</p>
<p>The Lorentz matrix in the Example 13.3.4 is not such a rotation. From the Exer-
</p>
<p>cise 11.2.3 we write
</p>
<p>euS1 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>γ βγ 0 0
</p>
<p>βγ γ 0 0
</p>
<p>0 0 1 0
</p>
<p>0 0 0 1
</p>
<p>⎞
⎟⎟⎠ with S1 =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 1 0 0
</p>
<p>1 0 0 0
</p>
<p>0 0 0 0
</p>
<p>0 0 0 0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>with sinh u = βγ and cosh u = γ so that tgh u = v/c.
We therefore have a closer look at the exponential of symmetric matrices of the
</p>
<p>form
</p>
<p>S(u) =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 u1 u2 u3
u1 0 0 0
</p>
<p>u2 0 0 0
</p>
<p>u3 0 0 0
</p>
<p>⎞
⎟⎟⎠ = u1S1 + u2S2 + u3S3, (13.2)
</p>
<p>with u = (u1, u2, u3) a triple of real parameters. If the matrix R = (Rij) represents a
spatial rotation, a direct computation shows that
</p>
<p>AR&minus;1 S AR =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0
&sum;3
</p>
<p>k=1 Rk1uk
&sum;3
</p>
<p>k=1 Rk2uk
&sum;3
</p>
<p>k=1 Rk3uk&sum;3
k=1 Rk1uk 0 0 0&sum;3
k=1 Rk2uk 0 0 0&sum;3
k=1 Rk3uk 0 0 0
</p>
<p>⎞
⎟⎟⎠ :
</p>
<p>We see that u = (u1, u2, u3) transforms like a vector in a three dimensional euclidean
space, and therefore we write the identity above as</p>
<p/>
</div>
<div class="page"><p/>
<p>228 13 Quadratic Forms
</p>
<p>S(R&minus;1u) = AR&minus;1 S(u)AR.
</p>
<p>This identity allows us to write (see the Proposition 11.2.2)
</p>
<p>eS(u) = AR&minus;1 eS(Ru)AR.
</p>
<p>If R is a proper rotation mapping u �&rarr; (‖u‖E, 0, 0), with ‖u‖2E = u21 + u22 + u23 the
square of the euclidean three-norm, we get
</p>
<p>eS(u) = AR&minus;1 e(‖u‖E S1)AR.
</p>
<p>Alternatively, one shows by direct computations that
</p>
<p>S2(u) =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>‖u‖2E 0 0 0
0
</p>
<p>0 Q
</p>
<p>0
</p>
<p>⎞
⎟⎟⎠ and S
</p>
<p>3(u) = ‖u‖2E S(u)
</p>
<p>&rArr; S2k(u) = ‖u‖2(k&minus;1)E S(u), S
2k+1(u) = ‖u‖2kE S(u),
</p>
<p>where Q &isin; R3,3 has entries Qij = uiuj, so that Q2 = ‖u‖2E Q. These identities give
then
</p>
<p>eS(u) = 1 + 1‖u‖2E (cosh ‖u‖
2
E &minus; 1) S
</p>
<p>2(u) + 1‖u‖E sinh ‖u‖E S(u).
</p>
<p>It is easy to show that eS(u) &isin; SO(3, 1)&uarr;. Such transformations are called Lorentz
boosts, or hyperbolic rotations. They give the matrices of change of bases M B,B
</p>
<p>&prime;
</p>
<p>where B&prime; is the orthonormal basis corresponding to an inertial reference system
</p>
<p>moving with constant velocity v = (v1, v2, v3) in the physical euclidean three dimen-
sional space with respect to the reference system represented by B, by identifying
</p>
<p>for the velocity,
</p>
<p>c (tgh ‖u‖E) = ‖v‖E .
</p>
<p>From the properties of the group SO(3) we know that each proper spatial rotation
</p>
<p>is the exponential of a suitable antisymmetric matrix, that is AR = eL̃ where L̃ is an
element in the three dimensional vector space spanned by the matrices L̃j &sub; R4,4 of
the form
</p>
<p>L̃j =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 0 0 0
</p>
<p>0
</p>
<p>0 Lj
0
</p>
<p>⎞
⎟⎟⎠
</p>
<p>with the antisymmetric matrices Lj, j = 1, 2, 3, those of the Exercise 11.1.10, the
generators of the Lie algebra so(3). With the symmetric matrices Sj in (13.2), we
</p>
<p>compute the commutators to be</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 The Minkowski Spacetime 229
</p>
<p>[̃Li, L̃j] =
3&sum;
</p>
<p>i,j=1
</p>
<p>εijk L̃k
</p>
<p>[Si, Sj] = &minus;
3&sum;
</p>
<p>i,j=1
</p>
<p>εijk L̃k
</p>
<p>[Si, L̃j] =
3&sum;
</p>
<p>i,j=1
</p>
<p>εijkSk ,
</p>
<p>thus proving that the six dimensional vector space L(̃L1, L̃2, L̃3, S1, S2, S3) is a matrix
</p>
<p>Lie algebra (see the Definition 11.1.7) which is denoted so(3, 1). What we have
</p>
<p>discussed gives the proof of the first part of the following proposition, which is
</p>
<p>analogous of the Proposition 11.2.6.
</p>
<p>Proposition 13.3.5 If M is a matrix in so(3, 1), then eM &isin; SO(3, 1)&uarr;. When
restricted to so(3, 1), the exponential map is surjective onto SO(3, 1)&uarr;.
</p>
<p>This means that the group of proper and ortochronous Lorentz matrices is given
</p>
<p>by spatial rotations, hyperbolic rotations (that is boosts) and their products.
</p>
<p>13.4 Electro-Magnetism
</p>
<p>By recalling the framework of Sect. 1.4, in the standard euclidean formulation on
</p>
<p>the space E3 representing the physical space S (and with an orthonormal basis) one
</p>
<p>describes the three dimensional electric E(t, x) field and the magnetic field B(t, x)
</p>
<p>as depending on both the three dimensional position vector x = (x1, x2, x3) and the
time coordinate t. In this section we show that the Maxwell equations for electro-
</p>
<p>magnetism can be naturally formulated in terms of the geometry of the Minkowski
</p>
<p>space M 4.
</p>
<p>Example 13.4.1 The Maxwell equations in vacuum for the pair (E(t, x),B(t, x)) are
</p>
<p>written as
</p>
<p>div B = 0, rot E + &part; B
&part; t
</p>
<p>= 0
div E = ρ
</p>
<p>ε0
, rot B = μ0J + μ0ε0 &part; E&part; t
</p>
<p>where ε0 and μ0 are the vacuum permittivity and permeability, with c
2ε0μ0 = 1. The
</p>
<p>sources of the fields are the electric charge density ρ (a scalar field) and the current
</p>
<p>density J (a vector field).
</p>
<p>For the homogeneous Maxwell equations (the first two) the vector fields E and B
</p>
<p>can be written in terms of a vector potential A(t, x) = (A1(t, x),A2(t, x),A3(t, x))
and a scalar potential φ(t, x), as</p>
<p/>
</div>
<div class="page"><p/>
<p>230 13 Quadratic Forms
</p>
<p>B = rot A, E = &minus;grad φ &minus; &part; A
&part; t
,
</p>
<p>that makes the homogeneous equations automatically satisfied from the identity
</p>
<p>div (rot) = 0 and rot (grad) = 0, in Exercise 1.4.1. If the potentials satisfy the so
called Lorentz gauge condition
</p>
<p>div A + 1
c2
</p>
<p>&part; φ
</p>
<p>&part; t
= 0,
</p>
<p>the two Maxwell equations depending on the sources can be written as
</p>
<p>&nabla;2 Aj &minus; 1c2
&part;2Aj
</p>
<p>&part;t2
= &minus;μ0Jj, for j = 1, 2, 3,
</p>
<p>&nabla;2φ &minus; 1
c2
</p>
<p>&part;2φ
</p>
<p>&part;t2
= &minus; ρ
</p>
<p>ε0
</p>
<p>where &nabla;2 =
&sum;3
</p>
<p>k=1 &part;
2
k is the spatial Laplacian operator with &part;k = &part;/&part;xk .
</p>
<p>If we define the four-potential as A = (A0 = &minus; φc ,A), then the Lorentz gauge
condition is written as (recall the Definition 13.3.2 for the metric ημν)
</p>
<p>3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν &part;μAν = 0,
</p>
<p>where we also define &part;0 = &part;/&part;x0 = &part;/c&part;t. In terms of the four-current
J = (J0 = &minus;ρ/cε0, μ0J), the inhomogeneous Maxwell equations are written as
</p>
<p>3&sum;
</p>
<p>μ,ν
</p>
<p>ημν&part;μ&part;νAρ = &minus;Jρ .
</p>
<p>Using the four-dimensional &lsquo;nabla&rsquo; operator &nabla;&nabla; = (&part;0, &part;1, &part;2, &part;3) we can then
write the Lorentz gauge condition as
</p>
<p>&nabla;&nabla; &middot; A =
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μAν = 0,
</p>
<p>and the inhomogeneous Maxwell equations as
</p>
<p>&nabla;&nabla;2Aρ =
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μ&part;νAρ = &minus;Jρ, for ρ = 0, 1, 2, 3,
</p>
<p>thus generalising to the Minkowski spacetime the analogous operations written for
</p>
<p>the euclidean space E3 in Sect. 1.4.
</p>
<p>Example 13.4.2 From the relations defining the vector fields E and B in terms of the
</p>
<p>four-potential vector A, we can write for their components in the physical space</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Electro-Magnetism 231
</p>
<p>Ba =
3&sum;
</p>
<p>b,c=1
</p>
<p>εabc&part;bAc
</p>
<p>Ea = c(&part;aA0 &minus; &part;0Aa)
</p>
<p>for a = 1, 2, 3. This shows that the quantity
</p>
<p>Fμν = &part;μAν &minus; &part;νAμ
</p>
<p>with μ, ν &isin; {0, . . . , 3}, defines the entries of the antisymmetric field strength matrix
(or more precisely field strength &lsquo;tensor&rsquo;) F given by
</p>
<p>F = (Fμν) =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 &minus;E1/c &minus;E2/c &minus;E3/c
E1/c 0 B3 &minus;B2
E2/c &minus;B3 0 B1
E3/c B2 &minus;B1 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Merging the definition of F with the Lorentz gauge condition we have
</p>
<p>3&sum;
</p>
<p>μ,ν
</p>
<p>ημν&part;μ&part;νAρ =
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μ(Fνρ + &part;ρAν)
</p>
<p>=
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μFνρ + &part;ρ(
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μAν) =
3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μFνρ
</p>
<p>so we can write the inhomogeneous Maxwell equations as
</p>
<p>3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μFνρ = &minus;Jρ for ρ = 0, 1, 2, 3.
</p>
<p>The homogeneous Maxwell equation can be written in a similar way by means
</p>
<p>of another useful quantity, the dual field strength matrix (or tensor) F̃μν . For this
</p>
<p>one needs the (four dimensional) totally antisymmetric symbol εa1a2a3a4 with indices
</p>
<p>aj = 0, 1, 2, 3 and defined by
</p>
<p>εa1a2a3a4 =
</p>
<p>⎧
⎨
⎩
+1 if (a1, a2, a3, a4) is an even permutation of (0,1,2,3)
&minus;1 if (a1, a2, a3, a4) is an odd permutation of (0,1,2,3) .
0 if any two indices are equal
</p>
<p>Also, let η&minus;1 = (ημν) be the inverse of the matrix η = (ημν). The dual field
strength matrix is the antisymmetric matrix defined by</p>
<p/>
</div>
<div class="page"><p/>
<p>232 13 Quadratic Forms
</p>
<p>F̃ = (F̃μν), F̃μν = 12
3&sum;
</p>
<p>α,β,γ,δ=0
εμνγ δ η
</p>
<p>γαηδβFαβ =
</p>
<p>⎛
⎜⎜⎝
</p>
<p>0 B1 B2 B3
&minus;B1 0 &minus;E3/c E2/c
&minus;B2 E3/c 0 &minus;E1/c
&minus;B3 &minus;E2/c E1/c 0
</p>
<p>⎞
⎟⎟⎠ .
</p>
<p>Notice that the elements of F̃ are obtained from those of F by the exchange
</p>
<p>E &harr; &minus;cB.
A straightforward computation shows that the homogeneous Maxwell equations
</p>
<p>can be written as
</p>
<p>3&sum;
</p>
<p>μ,ν=0
</p>
<p>ημν&part;μF̃νρ = 0, for ρ = 0, 1, 2, 3.
</p>
<p>In terms of Fμν rather then F̃μν , these homogeneous equation are the four equations
</p>
<p>&part;ρFμν + &part;μFνρ + &part;νFρμ = 0
</p>
<p>for μ, ν, ρ any three of the integers 0, 1, 2, 3.
</p>
<p>We now have a glimpse at the geometric nature of the four-potential A and of
</p>
<p>the antisymmetric matrix F , that is we study how they transform under a change of
</p>
<p>basis from B to B&prime; for M 4. If two inertial observers (for the orthonormal bases B and
</p>
<p>B&prime; for M 4) relate their spacetime components as in the Example (13.3.4), we know
</p>
<p>from physics that for the transformed electric and magnetic fields E&prime; and B&prime; one has
</p>
<p>E&prime;1 = E1, B
&prime;
1 = B1
</p>
<p>E&prime;2 = γ (E2 &minus; vB3), B
&prime;
2 = γ (B2 + (v/c
</p>
<p>2)E3)
</p>
<p>E&prime;3 = γ (E3 + vB2), B
&prime;
2 = γ (B3 &minus; (v/c
</p>
<p>2)E2)
</p>
<p>For the transformed potential A&prime; = (A&prime;ρ) and matrix F &prime; = (F &prime;μν) with
F &prime;μν = &part; &prime;kA&prime;s &minus; &part; &prime;sA&prime;k (where &part; &prime;a = &part;/&part;x&prime;a), one then finds
</p>
<p>A&prime; = M B&prime;,B A
</p>
<p>and
</p>
<p>F &prime; = t(M B,B&prime;)F M B,B&prime; .
</p>
<p>It is indeed possible to check that such identities are valid for any proper and
</p>
<p>ortochronous Lorentz matrix giving the change of orthonormal basis B &rarr; B&prime;.
If we denote by M 4&lowast; the space dual to (R4, η) with {ǫ0, ǫ1, ǫ2, ǫ3} the basis dual
</p>
<p>to B = (e0, . . . , e3), the definition
</p>
<p>η(ǫa, ǫb) = η(ea, eb)</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Electro-Magnetism 233
</p>
<p>clearly defines a Minkowski quadratic form on R4&lowast;, making then the space M 4&lowast;.
</p>
<p>Also, if B is orthonormal, then B&lowast; is orthonormal as well.
</p>
<p>Recall now the results described in Sect. 8.1 on the dual of a vector space. The
</p>
<p>previous relations, when compared with the Example 13.3.4, show that the vectors
</p>
<p>A = (A0,A) is indeed an element in the dual space M 4&lowast; to M 4 with respect to the
dual basis B&lowast; to B. From the Proposition 13.1.2 we see also that the matrix elements
</p>
<p>F transform as the entries of a quadratic form in M 4&lowast; (although F is antisymmetric).
</p>
<p>All this means that the components of the electro-magnetic fields E,B are the entries
</p>
<p>of an antisymmetric matrix F which transform as a &lsquo;contravariant&rsquo; quadratic form
</p>
<p>under (proper and orthochronous) Lorentz transformations.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
</p>
<p>Affine Linear Geometry
</p>
<p>14.1 Affine Spaces
</p>
<p>Intuitively, an affine space is a vector space without a &lsquo;preferred origin&rsquo;, that is as
</p>
<p>a set of points such that at each of these there is associated a model (a reference)
</p>
<p>vector space.
</p>
<p>Definition 14.1.1 The real affine space of dimension n, denoted by An(R) or simply
</p>
<p>A
n , is the set Rn equipped with the map
</p>
<p>α : An &times; An &rarr; Rn
</p>
<p>given by
</p>
<p>α((a1, . . . , an), (b1, . . . , bn)) = (b1 &minus; a1, . . . , bn &minus; an).
</p>
<p>Notice that the domain of α is the cartesian product of Rn &times; Rn , while the range
</p>
<p>of α is the vector space Rn . The notation An stresses the differences between an
</p>
<p>affine space structure and a vector space structure on the same set Rn . The n-tuples
</p>
<p>of An are called points.
</p>
<p>By A1 we have the affine real line, by A2 the affine real plane, by A3 the affine
</p>
<p>real space. There is an analogous notion of complex affine space An(C), modelled
</p>
<p>on the vector space Cn .
</p>
<p>Remark 14.1.2 The following properties for An easily follows from the above defi-
</p>
<p>nition:
</p>
<p>(p1) for any point P &isin; An and for any vector v &isin; Rn , there exists a unique point Q
</p>
<p>in An such that α(P, Q) = v,
</p>
<p>(p2) for any triple P, Q, R of points in An , it holds that α(P, Q)+ α(Q, R) =
</p>
<p>α(P, R).
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_14
</p>
<p>235</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_14&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_14&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>236 14 Affine Linear Geometry
</p>
<p>Fig. 14.1 The sum rule (Q &minus; P)+ (R &minus; Q) = R &minus; P
</p>
<p>The property (p2) amounts to the sum rule of vectors (see Fig. 14.1).
</p>
<p>Remark 14.1.3 Given the points P, Q &isin; An and the definition of the map α, the
</p>
<p>vector α(P, Q) will be also denoted by
</p>
<p>v = α(P, Q) = Q &minus; P.
</p>
<p>Then, from the property (p1), we shall write
</p>
<p>Q = P + v.
</p>
<p>And property (p2), the sum rule for vectors in Rn , is written as
</p>
<p>(Q &minus; P)+ (R &minus; Q) = R &minus; P.
</p>
<p>Remark 14.1.4 Given an affine space An , from (p2) we have that
</p>
<p>(a) for any P &isin; An it is α(P, P) = 0Rn (setting P = Q = R),
</p>
<p>(b) for any pair of points P, Q &isin; An it is (setting R = P), α(P, Q) = &minus;α(Q, P) .
</p>
<p>A reference system in an affine space is given by selecting a point O &isin; An so
</p>
<p>that from (p1) we have a bijection
</p>
<p>αO : A
n &rarr; Rn, αO(P) = α(O, P) = P &minus; O, (14.1)
</p>
<p>and then a basis B = (v1, . . . , vn) for R
n .
</p>
<p>Definition 14.1.5 The datum (O,B) is called an affine coordinate system or an
</p>
<p>affine reference system for An with origin O and basis B. With respect to a reference
</p>
<p>system (O,B) for An , if
</p>
<p>P &minus; O = (x1, . . . , xn)B = x1v1 + &middot; &middot; &middot; + xnvn
</p>
<p>we call (x1, . . . , xn) the coordinates of the point P &isin; A
n and often write
</p>
<p>P = (x1, . . . , xn). If E is the canonical basis for R
n , then (O, E) is the canonical
</p>
<p>reference system for An .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.1 Affine Spaces 237
</p>
<p>Remark 14.1.6 Once an origin has been selected, the affine space An has the struc-
</p>
<p>tures of Rn as a vector space. Given a reference system (O,B) for An , with
</p>
<p>B = (b1, . . . , bn), the points Ai in A
n given by
</p>
<p>Ai = O + bi
</p>
<p>for i = 1, . . . , n, are called the coordinate points of An with respect to B. They have
</p>
<p>coordinates
</p>
<p>A1 = (1, 0, . . . , 0)B, A2 = (0, 1, . . . , 0)B, . . . An = (0, 0, . . . , 1)B.
</p>
<p>With the canonical basis E = (e1, . . . , en), for R
n the coordinates points Ai = O + ei
</p>
<p>will have coordinates
</p>
<p>A1 = (1, 0, . . . , 0), A2 = (0, 1, . . . , 0), . . . An = (0, 0, . . . , 1).
</p>
<p>Definition 14.1.7 With w &isin; Rn , the map
</p>
<p>Tw : A
n &rarr; An, Tw(P) = P + w.
</p>
<p>is called the translation of An along ws.
</p>
<p>It is clear that Tw is a bijection between A
n and itself, since T&minus;w is the inverse
</p>
<p>map to Tw. Once a reference system has been introduced in A
n , a translation can be
</p>
<p>described by a set of equations, as the following exercise shows.
</p>
<p>Exercise 14.1.8 Let us fix the canonical cartesian coordinate system (O, E) for A3,
</p>
<p>and consider the vector w = (1,&minus;2, 1). If P = (x, y, z) &isin; A3, then
</p>
<p>P &minus; O = xe1 + ye2 + ze3 and we write
</p>
<p>Tw(P)&minus; O = (P + w)&minus; O
</p>
<p>= (P &minus; O)+ w
</p>
<p>= (xe1 + ye2 + ze3)+ (e1 &minus; 2e2 + e3)
</p>
<p>= (x + 1)e1 + (y &minus; 2)e2 + (z + 1)e3,
</p>
<p>so Tw
(
</p>
<p>(x, y, z)) = (x + 1, y &minus; 2, z + 1
)
</p>
<p>.
</p>
<p>Following this exercise, it is easy to obtain the equations for a generic translation.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 14 Affine Linear Geometry
</p>
<p>Proposition 14.1.9 Let An be an affine space with the reference system (O,B). With
</p>
<p>a vector w = (w1, . . . , wn)B in R
n , the translation Tw has the following equations
</p>
<p>Tw
(
</p>
<p>(x1, . . . , xn)B
)
</p>
<p>= (x1 + w1, . . . , xn + wn)B.
</p>
<p>Remark 14.1.10 The translation Tw induces an isomorphism of vector spaces φ :
</p>
<p>R
n &rarr; Rn given by
</p>
<p>P &minus; O �&rarr; Tw(P)&minus; Tw(O).
</p>
<p>It is easy to see that φ is the identity isomorphism. By fixing the orthogonal carte-
</p>
<p>sian reference system (O, E) for An , with corresponding coordinates (x1, . . . , xn)
</p>
<p>for a point P , and a vector w = w1e1 + &middot; &middot; &middot; + wnen , we can write
</p>
<p>R
n &ni; P &minus; O = x1e1 + &middot; &middot; &middot; + xnen
</p>
<p>and
</p>
<p>Tw(P) = (x1 + w1, . . . , xn + wn), Tw(O) = (w1, . . . , wn),
</p>
<p>so that we compute
</p>
<p>Tw(P) &minus; Tw(O) = (Tw(P)&minus; O)&minus; (Tw(O)&minus; O)
</p>
<p>=
(
</p>
<p>(x1 + w1)e1 + &middot; &middot; &middot; (xn + wn)en
)
</p>
<p>&minus; (w1e1 + &middot; &middot; &middot; + wnen)
</p>
<p>= x1e1 + &middot; &middot; &middot; + xnen = P &minus; O.
</p>
<p>More precisely, such an isomorphism is defined between two distinct copies of the
</p>
<p>vector space Rn , those associated to the points O and O &prime; = Tw(O) in A
n thought of
</p>
<p>as the origins of two different reference systems for An . This is depicted in Fig. 14.2.
</p>
<p>Fig. 14.2 The translation Tw</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Lines and Planes 239
</p>
<p>14.2 Lines and Planes
</p>
<p>From the notion of vector line in R2, using the bijection αO : A
2 �&rarr; R2, given in
</p>
<p>(14.1), it is natural to define a (straight) line by the origin the subset in A2 that
</p>
<p>corresponds to L(v) in R2.
</p>
<p>Exercise 14.2.1 Consider v = (1, 2) &isin; R2. The corresponding line by the origin in
</p>
<p>A
2 is the set
</p>
<p>{P &isin; A2 : (P &minus; O) &isin; L(v)} = {(x, y) = λ(1, 2), λ &isin; R}.
</p>
<p>Based on this, we have the following definition.
</p>
<p>Definition 14.2.2 A (straight) line by the origin in An is the subset
</p>
<p>rO = {P &isin; A
n : (P &minus; O) &isin; L(v)}
</p>
<p>for a vector v &isin; Rn\{0}. The vector v is called the direction vector of rO .
</p>
<p>Using the identification between An and Rn given in (14.1) we write
</p>
<p>rO = {P &isin; A
n : P = λv, λ &isin; R},
</p>
<p>or even
</p>
<p>rO : P = λv , λ &isin; R.
</p>
<p>We call such an expression the vector equation for the line rO . Once a reference
</p>
<p>system (O,B) for An is chosen, via the identification of the components of P &minus; O
</p>
<p>with respect to B with the coordinates of a point P , we write the vector equation
</p>
<p>above as
</p>
<p>rO : (x1, . . . , xn) = λ(v1, . . . , vn) , with λ &isin; R
</p>
<p>with v = (v1, . . . , vn) providing the direction of the line.
</p>
<p>Remark 14.2.3 It is clear that the subset rO coincides with L(v), although they
</p>
<p>belong to different spaces, that is rO &sub; A
n while L(v) &sub; Rn . With such a caveat,
</p>
<p>these sets will be often identified.
</p>
<p>Exercise 14.2.4 The line rO in A
3 with direction vector v = (1, 2, 3) has the vector
</p>
<p>equation,
</p>
<p>rO : (x, y, z) = λ(1, 2, 3), λ &isin; R.
</p>
<p>Exercise 14.2.5 Consider the affine space A2 with the orthogonal reference system
</p>
<p>(O, E). The subset given by
</p>
<p>r = {(x, y) = (1, 2)+ λ(0, 1), λ &isin; R}</p>
<p/>
</div>
<div class="page"><p/>
<p>240 14 Affine Linear Geometry
</p>
<p>clearly represents a line that runs parallel to the second reference axis. Under the
</p>
<p>translation Tu with u = (&minus;1,&minus;2) we get the set
</p>
<p>Tu(r) = {P + u, P &isin; r}
</p>
<p>= {(x, y) = λ(0, 1), λ &isin; R},
</p>
<p>which is a line by the origin (indeed the second axis of the reference system). If
</p>
<p>rO = Tu(r), a line by the origin, it is clear that r = Tw(rO), with w = &minus;u.
</p>
<p>This exercise suggests the following definition.
</p>
<p>Definition 14.2.6 A set r &sub; An is called a line if there exist a translation Tw in A
n
</p>
<p>and a line rO by the origin such that r = Tw(rO).
</p>
<p>Being the sets rO and L(v) in R
n coincident, we shall refer to L(v) as the direction
</p>
<p>of r , and we shall denote it by Sr (with the letter S referring to the fact that L(v) is
</p>
<p>a vector subspace in Rn). Notice that, for a line, it is dim(Sr ) = 1.
</p>
<p>The equation for an arbitrary line follows easily from that of a line by the origin.
</p>
<p>Let us consider a line by the origin,
</p>
<p>rO : P = λv , λ &isin; R,
</p>
<p>and the translation Tw with w &isin; R
n . If w = Q &minus; O , the line r = Tw(rO) is given by
</p>
<p>r = {P &isin; An : P = Tw(PO), PO &isin; rO}
</p>
<p>= {P &isin; An : P = Q + λv, λ &isin; R},
</p>
<p>so we write
</p>
<p>r : P = Q + λv. (14.2)
</p>
<p>With respect to a reference system (O,B), where Q = (q1, . . . , qn)B and
</p>
<p>v = (v1, . . . , vn)B, the previous equation can be written as
</p>
<p>r : (x1, . . . , xn) = (q1, . . . , qn)+ λ(v1, . . . , vn), (14.3)
</p>
<p>or equivalently
</p>
<p>r :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x1 = q1 + λv1
...
</p>
<p>xn = qn + λvn
</p>
<p>. (14.4)
</p>
<p>Definition 14.2.7 The expression (14.2) (or equivalently 14.3) is called the vector
</p>
<p>equation of the line r , while the expression (14.4) is called the parametric equation
</p>
<p>of r (stressing that λ is a real parameter).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Lines and Planes 241
</p>
<p>Fig. 14.3 The translation Tw&prime; with w
&prime; &minus; w &isin; L(v) maps r into rO
</p>
<p>Remark 14.2.8 Consider the line whose vector equation is r : P = Q + λv.
</p>
<p>(a) We have a unique point in r for each value of λ, and selecting a point of r gives
</p>
<p>a unique value for λ. The point in r is Q if and only if λ = 0;
</p>
<p>(b) The direction of r is clearly the vector line L(v). This means that the direction
</p>
<p>vector v is not uniquely determined by the equation, since each elementv&prime; &isin; L(v)
</p>
<p>is a direction vector for r . This arbitrariness can be re-absorbed by a suitable
</p>
<p>rescaling of the parameter λ: with a rescaling the equation for r can always be
</p>
<p>written in the given form with v its direction vector.
</p>
<p>(c) The point Q is not unique. As the Fig. 14.3 shows, if Q = O + w is a point in
</p>
<p>r , then any translation Tw&prime; with w
&prime; &minus; w &isin; L(v) maps r into the same line by
</p>
<p>the origin.
</p>
<p>Exercise 14.2.9 We check whether the following lines coincide:
</p>
<p>r : (x, y) = (1, 2)+ λ(1,&minus;1),
</p>
<p>r &prime; : (x, y) = (2, 1)+ &micro;(1,&minus;1).
</p>
<p>Clearly r and r &prime; have the same direction, which is Sr = Sr &prime; = L((1,&minus;1)) = rO .
</p>
<p>If we consider Q = (1, 2) &isin; r and Q&prime; = (2, 1) &isin; r &prime; with w = Q &minus; O = (1, 2) and
</p>
<p>w&prime; = Q&prime; &minus; O = (2, 1) we compute,
</p>
<p>r = Tw(rO), r
&prime; = Tw&prime;(rO).
</p>
<p>We have that r coincides with r &prime;: as described in the remark above,
</p>
<p>w &minus; w&prime; = (&minus;1, 1) &isin; L((1,&minus;1)).
</p>
<p>In analogy with the definition of affine lines, one defines planes in An .
</p>
<p>Definition 14.2.10 A plane through the origin in An is any subset of the form
</p>
<p>πO = {P &isin; A
n : (P &minus; O) &isin; L(u, v)},
</p>
<p>with two linearly independent vectors u, v &isin; Rn .</p>
<p/>
</div>
<div class="page"><p/>
<p>242 14 Affine Linear Geometry
</p>
<p>With the usual identification of a point P &isin; An with its image α(P) &isin; Rn
</p>
<p>(see 14.1), we write
</p>
<p>πO = {P &isin; A
n : P = λu + &micro;v, λ,&micro; &isin; R},
</p>
<p>or also
</p>
<p>πO : P = λu + &micro;v
</p>
<p>with λ,&micro; real parameters.
</p>
<p>Definition 14.2.11 A subset π &sub; An is called a plane if there exist a translation
</p>
<p>map Tw in A
n and a plane πO through the origin such that π = Tw(πO). Since we
</p>
<p>can identify the elements in πO with the vectors in L(u, v) &sub; R
n , generalising the
</p>
<p>analogue Definition 14.2.6 for a line, we define the space Sπ = L(u, v) to be the
</p>
<p>direction of π. Notice that dim(Sπ) = 2.
</p>
<p>If Q = Tw(O), that is w = Q &minus; O , the points P &isin; π are characterised by
</p>
<p>P = Q + λu + &micro;v. (14.5)
</p>
<p>Let (O,B) be a reference system for An . If Q = (q1, . . . , qn)B &isin; A
n , with
</p>
<p>u = (u1, . . . , un)B and v = (v1, . . . , vn)B &isin; R
n , the above equation can be written
</p>
<p>as
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x1 = q1 + λu1 + &micro;v1
...
</p>
<p>xn = qn + λuv + &micro;vn
</p>
<p>. (14.6)
</p>
<p>The relation (14.5) is the vector equation of the plane π, while (14.6) is a parametric
</p>
<p>equation of π.
</p>
<p>Exercise 14.2.12 Given the linearly independent vectors v1 = (1, 0, 1) and
</p>
<p>v2 = (1,&minus;1, 0) with respect to the basis B in R
3, the plane πO through the origin
</p>
<p>associated to them is the set of points P &isin; A3 given by the vector equation
</p>
<p>P = λ1v1 + λ2v2, λ1,λ2 &isin; R.
</p>
<p>With the reference system (O,B), with P = (x, y, z) its parametric equations is
</p>
<p>(x, y, z) = λ1(1, 0, 1)+ λ2(1,&minus;1, 0) &hArr; π :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = λ1 + λ2
y = &minus;λ2
z = λ1
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Lines and Planes 243
</p>
<p>Exercise 14.2.13 Given the translation Tw in A
3 with w = (1,&minus;1, 2) in a basis B,
</p>
<p>the plane πO of the previous exercise is mapped into the plane π = Tw(πO) whose
</p>
<p>vector equation is
</p>
<p>π : P = Q + λ1v1 + λ2v2,
</p>
<p>with Q = Tw(O) = (1,&minus;1, 2). We can equivalently represent the points in π as
</p>
<p>π : (x, y, z) = (1,&minus;1, 2)+ λ1(1, 0, 1)+ λ2(1,&minus;1, 0).
</p>
<p>Exercise 14.2.14 Let us consider the vectors v1, v2 in R
4 with the following com-
</p>
<p>ponents
</p>
<p>v1 = (1, 0, 1, 0), v2 = (2, 1, 0,&minus;1)
</p>
<p>in a basis B, and the point Q in A4 with coordinates
</p>
<p>Q = (2, 1, 1, 2).
</p>
<p>in the corresponding reference system (O,B). The plane π &sub; A4 through Q whose
</p>
<p>direction is Sπ = L(v1, v2) has the vector equation
</p>
<p>π : (x1, x2, x3, x4) = (2, 1, 1, 2)+ λ1(1, 0, 1, 0)+ λ2(2, 1, 0,&minus;1)
</p>
<p>and parametric equation
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x1 = 2 + λ1 + 2λ2
x2 = 1 + λ2
x3 = 1 + λ1
x4 = 2 &minus; λ2
</p>
<p>.
</p>
<p>Remark 14.2.15 The natural generalisation of the Remark 14.2.8 holds for planes as
</p>
<p>well. A vector equation for a given plane π is not unique. If
</p>
<p>π : P = Q + λu + &micro;v
</p>
<p>π&prime; : P = Q&prime; + λu&prime; + &micro;v&prime;
</p>
<p>are two planes in An , then
</p>
<p>π = π&prime; &hArr;
</p>
<p>{
</p>
<p>Sπ = Sπ&prime; (that is L(u, v) = L(u
&prime;, v&prime;))
</p>
<p>Q &minus; Q&prime; &isin; Sπ
</p>
<p>Proposition 14.2.16 Given two distinct points A, B in An (with n &ge; 2), there is only
</p>
<p>one line through A and B. A vector equation is
</p>
<p>rAB : P = A + λ(B &minus; A).</p>
<p/>
</div>
<div class="page"><p/>
<p>244 14 Affine Linear Geometry
</p>
<p>Proof Being A 
= B, this vector equation gives a line since B &minus; A is a non zero
</p>
<p>vector and the set of points P &minus; A is a one dimensional vector space (that is the
</p>
<p>direction is one dimensional). The equation rAB contains A (for λ = 0) and B (for
</p>
<p>λ = 1). This shows there exists a line through A and B.
</p>
<p>Let us consider another line rA through A. Its vector equation will be P = A + &micro;v,
</p>
<p>with v &isin; Rn and &micro; a real parameter. The point B is contained in rA if and only if
</p>
<p>there exists a value&micro;0 of the parameter such that B = A + &micro;0v, that is B &minus; A = &micro;0v.
</p>
<p>Thus the direction of rA would be SrA = L(v) = L(B &minus; A) = SrAB . The line rA then
</p>
<p>coincides with rAB . ⊓⊔
</p>
<p>Exercise 14.2.17 The line in A2 through the points A = (1, 2) and B = (1,&minus;2) has
</p>
<p>equation
</p>
<p>P = (x, y) = (1, 2)+ λ(0,&minus;4).
</p>
<p>Exercise 14.2.18 Let the points A and B in A3 have coordinates A = (1, 1, 1) and
</p>
<p>B = (1, 2,&minus;2). The line rAB through them has the vector
</p>
<p>(x, y, z) = (1, 1, 1)+ λ(0, 1,&minus;3).
</p>
<p>Does the point P = (1, 0, 4) belong to rAB? In order to answer this question we
</p>
<p>need to check whether there is a λ &isin; R that solves the linear system
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>1 = 1
</p>
<p>0 = 1 + λ
</p>
<p>4 = 1 &minus; 3λ.
</p>
<p>It is evident that λ = &minus;1 is a solution, so P is a point in rAB .
</p>
<p>An analogue of the Proposition 14.2.16 holds for three points in an affine space.
</p>
<p>Proposition 14.2.19 Let A, B,C be three points in an affine space An (with n &ge; 3).
</p>
<p>If they are not contained in the same line, there exists a unique plane πABC through
</p>
<p>them, with a vector equation given by
</p>
<p>πABC : P = A + λ(B &minus; A)+ &micro;(C &minus; A).
</p>
<p>Proof The vectors B &minus; A and C &minus; A are linearly independent, since they are not
</p>
<p>contained in the same line. The direction of πABC is then two dimensional, with
</p>
<p>SπABC = L(B &minus; A,C &minus; A). The point A is inπABC , corresponding to P(λ = &micro; = 0);
</p>
<p>the point B is in πABC , corresponding to P(λ = 1,&micro; = 0); the point C is in πABC
corresponding to P(λ = 0,&micro; = 1).
</p>
<p>We have then proven that a plane through A, B,C exists. Let us suppose that
</p>
<p>π&prime; : P = A + λu + &micro;v.
</p>
<p>gives a plane through the points A, B,C (which are not on the same line) with u and
</p>
<p>v linearly independent (so its direction is given by Sπ&prime; = L(u, v)). This means that</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Lines and Planes 245
</p>
<p>B &minus; A &isin; L(u, v) and C &minus; A &isin; L(u, v). Since the spaces are both two dimensional,
</p>
<p>this reads L(B &minus; A,C &minus; A) = L(u, v), proving that π&prime; coincides with πABC . ⊓⊔
</p>
<p>Exercise 14.2.20 Let A = (1, 2, 0), B = (1, 1, 1) and C = (0, 1,&minus;1) be three
</p>
<p>points in A3. They are not on the same line, since the vectors B &minus; A = (0,&minus;1, 1)
</p>
<p>and C &minus; A = (&minus;1,&minus;1,&minus;1) are linearly independent. A vector equation of the plane
</p>
<p>πABC is
</p>
<p>π : (x, y, z) = (1, 2, 0)+ λ(0,&minus;1, 1)+ &micro;(&minus;1,&minus;1,&minus;1).
</p>
<p>14.3 General Linear Affine Varieties and Parallelism
</p>
<p>The natural generalisation of (straight) lines and planes leads to the definition of a
</p>
<p>linear affine variety L in An , where the direction of L is a subspace in Rn of dimension
</p>
<p>greater than 2.
</p>
<p>Definition 14.3.1 A linear affine variety of dimension k in An is a set
</p>
<p>L = {P &isin; An : (P &minus; Q) &isin; V },
</p>
<p>where Q is a point in the affine space An and V &sub; Rn is a vector subspace of
</p>
<p>dimension k in Rn . The vector subspace V is called the direction of the variety L ,
</p>
<p>and denoted by SL = V . If V = L(v1, . . . , vk), a vector equation for L is
</p>
<p>L : P = Q + λ1v1 + &middot; &middot; &middot; + λkvk
</p>
<p>for scalars λ1, . . . ,λk in R.
</p>
<p>Remark 14.3.2 It is evident that a line is a one dimensional linear affine variety,
</p>
<p>while a plane is a two dimensional linear affine variety.
</p>
<p>Definition 14.3.3 An linear affine variety of dimension n &minus; 1 in An is called a
</p>
<p>hyperplane in An .
</p>
<p>It is clear that a line is a hyperplane in A2, while a plane is a hyperplane in A3.
</p>
<p>Exercise 14.3.4 We consider the affine space A4, the point Q with coordinates
</p>
<p>Q = (2, 1, 1, 2) with respect to a given reference system (O,B), and the vector sub-
</p>
<p>space S = L(v1, v2, v3) in R
4 with generators v1 = (1, 0, 1, 0), v2 = (2, 1, 0,&minus;1),
</p>
<p>v3 = (0, 0,&minus;1, 1) with respect to B. The vector equation of the linear affine variety
</p>
<p>L with direction SL = L(v1, v2, v3) and containing Q is
</p>
<p>L : (x1, x2, x3, x4) = (2, 1, 1, 2)+ λ1(1, 0, 1, 0)+ λ2(2, 1, 0,&minus;1)+ λ3(0, 0,&minus;1, 1),</p>
<p/>
</div>
<div class="page"><p/>
<p>246 14 Affine Linear Geometry
</p>
<p>while its parametric equation reads
</p>
<p>L :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x1 = 2 + λ1 + 2λ2
x2 = 1 + λ2
x3 = 1 + λ1 &minus; λ3
x4 = 2 &minus; λ2 + λ3
</p>
<p>.
</p>
<p>Definition 14.3.5 Let L , L &prime; be two linear affine varieties of the same dimension k
</p>
<p>in An . We say that L is parallel to L &prime; if they have the same directions, that is if
</p>
<p>SL = SL &prime; .
</p>
<p>Exercise 14.3.6 Let L O &sub; A
n be a line through the origin. A line L in An is parallel
</p>
<p>to L O if and only if L = Tw(L O), for w &isin; R
n . From the Remark 14.2.15 we know
</p>
<p>that L = L O if and only if w &isin; SL .
</p>
<p>Let us consider the line through the origin in A2 given by L O : (x, y) = λ(3,&minus;2).
</p>
<p>A line will be parallel to L O if and only if its vector equation is given by
</p>
<p>L : (x, y) = (α,β)+ λ(3,&minus;2),
</p>
<p>with (α,β) &isin; R2. The line L is moreover distinct from L &prime; if and only if (α,β) /&isin; SL .
</p>
<p>Definition 14.3.7 Let us consider in An a linear affine variety L of dimension k and
</p>
<p>a second linear affine variety L &prime; of dimension k &prime;, with k &gt; k &prime;. The variety L is said
</p>
<p>to be parallel to L &prime; if SL &prime; &sub; SL , that is if the direction of L
&prime; is a subspace of the
</p>
<p>direction of L .
</p>
<p>Exercise 14.3.8 Let us consider in A3 the plane given by
</p>
<p>π : (x, y, z) = (0, 2,&minus;1)+ λ1(1, 0, 1)+ λ2(0, 1, 1).
</p>
<p>We check whether the following lines,
</p>
<p>r1 : (x, y, z) = λ(1,&minus;1, 0)
</p>
<p>r2 : (x, y, z) = (0, 3, 0)+ λ(1, 1, 2)
</p>
<p>r3 : (x, y, z) = (1,&minus;1, 1)+ λ(1, 1, 1),
</p>
<p>are parallel to π.
</p>
<p>If Sπ denotes the direction ofπ, we clearly have that Sπ=L(w1, w2) = L((1, 0, 1),
</p>
<p>(0, 1, 1)), while we denote by vi a vector spanning the direction Sri of the line
</p>
<p>ri , i = 1, 2, 3. To verify whether Sri &sub; Sπ it is sufficient to compute the rank of the
</p>
<p>matrix whose rows are given by (w1, w2, vi ).
</p>
<p>&bull; For i = 1, after a reduction procedure we have,
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>w1
w2
v1
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>1 &minus;1 0
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 General Linear Affine Varieties and Parallelism 247
</p>
<p>Since this matrix has rank 2, we have that v1 &isin; L(w1, w2), that is Sr1 &sub; Sπ . We
</p>
<p>conclude that r1 is parallel to π. One also checks that r1 
&sub; π, since (0, 0, 0) &isin; r1
but (0, 0, 0) /&isin; π. To show this, one notices that the origin (0, 0, 0) is contained in
</p>
<p>π if and only if the linear system
</p>
<p>(0, 0, 0) = (0, 2,&minus;1)+ λ1(1, 0, 1)+ λ2(0, 1, 1) &rArr;
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>0 = λ1
0 = 2 + λ2
0 = &minus;1 + λ1 + λ2
</p>
<p>has a solution. It is evident that such a solution does not exist.
</p>
<p>&bull; For i = 2 we proceed as above. The following reduction by rows
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>w1
w2
v2
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>1 1 2
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>0 1 1
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>shows that v2 &isin; L(w1, w2), thus r2 is parallel to π. Now r2 &sub; π: a point P is in r2
if and only there exists a λ &isin; R such that P = (λ,λ+ 3, 2λ). For any value of λ,
</p>
<p>the linear system
</p>
<p>(λ,λ+ 3, 2λ) = (0, 2,&minus;1)+ λ1(1, 0, 1)+ λ2(0, 1, 1) &rArr;
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λ = λ1
λ+ 3 = 2 + λ2
2λ
</p>
<p>has the unique solution λ1 = λ, λ2 = λ+ 1.
</p>
<p>&bull; For i = 3 the following reduction by rows
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>w1
w2
v3
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>1 1 1
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 1
</p>
<p>0 1 1
</p>
<p>0 1 0
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>shows that the matrix t (w1, w2, v3) has rank 3, so r3 is not parallel to π.
</p>
<p>Definition 14.3.9 Let L , L &prime; &sube; An two distinct linear affine varieties. We say that L
</p>
<p>and L &prime; are incident if their intersection is non empty, while they are said to be skew
</p>
<p>if they are neither parallel nor incident.
</p>
<p>Remark 14.3.10 It is easy to see that two lines (or a line and a plane) are incident
</p>
<p>if they have a common point. Two distinct planes in An (with n &ge; 3) are incident if
</p>
<p>they have a common line.
</p>
<p>Exercise 14.3.11 In the affine space A3 we consider the line r3 and the plane π
</p>
<p>as in the Exercise 14.3.8. We know already that they are not parallel, and a point
</p>
<p>P = (x, y, z) belongs to the intersection r3 &cap; π if and only if there exists aλ such that</p>
<p/>
</div>
<div class="page"><p/>
<p>248 14 Affine Linear Geometry
</p>
<p>P = (1 + λ,&minus;1 + λ, 1 + λ) &isin; r3 and there exist scalars λ1,λ2 such that
</p>
<p>P = (λ1, 2 + λ2,&minus;1 + λ1 + λ2) &isin; π. These conditions are equivalent to the linear
</p>
<p>system
⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>1 + λ = λ1
&minus;1 + λ = 2 + λ2
1 + λ = &minus;1 + λ1 + λ2
</p>
<p>that has the unique solution (λ = 4,λ1 = 5,λ2 = 1). This corresponds to
</p>
<p>P = (5, 3, 5) &isin; r3 &cap; π.
</p>
<p>Exercise 14.3.12 Consider again the lines r1 and r2 in the Exercise 14.3.8. We know
</p>
<p>they are not parallel, since v1 /&isin; L(v2). They are not incident: there are indeed no
</p>
<p>values of λ and &micro; such that a point P = λ(1,&minus;1, 0) in r1 coincides with a point
</p>
<p>P = (0, 3, 0)+ &micro;(1, 1, 2) in r2, since the following linear system
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λ = &micro;
</p>
<p>&minus;λ = 3 + &micro;
</p>
<p>0 = 2&micro;
</p>
<p>has no solution. Thus r1 and r2 are skew.
</p>
<p>Exercise 14.3.13 Given the planes
</p>
<p>π : (x, y, z) = (0, 2,&minus;1)+ λ1(1, 0, 1)+ λ2(0, 1, 1)
</p>
<p>π&prime; : (x, y, z) = (1,&minus;1, 1)+ λ1(0, 0, 1)+ λ2(2, 1,&minus;1)
</p>
<p>in A3, we determine all the lines which are parallel to both π and π&prime;.
</p>
<p>We denote by r a generic line satisfying such a condition. From the Defini-
</p>
<p>tion 14.3.5, we require that Sr &sube; Sπ &cap; Sπ&prime; for the direction Sr of r . Since
</p>
<p>Sπ = L((1, 0, 1), (0, 1, 1)) while Sπ&prime; = L((0, 0, 1), (2, 1,&minus;1)), in order to compute
</p>
<p>Sπ &cap; Sπ&prime; we write the condition
</p>
<p>α(1, 0, 1)+ β(0, 1, 1) = α&prime;(0, 0, 1)+ β&prime;(2, 1,&minus;1)
</p>
<p>as the linear homogeneous system for (α,β,α&prime;,β&prime;) given by � : AX = 0 with
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 0 0 2
</p>
<p>0 1 0 1
</p>
<p>1 1 1 &minus;1
</p>
<p>⎞
</p>
<p>⎠ , X =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>α
</p>
<p>β
</p>
<p>&minus;α&prime;
</p>
<p>&minus;β&prime;
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>The space of solution for such a linear system is easily found to be
</p>
<p>S� = {(α,β,&minus;α
&prime;,&minus;β&prime;) = t (2, 1,&minus;4,&minus;1) : t &isin; R},</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 General Linear Affine Varieties and Parallelism 249
</p>
<p>so we have that the intersection Sπ &cap; Sπ&prime; is one dimensional and spanned by the
</p>
<p>vector
</p>
<p>2(1, 0, 1)+ (0, 1, 1) = 4(0, 0, 1)+ (2, 1,&minus;1) = (2, 1, 3).
</p>
<p>This gives that Sr = L((2, 1, 3)), so we finally write
</p>
<p>r : (x, y, z) = (a, b, c)+ λ(2, 1, 3).
</p>
<p>for an arbitrary (a, b, c) &isin; A3.
</p>
<p>14.4 The Cartesian Form of Linear Affine Varieties
</p>
<p>In the previous sections we have seen that a linear affine variety can be described
</p>
<p>either with a vector equation or a parametric equation. In this section we relate linear
</p>
<p>affine varieties to systems of linear equations.
</p>
<p>Proposition 14.4.1 A linear affine variety L &sube; An corresponds to the space of the
</p>
<p>solutions of an associated linear system with m equations in n unknowns, that is
</p>
<p>�L : AX = B, for A &isin; R
m,n . (14.7)
</p>
<p>Moreover, the space of solutions of the corresponding homogeneous linear system
</p>
<p>describes the direction space SL = L O of L, that is
</p>
<p>�L O : AX = 0.
</p>
<p>We say that the linear system �L given in (14.7) is the cartesian equation for
</p>
<p>the linear affine variety L of dimension n &minus; rk(A). By computing the space of the
</p>
<p>solutions of �L in terms of n &minus; rk(A) parameters, one gets the parametric equation
</p>
<p>for L . Conversely, given the parametric equation of L , its corresponding cartesian
</p>
<p>equation is given by consistently &lsquo;eliminating&rsquo; all the parameters in the parametric
</p>
<p>equation. This linear affine variety can be represented both by a cartesian equation
</p>
<p>and by a parametric equation, which are related as
</p>
<p>linear system � : AX = B
</p>
<p>(cartesian equation)
&lArr;&rArr;
</p>
<p>space of the solutions for � : AX = B
</p>
<p>(parametric equation)
</p>
<p>Notice that for a linear affine variety L a cartesian equation is not uniquely deter-
</p>
<p>mined: any linear system �&prime; which is equivalent to �L (that is for which the spaces
</p>
<p>of the solutions for �L and �
&prime; coincide) describe the same linear affine variety. An
</p>
<p>analogue result holds for the direction space of L , which is equivalently described
</p>
<p>by any homogenous linear system �&prime;O equivalent to �L O .</p>
<p/>
</div>
<div class="page"><p/>
<p>250 14 Affine Linear Geometry
</p>
<p>We avoid an explicit proof of the Proposition 14.4.1 in general, and analyse the
</p>
<p>equivalence between the two descriptions via the following examples.
</p>
<p>Exercise 14.4.2 Let us consider the line r &sub; A2 with parametric equation
</p>
<p>r :
</p>
<p>{
</p>
<p>x = 1 + λ
</p>
<p>y = 2 &minus; λ
.
</p>
<p>We can express the parameter λ in terms of x from the first relation, that is
</p>
<p>λ = x &minus; 1, and replace this in the second relation, having
</p>
<p>x + y &minus; 3 = 0.
</p>
<p>We set
</p>
<p>s = {(x, y) &isin; A2 : x + y &minus; 3 = 0}
</p>
<p>and show that s coincides with r . Clearly r &sube; s, since a point with coordinates
</p>
<p>(1 + λ, 2 &minus; λ) &isin; r solves the linear equation for s:
</p>
<p>(1 + λ)+ (2 &minus; λ)&minus; 3 = 0.
</p>
<p>In order to prove that s &sube; r , consider a point P = (x, y) &isin; s, so that
</p>
<p>P = (x, y = 3 &minus; x) for any value of x : this means considering x as a real parame-
</p>
<p>ter. By writing λ = x &minus; 1, we have P = (x = λ+ 1, y = 2 &minus; λ) for any λ &isin; R, so
</p>
<p>P &isin; r . We have then s = r as linear affine varieties.
</p>
<p>Proposition 14.4.3 Given a, b, c in R with (a, b) 
= (0, 0), the solutions of the equa-
</p>
<p>tion
</p>
<p>�r : ax + by + c = 0 (14.8)
</p>
<p>provide the coordinates of all the points P = (x, y) of a line r in A2 whose direc-
</p>
<p>tion Sr = L((&minus;b, a)) is given by the solutions of the associated linear homogenous
</p>
<p>equation
</p>
<p>�rO : ax + by = 0.
</p>
<p>Moreover, if r &sub; A2 is a line with direction Sr = L((&minus;b, a)), then there exists c &isin; R
</p>
<p>such that the cartesian form for the equation of r is given by (14.8).
</p>
<p>Proof We start by showing that the solutions of (14.8) give the coordinates of the
</p>
<p>points representing the line with direction L((&minus;b, a)) in parametric form.
</p>
<p>Let us assume a 
= 0. We can then write the space of the solutions for (14.8) as
</p>
<p>(x, y) = (&minus;
b
</p>
<p>a
&micro; &minus;
</p>
<p>c
</p>
<p>a
,&micro;)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Cartesian Form of Linear Affine Varieties 251
</p>
<p>where &micro; &isin; R is a parameter. By rescaling the parameter, that is defining λ = &micro;/a,
</p>
<p>we write the space of solutions as the points having coordinates,
</p>
<p>(x, y) = (&minus;bλ &minus;
c
</p>
<p>a
, aλ)
</p>
<p>= (&minus;
c
</p>
<p>a
, 0) + λ(&minus;b, a).
</p>
<p>This expression gives the vector (and the parametric) equation of a line through
</p>
<p>(&minus;c/a, 0) with direction Sr = L((&minus;b, a)).
</p>
<p>If a = 0, we can write the space of the solutions for (14.8) as
</p>
<p>(x, y) = (&micro;,&minus;
c
</p>
<p>b
)
</p>
<p>where &micro; &isin; R is a parameter. By rescaling the parameter, we can write
</p>
<p>(x, y) = (&minus;λb,&minus;
c
</p>
<p>b
) = (0,&minus;
</p>
<p>c
</p>
<p>b
)+ λ(&minus;b, 0),
</p>
<p>giving the vector equation of a line through the point (0,&minus;c/b) with direction
</p>
<p>Sr = L((&minus;b, 0)).
</p>
<p>Now let r be a line in A2 with direction Sr = L((&minus;b, a)). Its parametric equation
</p>
<p>is of the form
</p>
<p>r :
</p>
<p>{
</p>
<p>x = x0 &minus; bλ
</p>
<p>y = y0 + aλ
</p>
<p>where (x0, y0) is an arbitrary point in A
2. If a 
= 0, we can eliminate λ by setting
</p>
<p>λ =
y &minus; y0
</p>
<p>a
</p>
<p>from the second relation and then
</p>
<p>x = x0 &minus;
b
</p>
<p>a
(y &minus; y0),
</p>
<p>resulting into the linear equation
</p>
<p>ax + by + c = 0
</p>
<p>with c = &minus;(ax0 + by0).
</p>
<p>If a = 0 then b 
= 0, so by rescaling the parameter as &micro; = x0 &minus; λb, the points of
</p>
<p>the line r are (x = &micro;, y = y0). This is indeed the set of the solutions of the linear
</p>
<p>equation
</p>
<p>ax + by + c = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>252 14 Affine Linear Geometry
</p>
<p>with a = 0 and c = &minus;by0. We have then shown that any line with a given direction
</p>
<p>has the cartesian form given by a suitable linear equation (14.8). ⊓⊔
</p>
<p>The equation ax + by + c = 0 is called the cartesian equation of a line in A2.
</p>
<p>Remark 14.4.4 As already mentioned, a line does not uniquely determine its carte-
</p>
<p>sian equation. With ax + by + c = 0 the cartesian equation for r , any other linear
</p>
<p>equation
</p>
<p>ρax + ρby + ρc = 0, with ρ 
= 0
</p>
<p>yields a cartesian equation for the same line, since
</p>
<p>ρax + ρby + ρc = 0 &hArr; ρ(ax + by + c) = 0 &hArr; ax + by + c = 0.
</p>
<p>Exercise 14.4.5 The line�r : 2x &minus; y + 3 = 0 in A
2 has direction�rO : 2x &minus; y = 0,
</p>
<p>or Sr = L((1, 2)).
</p>
<p>Exercise 14.4.6 We turn now to the description of a plane in the three dimensional
</p>
<p>affine space in terms of a cartesian equation. Let us consider the plane π &sub; A3 with
</p>
<p>parametric equation
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2λ+ &micro;
</p>
<p>y = 2 &minus; λ&minus; &micro;
</p>
<p>z = &micro;
</p>
<p>.
</p>
<p>We eliminate the parameter &micro; by setting &micro; = z from the third relation, and write
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2λ+ z
</p>
<p>y = 2 &minus; λ&minus; z
</p>
<p>&micro; = z
</p>
<p>.
</p>
<p>We can then eliminate the parameter λ by using the second (for example) relation,
</p>
<p>so to have λ = 2 &minus; y &minus; z and write
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2(2 &minus; y &minus; z)+ z
</p>
<p>λ = 2 &minus; y &minus; z
</p>
<p>&micro; = z
</p>
<p>.
</p>
<p>Since these relations are valid for any choice of the parameters λ and &micro;, we have
</p>
<p>a resulting linear equation with three unknowns:
</p>
<p>�π : x + 2y + z &minus; 5 = 0.
</p>
<p>Such an equation still representsπ, since every point P &isin; π solves the equation (as
</p>
<p>easily seen by taking P = (1 + 2λ+ &micro;, 2 &minus; λ&minus; &micro;,&micro;)) and the space of solutions
</p>
<p>of �π coincides with the set π.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Cartesian Form of Linear Affine Varieties 253
</p>
<p>This example has a general validity for representing in cartesian form a plane in
</p>
<p>A
3. A natural generalisation of the proof of the previous Proposition 14.4.3 allows
</p>
<p>one to show the following result.
</p>
<p>Proposition 14.4.7 Given a, b, c, d in R with (a, b, c) 
= (0, 0, 0), the solutions of
</p>
<p>the equation
</p>
<p>�π : ax + by + cz + d = 0 (14.9)
</p>
<p>provide the coordinates of all the points P = (x, y, z) of a plane π in A3 whose
</p>
<p>direction Sπ is given by the solutions of the associated linear homogenous equation
</p>
<p>�πO : ax + by + cz = 0. (14.10)
</p>
<p>If π &sub; A3 is a plane with direction Sπ = R
2 given by the space of the solutions
</p>
<p>of (14.10), then there exists d &isin; R such that the cartesian form for the equation of
</p>
<p>π is given by (14.9).
</p>
<p>The equation
</p>
<p>ax + by + cz + d = 0
</p>
<p>is called the cartesian equation of a plane in A3.
</p>
<p>Remark 14.4.8 Analogously to what we noticed in the Remark 14.4.4, the cartesian
</p>
<p>equation of a planeπ in A3 is not uniquely determined, since it can be again multiplied
</p>
<p>by a non zero scalar.
</p>
<p>Exercise 14.4.9 We next look for a cartesian equation for a line in A3. As usual,
</p>
<p>by way of an example, we start by considering the parametric equation of the line
</p>
<p>r &sub; A3 given by
</p>
<p>r :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2λ
</p>
<p>y = 2 &minus; λ
</p>
<p>z = λ
</p>
<p>.
</p>
<p>By eliminating the parameter λ via (for example) the third relation λ = z we have
</p>
<p>r :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2z
</p>
<p>y = 2 &minus; z
</p>
<p>λ = z
</p>
<p>.
</p>
<p>Since the third relations formally amounts to redefine a parameter, we write
</p>
<p>�r :
</p>
<p>{
</p>
<p>x &minus; 2z &minus; 1 = 0
</p>
<p>y + z &minus; 2 = 0
,
</p>
<p>which is a linear system with three unknowns and rank 2, thus having &infin;1 solutions.
</p>
<p>In analogy with the procedure used above for the other examples, it is easy to show
</p>
<p>that the space of solutions of �r coincides with the line r in A
3.</p>
<p/>
</div>
<div class="page"><p/>
<p>254 14 Affine Linear Geometry
</p>
<p>The following result is the natural generalisation of the Propositions 14.4.3 and
</p>
<p>14.4.7.
</p>
<p>Proposition 14.4.10 Given the (complete, see the Definition 6.1.5) matrix
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>a1 b1 c1 &minus;d1
a2 b2 c2 &minus;d2
</p>
<p>)
</p>
<p>&isin; R2,4
</p>
<p>with
</p>
<p>rk(A) = rk
</p>
<p>(
</p>
<p>a1 b1 c1
a2 b2 c2
</p>
<p>)
</p>
<p>= 2,
</p>
<p>the solutions of the linear system
</p>
<p>�π : AX = B &hArr;
</p>
<p>{
</p>
<p>a1x + b1 y + c1z + d1 = 0
</p>
<p>a2x + b2 y + c2z + d2 = 0
(14.11)
</p>
<p>provide the coordinates of all the points P = (x, y, z)of a line r in A3 whose direction
</p>
<p>Sr is given by the solutions of the associated linear homogenous system
</p>
<p>�rO : AX = 0. (14.12)
</p>
<p>If r &sub; A3 is a line whose direction Sr = R is given by the space of the solutions
</p>
<p>of the linear homogenous system (14.12) with A &isin; R3,2 and rk(A) = 2, then there
</p>
<p>exists a vector B = t (&minus;d1,&minus;d2) such that the cartesian form for the equation of r
</p>
<p>is given by (14.11).
</p>
<p>The linear system
</p>
<p>�r :
</p>
<p>{
</p>
<p>a1x + b1 y + c1z + d1 = 0
</p>
<p>a2x + b2 y + c2z + d2 = 0
</p>
<p>with rk
</p>
<p>(
</p>
<p>a1 b1 c1
a2 b2 c2
</p>
<p>)
</p>
<p>= 2 is called the cartesian equation of the line r in A3.
</p>
<p>Remark 14.4.11 We notice again that the cartesian form (14.11) is not uniquely
</p>
<p>determined by the line r , since any linear system �&prime; which is equivalent to �r
describes the same line.
</p>
<p>We now a few examples of linear affine varieties described by cartesian equations
</p>
<p>obtained via removing parameters in their parametric equations.
</p>
<p>Exercise 14.4.12 We consider the hyperplane in A4 with parametric equation
</p>
<p>H :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x = 1 + λ+ &micro;+ ν
</p>
<p>y = λ&minus; &micro;
</p>
<p>z = &micro;+ ν
</p>
<p>t = ν
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Cartesian Form of Linear Affine Varieties 255
</p>
<p>Let us eliminate the parameters: we start by eliminating &micro; via the fourth relations,
</p>
<p>then ν by the third relation and eventually λ via the second relation. We have then
</p>
<p>H :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x = 1 + λ+ &micro;+ t
</p>
<p>y = λ&minus; &micro;
</p>
<p>z = &micro;+ t
</p>
<p>ν = t
</p>
<p>&hArr;
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x = 1 + λ+ (z &minus; t)+ t
</p>
<p>y = λ&minus; (z &minus; t)
</p>
<p>&micro; = z &minus; t
</p>
<p>ν = t
</p>
<p>&hArr;
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x = 1 + (y + z &minus; t)+ (z &minus; t)+ t
</p>
<p>λ = y + z &minus; t
</p>
<p>&micro; = z &minus; t
</p>
<p>ν = t
</p>
<p>.
</p>
<p>As we have noticed previously, since these relations are valid for each value of
</p>
<p>the parameters λ,&micro;, ν, the computations amount to a redefinition of the parameters
</p>
<p>to y, z, t , so we consider only the first relation, and write
</p>
<p>�H : x &minus; y &minus; 2z + t &minus; 1 = 0
</p>
<p>as the cartesian equation of the hyperplane H in A4 with the starting parametric
</p>
<p>equation. The direction SH = R
3 of such a hyperplane is given by the vector space
</p>
<p>corresponding to the space of the solutions of the homogeneous linear equation
</p>
<p>x &minus; y &minus; 2z + t = 0.
</p>
<p>Exercise 14.4.13 We consider the plane π in A3 whose vector equation is given by
</p>
<p>π : P = Q + λv1 + &micro;v2,
</p>
<p>with Q = (2, 3, 0) and v1 = (1, 0, 1), v2 = (1,&minus;1, 0). By denoting the coordinates
</p>
<p>P = (x, y, z) we write
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
</p>
<p>⎠ =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>2
</p>
<p>3
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠ + λ
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1
</p>
<p>0
</p>
<p>1
</p>
<p>⎞
</p>
<p>⎠ + &micro;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1
</p>
<p>&minus;1
</p>
<p>0
</p>
<p>⎞
</p>
<p>⎠ ,
</p>
<p>which reads as the parametric equation
</p>
<p>π :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 2 + λ+ &micro;
</p>
<p>y = 3 &minus; &micro;
</p>
<p>z = λ
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 14 Affine Linear Geometry
</p>
<p>If we eliminate the parameters we write
</p>
<p>H :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λ = z
</p>
<p>&micro; = 3 &minus; y
</p>
<p>x = 2 + z + 3 &minus; y
</p>
<p>so to have the following cartesian equation for π:
</p>
<p>�π : x + y &minus; z &minus; 5 = 0.
</p>
<p>The direction Sπ = R
2 of the plane π is the space of the solutions of the homo-
</p>
<p>geneous equation
</p>
<p>x + y &minus; z = 0,
</p>
<p>and it is easy to see that Sπ = L(v1, v2).
</p>
<p>Exercise 14.4.14 We consider the line r : P =Q + λv in A4, with Q = (1,&minus;1, 2, 1)
</p>
<p>and direction vector v = (1, 2, 2, 1). Its parametric equation is given by
</p>
<p>r :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>x1 = 1 + λ
</p>
<p>x2 = 2 &minus; λ
</p>
<p>x3 = 2 + 2λ
</p>
<p>x4 = 1 + λ
</p>
<p>.
</p>
<p>If we use the first relation to eliminate the parameter λ, we write
</p>
<p>r :
</p>
<p>⎧
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎨
</p>
<p>⎪
</p>
<p>⎪
</p>
<p>⎩
</p>
<p>λ = x1 &minus; 1
</p>
<p>x2 = 2 &minus; (x1 &minus; 1)
</p>
<p>x3 = 2 + 2(x1 &minus; 1)
</p>
<p>x4 = 1 + (x1 &minus; 1)
</p>
<p>which amounts to the following cartesian equation
</p>
<p>�r :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x1 + x2 &minus; 3 = 0
</p>
<p>2x1 + x3 = 0
</p>
<p>x1 + x4 = 0
</p>
<p>.
</p>
<p>Again, the direction Sr = R of the line r is given by the space of the solutions for
</p>
<p>the homogeneous linear system
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x1 + x2 = 0
</p>
<p>2x1 + x3 = 0
</p>
<p>x1 + x4 = 0
</p>
<p>.
</p>
<p>It is easy to see that SrO = L(v).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 The Cartesian Form of Linear Affine Varieties 257
</p>
<p>Exercise 14.4.15 We consider the plane π &sube; A3 whose cartesian equation is
</p>
<p>�π : 2x &minus; y + z &minus; 1 = 0.
</p>
<p>By choosing as free unknowns x, y, we have z = &minus;2x + y + 1, that is
</p>
<p>P = (x, y, z) &isin; π if and only if
</p>
<p>(x, y, z) = (a, b,&minus;2a + b + 1) = (0, 0, 1)+ a(1, 0,&minus;2)+ b(0, 1, 1)
</p>
<p>for any choice of the real parameters a, b. The former relation is then the vector
</p>
<p>equation of π.
</p>
<p>Exercise 14.4.16 We consider the line r &sube; A3 with cartesian equation
</p>
<p>�r :
</p>
<p>{
</p>
<p>x &minus; y + z &minus; 1 = 0
</p>
<p>2x + y + 2 = 0
.
</p>
<p>In order to have a vector equation for r we solve such a linear system, getting
</p>
<p>�r :
</p>
<p>{
</p>
<p>y = &minus;2x &minus; 2
</p>
<p>z = &minus;3x &minus; 3
.
</p>
<p>Then the space of the solutions for �r is given by the elements
</p>
<p>(x, y, z) = (a,&minus;2a &minus; 2,&minus;3a &minus; 3) = (0,&minus;2,&minus;3)+ a(1,&minus;2,&minus;3).
</p>
<p>This relation yields a vector equation for r .
</p>
<p>We conclude this section by rewriting the Proposition 14.4.1, whose formulation
</p>
<p>should appear now clearer.
</p>
<p>Proposition 14.4.17 Given the matrix
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n &minus;b1
a21 a22 . . . a2n &minus;b2
...
</p>
<p>...
...
</p>
<p>am1 am2 . . . amn &minus;bm
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>&isin; Rm,n
</p>
<p>with
</p>
<p>rk(A) = rk
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a11 a12 . . . a1n
a21 a22 . . . a2n
...
</p>
<p>...
</p>
<p>am1 am2 . . . amn
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= m &lt; n,</p>
<p/>
</div>
<div class="page"><p/>
<p>258 14 Affine Linear Geometry
</p>
<p>the solutions of the linear system
</p>
<p>�L : AX = B &hArr;
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>a11x1 + a12x2 + &middot; &middot; &middot; + a1n xn + b1 = 0
</p>
<p>. . .
</p>
<p>am1x1 + am2x2 + &middot; &middot; &middot; + amn xn + bm = 0
</p>
<p>(14.13)
</p>
<p>give the coordinates of all points P = (x1, x2, . . . , xn) of a linear affine variety L in
</p>
<p>A
n of dimension k = n &minus; m and whose direction SL is given by the solutions of the
</p>
<p>associated linear homogenous system
</p>
<p>�L O : AX = 0. (14.14)
</p>
<p>If L &sub; An is a linear affine variety of dimension k, whose direction SL &sim;= R
k is
</p>
<p>the space of solutions of the linear homogenous system AX = 0 with A &isin; Rm,n and
</p>
<p>rk(A) = m &lt; n, then there is a vector B = t (&minus;b1, . . . ,&minus;bm) such that the cartesian
</p>
<p>form for the equation of L is given by (14.13).
</p>
<p>14.5 Intersection of Linear Affine Varieties
</p>
<p>In this section, by studying particular examples, we introduce some aspects of the
</p>
<p>general problem of the intersection (that is of the mutual position) of different linear
</p>
<p>affine varieties.
</p>
<p>14.5.1 Intersection of two lines in A2
</p>
<p>Let r and r &prime; be the lines in A2 given by the cartesian equations
</p>
<p>�r : ax + by + c = 0; �r &prime; : a
&prime;x + b&prime;y + c&prime; = 0.
</p>
<p>Their intersection is given by the solutions of the linear system
</p>
<p>�r&cap;r &prime; :
</p>
<p>{
</p>
<p>ax + by = &minus;c
</p>
<p>a&prime;x + b&prime;y = &minus;c&prime;
.
</p>
<p>By defining
</p>
<p>A =
</p>
<p>(
</p>
<p>a b
</p>
<p>a&prime; b&prime;
</p>
<p>)
</p>
<p>, (A, B) =
</p>
<p>(
</p>
<p>a b &minus;c
</p>
<p>a&prime; b&prime; &minus;c&prime;
</p>
<p>)
</p>
<p>the matrices associated to such a linear system, we have three different possibilities:
</p>
<p>&bull; if rk(A) = rk((A, B)) = 1, the system�r&cap;r &prime; is solvable, with the space of solutions
</p>
<p>S�r&cap;r &prime; containing &infin;
1 solutions. This means that r = r &prime;, the two lines coincide;
</p>
<p>&bull; if rk(A) = rk((A, B)) = 2, the system�r&cap;r &prime; is solvable, with the space of solutions
</p>
<p>S�r&cap;r &prime; made of only one solution, the point P = (x0, y0) of intersection between
</p>
<p>the lines r and r &prime;;</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Intersection of Linear Affine Varieties 259
</p>
<p>&bull; if rk(A) = 1 and rk((A, B)) = 2, the system �r&cap;r &prime; is not solvable, which means
</p>
<p>that r &cap; r &prime; = &empty;; the lines r and r &prime; are therefore parallel, with common direction
</p>
<p>given by L((&minus;b, a)).
</p>
<p>We can summarise such cases as in the following table
</p>
<p>rk(A) rk((A, B)) S�r&cap;r &prime; r &cap; r
&prime;
</p>
<p>1 1 &infin;1 r = r &prime;
</p>
<p>2 2 1 P = (x0, y0)
</p>
<p>1 2 &empty; &empty;
</p>
<p>The following result comes easily from the analysis above.
</p>
<p>Corollary 14.5.2 Given the lines r and r &prime; in A2 with cartesian equations
</p>
<p>�r : ax + by + c = 0 and �r &prime; : a
&prime;x + b&prime;y + c&prime; = 0, we have that
</p>
<p>r = r &prime; &lArr;&rArr; rk
</p>
<p>(
</p>
<p>a b &minus;c
</p>
<p>a&prime; b&prime; &minus;c&prime;
</p>
<p>)
</p>
<p>= 1.
</p>
<p>Exercise 14.5.3 Given the lines r and s on A2 whose cartesian equations are
</p>
<p>�r : x + y &minus; 1 = 0, �s : x + 2y + 2 = 0,
</p>
<p>we study their mutual position. We consider therefore the linear system
</p>
<p>�r&cap;s :
</p>
<p>{
</p>
<p>x + y = 1
</p>
<p>x + 2y = &minus;2
.
</p>
<p>The reduction
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 1 1
</p>
<p>1 2 &minus;2
</p>
<p>)
</p>
<p>�&rarr;
</p>
<p>(
</p>
<p>1 1 1
</p>
<p>0 1 &minus;3
</p>
<p>)
</p>
<p>= (A&prime;, B &prime;)
</p>
<p>proves that rk(A, B) = rk(A&prime;, B &prime;) = 2 and rk(A) = rk(A&prime;) = 2. The lines r and s
</p>
<p>have a unique point of intersection, which is computed to be r &cap; s = {(4,&minus;3)}.
</p>
<p>Exercise 14.5.4 Consider the lines r and sα given by their cartesian equations
</p>
<p>�r : x + y &minus; 1 = 0, �sα : x + αy + 2 = 0
</p>
<p>with α &isin; R a parameter. We study the mutual position of r and sα as depending on
</p>
<p>the value of α. We therefore study the linear system
</p>
<p>�r&cap;sα :
</p>
<p>{
</p>
<p>x + y = 1
</p>
<p>x + αy = &minus;2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>260 14 Affine Linear Geometry
</p>
<p>We use the reduction
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 1 1
</p>
<p>1 α &minus;2
</p>
<p>)
</p>
<p>�&rarr;
</p>
<p>(
</p>
<p>1 1 1
</p>
<p>0 α&minus; 1 &minus;3
</p>
<p>)
</p>
<p>= (A&prime;, B &prime;),
</p>
<p>proving that rk(A, B) = rk(A&prime;, B &prime;) = 2 for any value ofα, while rk(A) = rk(A&prime;) = 2
</p>
<p>if and only if α 
= 1. This means that r is parallel to sα if and only if α = 1 (being
</p>
<p>in such a case �s1 : x + y + 2 = 0), while for any α 
= 1 the two lines intersects in
</p>
<p>one point, whose coordinates are computed to be
</p>
<p>r &cap; sα = (
α+ 2
</p>
<p>α&minus; 1
,
</p>
<p>3
</p>
<p>1 &minus; α
).
</p>
<p>The following examples show how to study the mutual position of two lines which
</p>
<p>are not given in the cartesian form. They present different methods without the need
</p>
<p>to explicitly transforming a parametric or a vector equation into its cartesian form.
</p>
<p>Exercise 14.5.5 We consider the line r in A2 with vector equation
</p>
<p>r : (x, y) = (1, 2)+ λ(1,&minus;1),
</p>
<p>and the line s whose cartesian equation is
</p>
<p>�s : 2x &minus; y &minus; 6 = 0.
</p>
<p>These line intersect for each value of the parameter λ giving a point in r whose
</p>
<p>coordinates solve the equation �s . From
</p>
<p>r :
</p>
<p>{
</p>
<p>x = 1 + λ
</p>
<p>y = 2 &minus; λ
</p>
<p>we have
</p>
<p>2(1 + λ)&minus; (2 &minus; λ)&minus; 6 = 0 &hArr; λ = 2.
</p>
<p>This means that r and s intersects in one point, the one with coordinates
</p>
<p>(x = 3, y = 0).
</p>
<p>Exercise 14.5.6 As in the exercise above we consider the line r given by the vector
</p>
<p>equation
</p>
<p>r : (x, y) = (1,&minus;1)+ λ(2,&minus;1)
</p>
<p>and the line s given by the cartesian equation
</p>
<p>�s : x + 2y &minus; 3 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Intersection of Linear Affine Varieties 261
</p>
<p>Their intersections correspond to the value of the parameter λ which solve the
</p>
<p>equation
</p>
<p>(1 + 2λ)+ 2(&minus;1 &minus; λ)&minus; 3 = 0 &hArr; &minus;4 = 0.
</p>
<p>This means that r &cap; s = &empty;; these two lines are parallel.
</p>
<p>Exercise 14.5.7 Consider the lines r and s in A2 both given by a vector equation,
</p>
<p>for example
</p>
<p>r : (x, y) = (1, 0)+ λ(1,&minus;2), s : (x, y) = (1,&minus;1)+ &micro;(&minus;1, 1).
</p>
<p>The intersection r &cap; s corresponds to values of the parameters λ and &micro; for which
</p>
<p>the coordinates of a point in r coincide with those of a point in s. We have then to
</p>
<p>solve the linear system
</p>
<p>{
</p>
<p>1 + λ = 1 &minus; &micro;
</p>
<p>&minus;2λ = &minus;1 + &micro;
&hArr;
</p>
<p>{
</p>
<p>λ = &minus;&micro;
</p>
<p>2&micro; = &minus;1 + &micro;
&hArr;
</p>
<p>{
</p>
<p>λ = 1
</p>
<p>&micro; = &minus;1
.
</p>
<p>Having such a linear system one solution, the intersection s &cap; r = P where the
</p>
<p>point P corresponds to the value λ = 1 in r or equivalently to the value &micro; = &minus;1 in s.
</p>
<p>Then r &cap; s = (2,&minus;2).
</p>
<p>Exercise 14.5.8 As in the previous exercise, we study the intersection of the lines
</p>
<p>r : (x, y) = (1, 1)+ λ(&minus;1, 2), s : (x, y) = (1, 2)+ &micro;(1,&minus;2).
</p>
<p>We proceed as above, and consider the linear system
</p>
<p>{
</p>
<p>1 &minus; λ = 1 + &micro;
</p>
<p>1 + 2λ = 2 &minus; 2&micro;
&hArr;
</p>
<p>{
</p>
<p>&minus;λ = &micro;
</p>
<p>1 &minus; 2&micro; = 2 &minus; 2&micro;
&hArr;
</p>
<p>{
</p>
<p>&minus;λ = &micro;
</p>
<p>1 = 2
.
</p>
<p>Since this linear system is not solvable, we conclude that r does not intersect s,
</p>
<p>and since the direction of r and s coincide, we have that r is parallel to s.
</p>
<p>14.5.9 Intersection of two planes in A3
</p>
<p>Consider the planes π and π&prime; in A3 with cartesian equations given by
</p>
<p>�π : ax + by + cz + d = 0, �π&prime; : a
&prime;x + b&prime;y + c&prime;z + d &prime; = 0.
</p>
<p>Their intersection is given by the solutions of the linear system
</p>
<p>�π&cap;π&prime; :
</p>
<p>{
</p>
<p>ax + by + cz + d = 0
</p>
<p>a&prime;x + b&prime;y + c&prime;z + d &prime; = 0</p>
<p/>
</div>
<div class="page"><p/>
<p>262 14 Affine Linear Geometry
</p>
<p>which is characterized by the matrices
</p>
<p>A =
</p>
<p>(
</p>
<p>a b c
</p>
<p>a&prime; b&prime; c&prime;
</p>
<p>)
</p>
<p>, (A, B) =
</p>
<p>(
</p>
<p>a b c &minus;d
</p>
<p>a&prime; b&prime; c&prime; &minus;d &prime;
</p>
<p>)
</p>
<p>.
</p>
<p>We have the following possible cases.
</p>
<p>rk(A) rk((A, B)) S�π&cap;π&prime; π &cap; π
&prime;
</p>
<p>1 1 &infin;2 π = π&prime;
</p>
<p>2 2 &infin;1 line
</p>
<p>1 2 &empty; &empty;
</p>
<p>Notice that the case π &cap; π&prime; = &empty; corresponds to π parallel to π&prime;.
</p>
<p>The following corollary parallels the one in Corollary 14.5.2.
</p>
<p>Corollary 14.5.10 Consider two planes π and π&prime; in A3 having cartesian equations
</p>
<p>�π : ax + by + cz + d = 0 and �π&prime; : a
&prime;x + b&prime;y + c&prime;z + d &prime; = 0. One has
</p>
<p>π = π&prime; &lArr;&rArr; rk
</p>
<p>(
</p>
<p>a b c &minus;d
</p>
<p>a&prime; b&prime; c&prime; &minus;d &prime;
</p>
<p>)
</p>
<p>= 1.
</p>
<p>Exercise 14.5.11 We consider the planes π and π&prime; in A3 whose cartesian equations
</p>
<p>are
</p>
<p>�π : x &minus; y + 3z + 2 = 0 �π&prime; : x &minus; y + z + 1 = 0.
</p>
<p>The intersection is given by the solutions of the system
</p>
<p>�π&cap;π&prime; :
</p>
<p>{
</p>
<p>x &minus; y + 3z = &minus;2
</p>
<p>x &minus; y + z = &minus;1
.
</p>
<p>By reducing the complete matrix of such a linear system,
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 &minus;1 3 &minus;2
</p>
<p>1 &minus;1 1 &minus;1
</p>
<p>)
</p>
<p>�&rarr;
</p>
<p>(
</p>
<p>1 &minus;1 3 &minus;2
</p>
<p>0 0 2 &minus;1
</p>
<p>)
</p>
<p>,
</p>
<p>we see that rk(A, B) = rk(A) = 2, so the linear system has &infin;1 solutions. The inter-
</p>
<p>section π &cap; π&prime; is therefore a line with cartesian equation given by �π&cap;π&prime; .
</p>
<p>Exercise 14.5.12 We consider the planes π and π&prime; in A3 given by
</p>
<p>�π : x &minus; y + z + 2 = 0 �π&prime; : 2x &minus; 2y + 2z + 1 = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Intersection of Linear Affine Varieties 263
</p>
<p>As in the previous exercise, we reduce the complete matrix of the linear system
</p>
<p>�π&cap;π&prime; ,
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 &minus;1 1 &minus;2
</p>
<p>2 &minus;2 2 &minus;1
</p>
<p>)
</p>
<p>�&rarr;
</p>
<p>(
</p>
<p>1 &minus;1 1 &minus;2
</p>
<p>0 0 0 3
</p>
<p>)
</p>
<p>,
</p>
<p>to get rk(A) = 1 while rk(A, B) = 2, so π &cap; π&prime; = &empty;. Since these planes are in A3,
</p>
<p>they are parallel.
</p>
<p>Exercise 14.5.13 We consider the planes π,π&prime;,π&prime;&prime; in A3 whose cartesian equations
</p>
<p>are given by
</p>
<p>�π : x &minus; 2y &minus; z + 1 = 0
</p>
<p>�π&prime; : x + y &minus; 2 = 0
</p>
<p>�π&prime;&prime; : 2x &minus; 4y &minus; 2z &minus; 5 = 0 .
</p>
<p>For the mutual positions of the pairs π,π&prime; and π,π&prime;&prime;, we start by considering the
</p>
<p>linear system
</p>
<p>�π&cap;π&prime; :
</p>
<p>{
</p>
<p>x &minus; 2y &minus; z = &minus;1
</p>
<p>x + y = 2
.
</p>
<p>For the complete matrix
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>)
</p>
<p>we easily see that rk(A) = rk(A, B) = 2, so the intersection π &cap; π&prime; is the line whose
</p>
<p>cartesian equation is the linear system �π&cap;π&prime; .
</p>
<p>For the intersections of π with π&prime;&prime; we consider the linear system
</p>
<p>�π&cap;π&prime;&prime; :
</p>
<p>{
</p>
<p>x &minus; 2y &minus; z = &minus;1
</p>
<p>2x &minus; 4y &minus; 2z = 5
.
</p>
<p>The complete matrix
</p>
<p>(A, B) =
</p>
<p>(
</p>
<p>1 &minus;2 1 &minus;1
</p>
<p>2 &minus;4 &minus;2 5
</p>
<p>)
</p>
<p>,
</p>
<p>has rk(A) = 1 and rk(A, B) = 2. This means that �π&cap;π&prime;&prime; has no solutions, that is
</p>
<p>the planes π and π&prime; are parallel, having the same direction given by the vector space
</p>
<p>solutions of SπO : x &minus; 2y &minus; z = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>264 14 Affine Linear Geometry
</p>
<p>14.5.14 Intersection of a line with a plane in A3
</p>
<p>We consider the line r and the plane π in A3 given by the cartesian equations
</p>
<p>�r :
</p>
<p>{
</p>
<p>a1x + b1 y + c1z + d1 = 0
</p>
<p>a2x + b2 y + c2z + d2 = 0
, �π : ax + by + cz + d = 0.
</p>
<p>Again, their intersection is given by the solutions of the linear system
</p>
<p>�π&cap;r :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>a1x + b1 y + c1z = &minus;d1
a2x + b2 y + c2z = &minus;d2
ax + by + cz = &minus;d
</p>
<p>,
</p>
<p>with its associated matrices
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>a1 b1 c1
a2 b2 c2
a b c
</p>
<p>⎞
</p>
<p>⎠ , (A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>a1 b1 c1 &minus;d1
a2 b2 c2 &minus;d2
a b c &minus;d
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>Since the upper two row vectors of both A and (A, B) matrices are linearly
</p>
<p>independent, because the corresponding equations represent a line in A3, only the
</p>
<p>following cases are possible.
</p>
<p>rk(A) rk((A, B)) S�π&cap;r π &cap; r
</p>
<p>2 2 &infin;1 r
</p>
<p>3 3 &infin;0 point
</p>
<p>2 3 &empty; &empty;
</p>
<p>Notice that, when rk(A) = rk(A, B) = 2, it is r &sub; π, while, if rk(A) = 2 and
</p>
<p>rk(A, B) = 3, then r is parallel to π. Indeed, when rk(A) = 2, then Sr &sub; Sπ , the
</p>
<p>direction of r is a subspace in the direction of π. In order to show this, we consider
</p>
<p>the linear systems for the directions Sr and Sπ ,
</p>
<p>�rO :
</p>
<p>{
</p>
<p>a1x + b1 y + c1z = 0
</p>
<p>a2x + b2 y + c2z = 0
, �πO : ax + by + cz = 0.
</p>
<p>Since rk(A) = 2 and the upper two row vectors are linearly independent, we can
</p>
<p>write
</p>
<p>(a, b, c) = λ1(a1, b1, c1)+ λ2(a2, b2, c2).
</p>
<p>If P = (x0, y0.z0) is a point in Sr , then ai x0 + bi y0 + ci z0 = 0 for i = 1, 2. We
</p>
<p>can then write
</p>
<p>ax0 + by0 + cz0 = (λ1a1 + λ2a2)x0 + (λ1b1 + λ2b2)y0 + (λ1c1 + λ2c2)z0
</p>
<p>= λ1(a1x0 + b1 y0 + c1z0)+ λ2(a2x0 + b2 y0 + c2z0)
</p>
<p>= 0
</p>
<p>and this proves that P &isin; Sπ , that is the inclusion Sr &sub; Sπ .</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Intersection of Linear Affine Varieties 265
</p>
<p>Exercise 14.5.15 Given in A3 the line r and the plane π with cartesian equations
</p>
<p>�r :
</p>
<p>{
</p>
<p>x &minus; 2y &minus; z + 1 = 0
</p>
<p>x + y &minus; 2 = 0
, �π : 2x + y &minus; 2z &minus; 5 = 0,
</p>
<p>their intersection is given by the solutions of the linear system�π&cap;r : AX = B whose
</p>
<p>associated complete matrix, suitably reduced, reads
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>2 1 &minus;2 5
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>0 5 0 7
</p>
<p>⎞
</p>
<p>⎠ = (A&prime;, B &prime;).
</p>
<p>Then rk(A) = 3 and rk(A, B) = 3, so the linear system �π&cap;r has a unique solu-
</p>
<p>tion, which corresponds to the unique point P of intersection between r and π. The
</p>
<p>coordinates of P are easily computed to be P = ( 3
5
, 7
</p>
<p>5
,&minus; 6
</p>
<p>5
).
</p>
<p>Exercise 14.5.16 We consider in A3 the line r and the plane πh with equations
</p>
<p>�r :
</p>
<p>{
</p>
<p>x &minus; 2y &minus; z + 1 = 0
</p>
<p>x + y &minus; 2 = 0
, �π : 2x + hy &minus; 2z &minus; 5 = 0,
</p>
<p>where h is a real parameter. The complete matrix of to the linear system
</p>
<p>�πh&cap;r : AX = B giving the intersection of πh and r is
</p>
<p>(Ah, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>2 h &minus;2 5
</p>
<p>⎞
</p>
<p>⎠ .
</p>
<p>We notice that the rank of Ah is at least 2, with rk(Ah) = 3 if and only if
</p>
<p>det(Ah) 
= 0. It is det(Ah) = &minus;h &minus; 4, so rk(Ah) = 3 if and only if h 
= &minus;4. In such
</p>
<p>a case rk(Ah) = 3 = rk(Ah, B), and this means that r and πh 
=&minus;4 have a unique point
</p>
<p>of intersection.
</p>
<p>If h = &minus;4, then rk(A&minus;4) = 2: the reduction
</p>
<p>(A&minus;4, B) =
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>2 &minus;4 &minus;2 5
</p>
<p>⎞
</p>
<p>⎠ �&rarr;
</p>
<p>⎛
</p>
<p>⎝
</p>
<p>1 &minus;2 &minus;1 &minus;1
</p>
<p>1 1 0 2
</p>
<p>0 0 0 7
</p>
<p>⎞
</p>
<p>⎠
</p>
<p>shows that rk(A&minus;4, B) = 3, so the linear system A&minus;4 X = B has no solutions, and r
</p>
<p>is parallel to π.</p>
<p/>
</div>
<div class="page"><p/>
<p>266 14 Affine Linear Geometry
</p>
<p>Exercise 14.5.17 As in the Exercise 14.5.15 we study the intersection of a plane π
</p>
<p>(represented by a cartesian equation) and a line r in A3 (represented by a parametric
</p>
<p>equation). Consider for instance,
</p>
<p>r : (x, y, z) = (3,&minus;1, 5)+ λ(1,&minus;1, 2), �π : x + y &minus; z + 1 = 0.
</p>
<p>As before, the intersection π &cap; r corresponds to the values of the parameter λ
</p>
<p>for which the coordinates P = (3 + λ,&minus;1 &minus; λ, 5 + 2λ) of a point in r solve the
</p>
<p>cartesian equation for π, that is
</p>
<p>(3 + λ)+ (&minus;1 &minus; λ)&minus; (5 + 2λ)+ 1 = 0 &rArr; &minus;2λ&minus; 2 = 0 &rArr; λ = &minus;1.
</p>
<p>We have then r &cap; π = (2, 0, 3).
</p>
<p>14.5.18 Intersection of two lines in A3
</p>
<p>We consider a line r and a line r &prime; in A3 with cartesian equations
</p>
<p>�r :
</p>
<p>{
</p>
<p>a1x + b1 y + c1z + d1 = 0
</p>
<p>a2x + b2 y + c2z + d2 = 0
, �&prime;r :
</p>
<p>{
</p>
<p>a&prime;1x + b
&prime;
1 y + c
</p>
<p>&prime;
1z + d
</p>
<p>&prime;
1 = 0
</p>
<p>a&prime;2x + b
&prime;
2 y + c
</p>
<p>&prime;
2z + d
</p>
<p>&prime;
2 = 0
</p>
<p>.
</p>
<p>The intersection is given by the linear system �r&cap;r &prime; whose associated matrices are
</p>
<p>A =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a1 b1 c1
a2 b2 c2
a&prime;1 b
</p>
<p>&prime;
1 c
</p>
<p>&prime;
1
</p>
<p>a&prime;2 b
&prime;
2 c
</p>
<p>&prime;
2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>, (A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>a1 b1 c1 &minus;d1
a2 b2 c2 &minus;d2
a&prime;1 b
</p>
<p>&prime;
1 c
</p>
<p>&prime;
1 &minus;d
</p>
<p>&prime;
1
</p>
<p>a&prime;2 b
&prime;
2 c
</p>
<p>&prime;
2 &minus;d
</p>
<p>&prime;
2
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>.
</p>
<p>Once again, different possibilities depending on the mutual ranks of these. As
</p>
<p>we stressed in the previous case 14.5.14, since r and r &prime; are lines, the upper two row
</p>
<p>vectors R1 and R2 of both A and (A, B) are linearly independent, as are the last two
</p>
<p>row vectors, R3 and R4. Then,
</p>
<p>rk(A) rk((A, B)) S�r&cap;r &prime; r &cap; r
&prime;
</p>
<p>2 2 &infin;1 r
</p>
<p>3 3 &infin;0 point
</p>
<p>2 3 &empty; &empty;
</p>
<p>3 4 &empty; &empty;
</p>
<p>In the first case, with rk(A) = rk(A, B) = 2, the lines r, r &prime; coincide, while in the
</p>
<p>second case, with rk(A) = rk(A, B) = 3, they have a unique point of intersection,
</p>
<p>whose coordinates are given by the solution of the system AX = B.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Intersection of Linear Affine Varieties 267
</p>
<p>In the third and the fourth case, the condition rk(A) 
= rk(A, B) means that the
</p>
<p>two lines do not intersect. If rk(A) = 2, then the row vectors R3 and R4 of A are both
</p>
<p>linearly dependent of R1 and R2, and therefore the homogeneous linear systems
</p>
<p>�rO :
</p>
<p>{
</p>
<p>a1x + b1 y + c1z = 0
</p>
<p>a2x + b2 y + c2z = 0
, �r &prime;O :
</p>
<p>{
</p>
<p>a&prime;1x + b
&prime;
1 y + c
</p>
<p>&prime;
1z = 0
</p>
<p>a&prime;2x + b
&prime;
2 y + c
</p>
<p>&prime;
2z = 0
</p>
<p>,
</p>
<p>are equivalent. We have then that Sr = Sr &prime; , the direction of r coincide with that of
</p>
<p>r &prime;, that is r is parallel to r &prime;. If rk(A) = 3 (the fourth case in the table above) the lines
</p>
<p>are not parallel and do not intersect, so they are skew.
</p>
<p>Exercise 14.5.19 We consider the line r and r &prime; in A3 whose cartesian equations are
</p>
<p>�r :
</p>
<p>{
</p>
<p>x &minus; y + 2z + 1 = 0
</p>
<p>x + z &minus; 1 = 0
, �r &prime; :
</p>
<p>{
</p>
<p>y &minus; z + 2 = 0
</p>
<p>x + y + z = 0
.
</p>
<p>We reduce the complete matrix associated to the linear system �r&cap;r &prime; , that is
</p>
<p>(A, B) =
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 &minus;1
</p>
<p>1 0 1 1
</p>
<p>0 1 &minus;1 &minus;2
</p>
<p>1 1 1 0
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 &minus;1
</p>
<p>0 1 &minus;1 2
</p>
<p>0 1 &minus;1 &minus;2
</p>
<p>0 2 &minus;1 1
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>�&rarr;
</p>
<p>⎛
</p>
<p>⎜
</p>
<p>⎜
</p>
<p>⎝
</p>
<p>1 &minus;1 2 &minus;1
</p>
<p>0 1 &minus;1 2
</p>
<p>0 0 0 &minus;4
</p>
<p>0 0 1 &minus;3
</p>
<p>⎞
</p>
<p>⎟
</p>
<p>⎟
</p>
<p>⎠
</p>
<p>= (A&prime;, B &prime;).
</p>
<p>Since rk(A&prime;) = 3 and rk(A&prime;, B &prime;) = 4, the two lines are skew.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
</p>
<p>Euclidean Affine Linear Geometry
</p>
<p>15.1 Euclidean Affine Spaces
</p>
<p>In the previous chapter we have dealt with the (real and linear) affine space An as
</p>
<p>modelled on the vector space Rn . In this chapter we study the additional structures
</p>
<p>on An that come when passing from Rn to the euclidean space En (see the Chap. 3).
</p>
<p>Taking into account the scalar product allows one to introduce metric notions (such
</p>
<p>as distances and angles) into an affine space.
</p>
<p>Definition 15.1.1 The affine space An associated to the Euclidean vector space
</p>
<p>En = (Rn, &middot;) is called the Euclidean affine space and denoted En . A reference system
(O,B) for En is called cartesian orthogonal if the basis B for En is orthonormal.
</p>
<p>Recall that, if B is an orthonormal basis for En , the matrix of change of basis
</p>
<p>ME,B (the matrix whose column vectors are the components of the vectors in B
</p>
<p>with respect to the canonical basis E) is orthogonal by definition (see the Chap. 10,
</p>
<p>Definition 10.1.1), and thus det(ME,B) = &plusmn;1.
In our analysis in this chapter we shall always consider cartesian orthogonal
</p>
<p>reference systems.
</p>
<p>Exercise 15.1.2 Let r be the (straight) line in E2 with vector equation
</p>
<p>(x, y) = (1,&minus;2)+ λ(1,&minus;1).
</p>
<p>We take A = (1,&minus;2) and v = (1,&minus;1). To determine a cartesian equation for r ,
in alternative to the procedure described at length in the previous chapter (that is
</p>
<p>removing the parameter λ), one observes that, since L(v) is the direction of r , and
</p>
<p>thus the vector u = (1, 1) is orthogonal to v, we can write
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_15
</p>
<p>269</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_15&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_15&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>270 15 Euclidean Affine Linear Geometry
</p>
<p>P = (x, y) &isin; r &lArr;&rArr; P &minus; A &isin; L(v)
&lArr;&rArr; (P &minus; A) &middot; u = 0
&lArr;&rArr; (x &minus; 1, y + 2) &middot; (1, 1) = 0.
</p>
<p>This condition can be written as
</p>
<p>x + y + 1 = 0,
</p>
<p>yielding a cartesian equation �r for r .
</p>
<p>This exercise shows that, if r is a line in E2 whose vector equation is
</p>
<p>r : P = A + λv, with u a vector orthogonal to v so that for the direction of r one
has Sr = L(u)&perp;, we have
</p>
<p>P &isin; r &lArr;&rArr; (P &minus; A) &middot; u = 0.
</p>
<p>This expression is called the normal equation for the line r .
</p>
<p>We can generalise this example to any hyperplane.
</p>
<p>Proposition 15.1.3 Let H &sub; En be a hyperplane, with A &isin; H. If u &isin; Rn is a non
zero vector orthogonal to the direction SH of the hyperplane, that is L(u) = (SH )&perp;,
then it holds that
</p>
<p>P &isin; H &lArr;&rArr; (P &minus; A) &middot; u = 0.
</p>
<p>Definition 15.1.4 The equation
</p>
<p>NH : (P &minus; A) &middot; u = 0
</p>
<p>is called the normal equation of the hyperplane H in En . If n = 2, it yields the normal
equation of a line; if n = 3, it yields the normal equation of a plane.
</p>
<p>Remark 15.1.5 Notice that, as we already seen for a cartesian equation in the previ-
</p>
<p>ous chapter (see the Remark 14.4.11), the normal equation NH for a given hyperplane
</p>
<p>in En is not uniquely determined, since A can range in H and the vector u is given
</p>
<p>up to an arbitrary non zero scalar.
</p>
<p>Remark 15.1.6 With a cartesian equation
</p>
<p>�H : a1x1 + &middot; &middot; &middot; + an xn = b
</p>
<p>for the hyperplane for H in En , one has S&perp;H = L((a1, . . . , an)). This follows from
the definition
</p>
<p>SH = {(x1, . . . , xn) &isin; Rn : a1x1 + &middot; &middot; &middot; + an xn = 0}
= {(x1, . . . , xn) &isin; Rn : (a1, . . . , an) &middot; (x1, . . . , xn) = 0}.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.1 Euclidean Affine Spaces 271
</p>
<p>With A an arbitrary point in H , a normal equation for H is indeed given by
</p>
<p>NH : (P &minus; A) &middot; (a1, . . . , an) = 0.
</p>
<p>Exercise 15.1.7 We determine both a cartesian and a normal equation for the plane
</p>
<p>π in A3 whose direction is orthogonal to u = (1, 2, 3) and that contains the point
A = (1, 0,&minus;1). We have
</p>
<p>Nπ : (x &minus; 1, y, z + 1) &middot; (1, 2, 3) = 0,
</p>
<p>equivalent to the cartesian equation
</p>
<p>�π : x + 2y + 3z + 2 = 0.
</p>
<p>Exercise 15.1.8 Given the (straight) line r in A2 with cartesian equation
</p>
<p>�r : 2x &minus; 3y + 3 = 0
</p>
<p>we look for its normal equation. We start by noticing (see the Remark 15.1.6) that the
</p>
<p>direction of r is orthogonal to the vector u = (2,&minus;3), and that the point A = (0, 1)
lays in r , so we can write
</p>
<p>Nr : (P &minus; (0, 1)) &middot; (2,&minus;3) = 0 &hArr; (x, y &minus; 1) &middot; (2,&minus;3) = 0
</p>
<p>as a normal equation for r .
</p>
<p>From what discussed above, it is clear that there exist deep relations between carte-
</p>
<p>sian and normal equations for an hyperplane in a Euclidean affine space. Moreover,
</p>
<p>as we have discussed in the previous chapter, a generic linear affine variety in An
</p>
<p>can be described as a suitable intersection of hyperplanes. Therefore it should come
</p>
<p>as no surprise that a linear affine variety can be described in a Euclidean affine space
</p>
<p>in terms of a suitable number of normal equations. The general case is illustrated by
</p>
<p>the following exercise.
</p>
<p>Exercise 15.1.9 Let r be the line through the point A = (1, 2,&minus;3) in E3 which is
orthogonal to the space L((1, 1, 0), (0, 1,&minus;1)). Its normal equation is given by
</p>
<p>Nr :
{
</p>
<p>(P &minus; A) &middot; (1, 1, 0) = 0
(P &minus; A) &middot; (0, 1,&minus;1) = 0 ,
</p>
<p>that is
</p>
<p>Nr :
{
</p>
<p>(x &minus; 1, y &minus; 2, z + 3) &middot; (1, 1, 0) = 0
(x &minus; 1, y &minus; 2, z + 3) &middot; (0, 1,&minus;1) = 0
</p>
<p>yielding then the cartesian equation</p>
<p/>
</div>
<div class="page"><p/>
<p>272 15 Euclidean Affine Linear Geometry
</p>
<p>�r :
{
</p>
<p>x + y &minus; 3 = 0
y &minus; z &minus; 5 = 0 .
</p>
<p>15.2 Orthogonality Between Linear Affine Varieties
</p>
<p>In the Euclidean affine space En there is the notion of orthogonality. Thus, we have:
</p>
<p>Definition 15.2.1 One says that
</p>
<p>(a) the lines r, r &prime; &sub; En are orthogonal if v &middot; v&prime; = 0 for any v &isin; Sr and any v&prime; &isin; Sr &prime; ,
(b) the planes π,π&prime; &sub; E3 are orthogonal if u &middot; u&prime; = 0 for any u &isin; S&perp;π and any
</p>
<p>u&prime; &isin; S&perp;π&prime; ,
(c) the line r with direction v is orthogonal to the plane π in E3 if v &isin; S&perp;π .
</p>
<p>Exercise 15.2.2 We consider the following lines in E2,
</p>
<p>�r1 : 2x &minus; 2y + 1 = 0,
�r2 : x + y + 3 = 0,
</p>
<p>r3 : (x, y) = (1,&minus;3)+ λ(1, 1),
Nr4 : (x + 1, y &minus; 4) &middot; (1, 2) = 0
</p>
<p>with directions spanned by the vectors
</p>
<p>v1 = (2, 2),
v2 = (1,&minus;1),
v3 = (1, 1),
v4 = (1,&minus;2).
</p>
<p>It is immediate to show that the only orthogonal pairs of lines among them are
</p>
<p>r1 &perp; r2 and r2 &perp; r3.
</p>
<p>Exercise 15.2.3 Consider the lines r, r &prime; &sub; E3 given by
</p>
<p>r : (x, y, z) = (1, 2, 1)+ λ(3, 0,&minus;1) , r &prime; :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 3 + &micro;
y = 2 &minus; 2&micro;
z = 3&micro;
</p>
<p>.
</p>
<p>We have Sr = L((3, 0,&minus;1)) and Sr &prime; = L((1,&minus;2, 3)). Since
</p>
<p>(3, 0,&minus;1) &middot; (1,&minus;2, 3) = 0
</p>
<p>we conclude that r is orthogonal to r &prime;.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Orthogonality Between Linear Affine Varieties 273
</p>
<p>Exercise 15.2.4 Let π be the plane in E3 whose cartesian equation is
</p>
<p>�π : x &minus; y + 2z &minus; 3 = 0.
</p>
<p>In order to find an equation for the line r through A = (1, 2, 1)which is orthogonal
to π we notice from the Remark 15.1.6, that it is S&perp;π = L((1,&minus;1, 2)): we can then
write
</p>
<p>r : (x, y, z) = (1, 2, 1)+ λ(1,&minus;1, 2).
</p>
<p>Exercise 15.2.5 Consider in E3 the line given by
</p>
<p>�r :
{
</p>
<p>x &minus; 2y + z &minus; 1 = 0
x + y = 0 .
</p>
<p>We seek to determine:
</p>
<p>(1) a cartesian equation for the plane π through the point A = (&minus;1,&minus;1,&minus;1) and
orthogonal to r ,
</p>
<p>(2) the intersection between r and π.
</p>
<p>We proceed as follows.
</p>
<p>(1) From the cartesian equation �r we have that
</p>
<p>S&perp;r = L((1,&minus;2, 1), (1, 1, 0))
</p>
<p>and this subspace yields the direction Sπ . Since A &isin; π, a vector equation for π
is given by
</p>
<p>π : (x, y, z) = &minus;(1, 1, 1)+ λ(1,&minus;2, 1)+ &micro;(1, 1, 0).
</p>
<p>By noticing that Sπ = L((1,&minus;1,&minus;3)), a normal equation for π is given by
</p>
<p>Nπ : (P &minus; A) &middot; (1,&minus;1,&minus;3) = 0
</p>
<p>yielding the cartesian equation
</p>
<p>�π : x &minus; y &minus; 3z &minus; 3 = 0.
</p>
<p>(2) The intersection π &cap; r is clearly given by the unique solution of the linear
system
</p>
<p>�π&cap;r :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x &minus; 2y + z &minus; 1 = 0
x + y = 0
x &minus; y &minus; 3z &minus; 3 = 0,
</p>
<p>,
</p>
<p>which is P = 1
11
(6,&minus;6,&minus;7).</p>
<p/>
</div>
<div class="page"><p/>
<p>274 15 Euclidean Affine Linear Geometry
</p>
<p>Exercise 15.2.6 We consider again the lines r and r &prime; in E3 from the Exercise 15.2.3.
We know that r &perp; r &prime;. We determine the plane π which is orthogonal to r &prime; and such
that r &sub; π. Since L((1,&minus;2, 3)) is the direction of r &prime;, we can write from the Remark
15.1.6 that
</p>
<p>�π : x &minus; 2y + 3z + d = 0
</p>
<p>with d a real parameter. The line r is in π if and only if the coordinates of every of
</p>
<p>its points P = (1 + 3λ, 2, 1 &minus; λ) &isin; r solve the equation �π , that is, if and only if
the equation
</p>
<p>(1 + 3λ)&minus; 2(2)+ 3(1 &minus; λ)+ d = 0
</p>
<p>has a solution for each value of λ. This is true if and only if d = 0, so a cartesian
equation for π is
</p>
<p>�π : x &minus; 2y + 3z = 0.
</p>
<p>Exercise 15.2.7 For the planes
</p>
<p>�π : 2x + y &minus; z &minus; 3 = 0, �π&prime; : x + y + 3z &minus; 1 = 0
</p>
<p>in E3 we have S&perp;π = L((2, 1,&minus;1)) and S&perp;π&prime; = L((1, 1, 3)). We conclude that π is
orthogonal to π&prime;, since (2, 1,&minus;1) &middot; (1, 1, 3) = 0. Notice that
</p>
<p>(2, 1,&minus;1) &isin; Sπ&prime; = {(a, b, c, ) : a + b + 3c = 0},
</p>
<p>that is S&perp;π &sub; Sπ&prime; . We can analogously show that S&perp;π&prime; &sub; Sπ . This leads to the following
remark.
</p>
<p>Remark 15.2.8 The planes π,π&prime; &sub; E3 are orthogonal if and only if S&perp;π &sub; Sπ&prime; (or
equivalently if and only if S&perp;π&prime; &sub; Sπ).
</p>
<p>In order to recap the results we described in the previous pages, we consider the
</p>
<p>following example.
</p>
<p>Exercise 15.2.9 Consider the point A = (1, 0, 1) in E3 and the lines r, s with equa-
tions
</p>
<p>r : (x, y, z) = (1, 2, 1)+ λ(3, 0,&minus;1), �s :
{
</p>
<p>x &minus; y + z + 2 = 0
x &minus; z + 1 = 0 .
</p>
<p>We seek to determine:
</p>
<p>(a) the set F of lines through A which are orthogonal to r ,
</p>
<p>(b) the line l &isin; F which is parallel to the plane π given by �π : x &minus; y + z + 2 = 0,
(c) the line l &prime; &isin; F which is orthogonal to s,
(d) the lines q &sub; π&prime; with �π&prime; : y &minus; 2 = 0 which are orthogonal to r .
</p>
<p>For these we proceed as follows.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Orthogonality Between Linear Affine Varieties 275
</p>
<p>(a) A line u through A has a vector equation
</p>
<p>(x, y, z) = (1, 0, 1)+ λ(a, b, c)
</p>
<p>with arbitrary direction Su = L((a, b, c)). Since u is to be orthogonal to r , we
have the condition (a, b, c) &middot; (3, 0, 1) = 3a &minus; c = 0. The set F is then given by
the union F = {rα}α&isin;R &cup; {r} with
</p>
<p>rα : (x, y, z) = (1, 0, 1)+ &micro;(1,α, 3) for a �= 0,
</p>
<p>and
</p>
<p>r : (x, y, z) = (1, 0, 1)+ &micro;(0, 1, 0) for a = c = 0.
</p>
<p>(b) Since the direction Sπ of the plane π is given by the subspace orthogonal to
</p>
<p>L((1,&minus;1, 1)), it is clear from (0, 1, 0) &middot; (1,&minus;1, 1) �= 0 that the line r is not
parallel to π. This means that the line l must be found within the set {rα}α&isin;R. If
we impose that (1,α, 3) &middot; (1,&minus;1, 1) = 0, we have α = 4, so the line l is given
by
</p>
<p>l : (x, y, z) = (1, 0, 1)+ &micro;(1, 4, 3).
</p>
<p>(c) A cartesian equation for s is given by solving the linear system �s in terms of
</p>
<p>one free unknown. It is immediate to show that
</p>
<p>s : (x, y, z) = (&minus;1 + η, 1 + 2η, η) = (&minus;1, 1, 0)+ η(1, 2, 1).
</p>
<p>The condition rα &perp; s is equivalent to (1,α, 3) &middot; (1, 2, 1) = 0, reading α = &minus;2,
so we have
</p>
<p>l &prime; : (x, y, z) = (1, 0, 1)+ &micro;(1,&minus;2, 3).
</p>
<p>This is the unique solution to the problem: we directly inspect that r is not
</p>
<p>orthogonal to s, since (0, 1, 0) &middot; (1, 2, 1) = 2 �= 0.
(d) A plane πh is orthogonal to r if and only if
</p>
<p>�πh : 3x &minus; z + h = 0.
</p>
<p>The lines qh are then given by the intersection
</p>
<p>�qh = �πh&cap;π&prime; :
{
</p>
<p>3x &minus; z + h = 0
y &minus; 2 = 0 with h &isin; R.</p>
<p/>
</div>
<div class="page"><p/>
<p>276 15 Euclidean Affine Linear Geometry
</p>
<p>15.3 The Distance Between Linear Affine Varieties
</p>
<p>It is evident that the distance between two points A and B on a plane is defined to
</p>
<p>be the length of the line segment whose endpoints are A and B. This definition can
</p>
<p>be consistently formulated in a Euclidean affine space.
</p>
<p>Definition 15.3.1 Let A and B be a pair of points in En . The distance d(A, B)
</p>
<p>between them is defined as
</p>
<p>d(A, B) = ‖B &minus; A‖ =
&radic;
</p>
<p>(B &minus; A) &middot; (B &minus; A).
</p>
<p>Exercise 15.3.2 If A = (1, 2, 0,&minus;1) and B = (0,&minus;1, 2, 2) are points in E4, then
</p>
<p>d(A, B) = ‖(&minus;1,&minus;3, 2, 3)‖ =
&radic;
</p>
<p>23.
</p>
<p>The well known properties of a Euclidean distance function are a consequence of
</p>
<p>the corresponding properties of the scalar product.
</p>
<p>Proposition 15.3.3 For any A, B,C points in En the following properties hold.
</p>
<p>(1) d(A, B) &ge; 0,
(2) d(A, B) = 0 if and only if A = B,
(3) d(A, B) = d(B, A).
(4) d(A, B)+ d(B,C) &ge; d(A,C).
</p>
<p>In order to introduce a notion of distance between a point and a linear affine
</p>
<p>variety, we start by looking at an example. Let us consider in E2 the point A = (0, 0)
and the line r whose vector equation is (x, y) = (1, 1)+ λ(1,&minus;1). By denoting
Pλ = (1 + λ, 1 &minus; λ) a generic point in r , we compute
</p>
<p>d(A, Pλ) =
&radic;
</p>
<p>2 + 2λ2.
</p>
<p>It is immediate to verify that, as a function of λ, the quantity d(A, P) ranges
</p>
<p>between
&radic;
</p>
<p>2 and +&infin;: it is therefore natural to consider the minimum of this range
as the distance between A and r . We have then d(A, r) =
</p>
<p>&radic;
2.
</p>
<p>Definition 15.3.4 If L is a linear affine variety and A is a point in En , the distance
</p>
<p>d(A, L) between A and L is defined to be
</p>
<p>d(A, L) = min{d(A, B) : B &isin; L}.
</p>
<p>Remark 15.3.5 It is evident from the definition above that d(A, L) = 0 if and only
if A &isin; L . We shall indeed prove that, given a point A and a linear affine variety L in
E
</p>
<p>n , there always exists a point A0 &isin; L such that d(A, L) = d(A0, L), thus showing
that the previous definition is well posed.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 The Distance Between Linear Affine Varieties 277
</p>
<p>Proposition 15.3.6 Let L be a linear affine variety and A /&isin; L a point in En . It holds
that
</p>
<p>d(A, L) = d(A, A0) with A0 = L &cap; (A + S&perp;L ).
</p>
<p>Here the set A + S&perp;L denotes the linear affine variety through A whose direction is
S&perp;L . The point A0 is called the orthogonal projection of A on L.
</p>
<p>Proof Since the linear system �L&cap;(A+S&perp;L ) given by the cartesian equations �L and
</p>
<p>�A+S&perp;L is of rank n with n unknowns, the intersection L &cap; (A + S
&perp;
L ) consists of a
</p>
<p>single point that we denote by A0.
</p>
<p>Let B be an arbitrary point in L . We can decompose
</p>
<p>A &minus; B = (A &minus; A0)+ (A0 &minus; B),
</p>
<p>with A0 &minus; B &isin; SL (since both A0 and B are in L) and A &minus; A0 &isin; S&perp;L (since both A
and A0 are points in the linear affine variety A + S&perp;L ). We have then
</p>
<p>(A &minus; A0) &middot; (A0 &minus; B) = 0
</p>
<p>and we write
</p>
<p>(d(A, B))2 = ‖A &minus; B‖2 = ‖(A &minus; A0)+ (A0 &minus; B)‖2
</p>
<p>= ‖A &minus; A0‖2 + ‖A0 &minus; B‖2.
</p>
<p>As a consequence,
</p>
<p>(d(A, B))2 &ge; ‖A &minus; A0‖2 = (d(A, A0))2
</p>
<p>for any B &isin; L , and this proves the claim. �
</p>
<p>Exercise 15.3.7 Let us compute the distance between the line r : 2x + y + 4 = 0
and the point A = (1,&minus;1) in E2. We start by finding the line sA = A + S&perp;r through
A which is orthogonal to r . The direction S&perp;r is spanned by the vector (2, 1), so we
have
</p>
<p>sA : (x, y) = (1,&minus;1)+ λ(2, 1).
</p>
<p>The intersection A0 = r &cap; sA is then given by the value of the parameter λ that
solves the equation
</p>
<p>2(1 + 2λ)+ (&minus;1 + λ)+ 4 = 0,
</p>
<p>that is λ = &minus;1 giving A0 = (&minus;1,&minus;2). Therefore we have
</p>
<p>d(A, r) = d(A, A0) = ‖(2, 1)‖ =
&radic;
</p>
<p>5.</p>
<p/>
</div>
<div class="page"><p/>
<p>278 15 Euclidean Affine Linear Geometry
</p>
<p>Exercise 15.3.8 Let us consider in E3 the point A = (1,&minus;1, 0) and the line r with
vector equation r : (x, y, z) = (1, 2, 1)+ λ(1,&minus;1, 2). In order to compute the dis-
tance between A and r we first determine the planeπA := A + S&perp;r . Since the direction
of r must be orthogonal to πA, from the Remark 15.1.6 the cartesian equation for πA
is given by
</p>
<p>�πA : x &minus; y + 2z + d = 0,
</p>
<p>with d &isin; R. The value of d if fixed by asking that A &isin; πA, that is 1 + 1 + d = 0
giving d = &minus;2. We then have
</p>
<p>�πA : x &minus; y + 2z &minus; 2 = 0.
</p>
<p>The point A0 is now the intersection r &cap; πA, which is given for the value of λ = 16
which solves,
</p>
<p>(1 + λ)&minus; (2 &minus; λ)+ 2(1 + 2λ)&minus; 2 = 0.
</p>
<p>It is therefore A0 = ( 76 ,
11
6
, 4
</p>
<p>3
), with
</p>
<p>d(A, r) = d(A, A0) = ‖(
1
</p>
<p>6
,
</p>
<p>17
</p>
<p>6
,
</p>
<p>4
</p>
<p>3
)‖ =
</p>
<p>&radic;
</p>
<p>59
</p>
<p>6
.
</p>
<p>The next theorem yields a formula which allows one to compute more directly
</p>
<p>the distance d(Q, H) between a point Q and an hyperplane H in En .
</p>
<p>Theorem 15.3.9 Let H be a hyperplane and Q a point in En with
</p>
<p>�H : a1x1 + &middot; &middot; &middot; + an xn + b = 0 and Q = (x &prime;1, . . . , x &prime;n). The distance between Q
and H is given by
</p>
<p>d(Q, H) =
|a1x &prime;1 + &middot; &middot; &middot; + an x &prime;n + b|
</p>
<p>&radic;
</p>
<p>a21 + &middot; &middot; &middot; + a2n
.
</p>
<p>Proof If we consider X = (x1, . . . , xn) and A = (a1, . . . , an) as vectors in Rn , using
the scalar product in En , the cartesian equation for H can be written as
</p>
<p>�H : A &middot; X + b = 0.
</p>
<p>We know that A &isin; S&perp;H , so the line through A which is orthogonal to H is made
of the points P such that
</p>
<p>r : P = Q + λA.
</p>
<p>The intersection point Q0 = r &cap; H is given by replacing X in �H with such a P ,
that is</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 The Distance Between Linear Affine Varieties 279
</p>
<p>A &middot; (Q + λA)+ b = 0 &rArr; A &middot; Q + λA &middot; A + b = 0
</p>
<p>&rArr; λ = &minus;
A &middot; Q + b
</p>
<p>A &middot; A
.
</p>
<p>The equation for r gives then
</p>
<p>Q0 = Q &minus;
A &middot; Q + b
‖A‖2
</p>
<p>A.
</p>
<p>We can now easily compute
</p>
<p>‖Q &minus; Q0‖2 =
∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>A &middot; Q + b
‖A‖2
</p>
<p>A
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>∥
</p>
<p>2
</p>
<p>=
|A &middot; Q + b|2
</p>
<p>‖A‖4
‖A‖2 =
</p>
<p>|A &middot; Q + b|2
</p>
<p>‖A‖2
,
</p>
<p>therefore getting
</p>
<p>d(Q, H) =
|A &middot; Q + b|
</p>
<p>‖A‖
=
</p>
<p>|a1x &prime;1 + &middot; &middot; &middot; + an x &prime;n + b|
&radic;
</p>
<p>a21 + &middot; &middot; &middot; + a2n
,
</p>
<p>as claimed. �
</p>
<p>Exercise 15.3.10 Consider the line r with cartesian equation �r : 2x + y + 4 = 0
and the point A = (1,&minus;1) in E2 as in the Exercise 15.3.7 above. From the Theorem
15.3.9 we have
</p>
<p>d(A, r) =
|2 &minus; 1 + 4|
&radic;
</p>
<p>4 + 1
=
</p>
<p>&radic;
5.
</p>
<p>Exercise 15.3.11 By making again use of the Theorem 15.3.9 it is easy to com-
</p>
<p>pute the distance between the point A = (1, 2,&minus;1) and the plane π in E3 with
�π : x + 2y &minus; 2z + 3 = 0. We have
</p>
<p>d(A,π) =
|1 + 4 + 2 + 3|
&radic;
</p>
<p>1 + 4 + 4
=
</p>
<p>10
</p>
<p>3
.
</p>
<p>We generalise the analysis above with a natural definition for the distance between
</p>
<p>any two linear affine varieties.
</p>
<p>Definition 15.3.12 Let L and L &prime; two linear affine varieties in En . The distance
between them is defined as the non negative real number
</p>
<p>d(L , L &prime;) = min{d(A, A&prime;) : A &isin; L , A&prime; &isin; L &prime;}.
</p>
<p>It is evident that d(L , L &prime;) = 0 if and only if L &cap; L &prime; �= &empty;. It is indeed possible to
show that the previous definition is consistent even when L &cap; L &prime; = &empty;. Moreover one</p>
<p/>
</div>
<div class="page"><p/>
<p>280 15 Euclidean Affine Linear Geometry
</p>
<p>can show that there exist a point Ā &isin; L and a point Ā&prime; &isin; L &prime;, such that the minimum
distance is attained for them, that is d( Ā, Ā&prime;) &le; d(A, A&prime;) for any A &isin; L and A&prime; &isin; L &prime;.
For such a pair of points it is d(L , L &prime;) = d( Ā, Ā&prime;).
</p>
<p>In the following pages we shall study the following cases of linear varieties which
</p>
<p>do not intersect:
</p>
<p>&bull; lines r, r &prime; in E2 which are parallel,
&bull; planes π,π&prime; in E3 which are parallel,
&bull; a plane π and a line r in E3 which are parallel,
&bull; lines r, r &prime; in E3 which are parallel.
</p>
<p>Remark 15.3.13 Consider lines r and r &prime; in E2 which are parallel and distinct. Their
cartesian equations are
</p>
<p>�r : ax + by + c = 0, �r &prime; : ax + by + c&prime; = 0,
</p>
<p>for c&prime; �= c. Let A = (x &prime;1, x &prime;2) &isin; r , that is ax &prime;1 + bx &prime;2 + c = 0. From the Theorem 15.3.9
it is
</p>
<p>d(A, r &prime;) =
|ax &prime;1 + bx &prime;2 + c&prime;|&radic;
</p>
<p>a2 + b2
=
</p>
<p>|c&prime; &minus; c|
&radic;
</p>
<p>a2 + b2
.
</p>
<p>From the Definition 15.3.12 we have d(A, A&prime;) &ge; d(A, r &prime;). Since the value d(A, r &prime;)
we have computed does not depend on the coordinates of A &isin; r , we have that d(A, r &prime;)
is the minimum value for d(A, A&prime;) when A ranges in r and A&prime; in r &prime;, so we conclude
that
</p>
<p>d(r, r &prime;) =
|c&prime; &minus; c|
&radic;
</p>
<p>a2 + b2
.
</p>
<p>Notice that, with respect to the same lines, we also have
</p>
<p>d(A&prime;, r) =
|c&prime; &minus; c|
&radic;
</p>
<p>a2 + b2
= d(A, r &prime;).
</p>
<p>Exercise 15.3.14 Consider the parallel lines r, r &prime; &sub; E2 with cartesian equations
</p>
<p>�r : 2x + y &minus; 3 = 0, �r &prime; : 2x + y + 2 = 0.
</p>
<p>The distance between them is
</p>
<p>d(r, r &prime;) =
|2 &minus; (&minus;3)|
</p>
<p>&radic;
5
</p>
<p>=
&radic;
</p>
<p>5.
</p>
<p>The distance between two parallel hyperplanes in En is given by generalising the
</p>
<p>proof of the Theorem 15.3.9.
</p>
<p>Proposition 15.3.15 If H and H &prime; are parallel hyperplanes in En with cartesian
equations</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 The Distance Between Linear Affine Varieties 281
</p>
<p>�H : a1x1 + &middot; &middot; &middot; + an xn + b = 0, �H &prime; : a1x1 + &middot; &middot; &middot; + an xn + b&prime; = 0,
</p>
<p>then d(H, H &prime;) = d(Q, H &prime;), where Q is an arbitrary point in H, and therefore it is
</p>
<p>d(H, H &prime;) =
|b &minus; b&prime;|
</p>
<p>&radic;
</p>
<p>a21 + &middot; &middot; &middot; + a2n
.
</p>
<p>Proof We proceed as in the Theorem 15.3.9, so we set X = (x1, . . . , xn) and
A = (a1, . . . , an) and write
</p>
<p>�H : A &middot; X + b = 0, �H &prime; : A &middot; X + b&prime; = 0.
</p>
<p>As we argued in the Remark 15.3.13, by setting Q = X̄ with AX̄ + b = 0, as an
arbitrary point in H , we have
</p>
<p>d(Q, H &prime;) =
|A &middot; X̄ + b&prime;|
</p>
<p>‖A‖
=
</p>
<p>|b&prime; &minus; b|
‖A‖
</p>
<p>and since such a distance does not depend on Q, we conclude that
</p>
<p>d(H, H &prime;) = d(Q, H &prime;). �
</p>
<p>Exercise 15.3.16 The planes
</p>
<p>�π : x + 2y &minus; z + 2 = 0, �π&prime; : x + 2y &minus; z &minus; 4 = 0
</p>
<p>are parallel and distinct. The distance between them is
</p>
<p>d(π,π&prime;) =
|2 + 4|
</p>
<p>&radic;
1 + 4 + 1
</p>
<p>=
&radic;
</p>
<p>6.
</p>
<p>It is clear that not all linear affine varieties which are parallel have the same
</p>
<p>dimension. The next proposition shows a result within this situation.
</p>
<p>Proposition 15.3.17 Let r be a line and H an hyperplane in En , with r parallel to
</p>
<p>H. It is
</p>
<p>d(r, H) = d(P̄, H),
</p>
<p>where P̄ is any point in r .
</p>
<p>Proof With the notations previously adopted, we have A = (a1, . . . , an) and
X = (x1, . . . , xn), we represent H by the cartesian equation
</p>
<p>�H : A &middot; X + b = 0
</p>
<p>and r by the vector equation</p>
<p/>
</div>
<div class="page"><p/>
<p>282 15 Euclidean Affine Linear Geometry
</p>
<p>r : P = P̄ + λv
</p>
<p>where P̄ &isin; r while v &middot; A = 0 since r is parallel to H . From the Theorem 15.3.9 we
have
</p>
<p>d(P, H) =
|A &middot; P + b|
</p>
<p>‖A‖
=
</p>
<p>|A &middot; (P̄ + λv)+ b|
‖A‖
</p>
<p>=
|A &middot; P̄ + b|
</p>
<p>‖A‖
.
</p>
<p>This expression does not depend on λ: this is the reason why d(P, H) = d(P̄, H) =
d(r, H). �
</p>
<p>Exercise 15.3.18 Consider in E3 the line r and the plane π given by:
</p>
<p>�r :
{
</p>
<p>2x &minus; y + z &minus; 2 = 0
y + 2z = 0 , �π : 2x &minus; y + z + 3 = 0.
</p>
<p>Since r is parallel to π, we take the point P = (1, 0, 0) in r and compute the
distance between P and π. One gets
</p>
<p>d(r,π) = d(P,π) =
5
&radic;
</p>
<p>6
.
</p>
<p>Exercise 15.3.19 Consider the lines r and r &prime; in E3 given by the vector equations
</p>
<p>r : (x, y, z) = (3, 1, 2)+ λ(1, 2, 0), r &prime; : (x, y, z) = (&minus;1,&minus;2, 3)+ λ(1, 2, 0).
</p>
<p>Since r is parallel to r &prime;, the distance between them can be computed by proceeding
as in the previous exercises, that is d(r, r &prime;) = d(A, r &prime;) = d(B, r), where A is an
arbitrary point in r and B an arbitrary point in r &prime;.
</p>
<p>We illustrate an alternative method. We notice that, if π is a plane orthogo-
</p>
<p>nal to both r and r &prime;, then the distance d(r, r &prime;) = d(P, P &prime;) where P = π &cap; r and
P &prime; = π &cap; r &prime;. We consider the plane π through the origin which is orthogonal to both
r and r &prime;, and whose cartesian equation is
</p>
<p>�π : x + 2y = 0.
</p>
<p>Direct calculations show that P = π &cap; r = (2,&minus;1, 2) and P &prime; = π &cap; r &prime; =
(0, 0, 3), so
</p>
<p>d(r, r &prime;) = d(P, P &prime;) =
&radic;
</p>
<p>6.
</p>
<p>We end the section by sketching how to define the distance between skew lines
</p>
<p>in E3.
</p>
<p>Remark 15.3.20 If r and r &prime; are skew lines in E3, then there exist a point P &isin; r and
a point P &isin; r &prime; which are the intersections of the lines r and r &prime; with the unique line s</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 The Distance Between Linear Affine Varieties 283
</p>
<p>orthogonally intersecting both r and r &prime;. The line s is the minimum distance line for
r and r &prime;, and the distance d(r, r &prime;) = d(P, P &prime;).
</p>
<p>Exercise 15.3.21 We consider the skew lines in E3,
</p>
<p>r : (x, y, z) = λ(1,&minus;1, 1), r &prime; : (x, y, z) = (0, 0, 1)+ &micro;(1, 0, 1).
</p>
<p>The subspace N &sub; E3 which is orthogonal to both the directions Sr and Sr &prime; is
N = L((1, 0,&minus;1)). The minimum distance line s for the given r and r &prime; has the
direction Ss = N , and intersects r in a point P and r &prime; in a point P &prime;. Since P &isin; r
and P &isin; r &prime;, there exists a value for λ and a value for &micro; such that P = Q(λ) and
P &prime; = Q&prime;(&micro;) with
</p>
<p>Q(λ)+ t (1, 0,&minus;1) = Q&prime;(&micro;),
</p>
<p>where ν is the parameter for s. The points P = s &cap; r and P &prime; = s &cap; r &prime; are then those
corresponding to the values of the parameters λ and &micro; solving such a relation, that is
</p>
<p>s :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>λ+ ν = &micro;
&minus;λ = 0
λ&minus; t = 1 + &micro;
</p>
<p>.
</p>
<p>One finds λ = 0, &micro; = ν = &minus; 1
2
, so P = (0, 0, 0), P &prime; = 1
</p>
<p>2
(&minus;1, 0, 1) and
</p>
<p>d(r, r &prime;) = d(P, P &prime;) =
1
&radic;
</p>
<p>2
.
</p>
<p>15.4 Bundles of Lines and of Planes
</p>
<p>A useful notion for several kinds of problems in affine geometry is that of bundle of
</p>
<p>lines and bundle of planes.
</p>
<p>Definition 15.4.1 Given a point A in E2, the bundle of concurrent lines with center
</p>
<p>(or point of concurrency) A is the set of all the lines through A in E2; we shall denote
</p>
<p>it by FA.
</p>
<p>The next result is immediate.
</p>
<p>Proposition 15.4.2 With A = (x0, y0) &isin; E2, the cartesian equation of an arbitrary
line in the bundle FA through A is given by
</p>
<p>�FA : α(x &minus; x0)+ β(y &minus; y0) = 0
</p>
<p>for any choice of the real parameters α and β such that (α,β) &isin; R2 \ {(0, 0)}.</p>
<p/>
</div>
<div class="page"><p/>
<p>284 15 Euclidean Affine Linear Geometry
</p>
<p>Notice that the parameters α and β label a line in FA, but there is not a bijection
</p>
<p>between pairs (α,β) and lines in FA: the pairs (α0,β0) and (ρα0, ρβ0), for ρ �= 0,
give the same line in FA.
</p>
<p>Exercise 15.4.3 The cartesian equation for the bundle FA of lines through
</p>
<p>A = (1,&minus;2) in E2 is
</p>
<p>�FA : α(x &minus; 1)+ β(y + 2) = 0.
</p>
<p>The result described in the next proposition (whose proof we omit) shows that the
</p>
<p>bundle FA can be generated by any pair of distinct lines concurrent in A.
</p>
<p>Proposition 15.4.4 Let A &isin; E2 be the unique intersection of the lines
</p>
<p>�r : ax + by + c = 0, �r &prime; : a&prime;x + b&prime;y + c&prime; = 0.
</p>
<p>Any relation
</p>
<p>�(α,β) : α(ax + by + c)+ β(a&prime;x + b&prime;y + c&prime;) = 0
</p>
<p>with R2 &ni; (α,β) �= (0, 0) is the cartesian equation for a line in the bundle FA of lines
with center A, and for any element s of FA there exists a pair R
</p>
<p>2 &ni; (α,β) �= (0, 0)
such that the cartesian equation of s can be written as
</p>
<p>�(α,β) : α(ax + by + c)+ β(a&prime;x + b&prime;y + c&prime;) = 0. (15.1)
</p>
<p>Definition 15.4.5 If the bundle FA is given by (15.1), the distinct lines r, r
&prime; are
</p>
<p>called the generators of the bundle. To stress the role of the generating lines, we also
</p>
<p>write in such a case FA = F(r, r &prime;).
</p>
<p>Exercise 15.4.6 The line r whose cartesian equation is �r : x + y + 1 is an ele-
ment in the bundle FA in the Exercise 15.4.3, corresponding to the parameters
</p>
<p>(α,β) = (1, 1) or equivalently (α,β) = (ρ, ρ) with ρ �= 0.
</p>
<p>Exercise 15.4.7 Consider the following cartesian equation,
</p>
<p>�(α,β) : α(x &minus; y + 3)+ β(2x + y + 3) = 0,
</p>
<p>depending on a pair of real parameters (α,β) �= (0, 0). Since the relations
x &minus; y + 3 = 0 and 2x + y + 3 = 0 yield the cartesian equations for a pair of non
parallel lines in E2, the equation �(α,β) is the cartesian equation for a bundle F of
</p>
<p>lines in E2. We compute:
</p>
<p>(a) the centre A of the bundle F ,
</p>
<p>(b) the line s1 &isin; F which is orthogonal to the line r1 whose cartesian equation is
�r1 : 3x + y &minus; 1 = 0,</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Bundles of Lines and of Planes 285
</p>
<p>(c) the line s2 &isin; F which is parallel to the line r2 whose cartesian equation is
�r2 : x &minus; y = 0,
</p>
<p>(d) the line s3 &isin; F through the point B = (1, 1).
We proceed as follows:
</p>
<p>(a) The centre of the bundle is given by the intersection
</p>
<p>{
</p>
<p>x &minus; y + 3 = 0
2x + y + 3 = 0 ,
</p>
<p>which is found to be A = (&minus;2, 1).
(b) We write the cartesian equation of the bundle F ,
</p>
<p>�F : (α+ 2β)x + (&minus;α+ β)y + 3(α+ β) = 0.
</p>
<p>As a consequence, the direction of an arbitrary line in the bundle F is spanned by
</p>
<p>the vectorv(α,β) = (α+ 2β,β &minus; α). In order for the line s1 &isin; F to be orthogonal
to r1 we require
</p>
<p>(α+ 2β,β &minus; α) &middot; (&minus;1, 3) = 0 &rArr; (α,β) = ρ(7,&minus;2)
</p>
<p>with ρ �= 0. The line s has the cartesian equation �(7,&minus;2) : x &minus; 3y + 5 = 0.
(c) In order for an element s2 &isin; F to be parallel to r2 we require that its direction
</p>
<p>coincides with the direction of r2, which is L((1,&minus;1)). We impose then
</p>
<p>α+ 2β = &minus;(β &minus; α) &rArr; (α,β) = ρ(1, 0)
</p>
<p>with ρ �= 0. So we have that s2 is given by the cartesian equation
�(1,0) : x &minus; y + 3 = 0. The line s2 turns out to be indeed one of the genera-
tors of the bundle F .
</p>
<p>(d) We have now to require that the coordinates of B solve the equation �(α,β), that
</p>
<p>is
</p>
<p>(α+ 2β)+ (β &minus; α)+ 3(α+ β) = 0 &rArr; 3α+ 6β = 0,
</p>
<p>giving (α,β) = ρ(2,&minus;1) with ρ �= 0. The line s3 is therefore given by
�(2,&minus;1) : y &minus; 1 = 0.
</p>
<p>Remark 15.4.8 Notice that the computations in (d) above can be generalised. If FA
is a bundle of lines through A, for any point B �= A there always exists a unique line
in FA which passes through B. We denote it as the line rAB &isin; FA.
</p>
<p>Definition 15.4.9 Let �r : ax + by + c = 0 be the cartesian equation of the line r
in E2. The set of all lines which are parallel to r is said to define a bundle of parallel
</p>
<p>lines or an improper bundle. The most convenient way to describe an improper bundle
</p>
<p>of lines is
</p>
<p>�F : ax + by + h = 0, with h &isin; R.</p>
<p/>
</div>
<div class="page"><p/>
<p>286 15 Euclidean Affine Linear Geometry
</p>
<p>Exercise 15.4.10 We consider the line r in E2 given by �r : 2x &minus; y + 3 = 0. We
wish to determine the lines s which are parallel to r and whose distance from r is
</p>
<p>d(s, r) =
&radic;
</p>
<p>5.
</p>
<p>The parallel lines to r are the elements rh of the improper bundle F whose cartesian
</p>
<p>equation is
</p>
<p>�h : 2x &minus; y + h = 0.
</p>
<p>From the Proposition 15.3.15 we have
</p>
<p>&radic;
5 = d(rh, r) =
</p>
<p>|h &minus; 3|
&radic;
</p>
<p>5
&rArr; |h &minus; 3| = 5 &rArr; h &minus; 3 = &plusmn;5.
</p>
<p>The solutions of the exercise are
</p>
<p>�r8 : 2x &minus; y + 8 = 0, �r&minus;2 : 2x &minus; y &minus; 2 = 0.
</p>
<p>In a way similar to above, one has the notion of bundle of planes in a three
</p>
<p>dimensional affine space.
</p>
<p>Definition 15.4.11 Let r be a line in E3. The bundle Fr of planes through r is the set
</p>
<p>of all planes π in E3 which contains r , that is r &sub; π. The line r is called the carrier
of the bundle Fr .
</p>
<p>Moreover, if π is a plane in E3, the set of all planes in E3 which are parallel to π
</p>
<p>gives the (improper) bundle of parallel planes to π.
</p>
<p>The following proposition is the analogue of the Proposition 15.4.2.
</p>
<p>Proposition 15.4.12 Let r be the line in E3 with cartesian equation given by
</p>
<p>�r :
{
</p>
<p>ax + by + cz + d = 0
a&prime;x + b&prime;y + c&prime;z + d &prime; = 0 .
</p>
<p>For any choice of the parameters (α,β) �= (0, 0) the relation
</p>
<p>�(α,β) : α(ax + by + cz + d)+ β(a&prime;x + b&prime;y + c&prime;z + d &prime;) = 0 (15.2)
</p>
<p>yields the cartesian equation for a plane in the bundle Fr with carrier line r , and for
</p>
<p>any plane π in such a bundle there is a pair (α,β) �= (0, 0) such that the cartesian
equation of π is given by (15.2).
</p>
<p>Definition 15.4.13 If the bundle Fr of planes is given by the cartesian equation
</p>
<p>(15.2), the planes �π : ax + by + cz + d = 0 and �π&prime; : a&prime;x + b&prime;y + c&prime;z + d &prime; = 0
are called the generators of Fr . In such a case the equivalent notation F(π,π
</p>
<p>&prime;) will
also be used.
</p>
<p>Remark 15.4.14 Clearly, the bundle Fr is generated by any two distinct planes π, π
&prime;
</p>
<p>through r .</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Bundles of Lines and of Planes 287
</p>
<p>Exercise 15.4.15 Given the line r whose vector equation is
</p>
<p>r : (x, y, z) = (1, 2,&minus;1)+ λ(2, 3, 1),
</p>
<p>we determine the bundle Fr of planes through r . In order to obtain a cartesian equation
</p>
<p>for r , we eliminate the parameter λ from the vector equation above, as follows
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = 1 + 2λ
y = 2 + 3λ
λ = z + 1
</p>
<p>&rArr;
{
</p>
<p>x = 1 + 2(z + 1)
y = 2 + 3(z + 1) &rArr; �r :
</p>
<p>{
</p>
<p>x &minus; 2z &minus; 3 = 0
y &minus; 3z &minus; 5 = 0 .
</p>
<p>The cartesian equation for the bundle is then given by
</p>
<p>�Fr : α(x &minus; 2z &minus; 3)+ β(y &minus; 3z &minus; 5) = 0
</p>
<p>with any (α,β) �= (0, 0).
Let us next find the plane π &isin; Fr which passes through A = (1, 2, 3). The con-
</p>
<p>dition A &isin; π yields
</p>
<p>α(1 &minus; 6 &minus; 3)+ β(2 &minus; 9 &minus; 5) = 0 &rArr; 2α+ 3β = 0.
</p>
<p>We can pick (λ,&micro;) = (3,&minus;2), giving �π : 3(x &minus; 2z &minus; 3)&minus; 2(y &minus; 3z &minus; 5) = 0,
that is �π : 3x &minus; 2y + 1 = 0.
</p>
<p>We also find the plane σ &isin; Fr which is orthogonal to v = (1,&minus;1, 1). We know
that a vector orthogonal to a plane π &isin; Fr with equation
</p>
<p>�Fr : αx + βy &minus; (2α+ 3β)z &minus; 3α&minus; 5β = 0,
</p>
<p>is given by (α,β,&minus;2α&minus; 3β). The conditions we have to meet are then
{
</p>
<p>α = &minus;β
α = &minus;2α&minus; 3β &rArr; α = &minus;β.
</p>
<p>If we fix (λ,&micro;) = (1,&minus;1), we have �σ : (x &minus; 2z &minus; 3)&minus; (y &minus; 3z &minus; 5) = 0, that is
</p>
<p>�σ : x &minus; y + z + 2 = 0.
</p>
<p>15.5 Symmetries
</p>
<p>We introduce a few notions related to symmetries which are useful to solve problems
</p>
<p>in several branches of geometry and physics.
</p>
<p>Definition 15.5.1 Consider a point C &isin; En .</p>
<p/>
</div>
<div class="page"><p/>
<p>288 15 Euclidean Affine Linear Geometry
</p>
<p>(a) Let P &isin; En be an arbitrary point in En . The symmetric point to P with respect
to C is the element P &prime; &isin; En that belongs to the line rC P passing through C and
P , and such that d(P &prime;,C) = d(P,C) with P &prime; �= P .
</p>
<p>(b) Let X &sub; En be a set of points. The symmetric points to X with respect to C is
the set X &prime; &sub; En given by every point P &prime; which is symmetric to any P in X with
respect to C .
</p>
<p>(c) Let X &sub; En . We say that X is symmetric with respect to C if X = X &prime;, that is if
X contains the symmetric point (with respect to C) to any of its points. In such
</p>
<p>a case, C is called a symmetry centre for X .
</p>
<p>Exercise 15.5.2 In the euclidean affine plane E2 consider the point C = (2, 3).
Given the point P = (1,&minus;1), we determine its symmetric P &prime; with respect to C .
And with the line �r : 2x &minus; y &minus; 3 = 0, we determine its symmetric r &prime; with respect
to C .
</p>
<p>We consider the line rC P through P and C , which has the vector equation
</p>
<p>rC P : (x, y) = (1,&minus;1)+ λ(1, 4).
</p>
<p>The distance between P and C is given by ‖P &minus; C‖ =
&radic;
</p>
<p>17, so the point P &prime; can
be obtained by finding the value for the parameter λ such that the distance
</p>
<p>‖Pλ &minus; C‖ = ‖(&minus;1 + λ,&minus;4 + 4λ)‖ =
&radic;
</p>
<p>(&minus;1 + λ)2 + (&minus;4 + 4λ)2
</p>
<p>be equal to ‖P &minus; C‖. We have then
&radic;
</p>
<p>(&minus;1 + λ)2 + 16(&minus;1 + λ)2 =
&radic;
</p>
<p>17 &rArr;
&radic;
</p>
<p>17(&minus;1 + λ)2 =
&radic;
</p>
<p>17 &rArr;
&radic;
</p>
<p>(&minus;1 + λ)2 = 1
</p>
<p>that is ‖ &minus; 1 + λ‖ = 1, giving λ = 2, λ = 0. For λ = 0 we have Pλ=0 = P , so
P &prime; = Pλ=2 = (3, 7).
</p>
<p>In order to determine r &prime; we observe that P &isin; r and we claim that, since r is a line,
the set r &prime; symmetric to r with respect to C is a line as well. It is then sufficient to write
the line through P &prime; and another point Q&prime; which is symmetric to Q &isin; r with respect
to C . By choosing Q = (0,&minus;3) &isin; r , it is immediate to compute, with the same steps
as above, that Q&prime; = (4, 9). We conclude that r &prime; = rC Q&prime; , with vector equation
</p>
<p>r &prime; : (x = 3 + λ, y = 7 + 2λ).
</p>
<p>Definition 15.5.3 Let A, B be points in En . The midpoint MAB of the line segment
</p>
<p>AB is the (unique) point of the line rAB with ‖MAB &minus; A‖ = ‖MAB &minus; B‖.
</p>
<p>Notice that A is the symmetric point to B with respect to MAB , and clearly B is
</p>
<p>the symmetric point to A with respect to MAB with MAB = MB A. One indeed has
the vector equality A &minus; MAB = MAB &minus; B, giving
</p>
<p>MAB =
A + B
</p>
<p>2
.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Symmetries 289
</p>
<p>The set HAB given by the points
</p>
<p>HAB =
{
</p>
<p>P &isin; En : ‖P &minus; A‖ = ‖P &minus; B‖
}
</p>
<p>can be shown to be the hyperplane passing through MAB and orthogonal to the line
</p>
<p>segment AB. The set HAB is called the bisecting hyperplane of the line segment AB.
</p>
<p>In E2 is the bisecting line of AB, while in E3 is the bisecting plane of AB.
</p>
<p>Exercise 15.5.4 Consider the line segment in E2 whose endpoints are A = (1, 2)
and B = (3, 4). Its midpoint is given by
</p>
<p>MAB =
A + B
</p>
<p>2
=
</p>
<p>(1, 2)+ (3, 4)
2
</p>
<p>= (2, 3).
</p>
<p>A point P = (x, y) belongs to the bisecting line if ‖P &minus; A‖2 = (x &minus; 1)2 + (y &minus; 2)2
equates ‖P &minus; B‖2 = (x &minus; 3)2 + (y &minus; 4)2 = ‖PB‖2, which gives
</p>
<p>(x &minus; 1)2 + (y &minus; 2)2 = (x &minus; 3)2 + (y &minus; 4)2 &rArr; &minus;2x + 1 &minus; 4y + 4 = &minus;6x + 9 &minus; 8y + 16,
</p>
<p>that is �HAB : x + y &minus; 5 = 0. It is immediate to check that M &isin; HAB . The direction
of the bisecting line is spanned by (1,&minus;1), which is orthogonal to the direction vector
B &minus; A = (2, 2) spanning the direction of the line rAB .
</p>
<p>Exercise 15.5.5 Consider the points A = (1, 2,&minus;1) and B = (3, 0, 1) in E3. The
corresponding midpoint is
</p>
<p>MAB =
A + B
</p>
<p>2
=
</p>
<p>(1, 2,&minus;1)+ (3, 0, 1)
2
</p>
<p>= (2, 1, 0).
</p>
<p>The bisecting plane HAB is given by the points P = (x, y, z) fulfilling the con-
dition
</p>
<p>(x &minus; 1)2 + (y &minus; 2)2 + (z + 1)2 = ‖P &minus; A‖2 = ‖P &minus; B‖2 = (x &minus; 3)2 + y2 + (z &minus; 1)2
</p>
<p>which gives
</p>
<p>�π : x &minus; y + z &minus; 1 = 0.
</p>
<p>The bisecting plane is then orthogonal to (1,&minus;1, 1), with rAB having a direction
vector given by B &minus; A = (2,&minus;2, 2).
</p>
<p>Having defined the notion of symmetry of a set in En with respect to a point, we
</p>
<p>might wonder about a meaningful definition of symmetry of a set with respect to an
</p>
<p>arbitrary linear affine variety in En . Such a task turns out to be quite hard in general,
</p>
<p>so we focus on the easy case of defining only the notion of symmetry with respect
</p>
<p>to a hyperplane.
</p>
<p>Firstly, a general definition.</p>
<p/>
</div>
<div class="page"><p/>
<p>290 15 Euclidean Affine Linear Geometry
</p>
<p>Definition 15.5.6 Let H &sub; En be a hyperplane.
</p>
<p>(a) Let P &isin; En be an arbitrary point in En . The symmetric point to P with respect
to H is the element P &prime; &isin; En such that H is the bisecting hyperplane of the line
segment P P &prime;.
</p>
<p>(b) Let X &sub; En be a set of points. The symmetric points to X with respect to H is
the set X &prime; &sub; En given by every point P &prime; which is symmetric to any P in X with
respect to H .
</p>
<p>(c) Let X &sub; En . We say that X is symmetric with respect to H if X = X &prime;, that is if
X contains the symmetric point (with respect to H ) to any of its points. In such
</p>
<p>a case, H is called a symmetry hyperplane for X .
</p>
<p>Remark 15.5.7 Notice that if P &prime; is the symmetric point to P with respect to the
hyperplane H , then the line rP P &prime; is orthogonal to H and d(P
</p>
<p>&prime;, H) = d(P, H).
</p>
<p>We finish with some examples on the simplest cases in E2 and E3.
</p>
<p>Exercise 15.5.8 A line is a hyperplane in E2. Given the point P = (1, 2) we deter-
mine its symmetric P &prime; with respect to the line whose equation is �r : 2x + y &minus; 2.
</p>
<p>We observe that if t is the line through P which is orthogonal to P , then P &prime; is
the point in t fixed by the condition d(P, r) = d(P &prime;, r). The direction of t is clearly
spanned by the vector (2, 1), so
</p>
<p>t :
{
</p>
<p>x = 1 + 2λ
y = 2 + λ
</p>
<p>and the points in t can be written as Qλ = (1 + 2λ, 2 + λ). By setting
</p>
<p>d(Qλ, r) = d(P, r) &rArr;
|2(1 + 2λ)+ (2 + λ)&minus; 2|
</p>
<p>&radic;
4 + 1
</p>
<p>=
|2 + 2 &minus; 2|
&radic;
</p>
<p>4 + 1
&rArr; |5λ+ 2| = 2
</p>
<p>we see that Qλ=0 = P , while Qλ=&minus;4/5 = P &prime; = 15 (&minus;3, 6).
</p>
<p>Exercise 15.5.9 Given P = (0, 1,&minus;2) &isin; E3, we determine its symmetric P &prime; with
respect to the hyperplane π (which is indeed a plane, since we are in E3) whose
</p>
<p>equation is �π : 2x + 4y + 4z &minus; 5 = 0.
We firstly find the line t through P which is orthogonal to π. The orthogonal
</p>
<p>subspace to π is spanned by the vector (2, 4, 4) or equivalently (1, 2, 2), so the line
</p>
<p>t has parametric equation
</p>
<p>t :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = λ
y = 1 + 2λ
z = &minus;2 + 2λ
</p>
<p>.
</p>
<p>Since for the symmetric point P &prime; it is d(P,π) = d(P &prime;,π), we label a point Q in
t by the parameter λ as Qλ = (λ, 1 + 2λ,&minus;2 + 2λ) and impose</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Symmetries 291
</p>
<p>d(Qλ,π) = d(P,π)
</p>
<p>&rArr;
|2λ+ 4(1 + 2λ)+ 4(&minus;2 + 2λ)&minus; 5|
</p>
<p>&radic;
36
</p>
<p>=
|4 &minus; 8 &minus; 5|
</p>
<p>&radic;
36
</p>
<p>&rArr; |18λ&minus; 9| = 9.
</p>
<p>We see that P = Qλ=0 and P &prime; = Qλ=1 = P &prime; = (1, 3, 0).
</p>
<p>Exercise 15.5.10 In E3 let us determine the line r &prime; which is symmetric to the line
with equation r : (x, y, z) = (0, 1,&minus;2)+ &micro;(1, 0, 0) with respect to the plane π with
equation π : 2x + 4y + 4z &minus; 5 = 0.
</p>
<p>The plane π is the same plane we considered in the previous exercise. Its orthog-
</p>
<p>onal space is spanned by the vector (1, 2, 2). By labelling a point of the line r as
</p>
<p>P&micro; = (&micro;, 1,&minus;2), we find the line t&micro; which passes through P&micro; and is orthogonal to
π. A parametric equation for t&micro; is given by
</p>
<p>t&micro; :
</p>
<p>⎧
</p>
<p>⎨
</p>
<p>⎩
</p>
<p>x = &micro;+ λ
y = 1 + 2λ
z = &minus;2 + 2λ
</p>
<p>.
</p>
<p>We label then points Q in t&micro; by writing Qλ,&micro; = (&micro;+ λ, 1 + 2λ,&minus;2 + 2λ). We
require
</p>
<p>d(Qλ,&micro;,π) = d(P&micro;,π)
</p>
<p>as a condition to determine λ, since &micro; will yield a parameter for the line r &prime;. We have
</p>
<p>d(Qλ,&micro;,π) =
|2(&micro;+ λ)+ 4(1 + 2λ)+ 4(&minus;2 + 2λ)&minus; 5|
</p>
<p>&radic;
36
</p>
<p>d(P&micro;,π) =
|2&micro;+ 4 &minus; 8 &minus; 5|
</p>
<p>&radic;
36
</p>
<p>.
</p>
<p>From d(Qλ,&micro;,π) = d(P&micro;,π) we have
</p>
<p>|2&micro;+ 18λ&minus; 9| = |2&micro;&minus; 9| &rArr; 2&micro;+ 18λ&minus; 9 = &plusmn;(2&micro;&minus; 9).
</p>
<p>For λ = 0 we recover Qλ=0,&micro; = P&micro;. The other solution is λ = &minus; 29 &micro;+ 1, giving
</p>
<p>Qλ=&minus;(2/9)&micro;+1,&micro; = P &prime;&micro; =
(
</p>
<p>7
</p>
<p>9
&micro;+ 1,&minus;
</p>
<p>4
</p>
<p>9
&micro;+ 3,&minus;
</p>
<p>4
</p>
<p>9
&micro;
</p>
<p>)
</p>
<p>.
</p>
<p>By a rescaling of the parameter &micro;, a vector equation for the line r &prime; can be written
as
</p>
<p>r &prime; : (x, y, z) = (1, 3, 0)+ &micro;(7,&minus;4,&minus;4).
</p>
<p>Exercise 15.5.11 Consider the set X &sub; E2 given by
</p>
<p>X = {(x, y) &isin; E2 : y = 5x2}</p>
<p/>
</div>
<div class="page"><p/>
<p>292 15 Euclidean Affine Linear Geometry
</p>
<p>and the line r whose cartesian equation is �r : x = 0. We wish to show that r is
a symmetry axis for X , that is X is symmetric with respect to r . We have then to
</p>
<p>prove that each point P &prime;, symmetric to any point P &isin; X with respect to r , is an
element in X .
</p>
<p>Let us consider a generic P = (x0, y0) &isin; X and determine its symmetric with
respect to r . The line t through P which is orthogonal to r has the following parametric
</p>
<p>equation
</p>
<p>t :
{
</p>
<p>x = x0 + λ
y = y0
</p>
<p>.
</p>
<p>A point in t is then labelled Pλ = (x0 + λ, y0). For its distance from r we compute
d(Pλ, r) = |x0 + λ|, while d(P, r) = |x0|. By imposing that these two distances
coincide, we have
</p>
<p>d(Pλ, r) = d(P, r) &hArr; |x0 + λ| = |x0|
&hArr; (x0 + λ)2 = x20
&hArr; λ(2x0 + λ) = 0.
</p>
<p>The solution λ = 0 corresponds to P , the solution λ = &minus;2x0 yields
P &prime; = (&minus;x0, y0). Such calculations do not depend on the fact that P is an element in
X . If we consider only points P in X , we have to require that y0 = 5x20 . It follows
that y0 = 5(&minus;x0)2, that is P &prime; &isin; X .</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
</p>
<p>Conic Sections
</p>
<p>This chapter is devoted to conics. We shall describe at length their algebraic and
</p>
<p>geometric properties and their use in physics, notably for the Kepler laws for the
</p>
<p>motion of celestial bodies.
</p>
<p>16.1 Conic Sections as Geometric Loci
</p>
<p>The conic sections (or simply conics) are parabol&aelig;, ellipses (with circles as limiting
</p>
<p>case), hyperbol&aelig;. They are also known as geometric loci, that is collections of points
</p>
<p>P(x, y) &isin; E2 satisfying one or more conditions, or determined by such conditions.
The following three relations, whose origins we briefly recall, should be well known
</p>
<p>x2 = 2py,
x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= 1,
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= 1. (16.1)
</p>
<p>Definition 16.1.1 (Parabol&aelig;) Given a straight line δ and a point F on the plane E2,
</p>
<p>the set (locus) of points P equidistant from δ and F is called parabola. The straight
</p>
<p>line δ is the directrix of the parabola, while the point F is the focus of the parabola.
</p>
<p>This is shown in Fig. 16.1.
</p>
<p>Fix a cartesian orthogonal reference system (O; x, y) for E2, with a generic point
P having coordinates P = (x, y). Consider the straight line δ given by the points
with equation y = &minus;p/2 and the focus F = (0, p/2) (with p &gt; 0). The parabola
with directrix δ and focus F is the set of points fulfilling the condition
</p>
<p>d(P, δ) = d(P, F). (16.2)
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1_16
</p>
<p>293</p>
<p/>
<div class="annotation"><a href="http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_16&amp;domain=pdf">http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-319-78361-1_16&amp;domain=pdf</a></div>
</div>
<div class="page"><p/>
<p>294 16 Conic Sections
</p>
<p>Fig. 16.1 The parabola y = x2/2p
</p>
<p>Since the point P &prime; = (x,&minus;p/2) is the orthogonal projection of P over δ, with
d(P, δ) = d(P, P &prime;), the condition (16.2) reads
</p>
<p>‖P &minus; P &prime;‖2 = ‖P &minus; F‖2 &rArr; ‖(0, y + p/2)‖2 = ‖(x, y &minus; p/2)‖2,
</p>
<p>that is
</p>
<p>(y + p/2)2 = x2 + (y &minus; p/2)2 &rArr; x2 = 2py.
</p>
<p>If C is a parabola with focus F and directrix δ then,
</p>
<p>&bull; the straight line through F which is orthogonal to δ is the axis of C ,
&bull; the point where the parabola C intersects its axis is the vertex of the parabola.
</p>
<p>Definition 16.1.2 (Ellipses) Given two points F1 ed F2 on the plane E
2, the set
</p>
<p>(locus) of points P for which the sum of the distances between P and the points F1
and F2 is constant is called ellipse. The points F1 and F2 are called the foci of the
</p>
<p>ellipse. This is shown in Fig. 16.2.
</p>
<p>Fix a cartesian orthogonal reference system (O; x, y) for E2, with a generic point
P having coordinates P = (x, y). Consider the points F1 = (&minus;q, 0), F2 = (q, 0)
(with q &ge; 0) and k a real parameter such that k &gt; 2q. The ellipse with foci F1, F2
and parameter k is the set of points P = (x, y) fulfilling the condition
</p>
<p>Fig. 16.2 The ellipse x2/a2 + y2/b2 = 1</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Conic Sections as Geometric Loci 295
</p>
<p>d(P, F1)+ d(P, F2) = k. (16.3)
</p>
<p>We denote by A = (a, 0) and B = (0, b) the intersection of the ellipse with the
positive x-axis half-line and the positive y-axis half-line, thus a &gt; 0 and b &gt; 0. From
</p>
<p>d(A, F1)+ d(A, F2) = k we have that k = 2a; from d(B, F1)+ d(B, F2) = k we
have that 2
</p>
<p>&radic;
q2 + b2 = k, so we write
</p>
<p>k = 2a, q2 = a2 &minus; b2,
</p>
<p>with a &ge; b. By squaring the condition (16.3) we have
</p>
<p>‖(x + q, y)‖2 + ‖(x &minus; q, y)‖2 + 2 ‖(x + q, y)‖ ‖(x &minus; q, y)‖ = 4a2,
</p>
<p>that is
</p>
<p>2(x2 + y2 + q2)+ 2
&radic;
(x2 + y2 + q2 + 2qx)(x2 + y2 + q2 &minus; 2qx) = 4a2
</p>
<p>that we write as
</p>
<p>&radic;
(x2 + y2 + q2)2 &minus; 4q2x2 = 2a2 &minus; (x2 + y2 + q2).
</p>
<p>By squaring such a relation we have
</p>
<p>&minus;q2x2 = a4 &minus; a2(x2 + y2 + q2).
</p>
<p>Since q2 = a2 &minus; b2, the equation of the ellipse depends on the real positive param-
eters a, b as follows
</p>
<p>b2x2 + a2y2 = a2b2,
</p>
<p>which is equivalent to
</p>
<p>x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= 1.
</p>
<p>Notice that, if q = 0, that is if a = b, the foci F1 ed F2 coincide with the origin
O of the reference system, and the ellipse reduces to a circle whose equation is
</p>
<p>x2 + y2 = r2
</p>
<p>with radius r = a = b &gt; 0.
If C is an ellipse with (distinct) foci F1 and F2, then
</p>
<p>&bull; the straight line passing through the foci is the major axis of the ellipse,
&bull; the straight line orthogonally bisecting the segment F1F2 is the minor axis of the
ellipse,
</p>
<p>&bull; the midpoint of the segment F1F2 is the centre of the ellipse,</p>
<p/>
</div>
<div class="page"><p/>
<p>296 16 Conic Sections
</p>
<p>Fig. 16.3 The hyperbola x2/a2 &minus; y2/b2 = 1
</p>
<p>&bull; the four points where the ellipse intersects its axes are the vertices of the ellipse,
&bull; the distance between the centre of the ellipse and the vertices on the major axis
(respectively on the minor axis) is called the major semi-axis (respectively minor
</p>
<p>semi-axis).
</p>
<p>Definition 16.1.3 (Hyperbol&aelig;) Given two points F1 and F2 on the plane E
2, the set
</p>
<p>(locus) of points P for which the absolute difference of the distances d(P, F1) and
</p>
<p>d(P, F2) is constant, is the hyperbola with foci F1, F2. This is shown in Fig. 16.3.
</p>
<p>Fix a cartesian orthogonal reference system (O; x, y) for E2, with a generic point
P having coordinates P = (x, y). Consider the points F1 = (&minus;q, 0), F2 = (q, 0)
(with q &ge; 0) and k a real parameter such that k &gt; 2q. The hyperbola with foci
F1, F2 and parameter k is the set of points P = (x, y) fulfilling the condition
</p>
<p>|d(P, F1)&minus; d(P, F2)| = k. (16.4)
</p>
<p>Notice that, since k &gt; 0, such a hyperbola does not intersect the y-axis, since the
</p>
<p>points on the y-axis are equidistant from the foci. By denoting by A = (a, 0) (with
a &gt; 0) the intersection of the hyperbola with the x-axis, we have
</p>
<p>k = |d(A, F1)&minus; d(A, F2)| =
∣∣a + q &minus; |a &minus; q|
</p>
<p>∣∣,
</p>
<p>which yields a &lt; q, since from a &gt; q it would follow that |a &minus; q| = a &minus; q, giving
k = 2q. The previous condition then show that
</p>
<p>k = |2a| = 2a.
</p>
<p>By squaring the relation (16.4) we have
</p>
<p>‖(x + q, y)‖2 + ‖(x &minus; q, y)‖2 &minus; 2 ‖(x + q, y)‖ ‖(x &minus; q, y)‖ = 4a2,</p>
<p/>
</div>
<div class="page"><p/>
<p>16.1 Conic Sections as Geometric Loci 297
</p>
<p>that is
</p>
<p>2(x2 + y2 + q2)&minus; 2
&radic;
(x2 + y2 + q2 + 2qx)(x2 + y2 + q2 &minus; 2qx) = 4a2
</p>
<p>which we write as
</p>
<p>&radic;
(x2 + y2 + q2)2 &minus; 4q2x2 = (x2 + y2 + q2)&minus; 2a2.
</p>
<p>By squaring once more, we have
</p>
<p>&minus;q2x2 = a4 &minus; a2(x2 + y2 + q2),
</p>
<p>that reads
</p>
<p>(a2 &minus; q2)x2 + a2y2 = a2(a2 &minus; q2).
</p>
<p>From a &lt; q we have q2 &minus; a2 &gt; 0, so we set q2 &minus; a2 = b2 and write the previous
relation as
</p>
<p>&minus;b2x2 + a2y2 = &minus;a2b2,
</p>
<p>which is equivalent to
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= 1.
</p>
<p>If C is a hyperbola with foci F1 and F2, then
</p>
<p>&bull; the straight line through the foci is the transverse axis of the hyperbola,
&bull; the straight line orthogonally bisecting the segment F1F2 is the axis of the hyper-
bola,
</p>
<p>&bull; the midpoint of the segment F1F2 is the centre of the hyperbola;
&bull; the points where the hyperbola intersects its transverse axis are the vertices of the
hyperbola,
</p>
<p>&bull; the distance between the centre of the hyperbola and its foci is the transverse
semi-axis of the hyperbola.
</p>
<p>Remark 16.1.4 The above analysis shows that, if C is a parabola with equation
</p>
<p>x2 = 2py,
</p>
<p>then its directrix is the line y = &minus;p/2 and its focus is the point (0, p/2), while the
equation
</p>
<p>y2 = 2px
</p>
<p>is a parabola C with directrix x = &minus;p/2 and focus (p/2, 0).
If C is an ellipse with equation</p>
<p/>
</div>
<div class="page"><p/>
<p>298 16 Conic Sections
</p>
<p>x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= 1
</p>
<p>(and a &ge; b), then its foci are the points F&plusmn; = (&plusmn;
&radic;
a2 &minus; b2, 0).
</p>
<p>If C is a hyperbola with equation
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= 1
</p>
<p>then its foci are the points F&plusmn; = (&plusmn;
&radic;
a2 + b2, 0).
</p>
<p>We see that the definition of a parabola requires one single focus and a straight
</p>
<p>line (not containing the focus), while the definition of an ellipse and of a hyperbola
</p>
<p>requires two distinct foci and a suitable distance k. This apparent diversity can be
</p>
<p>reconciled. If F is a point isE2 and δ a straight line with F /&isin; δ, then one can consider
the locus given by points P in E2 fulfilling the condition
</p>
<p>d(P, F) = e d(P, δ) (16.5)
</p>
<p>with e &gt; 0. It is clear that, if e = 1, this relation defines a parabola with focus F and
directrix δ. We shall show later on (in Sect. 16.4 and then Sect. 16.7) that the relation
</p>
<p>above gives an ellipse for 0 &lt; e &lt; 1 and a hyperbola if e &gt; 1. The parameter e &gt; 0
</p>
<p>is called the eccentricity of the conic.
</p>
<p>Since symmetry properties of conics do not depend on the reference system,
</p>
<p>when dealing with symmetries or geometric properties of conics one can refer to the
</p>
<p>Eqs. (16.1).
</p>
<p>Remark 16.1.5 With the symmetry notions given in the Sect. 15.5, the y-axis is a
</p>
<p>symmetry axis for the parabolaC whose equation is y = 2px2. If P = (x0, y0) &isin; C ,
the symmetric point P &prime; to P with respect to the y-axis is P &prime; = (&minus;x0, y0), which
belongs to C since 2py0 = (&minus;x20 ) = x20 . Furthermore, the axis of a parabola is a
symmetry axis and its vertex is equidistant from the focus and the directrix if the
</p>
<p>parabola.
</p>
<p>In a similar way one shows that the axes of an ellipse or of a hyperbola, are
</p>
<p>symmetry axes and the centre is a symmetry centre in both cases. For an ellipse with
</p>
<p>equation αx2 + βy2 = 1 or a hyperbola with equation αx2 &minus; βy2 = 1 the centre
coincided with the origin of the reference system.
</p>
<p>16.2 The Equation of a Conic in Matrix Form
</p>
<p>In the previous section we have shown how, in a given reference system, a parabola,
</p>
<p>an ellipse and a hyperbola are described by one of equations in (16.1). But evidently
</p>
<p>such equations are not the most general ones for the loci we are considering, since
</p>
<p>they have particular positions with respect to the axes of the reference system.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 The Equation of a Conic in Matrix Form 299
</p>
<p>A common feature of the Eqs. (16.1) is that they are formulated as quadratic
</p>
<p>polynomials in x and y. In the present section we study general quadratic polynomial
</p>
<p>equations in two variables.
</p>
<p>Since to a large extent one does not make use of the euclidean structure given
</p>
<p>by the scalar product in En , one can consider the affine plane A2(R). By taking
</p>
<p>complex coordinates, with the canonical inclusion R2 &rarr;֒ C2, one enlarges the real
affine plane to the complex one,
</p>
<p>A
2(R) &rarr;֒ A2(C).
</p>
<p>Definition 16.2.1 A conic section (or simply a conic) is the set of points (locus)
</p>
<p>whose coordinates (x, y) satisfy a quadratic polynomial equation in the variables
</p>
<p>x, y, that is
</p>
<p>a11 x
2 + 2 a12 xy + a22 y2 + 2 a13 x + 2 a23 y + a33 = 0 (16.6)
</p>
<p>with coefficients ai j &isin; R.
</p>
<p>Remark 16.2.2 We notice that
</p>
<p>(a) The equations of conics considered in the previous section are particular case of
</p>
<p>the general Eq. (16.6). As an example, for a parabola we have
</p>
<p>a11 = 1, a23 = &minus;2p, a12 = a22 = a13 = a33 = 0.
</p>
<p>Notice also that in all the equations considered in the previous section for a
</p>
<p>parabola or an ellipse or a hyperbola we have a12 = 0.
(b) There are polynomial equations like (16.6) which do not describe any of
</p>
<p>the conics presented before: neither a parabola, nor an ellipse or a hyper-
</p>
<p>bola. Consider for example the equation x2 &minus; y2 = 0, which is factorised as
(x + y)(x &minus; y) = 0. The set of solutions for such an equation is the union of
the two lines with cartesian equations x + y = 0 and x &minus; y = 0.
Any quadratic polynomial equation (16.6) that can be factorised as
</p>
<p>(ax + by + c)(a&prime;x + b&prime;y + c&prime;) = 0
</p>
<p>describes the union of two lines. Such lines are not necessarily real. Consider for
</p>
<p>example the equation x2 + y2 = 0. Its set of solutions is given only by the point
(0, 0) inA2(R), while inA2(C)we canwrite x2 + y2 = (x + iy)(x &minus; iy), so the
conic is the union of the two conjugate lines with cartesian equation x + iy = 0
and x &minus; iy = 0.
</p>
<p>Definition 16.2.3 A conic is called degenerate if it is the union of two lines. Such
</p>
<p>lines can be either real (coincident or distinct) or complex (in such a case they are
</p>
<p>also conjugate).</p>
<p/>
</div>
<div class="page"><p/>
<p>300 16 Conic Sections
</p>
<p>The polynomial equation (16.6) can be written in a more succinct form by means
</p>
<p>of two symmetric matrices associated with a conic. We set
</p>
<p>R
2,2 &ni; A =
</p>
<p>(
a11 a12
a12 a22
</p>
<p>)
, R3,3 &ni; B =
</p>
<p>⎛
⎝
a11 a12 a13
a12 a22 a23
a13 a23 a33
</p>
<p>⎞
⎠ .
</p>
<p>By introducing these matrices, we write the left end side of the Eq. (16.6) as
</p>
<p>(
x y 1
</p>
<p>)
⎛
⎝
a11 a12 a13
a12 a22 a23
a13 a23 a33
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>1
</p>
<p>⎞
⎠ = a11 x2 + 2 a12 xy + a22 y2 + 2 a13 x + 2 a23 y + a33.
</p>
<p>(16.7)
</p>
<p>The quadratic homogeneous part of the polynomial defining (16.6) and (16.7), is
</p>
<p>written as
</p>
<p>FC(x, y) = a11 x2 + 2 a12 xy + a22 y2 =
(
x y
</p>
<p>)
A
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
.
</p>
<p>Such an FC is a quadratic form, called thequadratic form associated to the conicC .
</p>
<p>Definition 16.2.4 Let C be the conic given by the equation
</p>
<p>a11 x
2 + 2 a12 xy + a22 y2 + 2 a13 x + 2 a23 y + a33 = 0.
</p>
<p>The matrices
</p>
<p>B =
</p>
<p>⎛
⎝
a11 a12 a13
a12 a22 a23
a13 a23 a33
</p>
<p>⎞
⎠ , A =
</p>
<p>(
a11 a12
a12 a22
</p>
<p>)
</p>
<p>are called respectively the matrix of the coefficients and the matrix of the quadratic
</p>
<p>form of C .
</p>
<p>Exercise 16.2.5 The matrices associated to the parabola with equation y = 3x2 are,
</p>
<p>B =
</p>
<p>⎛
⎝
3 0 0
</p>
<p>0 0 &minus;1/2
0 &minus;1/2 0
</p>
<p>⎞
⎠ , A =
</p>
<p>(
3 0
</p>
<p>0 0
</p>
<p>)
.
</p>
<p>Remark 16.2.6 Notice that the six coefficients ai j in (16.6) determine a conic, but a
</p>
<p>conic is not described by a single array of six coefficients since the equation
</p>
<p>ka11 x
2 + 2 ka12 xy + ka22 y2 + 2 ka13 x + 2 ka23 y + ka33 = 0
</p>
<p>defines the same locus for any k &isin; R \ {0}.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Reduction to Canonical Form of a Conic: Translations 301
</p>
<p>16.3 Reduction to Canonical Form of a Conic: Translations
</p>
<p>A natural question arises. Given a non degenerate conic with equation written as in
</p>
<p>(16.6) with respect to a reference frame, does there exist a new reference systemwith
</p>
<p>respect to which the equation for the conic has a form close to one of those given in
</p>
<p>(16.1)?
</p>
<p>Definition 16.3.1 We call canonical form of a non degenerate conic C one of the
</p>
<p>following equations for C in a given reference system (O; x, y).
(i) A parabola has equation
</p>
<p>x2 = 2py or y2 = 2px . (16.8)
</p>
<p>(ii) A real ellipse has equation
</p>
<p>x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= 1 (16.9)
</p>
<p>while an imaginary ellipse has equation
</p>
<p>x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= &minus;1. (16.10)
</p>
<p>(iii) A hyperbola has equation
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= 1 or
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= &minus;1. (16.11)
</p>
<p>A complete answer to the question above is given in two steps.
</p>
<p>One first considers only conics whose equation, in a given reference system,
</p>
<p>(O; x, y) has coefficient a12 = 0, that is conics whose equation lacks the mixed
term xy. The reference system (O &prime;; X,Y ) for a canonical form is obtained with a
translation from (O; x, y).
</p>
<p>The general case of a conic whose equation in a given reference system (O; x, y)
may have the mixed term xy will require the composition of a rotation and a transla-
</p>
<p>tion from (O; x, y) to obtain the reference system (O &prime;; X,Y ) for a canonical form.
Exercise 16.3.2 Let Ŵ : y = 2x2 describe a parabola in the canonical form, and let
us define the following translation on the plane
</p>
<p>T(x0,y0) :
{
x = X + x0
y = Y + y0
</p>
<p>.
</p>
<p>The equation for the conic Ŵ with respect to the reference system (O &prime;; X,Y ) is then
</p>
<p>Y = 2X2 + 4x0X + 2x20 &minus; y0.</p>
<p/>
</div>
<div class="page"><p/>
<p>302 16 Conic Sections
</p>
<p>Exercise 16.3.3 Let Ŵ&prime; : x2 + 2y2 = 1 be an ellipse in the canonical form. Under
the translation of the previous example, the equation for Ŵ&prime; with respect to the refer-
ence system (O &prime;; X,Y ) is
</p>
<p>X2 + 2Y 2 + 2x0X + 4y0Y + x20 + 2y20 &minus; 1 = 0.
</p>
<p>Notice that, after the translation by T(x0,y0), the equations for the conics Ŵ and Ŵ
&prime;
</p>
<p>are no longer in canonical form, but both still lack the mixed term xy. We prove now,
</p>
<p>with a constructive method, that the converse holds as well.
</p>
<p>Exercise 16.3.4 (Completing the squares) Let C be a non degenerate conic whose
</p>
<p>equation reads, with respect to the reference system (O; x, y),
</p>
<p>a11 x
2 + a22 y2 + 2 a13 x + 2 a23 y + a33 = 0. (16.12)
</p>
<p>Since the polynomial must be quadratic, there are two possibilities. Either both
</p>
<p>a11 and a22 different from zero, or one of them is zero. We then consider:
</p>
<p>(I) It is a11 = 0, a22 
= 0 (the case a11 
= 0 and a22 = 0 is analogue).
The Eq. (16.12) is then
</p>
<p>a22 y
2 + 2 a23 y + a33 + 2 a13 x = 0. (16.13)
</p>
<p>From the algebraic identities:
</p>
<p>a22 y
2 + 2 a23 y = a22
</p>
<p>(
y2 + 2
</p>
<p>a23
</p>
<p>a22
y
</p>
<p>)
</p>
<p>= a22
</p>
<p>[(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
&minus;
</p>
<p>(
a23
</p>
<p>a22
</p>
<p>)2]
</p>
<p>= a22
(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
&minus;
</p>
<p>a223
</p>
<p>a22
</p>
<p>we write the Eq. (16.13) as
</p>
<p>a22
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
&minus;
</p>
<p>a223
</p>
<p>a22
+ a33 + 2 a13 x = 0. (16.14)
</p>
<p>Since C is not degenerate, we have a13 
= 0 so we write (16.14) as
</p>
<p>a22
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
+ 2 a13
</p>
<p>(
x +
</p>
<p>a33a22 &minus; a223
2 a22a13
</p>
<p>)
= 0</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Reduction to Canonical Form of a Conic: Translations 303
</p>
<p>which reads
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
= &minus;
</p>
<p>2 a13
</p>
<p>a22
</p>
<p>(
x +
</p>
<p>a33a22 &minus; a223
2 a22a13
</p>
<p>)
.
</p>
<p>Under the translation
</p>
<p>⎧
⎨
⎩
</p>
<p>X = x + (a33a22 &minus; a223)/2 a22a13
</p>
<p>Y = y + a23/a22
</p>
<p>we get
</p>
<p>Y 2 = 2 p X (16.15)
</p>
<p>with p = &minus;a13/a22. This is the canonical form (16.8).
</p>
<p>If we drop the hypothesis that the conics C is non degenerate, we have
</p>
<p>a13 = 0 in the Eq. (16.13). Notice that, for the case a11 = 0 we are considering,
det B = &minus;a213a22. Thus the condition of non degeneracy can be expressed as a
condition on the determinant of the matrix of the coefficients, since
</p>
<p>a13 = 0 &hArr; det B = &minus;a213a22 = 0.
</p>
<p>The Eq. (16.14) is then
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
=
</p>
<p>a223 &minus; a33a22
a222
</p>
<p>and with the translation {
X = x
Y = y + a23/a22
</p>
<p>it reads
</p>
<p>Y 2 = q (16.16)
</p>
<p>with q = (a223 &minus; a33a22)/a222.
(II) It is a11 
= 0, a22 
= 0.
</p>
<p>With algebraic manipulation as above, we can write
</p>
<p>a11 x
2 + 2 a13 x = a11
</p>
<p>(
x +
</p>
<p>a13
</p>
<p>a11
</p>
<p>)2
&minus;
</p>
<p>a213
</p>
<p>a11
,
</p>
<p>a22 y
2 + 2 a23 y = a22
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
&minus;
</p>
<p>a223
</p>
<p>a22
.</p>
<p/>
</div>
<div class="page"><p/>
<p>304 16 Conic Sections
</p>
<p>So the Eq. (16.12) is written as
</p>
<p>a11
</p>
<p>(
x +
</p>
<p>a13
</p>
<p>a11
</p>
<p>)2
+ a22
</p>
<p>(
y +
</p>
<p>a23
</p>
<p>a22
</p>
<p>)2
+ a33 &minus;
</p>
<p>a213
</p>
<p>a11
&minus;
</p>
<p>a223
</p>
<p>a22
= 0. (16.17)
</p>
<p>If we consider the translation given by
</p>
<p>{
X = x + a13/a11
Y = y + a23/a22
</p>
<p>the conic C has the equation
</p>
<p>a11X
2 + a22Y 2 = h, with h = &minus;a33 +
</p>
<p>a213
</p>
<p>a11
+
</p>
<p>a223
</p>
<p>a22
, (16.18)
</p>
<p>and h 
= 0 since C is non degenerate. The coefficients a11 and a22 can be either
concordant or not. Up to a global factor (&minus;1), we can take a11 &gt; 0. So we have
the following cases.
</p>
<p>(IIa) It is a11 &gt; 0 and a22 &gt; 0. One distinguish according to the sign of the coeffi-
</p>
<p>cient h:
</p>
<p>&bull; If h &gt; 0, the Eq. (16.18) is equivalent to
</p>
<p>a11
</p>
<p>h
X2 +
</p>
<p>a22
</p>
<p>h
Y 2 = 1.
</p>
<p>Since a11/h &gt; 0 and a22/h &gt; 0, we have (positive) real numbers a, b by
</p>
<p>defining h/a11 = a2 and h/a22 = b2. The Eq. (16.18) is written as
</p>
<p>X2
</p>
<p>a2
+
</p>
<p>Y 2
</p>
<p>b2
= 1, (16.19)
</p>
<p>which is the canonical form of a real ellipse (16.9).
</p>
<p>&bull; If h &lt; 0, we have&minus;a11/h &gt; 0 and&minus;a22/h &gt; 0, we can again introduce (pos-
itive) real numbers a, b by &minus;h/a11 = a2 and &minus;h/a22 = b2. The Eq. (16.18)
can be written as
</p>
<p>X2
</p>
<p>a2
+
</p>
<p>Y 2
</p>
<p>b2
= &minus;1, (16.20)
</p>
<p>which is the canonical form of an imaginary ellipse (16.10).
</p>
<p>&bull; Ifh = 0 (whichmeans thatC is degenerate),we set 1/a11 = a2 and1/a22 = b2
with real number a, b, so to get from (16.18) the expression
</p>
<p>X2
</p>
<p>a2
+
</p>
<p>Y 2
</p>
<p>b2
= 0. (16.21)</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Reduction to Canonical Form of a Conic: Translations 305
</p>
<p>(IIb) It is a11 &gt; 0 and a22 &lt; 0. Again depending on the sign of the coefficient h we
</p>
<p>have:
</p>
<p>&bull; If h &gt; 0, the Eq. (16.18) is
</p>
<p>a11
</p>
<p>h
X2 +
</p>
<p>a22
</p>
<p>h
Y 2 = 1.
</p>
<p>Since a11/h &gt; 0 and a22/h &lt; 0, we can define h/a11 = a2 and&minus;h/a22 = b2
with a, b positive real numbers. The Eq. (16.18) becomes
</p>
<p>X2
</p>
<p>a2
&minus;
</p>
<p>X2
</p>
<p>b2
= 1, (16.22)
</p>
<p>which the first canonical form in (16.11).
</p>
<p>&bull; Ifh &lt; 0,wehave&minus;a11/h &gt; 0 and&minus;a22/h &lt; 0, sowecandefine&minus;h/a11 = a2
and h/a22 = 1/b2 with a, b positive real numbers. The Eq. (16.18) becomes
</p>
<p>X2
</p>
<p>a2
&minus;
</p>
<p>Y 2
</p>
<p>b2
= &minus;1, (16.23)
</p>
<p>which is the second canonical form in (16.11).
</p>
<p>&bull; If h = 0 (that is C is degenerate), we set 1/a11 = a2 and &minus;1/a22 = b2 with
a, b real number, so to get from (16.18) the expression
</p>
<p>X2
</p>
<p>a2
&minus;
</p>
<p>Y 2
</p>
<p>b2
= 0. (16.24)
</p>
<p>Once again, with B the matrix of the coefficients for C , the identity
</p>
<p>det B = a11a22 h
</p>
<p>shows that the condition of non degeneracy of the conic C is equivalently given by
</p>
<p>det B 
= 0.
The analysis done for the cases of degenerate conics makes it natural to introduce
</p>
<p>the following definition, which has to be compared with the Definition 16.3.1.
</p>
<p>We call canonical form of a degenerate conic C one of the following equations
</p>
<p>for C in a given reference system (O; x, y).
(i) A degenerate parabola has equation
</p>
<p>x2 = q or y2 = q. (16.25)
</p>
<p>(ii) A degenerate ellipse has equation
</p>
<p>x2
</p>
<p>a2
+
</p>
<p>y2
</p>
<p>b2
= 0. (16.26)</p>
<p/>
</div>
<div class="page"><p/>
<p>306 16 Conic Sections
</p>
<p>(iii) A degenerate hyperbola has equation
</p>
<p>x2
</p>
<p>a2
&minus;
</p>
<p>y2
</p>
<p>b2
= 0. (16.27)
</p>
<p>Remark 16.3.5 With the definition above, we have that
</p>
<p>(i) The conic C with equation x2 = q is the union of the lines with cartesian
equations x = &plusmn;&radic;q . If q &gt; 0 the lines are real and distinct, if q &lt; 0 the lines
are complex and conjugate. If q = 0 the conic C is the y-axis counted twice.
Analogue cases are obtained for the equation y2 = q.
</p>
<p>(ii) The equation b2 x2 + a2 y2 = 0 has the unique solution (0, 0) if we consider
real coordinates. On the complex affine planeA2(C) the solutions to such equa-
</p>
<p>tions give a degenerate conic C which is the union of two complex conjugate
</p>
<p>lines, since we can factorise
</p>
<p>b2 x2 + a2 y2 = (b x + i a y)(b x &minus; i a y).
</p>
<p>(iii) The solutions to the equation b2 x2 &minus; a2 y2 = 0 give the union of two real and
distinct lines, since we can factorise as follows
</p>
<p>b2 x2 &minus; a2 y2 = (b x + a y)(b x &minus; a y).
</p>
<p>What we have studied up to now is the proof of the following theorem.
</p>
<p>Theorem 16.3.6 Let C be a conic whose equation, with respect to a reference sys-
</p>
<p>tem (O; x, y) lacks the monomial xy. There exists a reference system (O &prime;; X,Y ),
obtained from (O; x, y) by a translation, with respect to which the equation for the
conic C has a canonical form.
</p>
<p>Exercise 16.3.7 We consider the conic C with equation
</p>
<p>x2 + 4y2 + 2x &minus; 12y + 3 = 0.
</p>
<p>We wish to determine a reference system (O &prime;; X,Y ) with respect to which the
equation for C is canonical. We complete the squares as follows:
</p>
<p>x2 + 2x = (x + 1)2 &minus; 1,
</p>
<p>4y2 &minus; 12y = 4
(
y &minus; 3
</p>
<p>2
</p>
<p>)2 &minus; 9
</p>
<p>and write
</p>
<p>x2 + 4y2 + 2x &minus; 12y + 3 = (x + 1)2 + 4
(
y &minus; 3
</p>
<p>2
</p>
<p>)2 &minus; 7.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Reduction to Canonical Form of a Conic: Translations 307
</p>
<p>With the translation {
X = x + 1
Y = y &minus; 3
</p>
<p>2
</p>
<p>the equation for C reads
</p>
<p>X2 + 4Y 2 = 7 &rArr;
X2
</p>
<p>7
+
</p>
<p>Y 2
</p>
<p>7/4
= 1.
</p>
<p>This is an ellipse with centre (X = 0,Y = 0) = (x = &minus;1, y = 3/2), with axes
given by the lines X = 0 and Y = 0 which are x = &minus;1 and y = 3/2, and semi-axes
given by
</p>
<p>&radic;
7,
&radic;
7/2.
</p>
<p>16.4 Eccentricity: Part 1
</p>
<p>We have a look now at the relation (16.5) for a particular class of examples. Consider
</p>
<p>the point F = (ax , ay) in E2 and the line δ whose points satisfy the equation x = u,
with u 
= ax . The relation d(P, F) = e d(P, δ) (with e &gt; 0) is satisfied by the points
P = (x, y) whose coordinates are the solutions of the equation
</p>
<p>(y &minus; ay)2 + (1&minus; e2)x2 + 2(ue2 &minus; ax )x + a2x &minus; u2e2 = 0. (16.28)
</p>
<p>We have different cases, depending on the parameter e.
</p>
<p>(a) We have already mentioned that for e = 1 we are describing the parabola with
focus F and directrix δ. Its equation from (16.28) is given by
</p>
<p>(y &minus; ay)2 + 2(u &minus; ax )x + a2x &minus; u2 = 0. (16.29)
</p>
<p>(b) Assume e 
= 1. Using the results of the Exercise16.3.4, we complete the square
and write
</p>
<p>(y &minus; ay)2 + (1&minus; e2)x2 + 2(ue2 &minus; ax )x + a2x &minus; u2e2 = 0
</p>
<p>or (y &minus; ay)2 + (1&minus; e2)
(
x +
</p>
<p>ue2 &minus; ax
1&minus; e2
</p>
<p>)2
&minus;
</p>
<p>e2(u &minus; ax )2
</p>
<p>1&minus; e2
= 0. (16.30)
</p>
<p>Then the translation given by
</p>
<p>{
Y = y &minus; ay
X = x + (ue2 &minus; ax )/(1&minus; e2)
</p>
<p>allows us to write, with respect to the reference system (O &prime;; X,Y ), the equation
as</p>
<p/>
</div>
<div class="page"><p/>
<p>308 16 Conic Sections
</p>
<p>Y 2 + (1&minus; e2)X2 =
e2(u &minus; ax )2
</p>
<p>1&minus; e2
.
</p>
<p>Depending on the value of e, we have the following possibilities.
</p>
<p>(b1) If 0 &lt; e &lt; 1, all the coefficients of the equation are positive, so we have the
</p>
<p>ellipse (
1&minus; e2
</p>
<p>e(u &minus; ax )
</p>
<p>)2
X2 +
</p>
<p>1&minus; e2
</p>
<p>e2(u &minus; ax )2
Y 2 = 1.
</p>
<p>An easy computation shows that its foci are given by
</p>
<p>F&plusmn; = (&plusmn;
e2(u &minus; ax )
1&minus; e2
</p>
<p>, ay)
</p>
<p>with respect to the reference system (O &prime;; X,Y ) and then clearly by
</p>
<p>F+ = (ax , ay), F&minus; = (
ax + e2ax &minus; 2ue2
</p>
<p>1&minus; e2
, ay)
</p>
<p>with respect to (O; x, y). Notice that F+ = F , the starting point.
(b2) If e &gt; 1 the equation
</p>
<p>(
1&minus; e2
</p>
<p>e(u &minus; ax )
</p>
<p>)2
X2 &minus;
</p>
<p>e2 &minus; 1
e2(u &minus; ax )2
</p>
<p>Y 2 = 1.
</p>
<p>represents a hyperbola with foci again given by the points F&plusmn; written before.
</p>
<p>Remark 16.4.1 Notice that, if e = 0, the relation (16.28) becomes
</p>
<p>(y &minus; ay)2 + (x &minus; ax )2 = 0,
</p>
<p>that is a degenerate imaginary conic, with
</p>
<p>(y &minus; ay + i(x &minus; ax ))(y &minus; ay &minus; i(x &minus; ax )) = 0.
</p>
<p>If we fix e2(u &minus; ax )2 = r2 
= 0 and consider the limit e &rarr; 0, the Eq. (16.28) can
be written as
</p>
<p>(x &minus; ax )2 + (y &minus; ay)2 = r2.
</p>
<p>This is another way of viewing a circle as a limiting case of a sequence of ellipses.
</p>
<p>The case for which the point F &isin; δ also gives a degenerate conic. In this case
u = ax and the Eq. (16.28) is
</p>
<p>(y &minus; ax )2 + (1&minus; e2)(x &minus; 2u)2 = 0
</p>
<p>which is the union of two lines either real (if 1 &lt; e) or imaginary (if 1 &gt; e).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Conic Sections and Kepler Motions 309
</p>
<p>16.5 Conic Sections and Kepler Motions
</p>
<p>Via the notion of eccentricity it is easier to describe a fundamental relation between
</p>
<p>the conic sections and the so called Keplerian motions.
</p>
<p>If x1(t) and x2(t) describe the motion in E
3 of two point masses m1 and m2, and
</p>
<p>the only force acting on them is the mutual gravitational attraction, the equations of
</p>
<p>motions are given by
</p>
<p>m1ẍ1 = &minus;Gm1m2
x1 &minus; x2
</p>
<p>‖x1 &minus; x2‖3
</p>
<p>m2ẍ2 = &minus;Gm1m2
x2 &minus; x1
</p>
<p>‖x1 &minus; x2‖3
.
</p>
<p>Here G is a constant, the gravitational constant. We know from physics that the
</p>
<p>centre of mass of this system moves with no acceleration, while for the relative
</p>
<p>motion r(t) = x1(t)&minus; x2(t) the Newton equations are
</p>
<p>&micro; r̈(t) = &minus;Gm1m2
r
</p>
<p>r3
(16.31)
</p>
<p>with the norm r = ‖x‖ and &micro; = m1m2/(m1 + m2) the so called reduced mass of the
system. A qualitative analysis of this motion can be given as follows.
</p>
<p>With a cartesian orthogonal reference system (O; x, y, z) in E3, we can write
r(t) = (x(t), y(t), z(t)) and ṙ(t) = (ẋ(t), ẏ(t), ż(t)) for the vector representing the
corresponding velocity. From the Newton equations (16.31) the angular momentum
</p>
<p>(recall its definition and main properties from Sects. 1.3 and 11.2) with respect to the
</p>
<p>origin O ,
dLO
</p>
<p>dt
= &micro; {ṙ &and; ṙ + r &and; r̈} = 0,
</p>
<p>is a constant of the motion, since r̈ is parallel to r from (16.31). This means that both
</p>
<p>vectors r(t) and ṙ(t) remain orthogonal to the direction of LO , which is constant: if
</p>
<p>the initial velocity ṙ(t = 0) is not parallel to the initial position r(t = 0), the motion
stays at any time t on the plane orthogonal to LO(t = 0).
</p>
<p>We can consider the plane of the motion as E2, and fix a cartesian orthogonal ref-
</p>
<p>erence system (O; x, y), so that the angular momentum conservation can be written
as
</p>
<p>&micro; (ẋ y &minus; ẏx) = l
</p>
<p>with the constant l fixed by the initial conditions. We also know that the gravitational
</p>
<p>force is conservative, thus the total energy
</p>
<p>1
</p>
<p>2
&micro; ‖ṙ‖2 &minus; Gm1m2
</p>
<p>1
</p>
<p>r
= E .</p>
<p/>
</div>
<div class="page"><p/>
<p>310 16 Conic Sections
</p>
<p>is also a constant of themotion. It iswell known that the Eq. (16.31) can be completely
</p>
<p>solved. We omit the proof of this claim, and mention that the possible trajectories of
</p>
<p>such motions are conic sections, with focus F = (0, 0) &isin; E2 and directrix δ given
by the equation x = l̃/e with
</p>
<p>l̃ =
l2
</p>
<p>Gm1m2&micro;
.
</p>
<p>and eccentricity parameter given by
</p>
<p>e =
</p>
<p>&radic;
1 +
</p>
<p>2&micro;El2
</p>
<p>(Gm1m2&micro;)2
.
</p>
<p>One indeed shows that
2&micro;El2
</p>
<p>(Gm1m2&micro;)2
&gt; &minus;1
</p>
<p>for any choice of initial values for position and velocity.
</p>
<p>This result is one of the reasons why conic sections deserve a special attention
</p>
<p>in affine geometry. From the analysis of the previous section, we conclude that for
</p>
<p>E &lt; 0, since 0 &lt; e &lt; 1, the trajectory of the motion is elliptic. If the point mass m2
represents the Sun, while m1 a planet in our solar system, this result gives the well
</p>
<p>observed fact that planet orbits are plane elliptic and the Sun is one of the foci of the
</p>
<p>orbit (Kepler law).
</p>
<p>The Sun is also the focus of hyperbolic orbits (E &gt; 0) or parabolic ones (E = 0),
orbits that are travelled by comets and asteroids.
</p>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations
</p>
<p>Let us consider two reference systems (O; x, y) and (O; X,Y ) having the same
origin and related by a rotation by an angle of α,
</p>
<p>{
x = cosα X + sinα Y
y = &minus; sinα X + cosα Y .
</p>
<p>With respect to (O; x, y), consider the parabola Ŵ: y = x2. In the rotated system
(O; X,Y ) the equation for Ŵ is easily found to be
</p>
<p>&minus; sinα X + cosα Y = (cosα X + sinα Y )2
</p>
<p>&rArr;
cosα2 X2 + sin 2α XY + sinα2 Y 2 + sinα X &minus; cosα Y = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations 311
</p>
<p>We see that as a consequence of the rotation, there is a mixed term XY in the
</p>
<p>quadratic polynomial equation for the parabola Ŵ. It is natural to wonder whether
</p>
<p>such a behaviour can be reversed.
</p>
<p>Example 16.6.1 With respect to (O; x, y), consider the conic C : xy = k for a real
parameter k. Clearly, for k = 0 this is degenerate (the union of the coordinate axes x
and y). On the other hand, the rotation to the system (O; X,Y ) by an angle α = π
</p>
<p>4
,
</p>
<p>{
x = 1&radic;
</p>
<p>2
(X + Y )
</p>
<p>y = 1&radic;
2
(X &minus; Y ) ,
</p>
<p>transforms the equation of the conic to
</p>
<p>X2 &minus; Y 2 = 2k.
</p>
<p>This is a hyperbola with foci F&plusmn; = (&plusmn;2
&radic;
k, 0 when k &gt; 0 or F&plusmn; = (0,&plusmn;2
</p>
<p>&radic;
|k|)
</p>
<p>when k &lt; 0.
</p>
<p>In general, if the equation of a conic has a mixed term, does there exist a reference
</p>
<p>systemwith respect to which the equation for the given conic does not have themixed
</p>
<p>term?
</p>
<p>It is clear that the answer to such a question is in the affirmative if and only if
</p>
<p>there exists a reference system with respect to which the quadratic form of the conic
</p>
<p>is diagonal. On the other hand, since the quadratic form associated to a conic is
</p>
<p>symmetric, we know from the Chap. 10 that it is always possible to diagonalise it
</p>
<p>with a suitable orthogonal matrix.
</p>
<p>Let us first study how the equation in (16.7) for a conic changes under a general
</p>
<p>change of the reference system of the affine euclidean plane we are considering.
</p>
<p>Definition 16.6.2 With a rotation of the plane we mean a change in the reference
</p>
<p>system from (O; x, y) to (O; x &prime;, y&prime;) that is given by
(
x
</p>
<p>y
</p>
<p>)
= P
</p>
<p>(
x &prime;
</p>
<p>y&prime;
</p>
<p>)
, (16.32)
</p>
<p>with P &isin; SO(2) a special orthogonal matrix, referred to as the rotation matrix. If
we write
</p>
<p>P =
(
p11 p12
p21 p22
</p>
<p>)
,
</p>
<p>the transformation above reads
</p>
<p>{
x = p11x &prime; + p12y&prime;
y = p21x &prime; + p22y&prime;
</p>
<p>(16.33)
</p>
<p>These relations give the equations of the rotation.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 16 Conic Sections
</p>
<p>A translation from the reference system (O; x &prime;, y&prime;) to another (O &prime;; X,Y ) is
described by the relations {
</p>
<p>x &prime; = X + x0
y&prime; = Y + y0
</p>
<p>(16.34)
</p>
<p>where (&minus;x0,&minus;y0) are the coordinates of the point O with respect to (O &prime;; X,Y ) and,
equivalently, (x0, y0) are the coordinates of the point O
</p>
<p>&prime; with respect to (O; x &prime;, y&prime;).
A proper rigid transformation on the affine euclidean plane E2 is a change of the
</p>
<p>reference system given by a rotation followed by a translation. We shall refer to a
</p>
<p>proper rigid transformation also under the name of roto-translation.
</p>
<p>Let us consider the composition of the rotation given by (16.33) followed by the
</p>
<p>translation givenby (16.34), so tomap the reference system (O; x, y) into (O &prime;; X,Y ).
The equation describing such a transformation are easily found to be
</p>
<p>{
x = p11X + p12Y + a
y = p21X + p22Y + b
</p>
<p>(16.35)
</p>
<p>where {
a = p11 x0 + p12 y0
b = p21 x0 + p22 y0
</p>
<p>are the coordinates of O &prime; with respect to (O; x, y). The transformation (16.35) can
be written as ⎛
</p>
<p>⎝
x
</p>
<p>y
</p>
<p>1
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
p11 p12 a
</p>
<p>p21 p22 b
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
X
</p>
<p>Y
</p>
<p>1
</p>
<p>⎞
⎠ , (16.36)
</p>
<p>and we call
</p>
<p>Q =
</p>
<p>⎛
⎝
p11 p12 a
</p>
<p>p21 p22 b
</p>
<p>0 0 1
</p>
<p>⎞
⎠ (16.37)
</p>
<p>the matrix of (associated to) the proper rigid transformation (roto-translation).
</p>
<p>Remark 16.6.3 A rotation matrix P is special orthogonal, that is tP = P&minus;1 and
det(P) = 1. A roto-translation matrix Q as in (16.37), although satisfies the identity
det(Q) = 1, is not orthogonal.
</p>
<p>Clearly, with a transposition, the action (16.32) of a rotation matrix also gives(
x y
</p>
<p>)
=
</p>
<p>(
x &prime; y&prime;
</p>
<p>)
tP , while the action (16.36) of a roto-translation can be written as(
</p>
<p>x y 1
)
=
</p>
<p>(
X Y 1
</p>
<p>)
tQ.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations 313
</p>
<p>Let us then describe how the matrices associated to the equation of a conic are
</p>
<p>transformed under a roto-translation of the reference system. Then, let us consider a
</p>
<p>conic C described, with respect to the reference system (O; x, y), by
</p>
<p>(
x y 1
</p>
<p>)
B
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>1
</p>
<p>⎞
⎠ = 0, FC(x, y) =
</p>
<p>(
x y
</p>
<p>)
A
</p>
<p>(
x
</p>
<p>y
</p>
<p>)
.
</p>
<p>Under the roto-translation transformation (16.36) the equation of the conicC with
</p>
<p>respect to the reference system (O &prime;; X,Y ) is easily found to becomes
</p>
<p>(
X Y 1
</p>
<p>)
tQ B Q
</p>
<p>⎛
⎝
X
</p>
<p>Y
</p>
<p>1
</p>
<p>⎞
⎠ = 0.
</p>
<p>Also, under the same transformations, the quadratic form for C reads
</p>
<p>FC(x
&prime;, y&prime;) =
</p>
<p>(
x &prime; y&prime;
</p>
<p>)
tP A P
</p>
<p>(
x &prime;
</p>
<p>y&prime;
</p>
<p>)
</p>
<p>with respect to the reference system (O; x &prime;, y&prime;) obtained from (O; x, y) under the
action of only the rotation P . Such a claim is made clearer by the following propo-
</p>
<p>sition.
</p>
<p>Proposition 16.6.4 The quadratic form associated to a conic C does not change for
</p>
<p>a translation of the reference system with respect to which it is defined.
</p>
<p>Proof Let us consider, with respect to the reference system (O; x &prime;, y&prime;), the conic
with quadratic form
</p>
<p>FC(x
&prime;, y&prime;) =
</p>
<p>(
x &prime; y&prime;
</p>
<p>)
A&prime;
</p>
<p>(
x &prime;
</p>
<p>y&prime;
</p>
<p>)
= a11 (x &prime;)2 + 2 a12 x &prime;y&prime; + a22 (y&prime;)2.
</p>
<p>Under the translation (16.34) we have x &prime; = X &minus; x0 e y&prime; = Y &minus; y0, that is
</p>
<p>a11 X
2 + 2 a12 XY + a22 Y 2 + {monomials of order &le; 1}.
</p>
<p>The quadratic form associated to C , with respect to the reference system
</p>
<p>(O &prime;; X,Y ), is then
</p>
<p>FC(X,Y ) = a11 X2 + 2 a12 XY + a22 Y 2 =
(
X Y
</p>
<p>)
A&prime;
</p>
<p>(
X
</p>
<p>Y
</p>
<p>)
,
</p>
<p>with the same matrix A&prime;. �</p>
<p/>
</div>
<div class="page"><p/>
<p>314 16 Conic Sections
</p>
<p>Given the quadratic form FC associated to the conic C in (O; x &prime;, y&prime;), we have
then the following:
</p>
<p>FC(x
&prime;, y&prime;) =
</p>
<p>(
x &prime; y&prime;
</p>
<p>)
tP A P
</p>
<p>(
x &prime;
</p>
<p>y&prime;
</p>
<p>)
&rArr; FC(X,Y ) =
</p>
<p>(
X Y
</p>
<p>)
tP A P
</p>
<p>(
X
</p>
<p>Y
</p>
<p>)
.
</p>
<p>All of the above proves the following theorem.
</p>
<p>Theorem 16.6.5 Let C be a conic with associated matrix of the coefficients B and
</p>
<p>matrix of the quadratic form A with respect to the reference system (O; x, y). If
Q is the matrix of the roto-translation mapping the reference system (O; x, y) to
(O &prime;; X,Y ), with P the corresponding rotation matrix, the matrix of the coefficients
associated to the conic C with respect to (O &prime;; X,Y ) is
</p>
<p>B &prime; = tQ B Q,
</p>
<p>while the matrix of the canonical form is
</p>
<p>A&prime; = tP A P = P&minus;1 A P.
</p>
<p>In light of the Definition13.1.4, the matrices A and A&prime; are quadratically equiva-
lent. �
</p>
<p>Exercise 16.6.6 Consider the conic C whose equation, in the reference system
</p>
<p>(O; x, y) is
x2 &minus; 2xy + y2 + 4x + 4y &minus; 1 = 0.
</p>
<p>Its associated matrices are
</p>
<p>B =
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 2
&minus;1 1 2
2 2 &minus;1
</p>
<p>⎞
⎠ , A =
</p>
<p>(
1 &minus;1
&minus;1 1
</p>
<p>)
.
</p>
<p>We first diagonalise the matrix A. Its characteristic polynomial is
</p>
<p>pA(T ) = |A &minus; T I | =
∣∣∣∣
1&minus; T &minus;1
&minus;1 1&minus; T
</p>
<p>∣∣∣∣ = (1&minus; T )
2 &minus; 1 = T (T &minus; 2).
</p>
<p>The eigenvalues are λ = 0 and λ = 2 with associated eigenspaces,
</p>
<p>V0 = ker( fA) = {(x, y) &isin; R2 : x &minus; y = 0} = L((1, 1)),
V2 = ker( fA&minus;2I ) = {(x, y) &isin; R2 : x + y = 0} = L((1,&minus;1)).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations 315
</p>
<p>It follows that the special orthogonal matrix P giving the change of the basis is
</p>
<p>P = 1&radic;
2
</p>
<p>(
1 1
</p>
<p>&minus;1 1
</p>
<p>)
</p>
<p>and eigenvectors ordered so that det(P) = 1. This rotated the reference system to
(O; x &prime;, y&prime;) with {
</p>
<p>x = 1&radic;
2
(x &prime; + y&prime;)
</p>
<p>y = 1&radic;
2
(&minus;x &prime; + y&prime;) .
</p>
<p>Without translation, the roto-translation matrix is
</p>
<p>Q&prime; = 1&radic;
2
</p>
<p>⎛
⎝
</p>
<p>1 1 0
</p>
<p>&minus;1 1 0
0 0
</p>
<p>&radic;
2
</p>
<p>⎞
⎠
</p>
<p>and from the Theorem 16.6.5, thematrix associated toC with respect to the reference
</p>
<p>system (O; x &prime;, y&prime;) is B̃ = tQ&prime; B Q&prime;. We have then
</p>
<p>B̃ = 1&radic;
2
</p>
<p>⎛
⎝
1 &minus;1 0
1 1 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
</p>
<p>1 &minus;1 2
&minus;1 1 2
2 2 &minus;1
</p>
<p>⎞
⎠ 1
</p>
<p>2
</p>
<p>⎛
⎝
</p>
<p>1 1 0
</p>
<p>&minus;1 1 0
0 0 1
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
2 0 0
</p>
<p>0 0 2
&radic;
2
</p>
<p>0 2
&radic;
2 &minus;1
</p>
<p>⎞
⎠ ,
</p>
<p>so that the equation for C reads
</p>
<p>2(x &prime;)2 + 4
&radic;
2y&prime; &minus; 1 = 0.
</p>
<p>By completing the square at the right hand side, we write this equation as
</p>
<p>(x &prime;)2 = &minus;2
&radic;
2
(
y&prime; &minus;
</p>
<p>&radic;
2
8
</p>
<p>)
.
</p>
<p>With the translation {
X = x &prime;
</p>
<p>Y = y&prime; &minus;
&radic;
2
8
</p>
<p>we see that C is a parabola with the canonical form
</p>
<p>X2 = &minus;2
&radic;
2 Y
</p>
<p>and the associated matrices
</p>
<p>B &prime; =
</p>
<p>⎛
⎝
1 0 0
</p>
<p>0 0
&radic;
2
</p>
<p>0
&radic;
2 0
</p>
<p>⎞
⎠ , A&prime; =
</p>
<p>(
1 0
</p>
<p>0 0
</p>
<p>)
.</p>
<p/>
</div>
<div class="page"><p/>
<p>316 16 Conic Sections
</p>
<p>Rather than splitting the reduction to canonical form into a first step given by a
</p>
<p>rotation and a second step given by a translation, we can reduce the equation for C
</p>
<p>with respect to (O; x, y) to its canonical form by a proper rigid transformation with
a matrix Q encoding both a rotation and a translation. Such a composition is given by
</p>
<p>{
x = 1&radic;
</p>
<p>2
(x &prime; + y&prime;)
</p>
<p>y = 1&radic;
2
(&minus;x &prime; + y&prime;) &rArr;
</p>
<p>{
x = 1&radic;
</p>
<p>2
(X + Y +
</p>
<p>&radic;
2
8
)
</p>
<p>y = 1&radic;
2
(&minus;X + Y +
</p>
<p>&radic;
2
8
)
</p>
<p>which we write as ⎛
⎝
x
</p>
<p>y
</p>
<p>1
</p>
<p>⎞
⎠ = Q
</p>
<p>⎛
⎝
X
</p>
<p>Y
</p>
<p>1
</p>
<p>⎞
⎠
</p>
<p>with
</p>
<p>Q =
1
&radic;
2
</p>
<p>⎛
⎝
</p>
<p>1 1
&radic;
2/8
</p>
<p>&minus;1 1
&radic;
2/8
</p>
<p>0 0 1
</p>
<p>⎞
⎠ .
</p>
<p>We end this example by checking that the matrix associated to the conic C with
</p>
<p>respect to the reference system (O &prime;; X,Y ) can be computed as it is described in the
Theorem 16.6.5, that is
</p>
<p>tQ B Q =
</p>
<p>⎛
⎝
2 0 0
</p>
<p>0 0 2
&radic;
2
</p>
<p>0 2
&radic;
2 0
</p>
<p>⎞
⎠ = 2B &prime;.
</p>
<p>We list the main steps of the method we described in order to reduce a conic to
</p>
<p>its canonical form as the proof of the following results.
</p>
<p>Theorem 16.6.7 Given a conic C whose equation is written in the reference system
</p>
<p>(O; x, y), there always exists a reference system (O &prime;; X, Y ), obtained with a roto-
translation from (O; x, y), with respect to which the equation for C is canonic.
</p>
<p>Proof Let C be a conic, with associated matrices A (of the quadratic form) and B
</p>
<p>(of the coefficients), with respect to the reference system (O; x, y). Then,
</p>
<p>(a) Diagonalise A, computing an orthonormal basis with eigenvectors
</p>
<p>v1 = (p11, p21), v2 = (p12, p22), given by the rotation
{
x = p11 x &prime; + p12 y&prime;
y = p21 x &prime; + p22 y&prime;
</p>
<p>(16.38)
</p>
<p>and define
</p>
<p>Q&prime; =
</p>
<p>⎛
⎝
p11 p12 0
</p>
<p>p21 p22 0
</p>
<p>0 0 1
</p>
<p>⎞
⎠ .</p>
<p/>
</div>
<div class="page"><p/>
<p>16.6 Reduction to Canonical Form of a Conic: Rotations 317
</p>
<p>With respect to the reference system (O; x &prime;, y&prime;), the conic C has matrix
B &prime; = tQ&prime; B Q&prime;, and the corresponding quadratic equation, which we write as
</p>
<p>(
x &prime; y&prime; 1
</p>
<p>)
B &prime;
</p>
<p>⎛
⎝
x &prime;
</p>
<p>y&prime;
</p>
<p>1
</p>
<p>⎞
⎠ = 0, (16.39)
</p>
<p>lacks the monomial term x &prime;y&prime;.
(b) Complete the square so to transform, by the corresponding translation, the ref-
</p>
<p>erence system (O; x &prime;, y&prime;) to (O &prime;; X,Y ), that is
{
X = x &prime; + a
Y = y&prime; + b . (16.40)
</p>
<p>From this, we can express the Eq. (16.39) for C with respect to the reference
</p>
<p>system (O &prime;; X,Y ). The resulting equation is canonical for C .
(c) The equations for the roto-translation from (O; x, y) to (O &prime;; X,Y ) are given by
</p>
<p>substituting the translation transformation (16.40) into (16.38).
</p>
<p>Corollary 16.6.8 Given a degree-two polynomial equation in the variable x and y,
</p>
<p>the set (locus) of zeros of such equation is one of the following loci: ellipse, hyperbola,
</p>
<p>parabola, union of lines (either coincident or distinct).
</p>
<p>The proof of the Proposition 16.3.4 together with the result of the Theorem 16.6.5,
</p>
<p>which give the transformation relations for the matrices associated to a given conic
</p>
<p>C under a proper rigid transformation, allows one to prove the next proposition.
</p>
<p>Proposition 16.6.9 AconicC whoseassociatedmatrices are A and B with respect to
</p>
<p>a given orthonormal reference system (O; x, y) is degenerate if and only if det B = 0.
Depending on the values of the determinant of A the following cases are possible
</p>
<p>det A &lt; 0 &hArr; C hyperbola
det A = 0 &hArr; C parabola
det A &gt; 0 &hArr; C ellipse .
</p>
<p>The relative signs of det(A) and det B determine whether the conic is real or
</p>
<p>imaginary.
</p>
<p>Exercise 16.6.10 As an example, we recall the results obtained in the Sect. 16.4. For
</p>
<p>the conic d(P, F) = e d(P, δ)with focus F = (ax , ay) and directrix δ : x = u, the
matrix of the coefficients associated to the Eq. (16.28) is
</p>
<p>B =
</p>
<p>⎛
⎝
</p>
<p>1&minus; e2 0 ue2 &minus; ax
0 1 ay
</p>
<p>ue2 &minus; ax ay a2x + a2y &minus; u2e2
</p>
<p>⎞
⎠</p>
<p/>
</div>
<div class="page"><p/>
<p>318 16 Conic Sections
</p>
<p>with then det B = &minus;e2(ax &minus; u)2. We recover that the sign of (1&minus; e2) determines
whether the conic C is an ellipse, or a parabola, or a hyperbola. We notice also that
</p>
<p>the conic is degenerate if and only if at least one of the conditions e = 0 or ax = u
is met.
</p>
<p>16.7 Eccentricity: Part 2
</p>
<p>We complete now the analysis of the conics defined by the relation
</p>
<p>d(P, F) = e d(P, δ)
</p>
<p>in terms of the eccentricity parameter. In Sect. 16.4 we have studied this equation
</p>
<p>with an arbitrary F and δ parallel to the y-axis, when it becomes the Eq. (16.28). In
</p>
<p>general, for a given eccentricity the previous relation depends only on the distance
</p>
<p>between F and δ. Using a suitable roto-translation as in the previous section, we
</p>
<p>have the following result.
</p>
<p>Proposition 16.7.1 Given a point F and a line δ in E2 such that F /&isin; δ, there exists
a cartesian orthogonal coordinate system (O &prime;; X,Y ) with F = O &prime; and with respect
to which the equation d(P, F) = e d(P, δ) (with e &gt; 0) is written as
</p>
<p>Y 2 + X2 &minus; e2(X &minus; u)2 = 0.
</p>
<p>Proof Given a point F and a line δ 
&ni; F , it is always possible to roto-translate the
starting coordinate system (O; x, y) to a new one (O &prime;; X,Y ) in such a way that
O &prime; = F and the line δ is given by the equation X = u 
= 0. The result then follows
from (16.28) being aX = aY = 0. �
</p>
<p>We know from the Sect. 16.4 that if e = 1, the equation represents a parabola
with directrix X = u 
= 0 and focus F = (0, 0). If 1 
= e, the equation represents
either an ellipse (0 &lt; e &lt; 1) or a hyperbola (e &gt; 1) with foci F+ = (0, 0) and
F&minus; = (&minus; 2ue
</p>
<p>2
</p>
<p>1&minus;e2 , 0). Also, e = 0 yields the degenerate conic X
2 + Y 2 = 0, while
</p>
<p>u = 0 (that is F &isin; δ) gives the degenerate conic Y 2 + (1&minus; e2)X2 = 0.
We can conclude that the Eq. (16.5) represents a conic whose type depends on the
</p>
<p>values of the eccentricity parameter. Its usefulness resides in yielding a constructive
</p>
<p>method to write the equation in canonical form, even for the degenerate cases.
</p>
<p>We address the inverse question: given a non degenerate conic C with equation
</p>
<p>a11 x
2 + 2 a12 xy + a22 y2 + 2 a13 x + 2 a23 y + a33 = 0
</p>
<p>is it possible to determine its eccentricity and its directrix?
</p>
<p>We give a constructive proof of the following theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7 Eccentricity: Part 2 319
</p>
<p>Theorem 16.7.2 Given the non degenerate conic C whose equation is
</p>
<p>a11 x
2 + 2 a12 xy + a22 y2 + 2 a13 x + 2 a23 y + a33 = 0,
</p>
<p>there exists a point F and a line δ with F /&isin; δ such that a point P &isin; C if and only if
</p>
<p>d(P, F) = e d(P, δ)
</p>
<p>for a suitable value e &gt; 0 of the eccentricity parameter.
</p>
<p>Proof As in the example Exercise16.6.6 we firstly diagonalise the matrix of the
</p>
<p>quadratic form of C finding a cartesian orthogonal system (O; x &prime;, y&prime;) with respect
to which the equation for C is written as
</p>
<p>α11(x
&prime;)2 + α22(y&prime;)2 + 2α13x &prime; + 2α23y&prime; + α33 = 0,
</p>
<p>with α11,α22 the eigenvalues of the quadratic form. This is the equation of the conic
</p>
<p>in the form studied in the Proposition 16.3.4, whose proof we now use. We have the
</p>
<p>following cases
</p>
<p>(a) One of the eigenvalues of the quadratic form is zero, say α11 = 0 (the case
α22 = 0 is analogous).
Up to a global (&minus;1) factor that we can rescale, the equation for C is
</p>
<p>α22(y
&prime;)2 + 2α13x &prime; + 2α23y&prime; + α33 = 0,
</p>
<p>with α22 &gt; 0 and α13 
= 0 (non degeneracy of C). Since there is no term (x &prime;)2,
this equation is of the form (16.28) only if e = 1. Thus it is of the form (16.29)
written as
</p>
<p>(y &minus; ay)2 + 2(u &minus; ax )
(
x &minus; 1
</p>
<p>2
(u + ax )
</p>
<p>)
= 0.
</p>
<p>The two expression are the same if and only if we have e = 1, and
</p>
<p>ay = &minus;
α23
</p>
<p>α22
and
</p>
<p>⎧
⎨
⎩
u &minus; ax = α13/α22
</p>
<p>u + ax = (α223 &minus; α33α22)/α13α22
.
</p>
<p>These say that C is the parabola with focus and directrix given, with respect to
</p>
<p>(O; x &prime;, y&prime;), by
</p>
<p>F = (
α223 &minus; α33α22 &minus; α213
</p>
<p>2α13α22
,&minus;
</p>
<p>α23
</p>
<p>α22
), x &prime; =
</p>
<p>α213 + α223 &minus; α33α22
2α13α22
</p>
<p>.</p>
<p/>
</div>
<div class="page"><p/>
<p>320 16 Conic Sections
</p>
<p>With the translation
</p>
<p>⎧
⎨
⎩
</p>
<p>X = x &prime; + (α33α22 &minus; α223)/2α22α13
</p>
<p>Y = y&prime; + α23/α22
</p>
<p>it can indeed be written as
</p>
<p>Y 2 + 2
α13
</p>
<p>α22
X = 0.
</p>
<p>If α22 = 0 and α11 
= 0 the result would be similar with the x &prime;-axis and y&prime; axis
interchanged.
</p>
<p>(b) Assume α11 
= 0 and α22 
= 0. We write the equation for C as in (16.17),
</p>
<p>α11
</p>
<p>α22
</p>
<p>(
x +
</p>
<p>α13
</p>
<p>α11
</p>
<p>)2
+
</p>
<p>(
y +
</p>
<p>α23
</p>
<p>α22
</p>
<p>)2
&minus;
</p>
<p>1
</p>
<p>α22
</p>
<p>(
&minus;α33 +
</p>
<p>α213
α11
</p>
<p>+
α223
α22
</p>
<p>)
= 0,
(16.41)
</p>
<p>and compare it with (16.30)
</p>
<p>(1&minus; e2)
(
x +
</p>
<p>ue2 &minus; ax
1&minus; e2
</p>
<p>)2
+ (y &minus; ay)2 &minus;
</p>
<p>e2(u &minus; ax )2
</p>
<p>1&minus; e2
= 0. (16.42)
</p>
<p>Notice that with this choice (that the directrix be parallel to the y-axis, x = u)
we are not treating the axes x and y in an equivalent way. We would have a
similar analysis when exchanging the role of the axes x and y. The conditions
to satisfy are
</p>
<p>⎧
⎨
⎩
1&minus; e2 = α11/α22
</p>
<p>ay = &minus;α23/α22
and
</p>
<p>⎧
⎨
⎩
</p>
<p>e2(u&minus;ax )2
1&minus;e2 =
</p>
<p>h
α22
</p>
<p>with h = &minus;α33 +
α213
α11
</p>
<p>+ α
2
23
</p>
<p>α22
ue2&minus;ax
1&minus;e2 =
</p>
<p>α13
α11
</p>
<p>.
</p>
<p>(16.43)
</p>
<p>We see that h = 0 would give a degenerate conic with either e = 0 or u = ax ,
that is the focus is on the directrix. As before, up to a global (&minus;1) factor we
may assume α22 &gt; 0. And as in Sect. 16.3 we have two possibilities according
</p>
<p>to the sign of α11.
</p>
<p>(b1) The eigenvalues have the same sign:α22 &gt; 0 andα11 &gt; 0. From the first condi-
</p>
<p>tion in (16.43)we needα22 &gt; α11 andwe get that e &lt; 1. Then the last condition
</p>
<p>requires that the parameter h &gt; 0 be positive. Thismeans thatC is a real ellipse.
</p>
<p>The case α22 &lt; α11 also results into a real ellipse but requires that the role of
</p>
<p>the axes x and y be exchanged. (The condition α11 = α22 would give a circle
and result in e = 0 which we are excluding.)
</p>
<p>(b2) The eigenvaluesα11 andα22 are discordant.Now the conditions (16.43) requires
</p>
<p>e &gt; 1 and the parameter h to be negative. This means that C is a hyperbola
</p>
<p>of the second type in (16.11). To get the other type in (16.11), once again one
</p>
<p>needs to exchange the axes x and y.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7 Eccentricity: Part 2 321
</p>
<p>As mentioned, the previous analysis is valid when the directrix is parallel to the
</p>
<p>y-axis. For the case when the directrix is parallel to the x-axis (the equation y = u),
one has a similar analysis with the relations analogous to (16.43) now written as
</p>
<p>⎧
⎨
⎩
1&minus; e2 = α22/α11
</p>
<p>ax = &minus;α13/α11
and
</p>
<p>{
e2(u&minus;ay)2
</p>
<p>1&minus;e2 =
h
α11
</p>
<p>with h = &minus;α33 + α
2
13
</p>
<p>α11
+ α
</p>
<p>2
23
</p>
<p>α22
ue2&minus;ay
1&minus;e2 =
</p>
<p>α23
α22
</p>
<p>.
</p>
<p>(16.44)
</p>
<p>In particular for 0 &lt; α22 &lt; α22 these are the data of a real ellipse, while for
</p>
<p>α11 &gt; 0 and α22 &lt; 0 (and h &lt; 0) this are the data for a hyperbola of the first type in
</p>
<p>(16.11). �
</p>
<p>In all cases above, the parameters e, u, ax , ay are given in terms of the conic
</p>
<p>coefficients by the relations (16.43) or (16.44). Being these quite cumbersome, we
</p>
<p>omit to write the complete solutions for these relations and rather illustrate with
</p>
<p>examples the general methods we developed.
</p>
<p>Exercise 16.7.3 Consider the hyperbolas
</p>
<p>y2 &minus; x2 + k = 0, k = &plusmn;1 .
</p>
<p>If k = 1, the relations (16.43) easily give the foci
</p>
<p>F&plusmn; = (&plusmn;
&radic;
2, 0)
</p>
<p>and corresponding directrix δ&plusmn; with equation
</p>
<p>x = &plusmn;
&radic;
2
2
.
</p>
<p>On the other hand, for k = 1, the relations (16.44) now give the foci
</p>
<p>F&plusmn; = (0,&plusmn;
&radic;
2)
</p>
<p>and corresponding directrix δ&plusmn;,
</p>
<p>y = &plusmn;
&radic;
2
2
.
</p>
<p>Exercise 16.7.4 Consider the C of the example Exercise16.3.7, whose equation we
</p>
<p>write as
</p>
<p>x2 + 4y2 + 2x &minus; 12y + 3 = (x + 1)2 + 4(y &minus; 3
2
)2 &minus; 7 = 0.
</p>
<p>It is easy now to compute that this ellipse has eccentricity e =
&radic;
3
4
</p>
<p>and foci
</p>
<p>F&plusmn; = (&minus;1&plusmn;
&radic;
21
2
, 3
</p>
<p>2
).</p>
<p/>
</div>
<div class="page"><p/>
<p>322 16 Conic Sections
</p>
<p>The directrix δ&plusmn; corresponding to the focus F&plusmn; is given by the line
</p>
<p>x = &minus;1&plusmn; 2
&radic;
21
3
</p>
<p>.
</p>
<p>Exercise 16.7.5 Consider the conic C with equation
</p>
<p>x2 &minus; ky2 &minus; 2x &minus; 2 = 0
</p>
<p>with a parameter k &isin; R. By completing the square, we write this equation as
</p>
<p>(x &minus; 1)2 &minus; ky2 &minus; 3 = 0.
</p>
<p>Depending on the value of k, we have different cases.
</p>
<p>(i) If k &lt; &minus;1, it is evident that C is a real ellipse with α11 &lt; α22, and the condition
(16.43) gives eccentricity e =
</p>
<p>&radic;
1+ 1
</p>
<p>k
, with foci
</p>
<p>F&plusmn; = (1&plusmn;
&radic;
</p>
<p>3(1+k)
k
</p>
<p>, 0) (16.45)
</p>
<p>and corresponding directrix δ&plusmn; with equation
</p>
<p>x = 1 &plusmn;
&radic;
</p>
<p>3
k(1+k) . (16.46)
</p>
<p>(ii) If &minus;1 &lt; k &lt; 0 the conic C is again a real ellipse, whose major axis is par-
allel to the y-axis, so α11 &gt; α22. Now the relations (16.44) yield eccentricity
</p>
<p>e =
&radic;
1+ k, with foci
</p>
<p>F&plusmn; = (1,&plusmn;
&radic;
&minus;3
</p>
<p>(
1+ 1
</p>
<p>k
</p>
<p>)
)
</p>
<p>and corresponding directrix δ&plusmn; given by the lines with equation
</p>
<p>y = &plusmn;
&radic;
</p>
<p>3
&minus;k(k+1) .
</p>
<p>(iii) If k = 0 the conic C is degenerate.
(iv) If k &gt; 0, the conic C is a hyperbola. It is easy to compute the eccentricity to be
</p>
<p>e =
&radic;
1+ 1
</p>
<p>k
(the same expression as for k &lt; &minus;1), with the foci and the directrix
</p>
<p>given by (16.45) and (16.46).
</p>
<p>The matrix of the coefficients of this conic C is given by
</p>
<p>B =
</p>
<p>⎛
⎝
</p>
<p>1 0 &minus;1
0 &minus;k 0
&minus;1 0 &minus;2
</p>
<p>⎞
⎠ ,</p>
<p/>
</div>
<div class="page"><p/>
<p>16.7 Eccentricity: Part 2 323
</p>
<p>with det A = &minus;k and det B = &minus;3k. By the Proposition16.6.9 we recover the listed
results: C is degenerate if and only if k = 0; it is a hyperbola if and only if k &gt; 0;
an ellipse if and only if k &lt; 0.
</p>
<p>16.8 Why Conic Sections
</p>
<p>We close the chapter by explaining where the loci on the affine euclidean plane E2
</p>
<p>that we have described, the conic sections, get their name from. This will also be
</p>
<p>related to finding solutions to a non-linear problem in E3.
</p>
<p>Fix a line γ and a point V &isin; γ in E3. A (double) cone with axis γ and vertex V
is the bundle of lines through V whose direction vectors form, with respect to γ, an
</p>
<p>angle of fixed width.
</p>
<p>Consider now a plane π &sub; E3 which does not contain the vertex of the cone. We
show that, depending on the relative orientation of π with the axis of the cone, the
</p>
<p>intersection π &cap; C &mdash; a conic section&mdash; is a non degenerate ellipse, or a parabola, or
a hyperbola.
</p>
<p>Let (O, E) = (O; x, y, z) be an orthonormal reference frame for E3, with E an
orthonormal basis for E3. To be definite, we take the z-axis as the axis of a cone C,
</p>
<p>its vertex to be V = O and its width an angle 0 &lt; θ &lt; π/2. It is immediate to see
that the cone C is given by the points P = (x, y, z) of the lines whose normalised
direction vectors are
</p>
<p>E3 &ni; u(α) = (sin θ cosα, sin θ sinα, cos θ)
</p>
<p>with α &isin; [0, 2π). The parametric equation for these lines (see the Definition14.2.7)
is then
</p>
<p>r(α) =
</p>
<p>⎧
⎨
⎩
x = λ sin θ cosα
y = λ sin θ sinα
z = λ cos θ
</p>
<p>.
</p>
<p>with λ a real parameter. This expression provides a vector equation for the cone C.
</p>
<p>By eliminating the parameter, one gets a cartesian equation for C as given by the
</p>
<p>relation
</p>
<p>�r(α) : x2 + y2 &minus; (tan2 θ)z2 = 0.
</p>
<p>Without loss of generality, we may intersect the cone C with a plane π which
</p>
<p>is orthogonal to the yz coordinate plane and meeting the z axis at the point
</p>
<p>A = (0, 0, k &gt; 0). If β &isin; (0,π/2) is the angle between the axis of the cone (the
z axis) and (its projection on) the plane π, the direction Sπ of the plane is orthogo-
</p>
<p>nal to the normalised vector v = (0, cosβ, sin β). We know from Chap.15 that the
cartesian equation for the plane π is then
</p>
<p>�π : (cosβ)y + (sin β)(z &minus; k) = 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>324 16 Conic Sections
</p>
<p>The intersection C &cap; π is then given by the solution of the system
{
x2 + y2 &minus; (tan2 θ)z2 = 0
(cosβ)y + (sin β)(z &minus; k) = 0 . (16.47)
</p>
<p>This is the only problem in this textbook which is formulated in terms of a system
</p>
<p>of non-linear equations. By inserting the second equation in the first one, elemen-
</p>
<p>tary algebra gives, for the projection on the plane xy of the intersection C &cap; π, the
equation,
</p>
<p>x2 + (1&minus; tan2 θ cot2 β) y2 + 2k tan2 θ cot β y &minus; k2 tan2 θ. (16.48)
</p>
<p>From what we have described above in this chapter, this equation represents a
</p>
<p>conic.
</p>
<p>Its matrix of the coefficients is
</p>
<p>B =
</p>
<p>⎛
⎝
1 0 0
</p>
<p>0 1&minus; tan2 θ cot2 β k tan2 θ cot β
0 k tan2 θ cot β &minus;k2 tan2 θ
</p>
<p>⎞
⎠ ,
</p>
<p>while the matrix of the quadratic form is
</p>
<p>A =
(
1 0
</p>
<p>0 1&minus; tan2 θ cot2 β
</p>
<p>)
.
</p>
<p>One then computes
</p>
<p>det(A) = 1&minus; tan2 θ cot2 β, det B = &minus;k2 tan2 θ .
</p>
<p>Having excluded the cases k = 0 and tan θ = 0, we know from the Proposition 16.6.9
that the intersection C &cap; π represents a non degenerate real conic. Some algebra
indeed shows that:
</p>
<p>det(A) &gt; 0 &hArr; tan2 β &gt; tan2 θ &hArr; β &gt; θ,
det(A) = 0 &hArr; tan2 β = tan2 θ &hArr; β = θ, (16.49)
det(A) &lt; 0 &hArr; tan2 β &lt; tan2 θ &hArr; β &lt; θ,
</p>
<p>thus giving an ellipse, a parabola, a hyperbola respectively. These are shown in
</p>
<p>Figs. 16.4 and 16.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.8 Why Conic Sections 325
</p>
<p>Fig. 16.4 The ellipse with the limit case of the circle
</p>
<p>Fig. 16.5 The parabola and the hyperbola
</p>
<p>Remark 16.8.1 As a particular case, if we take β = π
2
, from (16.48) we see that
</p>
<p>C &cap; π is a circle with radius R = k tan θ. On the other hand, with k = 0, that is π
contains the vertex of the cone, one has det B = 0. In such a case, the (projected)
Eq. (16.48) reduces to
</p>
<p>x2 + (1&minus; tan2 θ cot2 β) y2 = 0.
</p>
<p>Such equation represents:
</p>
<p>2a. the union of two complex conjugate lines for β &gt; θ,
</p>
<p>2b. the points (x = 0, y), that is the y-axis for β = θ,
2c. the union of two real lines for β &lt; θ.</p>
<p/>
</div>
<div class="page"><p/>
<p>326 16 Conic Sections
</p>
<p>We conclude giving a more transparent, in a sense, description of the intersection
</p>
<p>C &cap; π by using a new reference system (O; x &prime;, y&prime;, z&prime;), by a rotation around the x-axis
where the plane π is orthogonal to the axis z&prime;-axis. adapted to π. From Chap.11, the
transformation we consider is given in terms of the matrix in SO(3),
</p>
<p>⎛
⎝
x &prime;
</p>
<p>y&prime;
</p>
<p>z&prime;
</p>
<p>⎞
⎠ =
</p>
<p>⎛
⎝
1 0 0
</p>
<p>0 sin β &minus; cosβ
0 cosβ sin β
</p>
<p>⎞
⎠
</p>
<p>⎛
⎝
x
</p>
<p>y
</p>
<p>z
</p>
<p>⎞
⎠ .
</p>
<p>With respect to the new reference system, the system of Eq. (16.47) becomes
</p>
<p>{
(x &prime;)2 +
</p>
<p>(
(sin β)y&prime; + (cosβ)z&prime;
</p>
<p>)2 &minus; (tan2 θ)
(
(sin β)z&prime; &minus; (cosβ)y&prime;
</p>
<p>)2 = 0
z&prime; &minus; k sin β = 0 .
</p>
<p>It is then easy to see that the solutions of this system of equations are the points
having coordinates z&prime; = k sin β and (x &prime;, y&prime;) satisfying the equation
</p>
<p>(x &prime;)2 + (sin2 β &minus; tan2 θ cos2 β)(y&prime;)2 + 2k cosβ sin2 β(1+ tan2 θ)y&prime; + (cos2 β &minus; tan2 θ sin2 ρ) k2 sin2 β = 0.
(16.50)
</p>
<p>Clearly, this equation represents a conic on the plane z&prime; = k sin β with respect to the
orthonormal reference system (O; x &prime;, y&prime;). Its matrix of the coefficients is
</p>
<p>B =
</p>
<p>⎛
⎝
1 0 0
</p>
<p>0 sin2 β(1 &minus; tan2 θ cot2 β) k cosβ sin2 β(1+ tan2 θ)
0 k cosβ sin2 β(1+ tan2 θ) k2 sin2 β cos2 β(1 &minus; tan2 θ tan2 β)
</p>
<p>⎞
⎠ ,
</p>
<p>while the matrix of the quadratic form is
</p>
<p>A =
(
1 0
</p>
<p>0 sin2 β(1 &minus; tan2 θ cot2 β))
</p>
<p>)
.
</p>
<p>One then computes
</p>
<p>det(A) = sin2 β(1 &minus; tan2 θ cot2 β), det B = &minus;k2 sin2 β tan2 θ .
</p>
<p>With k 
= 0 and tan θ 
= 0, clearly also in this case the relations (16.49) are valid.
And as particular cases, if we take β = π/2, one has that C &cap; π is a circle with radius
R = k tan θ. On the other hand, for k = 0, (that is π contains the vertex of the cone)
so that det B = 0, the Eq. (16.50) reduces to
</p>
<p>(x &prime;)2 + (sin2 β &minus; tan2 θ cos2 β)(y&prime;)2 = 0.
</p>
<p>Such equation as before represents: the union of two complex conjugate lines for
</p>
<p>β &gt; θ; the points x &prime; = 0 for β = θ; the union of two real lines for β &lt; θ.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.8 Why Conic Sections 327
</p>
<p>Remark 16.8.2 We remark that both Eqs. (16.48) and (16.50) describe the same type
</p>
<p>of conic, depending on the relative width of the angles β and θ. What differs is their
</p>
<p>eccentricity. The content of the Sect. 16.7 allows us to compute that the eccentricity
</p>
<p>of the conic in (16.48) is e2 = tan2 θ cot2 β, while for the conic in (16.50) we have
e2 = (1+ tan2 θ) cos2 β.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A
</p>
<p>Algebraic Structures
</p>
<p>This appendix is an elementary introduction to basic notions of set theory, together
</p>
<p>with those of group, ring and field. The reader is only supposed to know about
</p>
<p>numbers, more precisely natural (containing the zero 0), integer, rational and real
</p>
<p>numbers, that will be denoted respectively by N, Z, Q, R. Some of their properties
</p>
<p>will also be recalled in the following. We shall also introduce complex numbers
</p>
<p>denoted C and (classes of) integers Zp = Z/pZ.
</p>
<p>A.1 A Few Notions of Set Theory
</p>
<p>Definition A.1.1 Given any two sets A and B, by A &times; B we denote their Cartesian
</p>
<p>product. This it is defined as the set of ordered pairs of elements from A and B,
</p>
<p>that is,
</p>
<p>A &times; B = {(a, b) | a &isin; A, b &isin; B}.
</p>
<p>Notice that A &times; B �= B &times; A since we are considering ordered pairs. The previous
</p>
<p>definition is valid for sets A, B of arbitrary cardinality. The set A &times; A is denoted A2.
</p>
<p>Exercise A.1.2 Consider the set A = {&diams;, &hearts;, &clubs;, &spades;}. The Cartesian product A2 is
</p>
<p>then
</p>
<p>A2 = A &times; A
</p>
<p>=
{
</p>
<p>(&diams;,&diams;), (&diams;,&hearts;), (&diams;,&clubs;), (&diams;,&spades;), (&hearts;,&diams;), (&hearts;,&hearts;), (&hearts;,&clubs;), (&hearts;,&spades;),
</p>
<p>(&clubs;,&diams;), (&clubs;,&hearts;), (&clubs;,&clubs;)(&clubs;,&spades;), (&spades;,&diams;), (&spades;,&hearts;), (&spades;,&clubs;), (&spades;,&spades;)
}
</p>
<p>.
</p>
<p>Definition A.1.3 Given any set A, a binary relation on A is any subset of the Carte-
</p>
<p>sian product A2 = A &times; A. If such a subset is denoted by R, we say that the pair of
</p>
<p>elements a, b in A are related or in relation if (a, b) &isin; R and write it as a R b.
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1
</p>
<p>329</p>
<p/>
</div>
<div class="page"><p/>
<p>330 Appendix A: Algebraic Structures
</p>
<p>Fig. A.1 A binary relation in N2
</p>
<p>Example A.1.4 Consider A = N, the set of natural numbers, with R the subset of
</p>
<p>N2 = N &times; N given by the points as in the Fig. A.1. We see that 2R1, but it is not true
</p>
<p>that 1R1. One may easily check that R can be written by the formula
</p>
<p>nRm &hArr; m = n &minus; 1, for any (n,m) &isin; N2.
</p>
<p>Definition A.1.5 A binary relation on a set A is called an equivalence relation if the
</p>
<p>following properties are satisfied
</p>
<p>&bull; R is reflexive, that is aRa for any a &isin; A,
</p>
<p>&bull; R is symmetric, that is aRb &rArr; bRa, for any a, b &isin; A,
</p>
<p>&bull; R is transitive, that is aRb and bRc &rArr; aRc for any a, b, c &isin; A.
</p>
<p>Exercise A.1.6 In any given set A, the equality is an equivalence relation. On the set
</p>
<p>T of all triangles, congruence of triangles and similarity of triangles are equivalence
</p>
<p>relations. The relation described in the Example A.1.4 is not an equivalence relation,
</p>
<p>since reflexivity does not hold.
</p>
<p>Definition A.1.7 Consider a set A and let R be an equivalence relation defined on
</p>
<p>it. For any a &isin; A, one defines the subset
</p>
<p>[a] = {x &isin; A | xRa} &sube; A
</p>
<p>as the equivalence class of a in A. Any element x &isin; [a] is called a representative
</p>
<p>of the class [a]. It is clear that an equivalence class has as many representatives as
</p>
<p>the elements it contains.
</p>
<p>Proposition A.1.8 With R an equivalence relation on the set A, the following prop-
</p>
<p>erties hold:
</p>
<p>(1) If a R b, then [a] = [b].</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 331
</p>
<p>(2) If (a, b) /&isin; R, then [a] &cap; [b] = &empty;.
</p>
<p>(3) A =
⋃
</p>
<p>a&isin;A[a]; this is a disjoint union.
</p>
<p>Proof (1) One shows that the mutual inclusions [a] &sube; [b] and [b] &sube; [a] are both
</p>
<p>valid if a R b. Let x &isin; [a]; this means x R a. From the hypothesis a R b, so by
</p>
<p>the transitivity of R one has x R b, that is x &isin; [b]. This proves the inclusion
</p>
<p>[a] &sube; [b]. The proof of the inclusion [b] &sube; [a] is analogue.
</p>
<p>(2) Let us suppose that A &ni; x &isin; [a] &cup; [b]. It would mean that x R a and x R b.
</p>
<p>From the symmetry of R we would then have a R x , and from the transitivity
</p>
<p>this would result in a R b, which contradicts the hypothesis.
</p>
<p>(3) It is obvious, from (2). �
</p>
<p>Definition A.1.9 The decomposition A =
⋃
</p>
<p>a&isin;A[a] is called the partition of A
</p>
<p>associated (or corresponding) to the equivalence relation R.
</p>
<p>Definition A.1.10 If R is an equivalence relation defined on the set A, the set whose
</p>
<p>elements are the corresponding equivalence classes is denoted A
/
</p>
<p>R and called the
</p>
<p>quotient of A modulo R. The map
</p>
<p>π : A &rarr; A
/
</p>
<p>R given by a �&rarr; [a]
</p>
<p>is called the canonical projection of A onto the quotient A
/
</p>
<p>R.
</p>
<p>A.2 Groups
</p>
<p>A set has an algebraic structure if it is equipped with one or more operations. When
</p>
<p>the operations are more than one, they are required to be compatible. In this section
</p>
<p>we describe the most elementary algebraic structures.
</p>
<p>Definition A.2.1 Given a set G, a binary operation &lowast; on it is a map
</p>
<p>&lowast; : G &times; G &minus;&rarr; G.
</p>
<p>The image of the operation between a and b is denoted by a &lowast; b. One also says that
</p>
<p>G is closed, or stable with respect to the operation &lowast;. One usually writes (G, &lowast;) for
</p>
<p>the algebraic structure &lowast; defined on G, that is for the set G equipped with the binary
</p>
<p>operation &lowast;.
</p>
<p>Example A.2.2 It is evident that the usual sum and the usual product in N are binary
</p>
<p>operations.
</p>
<p>As a further example we describe a binary operation which does not come from
</p>
<p>usual arithmetic operations in any set of numbers. Let T be an equilateral triangle
</p>
<p>whose vertices are ordered and denoted by ABC . Let R be the set of rotations on a
</p>
<p>plane under which each vertex is taken onto another vertex. The rotation that takes
</p>
<p>the vertices ABC to BC D, can be denoted by</p>
<p/>
</div>
<div class="page"><p/>
<p>332 Appendix A: Algebraic Structures
</p>
<p>(
</p>
<p>A B C
</p>
<p>B C A
</p>
<p>)
</p>
<p>.
</p>
<p>It is clear that R contains three elements, which are:
</p>
<p>e =
</p>
<p>(
</p>
<p>A B C
</p>
<p>A B C
</p>
<p>)
</p>
<p>x =
</p>
<p>(
</p>
<p>A B C
</p>
<p>B C A
</p>
<p>)
</p>
<p>y =
</p>
<p>(
</p>
<p>A B C
</p>
<p>C A B
</p>
<p>)
</p>
<p>.
</p>
<p>The operation&mdash;denoted now ◦&mdash;that we consider among elements in R is the com-
</p>
<p>position of rotations. The rotation x ◦ y is the one obtained by acting on the vertices
</p>
<p>of the triangle first with y and then with x . It is easy to see that x ◦ y = e. The
</p>
<p>Table A.1 shows the composition law among elements in R.
</p>
<p>◦ e x y
</p>
<p>e e x y
</p>
<p>x x y e
</p>
<p>y y e x
</p>
<p>(A.1)
</p>
<p>Remark A.2.3 The algebraic structures (N,+) and (N, &middot;) have the following well
</p>
<p>known properties, for all elements a, b, c &isin; N,
</p>
<p>a + (b + c) = (a + b)+ c, a + b = b + a,
</p>
<p>a &middot; (b &middot; c) = (a &middot; b) &middot; c, a &middot; b = b &middot; a .
</p>
<p>The set N has elements, denoted 0 and 1, whose properties are singled out,
</p>
<p>0 + a = a, 1a = a
</p>
<p>for any a &isin; N. We give the following definition.
</p>
<p>Definition A.2.4 Let (G, &lowast;) be an algebraic structure.
</p>
<p>(a) (G, &lowast;) is called associative if
</p>
<p>a &lowast; (b &lowast; c) = (a &lowast; b) &lowast; c
</p>
<p>for any a, b, c &isin; G.
</p>
<p>(b) (G, &lowast;) is called commutative (or abelian) if
</p>
<p>a &lowast; b = b &lowast; a
</p>
<p>for any a, b &isin; G.
</p>
<p>(c) An element e &isin; G is called an identity (or a neutral element) for (G, &lowast;) (and the
</p>
<p>algebraic structure is often denoted by (G, &lowast;, e)) if</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 333
</p>
<p>e &lowast; a = a &lowast; e
</p>
<p>for any a &isin; G.
</p>
<p>(d) Let (G, &lowast;, e) be an algebraic structure with an identity e. An element b &isin; G such
</p>
<p>that
</p>
<p>a &lowast; b = b &lowast; a = e
</p>
<p>is called the inverse of a, and denoted by a&minus;1. The elements for which an inverse
</p>
<p>exists are called invertible.
</p>
<p>Remark A.2.5 If the algebraic structure is given by a &lsquo;sum rule&rsquo;, like in
</p>
<p>(N,+), the neutral element is usually called a zero element, denoted by 0, with
</p>
<p>a + 0 = 0 + a = a. Also, the inverse of an element a is usually denoted by &minus;a and
</p>
<p>named the opposite of a.
</p>
<p>Example A.2.6 It is easy to see that for the sets considered above one has (N,+, 0),
</p>
<p>(N, &middot;, 1), (R, ◦, e). Every element in R is invertible (since one has x ◦ y = y ◦ x = e);
</p>
<p>the set (N, &middot;, 1) contains only one invertible element, which is the identity itself, while
</p>
<p>in (N,+, 0) no element is invertible.
</p>
<p>From the defining relation (c) above one clearly has that if a&minus;1 is the inverse
</p>
<p>of a &isin; (G, &lowast;), then a is the inverse of a&minus;1. This suggests a way to enlarge sets
</p>
<p>containing elements which are not invertible, so to have a new algebraic structure
</p>
<p>whose elements are all invertible. For instance, one could define the set of integer
</p>
<p>numbers Z = {&plusmn;n : n &isin; N} and sees that every element in (Z,+, 0) is invertible.
</p>
<p>Definition A.2.7 An algebraic structure (G, &lowast;) is called a group when the following
</p>
<p>properties are satisfied
</p>
<p>(a) the operation &lowast; is associative,
</p>
<p>(b) G contains an identity element e with respect to &lowast;,
</p>
<p>(c) every element in G is invertible with respect to e.
</p>
<p>A group (G, &lowast;, e) is called commutative (or abelian) if the operation&lowast; is commutative.
</p>
<p>Remark A.2.8 Both (Z,+, 0) and (R, ◦, e) are abelian groups.
</p>
<p>Proposition A.2.9 Let (G, &lowast;, e) be a group. Then
</p>
<p>(i) the identity element is unique,
</p>
<p>(ii) the inverse a&minus;1 of any element a &isin; G is unique.
</p>
<p>Proof (i) Let us suppose that e, e&prime; are both identities for (G, &lowast;). Then it should be
</p>
<p>e &lowast; e&prime; = e since e&prime; is an identity, and also e &lowast; e&prime; = e&prime; since e is an identity; this
</p>
<p>would then mean e = e&prime;.
</p>
<p>(ii) Let b, c be both inverse elements to a &isin; G; this would give a &lowast; b = b &lowast; a = e and
</p>
<p>a &lowast; c = c &lowast; a = e. Since the binary operation is associative, one has
</p>
<p>b &lowast; (a &lowast; c) = (b &lowast; a) &lowast; c, resulting in b &lowast; e = e &lowast; c and then b = c. �</p>
<p/>
</div>
<div class="page"><p/>
<p>334 Appendix A: Algebraic Structures
</p>
<p>A.3 Rings and Fields
</p>
<p>Next we introduce and study the properties of a set equipped with two binary
</p>
<p>operations&mdash;compatible in a suitable sense&mdash;which resemble the sum and the product
</p>
<p>of integer numbers in Z.
</p>
<p>Definition A.3.1 Let A = (A,+, 0A, &middot;, 1A) be a set with two operations, called sum
</p>
<p>(+) and product (&middot;), with two distinguished elements called 0A and 1A. The set A is
</p>
<p>called a ring if the following conditions are satisfied:
</p>
<p>(a) (A,+, 0A) is an abelian group,
</p>
<p>(b) the product &middot; is associative,
</p>
<p>(c) 1A is the identity element with respect to the product,
</p>
<p>(d) one has a &middot; (b + c) = (a &middot; b)+ (a &middot; c) for any a, b, c &isin; A.
</p>
<p>If moreover the product is abelian, A is called an abelian ring.
</p>
<p>Example A.3.2 The set (Z,+, 0, &middot;, 1) is clearly an abelian ring.
</p>
<p>Definition A.3.3 By Z[X ] one denotes the set of polynomials in the indeterminate
</p>
<p>(or variable) X with coefficients in Z, that is the set of formal expressions,
</p>
<p>Z[X ] =
</p>
<p>{
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ai X
i = an X
</p>
<p>n + an&minus;1 X
n&minus;1 + . . .+ a1 X + a0 : n &isin; N, ai &isin; Z
</p>
<p>}
</p>
<p>.
</p>
<p>If Z[X ] &ni; p(X) = an X
n + an&minus;1 X
</p>
<p>n&minus;1 + . . .+ a1 X + a0 then a0, a1, . . . , an are the
</p>
<p>coefficients of the polynomial p(X), while the term ai X
i is a monomial of degree i .
</p>
<p>The degree of the polynomial p(X) is the highest degree among those of its non zero
</p>
<p>monomials. If p(X) is the polynomial above, its degree is n provided an �= 0, and
</p>
<p>one denotes deg p(X) = n. The two usual operations of sum and product in Z[X ]
</p>
<p>are defined as follows. Let p(X), q(X) be two arbitrary polynomials in Z[X ],
</p>
<p>p(X) =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ai X
i , q(X) =
</p>
<p>m
&sum;
</p>
<p>i=0
</p>
<p>bi X
i .
</p>
<p>Let us suppose n &le; m. One sets
</p>
<p>p(X)+ q(X) =
</p>
<p>m
&sum;
</p>
<p>j=0
</p>
<p>c j X
j ,</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 335
</p>
<p>with c j = a j + b j for 0 &le; j &le; n and c j = b j for n &lt; j &le; m. One would have an
</p>
<p>analogous results were n &ge; m. For the product one sets
</p>
<p>p(X) &middot; q(X) =
</p>
<p>m+n
&sum;
</p>
<p>h=0
</p>
<p>dh X
h,
</p>
<p>where dh =
&sum;
</p>
<p>i+ j=h ai b j .
</p>
<p>Proposition A.3.4 Endowed with the sum and the product as defined above, the
</p>
<p>set Z[X ] is an abelian ring, the ring of polynomials in one variable with integer
</p>
<p>coefficients.
</p>
<p>Proof One simply transfer to Z[X ] the analogous structures and properties of the
</p>
<p>ring (Z,+, 0, &middot;, 1). Let 0Z[X ] be the null polynomial, that is the polynomial whose
</p>
<p>coefficients are all equal to 0Z, and let 1Z[X ] = 1Z be the polynomial of degree 0
</p>
<p>whose only non zero coefficient is equal to 1Z. We limit ourselves to prove that
</p>
<p>(Z[X ],+, 0Z[X ]) is an abelian group.
</p>
<p>&bull; Clearly, the null polynomial 0Z[X ] is the identity element with respect to the sum
</p>
<p>of polynomials.
</p>
<p>&bull; Let us consider three arbitrary polynomials in Z[X ],
</p>
<p>p(X) =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ai X
i , q(X) =
</p>
<p>m
&sum;
</p>
<p>i=0
</p>
<p>bi X
i , r(X) =
</p>
<p>p
&sum;
</p>
<p>i=0
</p>
<p>ci X
i .
</p>
<p>We show that
</p>
<p>(
</p>
<p>p(X)+ q(X)
)
</p>
<p>+ r(X) = p(X)+
(
</p>
<p>q(X)+ r(X)
)
</p>
<p>.
</p>
<p>For simplicity we consider the case n = m = p, since the proof for the general
</p>
<p>case is analogue. From the definition of sum of polynomials, one has
</p>
<p>A(X) = (p(X)+ q(X))+ r(X)
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(ai + bi )X
i +
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ci X
i =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>[(ai + bi )+ ci ]X
i
</p>
<p>and
</p>
<p>B(X) = p(X)+ (q(X)+ r(X))
</p>
<p>=
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ai X
i +
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(bi + ci )X
i =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>[ai + (bi + ci )]X
i .</p>
<p/>
</div>
<div class="page"><p/>
<p>336 Appendix A: Algebraic Structures
</p>
<p>The coefficients of A(X) and B(X) are given, for any i = 0, . . . , n, by
</p>
<p>[(ai + bi )+ ci ] and [ai + (bi + ci )]
</p>
<p>and they coincide being the sum in Z associative. This means that A(X) = B(X).
</p>
<p>&bull; We show next that any polynomial p(X) =
&sum;n
</p>
<p>i=0 ai X
i is invertible with respect
</p>
<p>to the sum in Z[X ]. Let us define the polynomial p&prime;(X) =
&sum;n
</p>
<p>i=0(&minus;ai )X
i , with
</p>
<p>(&minus;ai ) denoting the inverse of ai &isin; Z with respect to the sum. From the definition
</p>
<p>of the sum of polynomials, one clearly has
</p>
<p>p(X)+ p&prime;(X) =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>ai X
i +
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(&minus;ai )X
i =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(ai &minus; ai )X
i .
</p>
<p>Since ai &minus; ai = 0Z for any i , one has p(X)+ p
&prime;(X) = 0Z[X ]; thus p
</p>
<p>&prime;(X) is the
</p>
<p>inverse of p(X).
</p>
<p>&bull; Finally, we show that the sum in Z[X ] is abelian. Let p(X) and q(X) be two arbi-
</p>
<p>trary polynomials in Z[X ] of the same degree deg p(X) = n = deg q(X) (again
</p>
<p>for simplicity); we wish to show that
</p>
<p>p(X)+ q(X) = q(X)+ p(X).
</p>
<p>From the definition of sum of polynomials,
</p>
<p>U (X) = p(X)+ q(X) =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(ai + bi )X
i
</p>
<p>V (X) = q(X)+ p(X) =
</p>
<p>n
&sum;
</p>
<p>i=0
</p>
<p>(bi + ai )X
i :
</p>
<p>the coefficients of U (X) and V (X) are given, for any i = 0, . . . , n by
</p>
<p>ai + bi and bi + ai
</p>
<p>which coincide since the sum is abelian in Z. This means U (X) = V (X).
</p>
<p>We leave as an exercise to finish showing that Z[X ] with the sum and the product
</p>
<p>above fulfill the conditions (b)&ndash;(d) in the Definition A.3.1 of a ring. �
</p>
<p>Remark A.3.5 Direct computation show the following well known properties of the
</p>
<p>abelian ring Z[X ] of polynomials. With f (X), g(X) &isin; Z[X ] it holds that:
</p>
<p>(i) deg( f (X)+ g(X)) &le; max{deg( f (X)), deg(g(X))};
</p>
<p>(ii) deg( f (X) &middot; g(X)) = deg( f (X))+ deg(g(X)).
</p>
<p>It is easy to see that the set (Q,+, &middot;, 0, 1) of rational numbers is an abelian ring as
</p>
<p>well. The set Q has indeed a richer algebraic structure than Z: any non zero element</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 337
</p>
<p>0 �= a &isin; Q is invertible with respect to the product. If a = p/q with p �= 0, then
</p>
<p>a&minus;1 = q/p &isin; Q.
</p>
<p>Definition A.3.6 An abelian ring K = (K ,+, 0, &middot;, 1) such that each element
</p>
<p>0 �= a &isin; K is invertible with respect to the product &middot;, is called a field. Equivalently
</p>
<p>one sees that (K ,+, 0, &middot;, 1) is a field if and only if both (K ,+, 0) and (K , &middot;, 1) are
</p>
<p>abelian groups and the product is distributive with respect to the sum, that is the
</p>
<p>condition (d) of the Definition A.3.1 is satisfied.
</p>
<p>Example A.3.7 Clearly (Q,+, 0, &middot;, 1) is a field, while (Z,+, 0, &middot;, 1) is not. The fun-
</p>
<p>damental example of a field for us will be the set R = (R,+, 0, &middot;, 1) of real numbers
</p>
<p>equipped with the usual definitions of sum and product.
</p>
<p>Analogously to the Definition A.3.3 one can define the sets Q[X ] and R[X ] of
</p>
<p>polynomials with rational and real coefficients. For them one naturally extends the
</p>
<p>definitions of sum and products, as well as that of degree.
</p>
<p>Proposition A.3.8 The set Q[X ] and R[X ] are both abelian rings. �
</p>
<p>It is worth stressing that in spite of the fact that Q and R are fields, neither Q[X ]
</p>
<p>nor R[X ] are such since a polynomial need not admit an inverse with respect to the
</p>
<p>product.
</p>
<p>A.4 Maps Preserving Algebraic Structures
</p>
<p>The Definition A.2.1 introduces the notion of algebraic structure (G, &lowast;) and we have
</p>
<p>described what groups, rings and fields are. We now briefly deal with maps between
</p>
<p>algebraic structures of the same kind, which preserve the binary operations defined
</p>
<p>in them. We have the following definition
</p>
<p>Definition A.4.1
</p>
<p>A map f : G &rarr; G &prime; between two groups (G, &lowast;G, eG) and (G
&prime;, &lowast;G &prime; , eG &prime;) is a group
</p>
<p>homomorphism if
</p>
<p>f (x &lowast;G y) = f (x) &lowast;G &prime; f (y) for all x, y &isin; G.
</p>
<p>A map f : A &rarr; B between two rings (A,+A, 0A, &middot;A, 1A) and (B,+B, 0B, &middot;B, 1B)
</p>
<p>is a ring homomorphism if
</p>
<p>f (x +A y) = f (x)+B f (y), f (x &middot;A y) = f (x) &middot;B f (y) for all x, y &isin; A.
</p>
<p>Example A.4.2 The natural inclusions Z &sub; Q, Q &sub; R are rings homomorphisms, as
</p>
<p>well as the inclusion Z &sub; Z[x] and similar ones.
</p>
<p>Exercise A.4.3 The map Z &rarr; Z defined by n �&rarr; 2n is a group homomorphism
</p>
<p>with respect to the group structure (Z,+, 0), but not a ring homomorphism with
</p>
<p>respect to the ring structure (Z,+, 0, &middot;, 1).</p>
<p/>
</div>
<div class="page"><p/>
<p>338 Appendix A: Algebraic Structures
</p>
<p>To lighten notations, from now on we shall denote a sum by + and a product by &middot;
</p>
<p>(and more generally a binary operation by &lowast;), irrespectively of the set in which they
</p>
<p>are defined. It will be clear from the context which one they refers to.
</p>
<p>Group homomorphisms present some interesting properties, as we now show.
</p>
<p>Proposition A.4.4 Let (G, &lowast;, eG) and (G
&prime;, &lowast;, eG &prime;) be two groups, and f : G &rarr; G
</p>
<p>&prime;
</p>
<p>a group homomorphism. Then,
</p>
<p>(i) f (eG) = eG &prime; ,
</p>
<p>(ii) f (a&minus;1) = ( f (a))&minus;1, for any a &isin; G.
</p>
<p>Proof (i) Since eG is the identity element with respect to the sum, we can write
</p>
<p>f (eG) = f (eG &lowast; eG) = f (eG) &lowast; f (eG),
</p>
<p>where the second equality is valid as f is a group homomorphism. Being
</p>
<p>f (eG) &isin; G
&prime;, it has a unique inverse (see the Proposition A.2.9), ( f (eG))
</p>
<p>&minus;1 &isin; G &prime;,
</p>
<p>that we can multiply with both sides of the previous equality, thus yielding
</p>
<p>f (eG) &lowast; ( f (eG))
&minus;1 = f (eG) &lowast; f (eG) &lowast; ( f (eG))
</p>
<p>&minus;1.
</p>
<p>This relation results in
</p>
<p>eG &prime; = f (eG) &lowast; eG &prime; &rArr; eG &prime; = f (eG).
</p>
<p>(ii) Making again use of the Proposition A.2.9, in order to show that ( f (a))&minus;1 is the
</p>
<p>inverse (with respect to the product in G &prime;) of f (a) it suffices to show that
</p>
<p>f (a) &lowast; ( f (a))&minus;1 = eG &prime; .
</p>
<p>From the definition of group homomorphism, it is
</p>
<p>f (a) &lowast; ( f (a))&minus;1 = f (a &lowast; a&minus;1) = f (eG) = eG &prime;
</p>
<p>where the last equality follows from (i).
</p>
<p>If f : A &rarr; B is a ring homomorphism, the previous properties are valid with
</p>
<p>respect to both the sum and to the product, that is
</p>
<p>(i&rsquo;) f (0A) = 0B and f (1A) = 1B ;
</p>
<p>(ii&rsquo;) f (&minus;a) = &minus; f (a) for any a &isin; A, while f (a&minus;1) = ( f (a))&minus;1 for any invertible
</p>
<p>(with respect to the product) element a &isin; A with inverse a&minus;1. �
</p>
<p>If A, B are fields, a ring homomorphism f : A &rarr; B is called a field homomor-
</p>
<p>phism. A bijective homomorphism between algebraic structures is called an isomor-
</p>
<p>phism.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 339
</p>
<p>A.5 Complex Numbers
</p>
<p>It is soon realised that one needs enlarging the field R of real numbers to consider zeros
</p>
<p>of polynomials with real coefficients. The real coefficient polynomial p(x) = x2 + 1
</p>
<p>has &lsquo;complex&rsquo; zeros usually denoted &plusmn;i, and their presence leads to defining the field
</p>
<p>of complex numbers C. One considers the smallest field containing R, &plusmn;i and all
</p>
<p>possible sums and products of them.
</p>
<p>Definition A.5.1 The set of complex numbers is given by formal expressions
</p>
<p>C = {z = a + ib | a, b &isin; R}.
</p>
<p>The real number a is called the real part of z, denoted a = &real;(z); the real number b
</p>
<p>is called the imaginary part of z, denoted b = &image;(z).
</p>
<p>The following proposition comes as an easy exercise.
</p>
<p>Proposition A.5.2 The binary operations of sum and product defined in C by
</p>
<p>(a + ib)+ (c + id) = (a + c)+ i(b + d),
</p>
<p>(a + ib) &middot; (c + id) = (ac &minus; bd)+ i(bc + ad)
</p>
<p>make (C,+, 0C, &middot;, 1C) a field, with 0C = 0R + i0R = 0R and
</p>
<p>1C = 1R + i0R = 1R. �
</p>
<p>Exercise A.5.3 An interesting part of the proof of the proposition above is to deter-
</p>
<p>mine the inverse z&minus;1 of the complex number z = a + ib. One easily checks that
</p>
<p>(a + ib)&minus;1 =
a
</p>
<p>a2 + b2
&minus; i
</p>
<p>b
</p>
<p>a2 + b2
=
</p>
<p>1
</p>
<p>a2 + b2
(a &minus; ib).
</p>
<p>Again an easy exercise establishes the following proposition.
</p>
<p>Proposition A.5.4 Given z = a + ib &isin; C one defines its conjugate number to be
</p>
<p>z̄ = a &minus; ib. Then, for any complex number z = a + ib the following properties hold:
</p>
<p>(i) z = z,
</p>
<p>(ii) z = z if and only if z &isin; R,
</p>
<p>(iii) zz = a2 + b2,
</p>
<p>iv) z + z = 2&real;(z). �
</p>
<p>Exercise A.5.5 The natural inclusions R &sub; C given by R &ni; a �&rarr; a + i0R is a field
</p>
<p>homomorphism, while the corresponding inclusion R[x] &sub; C[x] is a ring homomor-
</p>
<p>phism.
</p>
<p>Remark A.5.6 We mentioned above that the polynomial x2 + 1 = p(x) &isin; R[x] can-
</p>
<p>not be decomposed (i.e. cannot be factorised) as a product of degree 1 poly-
</p>
<p>nomials in R[x], that is, with real coefficients. On the other hand, the identity</p>
<p/>
</div>
<div class="page"><p/>
<p>340 Appendix A: Algebraic Structures
</p>
<p>x2 + 1 = (x &minus; i)(x + i) &isin; C[x] shows that the same polynomial can be decomposed
</p>
<p>into degree 1 terms if the coefficients of the latter are taken in C. This is not sur-
</p>
<p>prising, since the main reason to enlarge the field R to C was exactly to have a field
</p>
<p>containing the zero of the polynomial p(x).
</p>
<p>What is indeed surprising is that the field C contains the zeros of any polynomial
</p>
<p>with real coefficients. This is the result that we recall as the next theorem.
</p>
<p>Proposition A.5.7 (Fundamental theorem of algebra) Let f (x) &isin; R[x] be a polyno-
</p>
<p>mial with real coefficients and deg f (x) &ge; 1. Then, f (x) has at least a zero (that is
</p>
<p>a root) in C. More precisely, if deg f (x) = n, then f (x) has n (possibly non distinct)
</p>
<p>roots in C. If z1, . . . , zs are these distinct roots, the polynomial f (x) can be written
</p>
<p>as
</p>
<p>f (x) = a(x &minus; z1)
m(1)(x &minus; z2)
</p>
<p>m(2) &middot; &middot; &middot; (x &minus; zs)
m(s),
</p>
<p>with the root multiplicities m( j) for j = 1, . . . s, such that
</p>
<p>s
&sum;
</p>
<p>j=1
</p>
<p>m( j) = n.
</p>
<p>That is the polynomial f (x) it is completely factorisable on C. �
</p>
<p>A more general result states that C is an algebraically closed field, that is one has
</p>
<p>the following:
</p>
<p>Theorem A.5.8 Let f (x) &isin; C[x] be a degree n polynomial with complex coeffi-
</p>
<p>cients. Then there exist n complex (non distinct in generall ) roots of f (x). Thus the
</p>
<p>polynomial f (x) is completely factorisable on C. �
</p>
<p>A.6 Integers Modulo A Prime Number
</p>
<p>We have seen that the integer numbers Z form only a ring and not a field. Out of
</p>
<p>it one can construct fields of numbers by going to the quotient with respect to an
</p>
<p>equivalence relation of &lsquo;modulo an integer&rsquo;. As an example, consider the set Z3 of
</p>
<p>integer modulo 3. It has three elements
</p>
<p>Z3 = {[0], [1], [2]}
</p>
<p>which one also simply write Z3 = {0, 1, 2}, although one should not confuse them
</p>
<p>with the corresponding classes.
</p>
<p>One way to think of the three elements of Z3 is that each one represents the
</p>
<p>equivalence class of all integers which have the same remainder when divided by 3.
</p>
<p>For instance, [2] denotes the set of all integers which have remainder 2 when divided
</p>
<p>by 3 or equivalently, [2] denotes the set of all integers which are congruent to 2</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix A: Algebraic Structures 341
</p>
<p>modulo 3, thus [2] = {2, 5, 8, 11, . . . }. The usual arithmetic operations determine
</p>
<p>the addition and multiplication tables for this set as show in Table A.2.
</p>
<p>+ 0 1 2
</p>
<p>0 0 1 2
</p>
<p>1 1 2 0
</p>
<p>2 2 0 1
</p>
<p>and
</p>
<p>&lowast; 0 1 2
</p>
<p>0 0 0 0
</p>
<p>1 0 1 2
</p>
<p>2 0 2 1
</p>
<p>. (A.2)
</p>
<p>Thus&minus;[1] = [2] and&minus;[2] = [1] and Z3 is an abelian group for the addition. Further-
</p>
<p>more, [1] &lowast; [1] = [1] and [2] &lowast; [2] = [1] and both nonzero elements have inverse:
</p>
<p>[1]&minus;1 = [1] and [2]&minus;1 = [2]. All of this makes Z3 a field.
</p>
<p>The previous construction works when 3 is substituted with any prime number p.
</p>
<p>We recall that a positive integer p is called prime if it is only divisible by itself and
</p>
<p>by 1. Thus, for any prime number one gets the field of integers modulo p:
</p>
<p>Zp = Z/pZ = {[0], [1], . . . , [p &minus; 1]}.
</p>
<p>Each of its elements represents the equivalence class of all integers which have
</p>
<p>the given remainder when divided by p. Equivalently, each element denotes the
</p>
<p>equivalence class of all integers which are congruent modulo p. The corresponding
</p>
<p>addition and multiplication tables, defines as in Z but now taken modulo p, can be
</p>
<p>easily worked out. Notice that the construction does not work, that is Zp is not a ring,
</p>
<p>if p is not a prime number: were this the case there would be divisors of zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
</p>
<p>Affine line, 241
</p>
<p>Affine plane, 288, 299, 306
</p>
<p>Affine space, 183, 235, 236, 238, 244, 245,
</p>
<p>247, 252, 269, 271, 272, 275
</p>
<p>Algebraic multiplicity of an eigenvalue, 148,
</p>
<p>149, 170
</p>
<p>Angle between vectors, 35
</p>
<p>Angular momentum, 14, 194, 309
</p>
<p>Angular velocity, 14, 191&ndash;194
</p>
<p>Applied vector, 1&ndash;3
</p>
<p>Axial vector, 189&ndash;193
</p>
<p>B
</p>
<p>Basis in a vector space, change of, 118
</p>
<p>Basis of a vector space, 65
</p>
<p>C
</p>
<p>Characteristic polynomial of a matrix, 138
</p>
<p>Characteristic polynomial of an endomor-
</p>
<p>phism, 138
</p>
<p>Cofactor, 77, 78
</p>
<p>Commutator, 176, 187, 188, 206, 228, 229
</p>
<p>Commuting endomorphisms, 137
</p>
<p>Complex numbers, 129, 201, 329, 339
</p>
<p>Component of a vector, 224, 225
</p>
<p>Composition of linear maps, 116
</p>
<p>Composition of maps, 104, 117, 130
</p>
<p>Conic sections, 293, 309, 310
</p>
<p>Coordinate system, 1, 5, 6, 8, 11, 13, 14, 191,
</p>
<p>237, 318
</p>
<p>Coriolis acceleration, 193, 194
</p>
<p>D
</p>
<p>Degenerate conic, 301, 302, 305, 308, 318,
</p>
<p>320
</p>
<p>Diagonalisation of a matrix, 145
</p>
<p>Diagonalisation of an endomorphism, 143
</p>
<p>Diagonal matrix, 56, 133, 144, 147, 215, 216,
</p>
<p>221, 222
</p>
<p>Dimension of a vector space, 55
</p>
<p>Dirac&rsquo;s bra-ket notations, 129
</p>
<p>Directrix of a conic, 293, 297, 318, 321, 322
</p>
<p>Direct sum, 24, 34, 143, 158, 162
</p>
<p>Distance between linear affine varieties, 275
</p>
<p>Divergence, 15
</p>
<p>Dual basis, 126, 128, 129, 233
</p>
<p>Dual space, 125, 126, 197, 233
</p>
<p>E
</p>
<p>Eccentricity of a conic, 327
</p>
<p>Eigenvalues, 134&ndash;139, 142&ndash;145, 147, 149,
</p>
<p>156, 158, 168, 169, 171, 194, 195,
</p>
<p>203, 205, 211, 215, 216, 223, 314,
</p>
<p>319, 320
</p>
<p>Eigenvector, 134, 135, 137, 149, 163, 194,
</p>
<p>195, 207
</p>
<p>Ellipse, 294, 295, 297&ndash;299, 302, 305, 318,
</p>
<p>321, 322, 324
</p>
<p>Endomorphism, 131&ndash;139, 142&ndash;145, 155&ndash;
</p>
<p>159, 163, 166, 169, 170, 173, 174,
</p>
<p>188, 198, 200, 202, 203, 205, 206,
</p>
<p>225, 226
</p>
<p>&copy; Springer International Publishing AG, part of Springer Nature 2018
</p>
<p>G. Landi and A. Zampini, Linear Algebra and Analytic Geometry
</p>
<p>for Physical Sciences, Undergraduate Lecture Notes in Physics,
</p>
<p>https://doi.org/10.1007/978-3-319-78361-1
</p>
<p>343</p>
<p/>
</div>
<div class="page"><p/>
<p>344 Index
</p>
<p>Equivalence relation, 156, 218, 223, 330,
</p>
<p>331, 340
</p>
<p>Euclidean affine space, 271, 275
</p>
<p>Euclidean structure, 173, 299
</p>
<p>Euclidean vector space, 37&ndash;41, 153, 159,
</p>
<p>174
</p>
<p>Euler angles, 186
</p>
<p>Exponential of a matrix, 208, 210
</p>
<p>F
</p>
<p>Field, 15&ndash;17, 20, 190, 191, 204, 329, 337,
</p>
<p>339&ndash;341
</p>
<p>Field strength matrix, electro-magnetic, 231,
</p>
<p>232
</p>
<p>G
</p>
<p>Gauss&rsquo;algorithm, 61, 85
</p>
<p>Geometric multiplicity of an eigenvalue,
</p>
<p>145, 148
</p>
<p>Gradient, 15
</p>
<p>Gram-Schmidt orthogonalization, 43
</p>
<p>Grassmann theorem, 34, 144
</p>
<p>Group, 2, 4, 17&ndash;19, 51, 52, 153, 154, 178,
</p>
<p>180&ndash;182, 184, 207, 211, 225&ndash;227,
</p>
<p>333, 335, 337, 338
</p>
<p>H
</p>
<p>Hermitian endomorphism, 197, 204&ndash;206
</p>
<p>Hermitian structure, 205
</p>
<p>Homomorphism, 211, 337&ndash;339
</p>
<p>Hyperbola, 296&ndash;298, 308, 317, 318, 320,
</p>
<p>322, 324
</p>
<p>I
</p>
<p>Image, 61, 105, 108, 109, 115, 142, 331
</p>
<p>Inertia matrix of a rigid body, 195
</p>
<p>Injectivity of a linear map, 104, 109, 114
</p>
<p>Intrinsic rotation, 187
</p>
<p>Invertible matrix, 69, 117, 120, 132, 145, 178
</p>
<p>Isometry, linear, 183
</p>
<p>Isomorphism, 35, 107&ndash;109, 111, 112, 115,
</p>
<p>117, 118, 130, 155, 238
</p>
<p>J
</p>
<p>Jordan normal form of a matrix, 147
</p>
<p>K
</p>
<p>Keplerian motions, 309
</p>
<p>Kernel, 105, 106, 109, 110, 137, 138, 177,
</p>
<p>182
</p>
<p>Kinetic energy, 11
</p>
<p>L
</p>
<p>Laplacian, 16, 230
</p>
<p>Levi-Civita symbol, 187&ndash;190, 194
</p>
<p>Lie algebra of antisymmetric matrices, 175,
</p>
<p>176, 180, 206, 229
</p>
<p>Lie algebra of skew-adjoint matrices, 206
</p>
<p>Lie algebra, matrix, 176, 187, 188, 206
</p>
<p>Linear affine variety
</p>
<p>cartesian equation, 249, 252, 253, 262
</p>
<p>parametric equation, 242, 243, 249, 251,
</p>
<p>253, 254, 256, 291, 323
</p>
<p>skew, 247
</p>
<p>vector equation, 245, 257, 260, 261, 323
</p>
<p>Linear combinations, 24, 134
</p>
<p>Linear independence, 26, 69, 106
</p>
<p>Linear transformation, 97
</p>
<p>image, 105, 109, 111
</p>
<p>kernel, 104, 109, 110, 122
</p>
<p>Line of nodes, 187
</p>
<p>Lorentz boost
</p>
<p>event, 226
</p>
<p>Lorentz force, 190, 191
</p>
<p>Lorentz group, 225&ndash;227
</p>
<p>special, 226
</p>
<p>M
</p>
<p>Matrix, 151&ndash;153, 155, 157, 160, 161, 164,
</p>
<p>166&ndash;168, 172, 173, 175, 176
</p>
<p>Matrix determinant, 69, 72, 73, 76, 215, 226,
</p>
<p>303
</p>
<p>Laplace expansion, 73, 74, 77
</p>
<p>Matrix trace, 66, 67, 150, 160
</p>
<p>Matrix transposition, 78, 199, 312
</p>
<p>Maxwell equations, 229&ndash;232
</p>
<p>Minkowski spacetime, 230, 231
</p>
<p>Minor, 45, 72, 188, 189
</p>
<p>Mixed product, 9, 14, 16
</p>
<p>Momentum of a vector, 13
</p>
<p>N
</p>
<p>Normal endomorphism, 203&ndash;206
</p>
<p>Norm of a vector, 10, 37
</p>
<p>Nutation, 187
</p>
<p>O
</p>
<p>One parameter group of unitary matrices,
</p>
<p>211</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 345
</p>
<p>Orthogonal basis, 181
</p>
<p>Orthogonal group, 153, 176, 178, 180, 181
</p>
<p>special, 153, 181, 184
</p>
<p>Orthogonality between affine linear variety,
</p>
<p>271, 272, 276
</p>
<p>Orthogonal map, 156
</p>
<p>Orthogonal matrix, 153, 167, 171, 177, 179,
</p>
<p>182, 184, 191&ndash;193, 315
</p>
<p>Orthogonal projection, 11, 42, 158&ndash;162,
</p>
<p>276, 294
</p>
<p>P
</p>
<p>Parabola, 293, 294, 297&ndash;301, 307, 311, 317&ndash;
</p>
<p>319, 324, 325
</p>
<p>Parallelism between affine linear variety, 245
</p>
<p>Parallelogramm
</p>
<p>sum rule, 236, 333
</p>
<p>Polar vector, 190&ndash;193
</p>
<p>Precession, 187
</p>
<p>Pseudo vector, 189, 190
</p>
<p>Q
</p>
<p>Quadratic form, 213&ndash;220, 222, 224, 225,
</p>
<p>233, 300, 311, 313, 314, 319, 326
</p>
<p>R
</p>
<p>Rank of a matrix, 55, 58
</p>
<p>Reduced mass, 309
</p>
<p>Ring, 51, 329, 334&ndash;337, 339, 341
</p>
<p>Rotation, 6, 173, 183, 184, 186, 192, 194,
</p>
<p>227, 228, 301, 311, 312, 316, 326,
</p>
<p>332
</p>
<p>Rotation angle, 184
</p>
<p>Rotation axis, 183&ndash;185
</p>
<p>Rotor, 15
</p>
<p>Rouch&eacute;-Capelli theorem, 94
</p>
<p>Row by column product, 50, 67, 130, 152
</p>
<p>S
</p>
<p>Scalar field, 16, 229, 230
</p>
<p>Scalar product, 9, 11, 12, 16, 35, 36, 41, 42,
</p>
<p>45, 49, 154, 166, 213, 218, 220, 269,
</p>
<p>299
</p>
<p>Self-adjoint endomorphism, 156, 157, 159,
</p>
<p>163, 166, 169, 175, 205
</p>
<p>Signature of a quadratic form, 216, 218, 219
</p>
<p>Skew-adjoint endomorphism, 174&ndash;176, 206
</p>
<p>Spatial parity, 226, 227
</p>
<p>Spectral theorems, 197, 203
</p>
<p>Spectrum of an endomorphism, 134, 205
</p>
<p>Surjectivity of a linear map, 114
</p>
<p>Symmetric matrix, 164, 165, 178, 213, 216,
</p>
<p>220, 221
</p>
<p>System of linear equations, 47, 249
</p>
<p>homogeneous, 137, 146, 149, 164, 248,
</p>
<p>249
</p>
<p>T
</p>
<p>Time reversal, 226, 227
</p>
<p>Triangular matrix
</p>
<p>lower, 58
</p>
<p>upper, 56&ndash;59, 76, 83, 200
</p>
<p>U
</p>
<p>Unitary endomorphism, 205
</p>
<p>Unitary group, 207
</p>
<p>special, 207
</p>
<p>Unitary matrix, 202, 208, 210, 223
</p>
<p>V
</p>
<p>Vector, 1&ndash;8, 11, 13, 15, 19, 22, 23, 26, 30,
</p>
<p>39, 44, 49, 183, 189, 190, 202, 225,
</p>
<p>226, 235, 239, 241, 243, 256, 270,
</p>
<p>271, 274, 285, 288, 291, 309, 323
</p>
<p>Vector field, 15, 16, 190, 191, 229, 230
</p>
<p>Vector line, 23, 166, 183&ndash;185, 239
</p>
<p>Vector plane, 242, 245
</p>
<p>Vector product, 9, 12&ndash;14, 189, 190, 192&ndash;194
</p>
<p>Vector space, 4, 18&ndash;24, 26, 28, 30&ndash;33, 35,
</p>
<p>38, 39, 42, 48, 60, 65, 97, 100, 107,
</p>
<p>118, 125, 128, 131&ndash;133, 137, 142,
</p>
<p>143, 166, 173, 176, 182, 198, 206,
</p>
<p>213, 214, 218, 222, 229, 235, 238,
</p>
<p>255, 263, 269
</p>
<p>complex, 45, 128, 163, 222
</p>
<p>Vector subspace, 21&ndash;24, 40, 53, 104, 105,
</p>
<p>134, 159, 162, 176, 240, 245</p>
<p/>
</div>
<ul>	<li>Contents</li>
	<li>Introduction</li>
	<li>1 Vectors and Coordinate Systems</li>
<ul>	<li>1.1 Applied Vectors</li>
	<li>1.2 Coordinate Systems</li>
	<li>1.3 More Vector Operations</li>
	<li>1.4 Divergence, Rotor, Gradient and Laplacian</li>
</ul>
	<li>2 Vector Spaces</li>
<ul>	<li>2.1 Definition and Basic Properties</li>
	<li>2.2 Vector Subspaces</li>
	<li>2.3 Linear Combinations</li>
	<li>2.4 Bases of a Vector Space</li>
	<li>2.5 The Dimension of a Vector Space</li>
</ul>
	<li>3 Euclidean Vector Spaces</li>
<ul>	<li>3.1 Scalar Product, Norm</li>
	<li>3.2 Orthogonality</li>
	<li>3.3 Orthonormal Basis</li>
	<li>3.4 Hermitian Products</li>
</ul>
	<li>4 Matrices</li>
<ul>	<li>4.1 Basic Notions</li>
	<li>4.2 The Rank of a Matrix</li>
	<li>4.3 Reduced Matrices</li>
	<li>4.4 Reduction of Matrices</li>
	<li>4.5 The Trace of a Matrix</li>
</ul>
	<li>5 The Determinant</li>
<ul>	<li>5.1 A Multilinear Alternating Mapping</li>
	<li>5.2 Computing Determinants via a Reduction Procedure</li>
	<li>5.3 Invertible Matrices</li>
</ul>
	<li>6 Systems of Linear Equations</li>
<ul>	<li>6.1 Basic Notions</li>
	<li>6.2 The Space of Solutions for Reduced Systems</li>
	<li>6.3 The Space of Solutions for a General Linear System</li>
	<li>6.4 Homogeneous Linear Systems</li>
</ul>
	<li>7 Linear Transformations</li>
<ul>	<li>7.1 Linear Transformations and Matrices</li>
	<li>7.2 Basic Notions on Maps</li>
	<li>7.3 Kernel and Image of a Linear Map</li>
	<li>7.4 Isomorphisms</li>
	<li>7.5 Computing the Kernel of a Linear Map</li>
	<li>7.6 Computing the Image of a Linear Map</li>
	<li>7.7 Injectivity and Surjectivity Criteria</li>
	<li>7.8 Composition of Linear Maps</li>
	<li>7.9 Change of Basis in a Vector Space</li>
</ul>
	<li>8 Dual Spaces</li>
<ul>	<li>8.1 The Dual of a Vector Space</li>
	<li>8.2 The Dirac's Bra-Ket Formalism</li>
</ul>
	<li>9 Endomorphisms and Diagonalization</li>
<ul>	<li>9.1 Endomorphisms</li>
	<li>9.2 Eigenvalues and Eigenvectors</li>
	<li>9.3 The Characteristic Polynomial of an Endomorphism</li>
	<li>9.4 Diagonalisation of an Endomorphism</li>
	<li>9.5 The Jordan Normal Form</li>
</ul>
	<li>10 Spectral Theorems on Euclidean Spaces</li>
<ul>	<li>10.1 Orthogonal Matrices and Isometries</li>
	<li>10.2 Self-adjoint Endomorphisms</li>
	<li>10.3 Orthogonal Projections</li>
	<li>10.4 The Diagonalization of Self-adjoint Endomorphisms</li>
	<li>10.5 The Diagonalization of Symmetric Matrices</li>
</ul>
	<li>11 Rotations</li>
<ul>	<li>11.1 Skew-Adjoint Endomorphisms</li>
	<li>11.2 The Exponential of a Matrix</li>
	<li>11.3 Rotations in Two Dimensions</li>
	<li>11.4 Rotations in Three Dimensions</li>
	<li>11.5 The Lie Algebra mathfrakso(3)</li>
	<li>11.6 The Angular Velocity</li>
	<li>11.7 Rigid Bodies and Inertia Matrix</li>
</ul>
	<li>12 Spectral Theorems on Hermitian Spaces</li>
<ul>	<li>12.1 The Adjoint Endomorphism</li>
	<li>12.2 Spectral Theory for Normal Endomorphisms</li>
	<li>12.3 The Unitary Group</li>
</ul>
	<li>13 Quadratic Forms</li>
<ul>	<li>13.1 Quadratic Forms on Real Vector Spaces</li>
	<li>13.2 Quadratic Forms on Complex Vector Spaces</li>
	<li>13.3 The Minkowski Spacetime</li>
	<li>13.4 Electro-Magnetism</li>
</ul>
	<li>14 Affine Linear Geometry</li>
<ul>	<li>14.1 Affine Spaces</li>
	<li>14.2 Lines and Planes</li>
	<li>14.3 General Linear Affine Varieties and Parallelism</li>
	<li>14.4 The Cartesian Form of Linear Affine Varieties</li>
	<li>14.5 Intersection of Linear Affine Varieties</li>
</ul>
	<li>15 Euclidean Affine Linear Geometry</li>
<ul>	<li>15.1 Euclidean Affine Spaces</li>
	<li>15.2 Orthogonality Between Linear Affine Varieties</li>
	<li>15.3 The Distance Between Linear Affine Varieties</li>
	<li>15.4 Bundles of Lines and of Planes</li>
	<li>15.5 Symmetries</li>
</ul>
	<li>16 Conic Sections</li>
<ul>	<li>16.1 Conic Sections as Geometric Loci</li>
	<li>16.2 The Equation of a Conic in Matrix Form</li>
	<li>16.3 Reduction to Canonical Form of a Conic: Translations</li>
	<li>16.4 Eccentricity: Part 1</li>
	<li>16.5 Conic Sections and Kepler Motions</li>
	<li>16.6 Reduction to Canonical Form of a Conic: Rotations</li>
	<li>16.7 Eccentricity: Part 2</li>
	<li>16.8 Why Conic Sections</li>
</ul>
	<li>A Algebraic Structures</li>
<ul>	<li>A.1 A Few Notions of Set Theory</li>
	<li>A.2 Groups</li>
	<li>A.3 Rings and Fields</li>
	<li>A.4 Maps Preserving Algebraic Structures</li>
	<li>A.5 Complex Numbers</li>
	<li>A.6 Integers Modulo A Prime Number</li>
</ul>
	<li>Index</li>
</ul>
</body></html>