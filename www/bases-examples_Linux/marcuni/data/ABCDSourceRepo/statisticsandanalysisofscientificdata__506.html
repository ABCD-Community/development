<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>Massimiliano&nbsp;Bonamente
</p>
<p>Statistics and 
Analysis of 
Scientiï¿½ c Data
 Second Edition </p>
<p/>
</div>
<div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>Series editors
</p>
<p>Kurt H. Becker, Polytechnic School of Engineering, Brooklyn, USA
Jean-Marc Di Meglio, Universit&eacute; Paris Diderot, Paris, France
Sadri Hassani, Illinois State University, Normal, USA
Bill Munro, NTT Basic Research Laboratories, Atsugi, Japan
Richard Needs, University of Cambridge, Cambridge, UK
William T. Rhodes, Florida Atlantic University, Boca Raton, USA
Susan Scott, Australian National University, Acton, Australia
H. Eugene Stanley, Boston University, Boston, USA
Martin Stutzmann, TU M&uuml;nchen, Garching, Germany
Andreas Wipf, Friedrich-Schiller-Univ Jena, Jena, Germany</p>
<p/>
</div>
<div class="page"><p/>
<p>Graduate Texts in Physics
</p>
<p>Graduate Texts in Physics publishes core learning/teaching material for graduate-
and advanced-level undergraduate courses on topics of current and emerging fields
within physics, both pure and applied. These textbooks serve students at the
MS- or PhD-level and their instructors as comprehensive sources of principles,
definitions, derivations, experiments and applications (as relevant) for their mastery
and teaching, respectively. International in scope and relevance, the textbooks
correspond to course syllabi sufficiently to serve as required reading. Their didactic
style, comprehensiveness and coverage of fundamental material also make them
suitable as introductions or references for scientists entering, or requiring timely
knowledge of, a research field.
</p>
<p>More information about this series at http://www.springer.com/series/8431</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/8431">http://www.springer.com/series/8431</a></div>
</div>
<div class="page"><p/>
<p>Massimiliano Bonamente
</p>
<p>Statistics and Analysis
of Scientific Data
</p>
<p>Second Edition
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Massimiliano Bonamente
University of Alabama
Huntsville
Alabama, USA
</p>
<p>ISSN 1868-4513 ISSN 1868-4521 (electronic)
Graduate Texts in Physics
ISBN 978-1-4939-6570-0 ISBN 978-1-4939-6572-4 (eBook)
DOI 10.1007/978-1-4939-6572-4
</p>
<p>Library of Congress Control Number: 2016957885
</p>
<p>1st edition: &copy; Springer Science+Business Media New York 2013
2nd edition: &copy; Springer Science+Business Media LLC 2017
&copy; Springer Science+Busines Media New York 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>This Springer imprint is published by Springer Nature
The registered company is Springer Science+Business Media LLC
The registered company address is: 233 Spring Street, New York, NY 10013, U.S.A</p>
<p/>
</div>
<div class="page"><p/>
<p>To Giorgio and Alida, who taught me the
value of a book.
To Carlo and Gaia, to whom I teach the
</p>
<p>same.
And to Kerry, with whom I share the love
</p>
<p>of books, and everything else.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the First Edition
</p>
<p>Across all sciences, a quantitative analysis of data is necessary to assess the
significance of experiments, observations, and calculations. This book was written
over a period of 10 years, as I developed an introductory graduate course on
statistics and data analysis at the University of Alabama in Huntsville. My goal
was to put together the material that a student needs for the analysis and statistical
interpretation of data, including an extensive set of applications and problems that
illustrate the practice of statistical data analysis.
</p>
<p>The literature offers a variety of books on statistical methods and probability
theory. Some are primarily on the mathematical foundations of statistics, some
are purely on the theory of probability, and others focus on advanced statistical
methods for specific sciences. This textbook contains the foundations of probability,
statistics, and data analysis methods that are applicable to a variety of fields&mdash;
from astronomy to biology, business sciences, chemistry, engineering, physics, and
more&mdash;with equal emphasis on mathematics and applications. The book is therefore
not specific to a given discipline, nor does it attempt to describe every possible
statistical method. Instead, it focuses on the fundamental methods that are used
across the sciences and that are at the basis of more specific techniques that can
be found in more specialized textbooks or research articles.
</p>
<p>This textbook covers probability theory and random variables, maximum-
likelihood methods for single variables and two-variable datasets, and more complex
topics of data fitting, estimation of parameters, and confidence intervals. Among the
topics that have recently become mainstream, Monte Carlo Markov chains occupy
a special role. The last chapter of the book provides a comprehensive overview of
Markov chains and Monte Carlo Markov chains, from theory to implementation.
</p>
<p>I believe that a description of the mathematical properties of statistical tests is
necessary to understand their applicability. This book therefore contains mathemat-
ical derivations that I considered particularly useful for a thorough understanding of
the subject; the book refers the reader to other sources in case of mathematics that
goes beyond that of basic calculus. The reader who is not familiar with calculus may
skip those derivations and continue with the applications.
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface to the First Edition
</p>
<p>Nonetheless, statistics is necessarily slanted toward applications. To highlight
the relevance of the statistical methods described, I have reported original data
from four fundamental scientific experiments from the past two centuries: J.J.
Thomson&rsquo;s experiment that led to the discovery of the electron, G. Mendel&rsquo;s data
on plant characteristics that led to the law of independent assortment of species,
E. Hubble&rsquo;s observation of nebulae that uncovered the expansion of the universe,
and K. Pearson&rsquo;s collection of biometric characteristics in the UK in the early
twentieth century. These experiments are used throughout the book to illustrate how
statistical methods are applied to actual data and are used in several end-of-chapter
problems. The reader will therefore have an opportunity to see statistics in action
on these classic experiments and several additional examples.
</p>
<p>The material presented in this book is aimed at upper-level undergraduate
students or beginning graduate students. The reader is expected to be familiar
with basic calculus, and no prior knowledge of statistics or probability is assumed.
Professional scientists and researchers will find it a useful reference for fundamental
methods such as maximum-likelihood fit, error propagation formulas, goodness of
fit and model comparison, Monte Carlo methods such as the jackknife and bootstrap,
Monte Carlo Markov chains, Kolmogorov-Smirnov tests, and more. All subjects
are complemented by an extensive set of numerical tables that make the book
completely self-contained.
</p>
<p>The material presented in this book can be comfortably covered in a one-semester
course and has several problems at the end of each chapter that are suitable as
homework assignments or exam questions. Problems are both of theoretical and
numerical nature, so that emphasis is equally placed on conceptual and practical
understanding of the subject. Several datasets, including those in the four &ldquo;classic
experiments,&rdquo; are used across several chapters, and the students can therefore use
them in applications of increasing difficulty.
</p>
<p>Huntsville, AL, USA Massimiliano Bonamente</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface to the Second Edition
</p>
<p>The second edition of Statistics and Analysis of Scientific Data was motivated by
the overall goal to provide a textbook that is mathematically rigorous and easy to
read and use as a reference at the same time. Basically, it is a book for both the
student who wants to learn in detail the mathematical underpinnings of statistics
and the reader who wants to just find the practical description on how to apply a
given statistical method or use the book as a reference.
</p>
<p>To this end, first I decided that a more clear demarcation between theoretical and
practical topics would improve the readability of the book. As a result, several pages
(i.e., mathematical derivations) are now clearly marked throughout the book with a
vertical line, to indicate material that is primarily aimed to those readers who seek
a more thorough mathematical understanding. Those parts are not required to learn
how to apply the statistical methods presented in the book. For the reader who uses
this book as a reference, this makes it easy to skip such sections and go directly
to the main results. At the end of each chapter, I also provide a summary of key
concepts, intended for a quick look-up of the results of each chapter.
</p>
<p>Secondly, certain existing material needed substantial re-organization and expan-
sion. The second edition is now comprised of 16 chapters, versus ten of the first
edition. A few chapters (Chap. 6 on mean, median, and averages, Chap. 9 on multi-
variable regression, and Chap. 11 on systematic errors and intrinsic scatter) contain
material that is substantially new. In particular, the topic of multi-variable regression
was introduced because of its use in many fields such as business and economics,
where it is common to apply the regression method to many independent variables.
Other chapters originate from re-arranging existing material more effectively. Some
of the numerical tables in both the main body and the appendix have been expanded
and re-arranged, so that the reader will find it even easier to use them for a variety
of applications and as a reference.
</p>
<p>The second edition also contains a new classic experiment, that of the measure-
ment of iris characteristics by R.A. Fisher and E. Anderson. These new data are used
to illustrate primarily the method of regression with many independent variables.
The textbook now features a total of five classic experiments (including G. Mendel&rsquo;s
data on the independent assortment of species, J.J. Thomson&rsquo;s data on the discovery
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface to the Second Edition
</p>
<p>of the electron, K. Pearson&rsquo;s collection of data of biometric characteristics, and
E. Hubble&rsquo;s measurements of the expansion of the universe). These data and their
analysis provide a unique way to learn the statistical methods presented in the book
and a resource for the student and the teacher alike. Many of the end-of-chapter
problems are based on these experimental data.
</p>
<p>Finally, the new edition contains corrections to a number of typos that had
inadvertently entered the manuscript. I am very much in debt to many of my students
at the University of Alabama in Huntsville for pointing out these typos to me over the
past few years, in particular, to Zachary Robinson, who has patiently gone through
much of the text to find typographical errors.
</p>
<p>Huntsville, AL, USA Massimiliano Bonamente</p>
<p/>
</div>
<div class="page"><p/>
<p>Acknowledgments
</p>
<p>In my early postdoc years, I was struggling to solve a complex data analysis
problem. My longtime colleague and good friend Dr. Marshall Joy of NASA&rsquo;s
Marshall Space Flight Center one day walked down to my office and said something
like &ldquo;Max, I have a friend in Chicago who told me that there is a method that maybe
can help us with our problem. I don&rsquo;t understand any of it, but here&rsquo;s a paper that
talks about Monte Carlo Markov chains. See if it can help us.&rdquo; That conversation
led to the appreciation of one of statistics and data analysis, most powerful tools
and opened the door for virtually all the research papers that I wrote ever since. For
over a decade, Marshall taught me how to be careful in the analysis of data and
interpretation of results&mdash;and always used a red felt-tip marker to write comments
on my papers.
</p>
<p>The journey leading to this book started about 10 years ago, when Prof. A. Gor-
don Emslie, currently provost at Western Kentucky University, and I decided to offer
a new course in data analysis and statistics for graduate students in our department.
Gordon&rsquo;s uncanny ability to solve virtually any problem presented to him&mdash;and
likewise make even the experienced scientist stumble with his questions&mdash;has been
a great source of inspiration for this book.
</p>
<p>Some of the material presented in this book is derived from Prof. Kyle Siegrist&rsquo;s
lectures on probability and stochastic processes at the University of Alabama in
Huntsville. Kyle reinforced my love for mathematics and motivated my desire to
emphasize both mathematics and applications for the material presented in this
book.
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Theory of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Experiments, Events, and the Sample Space . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Probability of Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
</p>
<p>1.2.1 The Kolmogorov Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2.2 Frequentist or Classical Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2.3 Bayesian or Empirical Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
</p>
<p>1.3 Fundamental Properties of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Statistical Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.5 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.6 A Classic Experiment: Mendel&rsquo;s Law of Heredity
</p>
<p>and the Independent Assortment of Species . . . . . . . . . . . . . . . . . . . . . . . . 8
1.7 The Total Probability Theorem and Bayes&rsquo; Theorem . . . . . . . . . . . . . . 10
</p>
<p>2 Random Variables and Their Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2 Probability Distribution Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3 Moments of a Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
</p>
<p>2.3.1 The Mean and the Sample Mean. . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.3.2 The Variance and the Sample Variance .. . . . . . . . . . . . . . . . . . . 22
</p>
<p>2.4 A Classic Experiment: J.J. Thomson&rsquo;s Discovery
of the Electron .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
</p>
<p>2.5 Covariance and Correlation Between Random Variables . . . . . . . . . . 26
2.5.1 Joint Distribution and Moments of Two
</p>
<p>Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.2 Statistical Independence of Random Variables. . . . . . . . . . . . 28
</p>
<p>2.6 A Classic Experiment: Pearson&rsquo;s Collection of Data on
Biometric Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
</p>
<p>3 Three Fundamental Distributions: Binomial, Gaussian,
and Poisson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.1 The Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>3.1.1 Derivation of the Binomial Distribution . . . . . . . . . . . . . . . . . . . 35
3.1.2 Moments of the Binomial Distribution . . . . . . . . . . . . . . . . . . . . 38
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>3.2 The Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2.1 Derivation of the Gaussian Distribution
</p>
<p>from the Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2.2 Moments and Properties of the Gaussian Distribution . . . . 44
3.2.3 How to Generate a Gaussian Distribution
</p>
<p>from a Standard Normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.3 The Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.3.1 Derivation of the Poisson Distribution.. . . . . . . . . . . . . . . . . . . . 46
3.3.2 Properties and Interpretation of the Poisson
</p>
<p>Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.3.3 The Poisson Distribution and the Poisson Process . . . . . . . . 48
3.3.4 An Example on Likelihood and Posterior
</p>
<p>Probability of a Poisson Variable . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.4 Comparison of Binomial, Gaussian, and Poisson Distributions . . . 51
</p>
<p>4 Functions of Random Variables and Error Propagation . . . . . . . . . . . . . . 55
4.1 Linear Combination of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>4.1.1 General Mean and Variance Formulas . . . . . . . . . . . . . . . . . . . . . 55
4.1.2 Uncorrelated Variables and the 1=
</p>
<p>p
N Factor . . . . . . . . . . . . . 56
</p>
<p>4.2 The Moment Generating Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2.1 Properties of the Moment Generating Function .. . . . . . . . . . 59
4.2.2 The Moment Generating Function of the
</p>
<p>Gaussian and Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . 59
4.3 The Central Limit Theorem.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.4 The Distribution of Functions of Random Variables . . . . . . . . . . . . . . . 64
</p>
<p>4.4.1 The Method of Change of Variables . . . . . . . . . . . . . . . . . . . . . . . 65
4.4.2 A Method for Multi-dimensional Functions . . . . . . . . . . . . . . 66
</p>
<p>4.5 The Law of Large Numbers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.6 The Mean of Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . 69
4.7 The Variance of Functions of Random Variables
</p>
<p>and Error Propagation Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.7.1 Sum of a Constant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.7.2 Weighted Sum of Two Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.7.3 Product and Division of Two Random Variables. . . . . . . . . . 73
4.7.4 Power of a Random Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.7.5 Exponential of a Random Variable . . . . . . . . . . . . . . . . . . . . . . . . 75
4.7.6 Logarithm of a Random Variable . . . . . . . . . . . . . . . . . . . . . . . . . . 75
</p>
<p>4.8 The Quantile Function and Simulation of Random Variables . . . . . . 76
4.8.1 General Method to Simulate a Variable . . . . . . . . . . . . . . . . . . . 78
4.8.2 Simulation of a Gaussian Variable . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>5 Maximum Likelihood and Other Methods to Estimate
Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.1 The Maximum Likelihood Method for Gaussian Variables . . . . . . . . 85
</p>
<p>5.1.1 Estimate of the Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.1.2 Estimate of the Variance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.1.3 Estimate of Mean for Non-uniform Uncertainties . . . . . . . . 88</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>5.2 The Maximum Likelihood Method for Other Distributions . . . . . . . . 90
5.3 Method of Moments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
5.4 Quantiles and Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>5.4.1 Confidence Intervals for a Gaussian Variable . . . . . . . . . . . . . 94
5.4.2 Confidence Intervals for the Mean of a Poisson
</p>
<p>Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.5 Bayesian Methods for the Poisson Mean. . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
</p>
<p>5.5.1 Bayesian Expectation of the Poisson Mean . . . . . . . . . . . . . . . 102
5.5.2 Bayesian Upper and Lower Limits for a
</p>
<p>Poisson Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>6 Mean, Median, and Average Values of Variables . . . . . . . . . . . . . . . . . . . . . . . 107
6.1 Linear and Weighted Average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
6.2 The Median .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.3 The Logarithmic Average and Fractional
</p>
<p>or Multiplicative Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.3.1 The Weighted Logarithmic Average . . . . . . . . . . . . . . . . . . . . . . . 110
6.3.2 The Relative-Error Weighted Average .. . . . . . . . . . . . . . . . . . . . 113
</p>
<p>7 Hypothesis Testing and Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.1 Statistics and Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.2 The ï¿½2 Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
</p>
<p>7.2.1 The Probability Distribution Function .. . . . . . . . . . . . . . . . . . . . 122
7.2.2 Moments and Other Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.2.3 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
</p>
<p>7.3 The Sampling Distribution of the Variance . . . . . . . . . . . . . . . . . . . . . . . . . 127
7.4 The F Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>7.4.1 The Probability Distribution Function .. . . . . . . . . . . . . . . . . . . . 132
7.4.2 Moments and Other Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.4.3 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>7.5 The Sampling Distribution of the Mean
and the Student&rsquo;s t Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.5.1 Comparison of Sample Mean with Parent Mean . . . . . . . . . . 137
7.5.2 Comparison of Two Sample Means and
</p>
<p>Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
</p>
<p>8 Maximum Likelihood Methods for Two-Variable Datasets . . . . . . . . . . . 147
8.1 Measurement of Pairs of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.2 Maximum Likelihood Method for Gaussian Data . . . . . . . . . . . . . . . . . . 149
8.3 Least-Squares Fit to a Straight Line, or Linear Regression . . . . . . . . 150
8.4 Multiple Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>8.4.1 Best-Fit Parameters for Multiple Regression .. . . . . . . . . . . . . 152
8.4.2 Parameter Errors and Covariances for
</p>
<p>Multiple Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.4.3 Errors and Covariance for Linear Regression . . . . . . . . . . . . . 154
</p>
<p>8.5 Special Cases: Identical Errors or No Errors Available . . . . . . . . . . . . 155</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>8.6 A Classic Experiment: Edwin Hubble&rsquo;s Discovery
of the Expansion of the Universe .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
</p>
<p>8.7 Maximum Likelihood Method for Non-linear Functions . . . . . . . . . . 160
8.8 Linear Regression with Poisson Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
</p>
<p>9 Multi-Variable Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.1 Multi-Variable Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.2 A Classic Experiment: The R.A. Fisher and
</p>
<p>E. Anderson Measurements of Iris Characteristics . . . . . . . . . . . . . . . . . 166
9.3 The Multi-Variable Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
9.4 Tests for Significance of the Multiple Regression Coefficients . . . . 170
</p>
<p>9.4.1 T-Test for the Significance of Model Components .. . . . . . . 170
9.4.2 F-Test for Goodness of Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
9.4.3 The Coefficient of Determination .. . . . . . . . . . . . . . . . . . . . . . . . . 174
</p>
<p>10 Goodness of Fit and Parameter Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
10.1 Goodness of Fit for the ï¿½2min Fit Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
10.2 Goodness of Fit for the Cash C Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
10.3 Confidence Intervals of Parameters for Gaussian Data . . . . . . . . . . . . . 181
</p>
<p>10.3.1 Confidence Interval on All Parameters . . . . . . . . . . . . . . . . . . . . 183
10.3.2 Confidence Intervals on Reduced Number
</p>
<p>of Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
10.4 Confidence Intervals of Parameters for Poisson Data . . . . . . . . . . . . . . 186
10.5 The Linear Correlation Coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>10.5.1 The Probability Distribution Function .. . . . . . . . . . . . . . . . . . . . 188
10.5.2 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
</p>
<p>11 Systematic Errors and Intrinsic Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
11.1 What to Do When the Goodness-of-Fit Test Fails . . . . . . . . . . . . . . . . . . 195
11.2 Intrinsic Scatter and Debiased Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
</p>
<p>11.2.1 Direct Calculation of the Intrinsic Scatter . . . . . . . . . . . . . . . . . 196
11.2.2 Alternative Method to Estimate the Intrinsic Scatter . . . . . 197
</p>
<p>11.3 Systematic Errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
11.4 Estimate of Model Parameters with Systematic Errors
</p>
<p>or Intrinsic Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
</p>
<p>12 Fitting Two-Variable Datasets with Bivariate Errors . . . . . . . . . . . . . . . . . . 203
12.1 Two-Variable Datasets with Bivariate Errors . . . . . . . . . . . . . . . . . . . . . . . 203
12.2 Generalized Least-Squares Linear Fit to Bivariate Data . . . . . . . . . . . 204
12.3 Linear Fit Using Bivariate Errors in the ï¿½2 Statistic . . . . . . . . . . . . . . . . 209
</p>
<p>13 Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
13.1 The F Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
</p>
<p>13.1.1 F-Test for Two Independent ï¿½2 Measurements . . . . . . . . . . . 212
13.1.2 F-Test for an Additional Model Component . . . . . . . . . . . . . . 214
</p>
<p>13.2 Kolmogorov&ndash;Smirnov Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
13.2.1 Comparison of Data to a Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
13.2.2 Two-Sample Kolmogorov&ndash;Smirnov Test . . . . . . . . . . . . . . . . . . 219</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xvii
</p>
<p>14 Monte Carlo Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
14.1 What is a Monte Carlo Analysis?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
14.2 Traditional Monte Carlo Integration.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
14.3 Dart Monte Carlo Integration and Function Evaluation . . . . . . . . . . . . 227
14.4 Simulation of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
14.5 Monte Carlo Estimates of Errors for Two-Variable Datasets. . . . . . . 230
</p>
<p>14.5.1 The Bootstrap Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
14.5.2 The Jackknife Method .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
</p>
<p>15 Introduction to Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
15.1 Stochastic Processes and Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
15.2 Mathematical Properties of Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . 238
15.3 Recurrent and Transient States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
15.4 Limiting Probabilities and Stationary Distribution . . . . . . . . . . . . . . . . . 243
</p>
<p>16 Monte Carlo Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
16.1 Introduction to Monte Carlo Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 249
16.2 Requirements and Goals of a Monte Carlo Markov Chain . . . . . . . . . 250
16.3 The Metropolis&ndash;Hastings Algorithm .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
16.4 The Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
16.5 Tests of Convergence.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
</p>
<p>16.5.1 The Geweke Z Score Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
16.5.2 The Gelman&ndash;Rubin Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
16.5.3 The Raftery&ndash;Lewis Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
</p>
<p>Appendix: Numerical Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
A.1 The Gaussian Distribution and the Error Function . . . . . . . . . . . . . . . . . 273
A.2 Upper and Lower Limits for a Poisson Distribution . . . . . . . . . . . . . . . . 277
A.3 The ï¿½2 Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
A.4 The F Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
A.5 The Student&rsquo;s t Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
A.6 The Linear Correlation Coefficient r . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
A.7 The Kolmogorov&ndash;Smirnov Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 1
Theory of Probability
</p>
<p>Abstract The theory of probability is the mathematical framework for the study
of the probability of occurrence of events. The first step is to establish a method
to assign the probability of an event, for example, the probability that a coin lands
heads up after a toss. The frequentist&mdash;or empirical&mdash;approach and the subjective&mdash;
or Bayesian&mdash; approach are two methods that can be used to calculate probabilities.
The fact that there is more than one method available for this purpose should not
be viewed as a limitation of the theory, but rather as the fact that for certain parts
of the theory of probability, and even more so for statistics, there is an element
of subjectivity that enters the analysis and the interpretation of the results. It is
therefore the task of the statistician to keep track of any assumptions made in the
analysis, and to account for them in the interpretation of the results. Once a method
for assigning probabilities is established, the Kolmogorov axioms are introduced
as the &ldquo;rules&rdquo; required to manipulate probabilities. Fundamental results known as
Bayes&rsquo; theorem and the theorem of total probability are used to define and interpret
the concepts of statistical independence and of conditional probability, which play
a central role in much of the material presented in this book.
</p>
<p>1.1 Experiments, Events, and the Sample Space
</p>
<p>Every experiment has a number of possible outcomes. For example, the experiment
consisting of the roll of a die can have six possible outcomes, according to the
number that shows after the die lands. The sample space Ë is defined as the set of
all possible outcomes of the experiment, in this caseË D f1; 2; 3; 4; 5; 6g. An event
A is a subset of Ë , A ï¿½ Ë , and it represents a number of possible outcomes for the
experiment. For example, the event &ldquo;even number&rdquo; is represented by A D f2; 4; 6g,
and the event &ldquo;odd number&rdquo; as B D f1; 3; 5g. For each experiment, two events
always exist: the sample space itself, Ë , comprising all possible outcomes, and
A D ;, called the impossible event, or the event that contains no possible outcome.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Theory of Probability
</p>
<p>Events are conveniently studied using set theory, and the following definitions
are very common in theory of probability:
</p>
<p>&bull; The complementary A of an event A is the set of all possible outcomes except
those in A. For example, the complementary of the event &ldquo;odd number&rdquo; is the
event &ldquo;even number.&rdquo;
</p>
<p>&bull; Given two events A and B, the union C D A [ B is the event comprising all
outcomes of A and those of B. In the roll of a die, the union of odd and even
numbers is the sample space itself, consisting of all possible outcomes.
</p>
<p>&bull; The intersection of two events C D A \ B is the event comprising all outcomes
of A that are also outcomes of B. When A \ B D ;, the events are said to be
mutually exclusive. The union and intersection can be naturally extended to more
than two events.
</p>
<p>&bull; A number of events Ai are said to be a partition of the sample space if they are
mutually exclusive, and if their union is the sample space itself, [Ai D Ë .
</p>
<p>&bull; When all outcomes in A are comprised in B, we will say that A ï¿½ B or B ï¿½ A.
</p>
<p>1.2 Probability of Events
</p>
<p>The probability P of an event describes the odds of occurrence of an event in a
single trial of the experiment. The probability is a number between 0 and 1, where
P D 0 corresponds to an impossible event, and P D 1 to a certain event. Therefore
the operation of &ldquo;probability&rdquo; can be thought of as a function that transforms each
possible event into a real number between 0 and 1.
</p>
<p>1.2.1 The Kolmogorov Axioms
</p>
<p>The first step to determine the probability of the events associated with a given
experiment is to establish a number of basic rules that capture the meaning of
probability. The probability of an event is required to satisfy the three axioms
defined by Kolmogorov [26]:
</p>
<p>1. The probability of an event A is a non-negative number, P.A/ ï¿½ 0;
2. The probability of all possible outcomes, or sample space, is normalized to the
</p>
<p>value of unity, P.Ë/ D 1;
3. If A ï¿½ Ë and B ï¿½ Ë are mutually exclusive events, then
</p>
<p>P.A [ B/ D P.A/C P.B/ (1.1)
</p>
<p>Figure 1.1 illustrates this property using set diagrams. For events that are not
mutually exclusive, this property does not apply. The probability of the union is</p>
<p/>
</div>
<div class="page"><p/>
<p>1.2 Probability of Events 3
</p>
<p>Î©Î©
</p>
<p>BB
</p>
<p>A A
</p>
<p>Fig. 1.1 The probability of the event P.A[B/ is the sum of the two individual probabilities, only
if the two events are mutually exclusive. This property enables the interpretation of probability as
the &ldquo;area&rdquo; of a given event within the sample space
</p>
<p>represented by the area of A [ B, and the outcomes that overlap both events are
not double-counted.
</p>
<p>These axioms should be regarded as the basic &ldquo;ground rules&rdquo; of probability, but
they provide no unique specification on how event probabilities should be assigned.
Two major avenues are available for the assignment of probabilities. One is based on
the repetition of the experiments a large number of times under the same conditions,
and goes under the name of the frequentist or classical method. The other is based
on a more theoretical knowledge of the experiment, but without the experimental
requirement, and is referred to as the Bayesian approach.
</p>
<p>1.2.2 Frequentist or Classical Method
</p>
<p>Consider performing an experiment for a number N ï¿½ 1 of times, under the same
experimental conditions, and measuring the occurrence of the event A as the number
N.A/. The probability of event A is given by
</p>
<p>P.A/ D lim
N!1
</p>
<p>N.A/
</p>
<p>N
I (1.2)
</p>
<p>that is, the probability is the relative frequency of occurrence of a given event from
many repetitions of the same experiment. The obvious limitation of this definition
is the need to perform the experiment an infinite number of times, which is not only
time consuming, but also requires the experiment to be repeatable in the first place,
which may or may not be possible.
</p>
<p>The limitation of this method is evident by considering a coin toss: no matter the
number of tosses, the occurrence of heads up will never be exactly 50 %, which is
what one would expect based on a knowledge of the experiment at hand.</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Theory of Probability
</p>
<p>1.2.3 Bayesian or Empirical Method
</p>
<p>Another method to assign probabilities is to use the knowledge of the experiment
and the event, and the probability one assigns represents the degree of belief that the
event will occur in a given try of the experiment. This method implies an element
of subjectivity, which will become more evident in Bayes&rsquo; theorem (see Sect. 1.7).
The Bayesian probability is assigned based on a quantitative understanding of
the nature of the experiment, and in accord with the Kolmogorov axioms. It is
sometimes referred to as empirical probability, in recognition of the fact that
sometimes the probability of an event is assigned based upon a practical knowledge
of the experiment, although without the classical requirement of repeating the
experiment for a large number of times. This method is named after the Rev. Thomas
Bayes, who pioneered the development of the theory of probability [3].
</p>
<p>Example 1.1 (Coin Toss Experiment) In the coin toss experiment, the determi-
nation of the empirical probability for events &ldquo;heads up&rdquo; or &ldquo;tails up&rdquo; relies on
the knowledge that the coin is unbiased, and that therefore it must be true that
P.tails/ D P.heads/. This empirical statement signifies the use of the Bayesian
method to determine probabilities. With this information, we can then simply use
the Kolmogorov axioms to state that P.tails/C P.heads/ D 1, and therefore obtain
the intuitive result that P.tails/ D P.heads/ D 1=2. }
</p>
<p>1.3 Fundamental Properties of Probability
</p>
<p>The following properties are useful to improve our ability to assign and manipulate
event probabilities. They are somewhat intuitive, but it is instructive to derive them
formally from the Kolmogorov axioms.
</p>
<p>1. The probability of the null event is zero, P.;/ D 0.
Proof Start with the mutually exclusive events ; and Ë . Since their union is Ë ,
it follows from the Third Axiom that P.Ë/ D P.Ë/C P.;/. From the Second
Axiom we know that P.Ë/ D 1, from this it follows that P.;/ D 0. ut
The following property is a generalization of the one described above:
</p>
<p>2. The probability of the complementary event A satisfies the property
</p>
<p>P.A/ D 1 ï¿½ P.A/: (1.3)
</p>
<p>Proof By definition, it is true that A [ A D Ë , and that A, A are mutually
exclusive. Using the Second and Third axiom, P.A [ A/ D P.A/ C P.A/ D 1,
from which it follows that P.A/ D 1 ï¿½ P.A/. ut</p>
<p/>
</div>
<div class="page"><p/>
<p>1.4 Statistical Independence 5
</p>
<p>3. The probability of the union of two events satisfies the general property that
</p>
<p>P.A [ B/ D P.A/C P.B/ï¿½ P.A \ B/: (1.4)
</p>
<p>This property generalizes the Third Kolmogorov axiom, and can be interpreted as
the fact that outcomes in the overlap region of the two events should be counted
only once, as illustrated in Fig. 1.1.
</p>
<p>Proof First, realize that the event A [ B can be written as the union of three
mutually exclusive sets, A [ B D .A \ B/ [ .B \ A/ [ .A \ B/, see Fig. 1.1.
Therefore, using the Third axiom, P.A[B/ D P.A\B/CP.B\A/CP.A\B/.
</p>
<p>Then, notice that for any event A and B, it is true that A D .A\ B/[ .A\ B/,
since {B;B} is a partition ofË . This implies that P.A/ D P.A\B/CP.A\B/ due
to the fact that the two sets are again mutually exclusive, and likewise for eventB.
It thus follows that P.A[B/ D P.A/ï¿½P.A\B/CP.B/ï¿½P.B\A/CP.A\B/D
P.A/C P.B/ï¿½ P.A\ B/. ut
</p>
<p>Example 1.2 An experiment consists of drawing a number between 1 and 100 at
random. Calculate the probability of the event: &ldquo;drawing either a number greater
than 50, or an odd number, at each try.&rdquo;
</p>
<p>The sample space for this experiment is the set of numbers i D 1; : : : ; 100, and
the probability of drawing number i is P.Ai/ D 1=100, since we expect that each
number will have the same probability of being drawn at each try. Ai is the event
that consists of drawing number i. If we call B the event consisting of all numbers
greater than 50, and C the event with all odd numbers, it is clear that P.B/ D 0:5,
and likewise P.C/ D 0:5. The event A\B contains all odd numbers greater than 50,
and therefore P.A\ B/ D 0:25. Using (1.4), we find that the probability of drawing
either a number greater than 50, or an odd number, is 0.75. This can be confirmed
by a direct count of the possible outcomes. }
</p>
<p>1.4 Statistical Independence
</p>
<p>Statistical independence among events means that the occurrence of one event has
no influence on the occurrence of other events. Consider, for example, rolling two
dice, one after the other: the outcome of one die is independent of the other, and
the two tosses are said to be statistically independent. On the other hand, consider
the following pair of events: the first is the roll of die 1, and the second is the roll
of die 1 and die 2, so that for the second event we are interested in the sum of the
two tosses. It is clear that the outcome of the second event&mdash;e.g., the sum of both
dice&mdash;depends on the first toss, and the two events are not independent.
</p>
<p>Two events A and B are said to be statistically independent if and only if
</p>
<p>P.A\ B/ D P.A/ ï¿½ P.B/: (1.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Theory of Probability
</p>
<p>At this point, it is not obvious that the concept of statistical independence is
embodied by (1.5). A few examples will illustrate the meaning of this definition,
which will be explored further in the following section on conditional probability.
</p>
<p>Example 1.3 Determine the probability of obtaining two 3 when rolling two dice.
This event can be decomposed in two events: A D {die 1 shows 3 and die 2 shows
any number} and B D {die 2 shows 3 and die 1 shows any number}.
</p>
<p>It is natural to assume that P.A/ D 1=6, P.B/ D 1=6 and state that the two events
A and B are independent by nature, since each event involves a different die, which
has no knowledge of the other one. The event we are interested in is C D A\B and
the definition of probability of two statistically independent events leads to P.C/ D
P.A\ B/ D P.A/ ï¿½ P.B/ D 1=36. This result can be confirmed by the fact that there
is only one combination out of 36 that gives rise to two consecutive 3. }
</p>
<p>The example above highlights the importance of a proper, and sometimes
extended, definition of an event. The more careful the description of the event and of
the experiment that it is drawn from, the easier it is to make probabilistic calculation
and the assessment of statistical independence.
</p>
<p>Example 1.4 Consider the events A D {die 1 shows 3 and die 2 shows any number}
and B D {the sum of the two dice is 9}. Determine whether they are statistically
independent.
</p>
<p>In this case, we will calculate the probability of the two events, and then check
whether they obey (1.5) or not. This calculation will illustrate that the two events
are not statistically independent.
</p>
<p>Event A has a probability P.A/ D 1=6; in order to calculate the probability
of event B, we realize that a sum of 9 is given by the following combinations of
outcomes of the two rolls: (3,6), (4,5), (5,4) and (6,3). Therefore, P.B/ D 1=9.
The event A \ B is the situation in which both event A and B occur, which
corresponds to the single combination (3,6); therefore, P.A \ B/ D 1=36. Since
P.A/ ï¿½ P.B/ D 1=6 ï¿½ 1=9 D 1=54 &curren; P.A \ B/ D 1=36, we conclude that the
two events are not statistically independent. This conclusion means that one event
influences the other, since a 3 in the first toss has certainly an influence on the
possibility of both tosses having a total of 9. }
</p>
<p>There are two important necessary (but not sufficient) conditions for statistical
independence between two events. These properties can help identify whether two
events are independent.
</p>
<p>1. If A \ B D ;, A and B cannot be independent, unless one is the empty set. This
property states that there must be some overlap between the two events, or else it
is not possible for the events to be independent.
</p>
<p>Proof For A and B to be independent, it must be true that P.A\B/ D P.A/ ï¿½P.B/,
which is zero by hypothesis. This can be true only if P.A/ D 0 or P.B/ D 0,
which in turn means A D ; or B D ; as a consequence of the Kolmogorov
axioms. ut</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Conditional Probability 7
</p>
<p>2. If A ï¿½ B, then A and B cannot be independent, unless B is the entire sample
space. This property states that the overlap between two events cannot be such
that one event is included in the other, in order for statistical independence to be
possible.
</p>
<p>Proof In order for A and B to be independent, it must be that P.A \ B/ D P.A/ ï¿½
P.B/ D P.A/, given that A ï¿½ B. This can only be true if B D Ë , since P.Ë/ D 1.
</p>
<p>ut
Example 1.5 Consider the above Example 1.3 of the roll of two dice; each event
was formulated in terms of the outcome of both rolls, to show that there was in fact
overlap between two events that are independent of one another. }
Example 1.6 Consider the following two events: A D {die 1 shows 3 and die 2
shows any number} and B D {die 1 shows 3 or 2 and die 2 shows any number}. It
is clear that A ï¿½ B, P.A/ D 1=6 and P.B/ D 1=3. The event A \ B is identical to
A and P.A \ B/ D 1=6. Therefore P.A \ B/ &curren; P.A/ ï¿½ P.B/ and the two events
are not statistically independent. This result can be easily explained by the fact
that the occurrence of A implies the occurrence of B, which is a strong statement
of dependence between the two events. The dependence between the two events
can also be expressed with the fact that the non-occurrence of B implies the non-
occurrence of A. }
</p>
<p>1.5 Conditional Probability
</p>
<p>The conditional probability describes the probability of occurrence of an event A
given that another event B has occurred and it is indicated as P.A=B/. The symbol
&ldquo;/&rdquo; indicates the statement given that or knowing that. It states that the event after the
symbol is known to have occurred. When two or more events are not independent,
the probability of a given event will in general depend on the occurrence of another
event. For example, if one is interested in obtaining a 12 in two consecutive rolls of
a die, the probability of such event does rely on the fact that the first roll was (or
was not) a 6.
</p>
<p>The following relationship defines the conditional probability:
</p>
<p>P.A\ B/ D P.A=B/ ï¿½ P.B/ D P.B=A/ ï¿½ P.A/I (1.6)
</p>
<p>Equation (1.6) can be equivalently expressed as
</p>
<p>P.A=B/ D
8
&lt;
</p>
<p>:
</p>
<p>P.A \ B/
P.B/
</p>
<p>if P.B/ &curren; 0
0 if P.B/ D 0:
</p>
<p>(1.7)</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Theory of Probability
</p>
<p>A justification for this definition is that the occurrence of B means that the
probability of occurrence of A is that of A \ B. The denominator of the conditional
probability is P.B/ because B is the set of all possible outcomes that are known to
have happened. The situation is also depicted in the right-hand side panel of Fig. 1.1:
knowing that B has occurred, leaves the probability of occurrence of A to the
occurrence of the intersection A\B, out of all outcomes in B. It follows directly from
(1.6) that if A and B are statistically independent, then the conditional probability is
P.A=B/ D P.A/, i.e., the occurrence of B has no influence on the occurrence of A.
This observation further justifies the definition of statistical independence according
to (1.5).
</p>
<p>Example 1.7 Calculate the probability of obtaining 8 as the sum of two rolls of a
die, given that the first roll was a 3.
</p>
<p>Call event A the sum of 8 in two separate rolls of a die and event B the event
that the first roll is a 3. Event A is given by the probability of having tosses (2,6),
(3,5), (4,4), (5,3), (6,2). Since each such combination has a probability of 1/36,
P.A/ D 5=36. The probability of event B is P.B/ D 1=6. Also, the probability of
A \ B is the probability that the first roll is a 3 and the sum is 8, which can clearly
occur only if a sequence of (3,5) takes place, with probability P.A \ B/ D 1=36.
</p>
<p>According to the definition of conditional probability, P.A=B/ D P.A \
B/=P.B/ D 6=36 D 1=6, and in fact only combination (5,3)&mdash;of the six available
with 3 as the outcome of the second toss&mdash;gives rise to a sum of 8. The occurrence
of 3 in the first roll has therefore increased the probability of A from P.A/ D 5=36 to
P.A=B/ D 1=6, since not any outcome of the first roll would be equally conducive
to a sum of 8 in two rolls. }
</p>
<p>1.6 A Classic Experiment: Mendel&rsquo;s Law of Heredity
and the Independent Assortment of Species
</p>
<p>The experiments performed in the nineteenth century by Gregor Mendel in
the monastery of Brno led to the discovery that certain properties of plants,
such as seed shape and color, are determined by a pair of genes. This pair of
genes, or genotype, is formed by the inheritance of one gene from each of the
parent plants.
</p>
<p>Mendel began by crossing two pure lines of pea plants which differed in
one single characteristic. The first generation of hybrids displayed only one
of the two characteristics, called the dominant character. For example, the
first-generation plants all had round seed, although they were bred from a
population of pure round seed plants and one with wrinkled seed. When the
first-generation was allowed to self-fertilize itself, Mendel observed the data
shown in Table 1.1 [31].
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.6 A Classic Experiment: Mendel&rsquo;s Law of Heredity and the Independent. . . 9
</p>
<p>Table 1.1 Data from G. Mendel&rsquo;s experiment
</p>
<p>Character No. of dominant No. of recessive Fract. of dominant
</p>
<p>Round vs. wrinkled seed 5474 1850 0.747
</p>
<p>Yellow vs. green seed 6022 2001 0.751
</p>
<p>Violet-red vs. white flower 705 224 0.759
</p>
<p>Inflated vs. constricted pod 882 299 0.747
</p>
<p>Green vs. yellow unripe pod 428 152 0.738
</p>
<p>Axial vs. terminal flower 651 207 0.759
</p>
<p>Long vs. short stem 787 277 0.740
</p>
<p>Table 1.2 Data from G. Mendel&rsquo;s experiment for plants with two different characters
</p>
<p>Yellow seed Green seed
</p>
<p>Round seed 315 108
</p>
<p>Wrinkled seed 101 32
</p>
<p>In addition, Mendel performed experiments in which two pure lines that
differed by two characteristics were crossed. In particular, a line with yellow
and round seed was crossed with one that had green and wrinkled seeds.
As in the previous case, the first-generation plants had a 100 % occurrence
of the dominant characteristics, while the second-generation was distributed
according to the data in Table 1.2.
</p>
<p>One of the key results of these experiments goes under the name of Law
of independent assortment, stating that a daughter plant inherits one gene
from each parent plant independently of the other parent. If we denote the
genotype of the dominant parent as DD (a pair of dominant genes) and that
of the recessive parent as RR, then the data accumulated by Mendel support
the hypothesis that the first-generation plants will have the genotype DR
(the order of genes in the genome is irrelevant) and the second generation
plants will have the following four genotypes: DD, DR, RD and RR, in
equal proportions. Since the first three genomes will display the dominant
characteristic, the ratio of appearance of the dominant characteristic is
expected to be 0.75. The data appear to support in full this hypothesis.
</p>
<p>In probabilistic terms, one expects that each second-generation plant has
P.D/ D 0:5 of drawing a dominant first gene from each parent and P.R/ D
0:5 of drawing a recessive gene from each parent. Therefore, according to the
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Theory of Probability
</p>
<p>hypothesis of independence in the inheritance of genes, we have
</p>
<p>8
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
:
</p>
<p>P.DD/ D P.D/ ï¿½ P.D/ D 0:25
P.DR/ D P.D/ ï¿½ P.R/ D 0:25
P.RD/ D P.R/ ï¿½ P.D/ D 0:25
P.RR/ D P.R/ ï¿½ P.R/ D 0:25:
</p>
<p>(1.8)
</p>
<p>When plants differing by two characteristics are crossed, as in the case
of the data in Table 1.2, then each of the four events in (1.8) is indepen-
dently mixed between the two characters. Therefore, there is a total of 16
possibilities, which give rise to 4 possible combinations of the two characters.
For example, a display of both recessive characters will have a probability
of 1=16 D 0:0625. The data seemingly support this hypothesis with a
measurement of a fraction of 0.0576.
</p>
<p>1.7 The Total Probability Theorem and Bayes&rsquo; Theorem
</p>
<p>In this section we describe two theorems that are of great importance in a number of
practical situations. They make use of a partition of the sample space Ë , consisting
of n events Ai that satisfy the following two properties:
</p>
<p>Ai \ Aj D ;; 8i &curren; j
n[
</p>
<p>iD1
Ai D Ë:
</p>
<p>(1.9)
</p>
<p>For example, the outcomes 1, 2, 3, 4, 5 and 6 for the roll of a die partition the sample
space into a number of events that cover all possible outcomes, without any overlap
among each other.
</p>
<p>Theorem 1.1 (Total Probability Theorem) Given an event B and a set of events
Ai with the properties (1.9),
</p>
<p>P.B/ D
nX
</p>
<p>iD1
P.B \ Ai/ D
</p>
<p>nX
</p>
<p>iD1
P.B=Ai/ ï¿½ P.Ai/: (1.10)
</p>
<p>Proof The first equation is immediately verified given that the B \ Ai are mutually
exclusive events such that B D [i.B \ Ai/. The second equation derives from the
application of the definition of conditional probability. ut</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 The Total Probability Theorem and Bayes&rsquo; Theorem 11
</p>
<p>The total probability theorem is useful when the probability of an event B cannot
be easily calculated and it is easier to calculate the conditional probability B=Ai
given a suitable set of conditions Ai. The example at the end of Sect. 1.7 illustrates
one such situation.
</p>
<p>Theorem 1.2 (Bayes&rsquo; Theorem) Given an event B and a set of events Ai with
properties (1.9),
</p>
<p>P.Ai=B/ D P.B=Ai/P.Ai/
P.B/
</p>
<p>D P.B=Ai/P.Ai/nX
iD1
</p>
<p>P.B \ Ai/
(1.11)
</p>
<p>Proof The proof is an immediate consequence of the definition of conditional
probability, (1.6), and of the Total Probability theorem, (1.10). ut
</p>
<p>Bayes&rsquo; theorem is often written in a simpler form by taking into account two
events only, Ai D A and B:
</p>
<p>P.A=B/ D P.B=A/P.A/
P.B/
</p>
<p>(1.12)
</p>
<p>In this form, Bayes&rsquo; theorem is just a statement of how the order of conditioning
between two events can be inverted.
</p>
<p>Equation (1.12) plays a central role in probability and statistics. What is
especially important is the interpretation that each term assumes within the context
of a specific experiment. Consider B as the data collected in a given experiment&mdash;
these data can be considered as an event, containing the outcome of the experiment.
The eventA is a model that is used to describe the data. The model can be considered
as an ideal outcome of the experiment, therefore both A and B are events associated
with the same experiment. Following this interpretation, the quantities involved in
Bayes&rsquo; theorem can be interpreted as in the following:
</p>
<p>&bull; P.B=A/ is the probability, or likelihood, of the data given the specified model,
and indicated as L . The likelihood represents the probability of making the
measurement B given that the model A is a correct description of the experiment.
</p>
<p>&bull; P.A/ is the probability of the model A, without any knowledge of the data. This
term is interpreted as a prior probability, or the degree belief that the model
is true before the measurements are made. Prior probabilities should be based
upon quantitative knowledge of the experiment, but can also reflect the subjective
belief of the analyst. This step in the interpretation of Bayes&rsquo; theorem explicitly
introduces an element of subjectivity that is characteristic of Bayesian statistics.
</p>
<p>&bull; P.B/ is the probability of collecting the dataset B. In practice, this probability acts
as a normalization constant and its numerical value is typically of no practical
consequence.
</p>
<p>&bull; Finally, P.A=B/ is the probability of the model after the data have been collected.
This is referred to as the posterior probability of the model. The posterior</p>
<p/>
</div>
<div class="page"><p/>
<p>12 1 Theory of Probability
</p>
<p>probability is the ultimate goal of a statistical analysis, since it describes the
probability of the model based on the collection of data. According to the value
of the posterior probability, a model can be accepted or discarded.
</p>
<p>This interpretation of Bayes&rsquo; theorem is the foundation of Bayesian statistics.
Models of an experiment are usually described in terms of a number of parameters.
One of the most common problems of statistical data analysis is to estimate what
values for the parameters are permitted by the data collected from the experiment.
Bayes&rsquo; theorem provides a way to update the prior knowledge on the model
parameters given the measurements, leading to posterior estimates of parameters.
One key feature of Bayesian statistics is that the calculation of probabilities are
based on a prior probability, which may rely on a subjective interpretation of what
is known about the experiment before any measurements are made. Therefore, great
attention must be paid to the assignment of prior probabilities and the effect of priors
on the final results of the analysis.
</p>
<p>Example 1.8 Consider a box in which there are red and blue balls, for a total of
N D 10 balls. What is known a priori is just the total number of balls in the
box. Of the first 3 balls drawn from the box, 2 are red and 1 is blue (drawing is
done with re-placement of balls after drawing). We want to use Bayes&rsquo; theorem
to make inferences on the number of red balls (i) present in the box, i.e., we seek
P.Ai=B/, the probability of having i red balls in the box, given that we performed
the measurement B D {Two red balls were drawn in the first three trials}.
</p>
<p>Initially, we may assume that P.Ai/ D 1=11, meaning that there is an equal
probability of having 0, 1, : : : or 10 red balls in the box (for a total of 11 possibilities)
before we make any measurements. Although this is a subjective statement, a
uniform distribution is normally the logical assumption in the absence of other
information. We can use basic combinatorial mathematics to determine that the
likelihood of drawing D D 2 red balls out of T D 3 trials, given that there are i
red balls (also called event Ai):
</p>
<p>P.B=Ai/ D
 
T
</p>
<p>D
</p>
<p>!
</p>
<p>pDqTï¿½D: (1.13)
</p>
<p>In this equation p is the probability of drawing one of the red balls in a given drawing
assuming that there are i red balls, p D i=N, and q is the probability of drawing one
of the blue balls, q D 1ï¿½ p D .N ï¿½ i/=N. The distribution in (1.13) is known as the
binomial distribution and it will be derived and explained in more detail in Sect. 3.1.
The likelihood P.B=Ai/ can therefore be rewritten as
</p>
<p>P.B=Ai/ D
 
3
</p>
<p>2
</p>
<p>!ï¿½
i
</p>
<p>N
</p>
<p>ï¿½2 ï¿½N ï¿½ i
N
</p>
<p>ï¿½
</p>
<p>(1.14)
</p>
<p>The probability P.B/ is the probability of drawing D D 2 red balls out of T D 3
trial, for all possible values of the true number of red balls, i D 0; : : : ; 10. This</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 The Total Probability Theorem and Bayes&rsquo; Theorem 13
</p>
<p>probability can be calculated from the Total Probability theorem,
</p>
<p>P.B/ D
NX
</p>
<p>iD0
P.B=Ai/ ï¿½ P.Ai/ (1.15)
</p>
<p>We can now put all the pieces together and determine the posterior probability of
having i red balls, P.Ai=B/, using Bayes&rsquo; theorem, P.Ai=B/ D P.B=Ai/P.Ai/=P.B/.
</p>
<p>The equation above is clearly a function of i, the true number of red balls.
Consider the case of i D 0, i.e., what is the posterior probability of having no
red balls in the box. Since
</p>
<p>P.B=A0/ D
 
3
</p>
<p>2
</p>
<p>!ï¿½
0
</p>
<p>N
</p>
<p>ï¿½2 ï¿½N ï¿½ 0
N
</p>
<p>ï¿½
</p>
<p>D 0;
</p>
<p>it follows that P.Ao=B/ D 0, i.e., it is impossible that there are no red balls. This is
obvious, since two times a red ball was in fact drawn, meaning that there is at least
one red ball in the box. Other posterior probabilities can be calculated in a similar
way. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Event: A set of possible outcomes of an experiment.
ï¿½ Sample space: All possible outcomes of an experiment.
ï¿½ Probability of an Event: A number between 0 and 1 that follows the
</p>
<p>Kolmogorov axioms.
ï¿½ Frequentist or Classical approach: A method to determine the probability
</p>
<p>of an event based on many repetitions of the experiment.
ï¿½ Bayesian or Empirical approach:A method to determine probabilities that
</p>
<p>uses prior knowledge of the experiment.
ï¿½ Statistical independence: Two events are statistically independent when
</p>
<p>the occurrence of one has no influence on the occurrence of the other,P.A\
B/ D P.A/P.B/.
</p>
<p>ï¿½ Conditional probability: Probability of occurrence of an event given that
another event is known to have occurred, P.A=B/ D P.A \ B/=P.B/.
</p>
<p>ï¿½ Total Probability theorem: A relationship among probabilities of events
that form a partition of the sample space, P.B/ DPP.B=Ai/P.Ai/.
</p>
<p>ï¿½ Bayes&rsquo; theorem: A relationship among conditional probabilities that
enables the change in the order of conditioning of the events, P.A=B/ D
P.B=A/P.A/=P.B/.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Theory of Probability
</p>
<p>Problems
</p>
<p>1.1 Describe the sample space of the experiment consisting of flipping four coins
simultaneously. Assign the probability to the event consisting of &ldquo;two heads up and
two tails up.&rdquo; In this experiment it is irrelevant to know which specific coin shows
heads up or tails up.
</p>
<p>1.2 An experiment consists of rolling two dice simultaneously and independently
of one another. Find the probability of the event consisting of having either an odd
number in the first roll or a total of 9 in both rolls.
</p>
<p>1.3 In the roll of a die, find the probability of the event consisting of having either
an even number or a number greater than 4.
</p>
<p>1.4 An experiment consists of rolling two dice simultaneously and independently
of one another. Show that the two events, &ldquo;the sum of the two rolls is 8&rdquo; and &ldquo;the
first roll shows 5&rdquo; are not statistically independent.
</p>
<p>1.5 An experiment consists of rolling two dice simultaneously and independently
of one another. Show that the two events, &ldquo;first roll is even&rdquo; and &ldquo;second roll is
even&rdquo; are statistically independent.
</p>
<p>1.6 A box contains 5 balls, of which 3 are red and 2 are blue. Calculate (a) the
probability of drawing two consecutive red balls and (b) the probability of drawing
two consecutive red balls, given that the first draw is known to be a red ball. Assume
that after each draw the ball is replaced in the box.
</p>
<p>1.7 A box contains 10 balls that can be either red or blue. Of the first three draws,
done with replacement, two result in the draw of a red ball. Calculate the ratio of the
probability that there are 2 or just 1 red ball in the box and the ratio of probability
that there are 5 or 1 red balls.
</p>
<p>1.8 In the game of baseball a player at bat either reaches base or is retired. Consider
three baseball players: player A was at bat 200 times and reached base 0.310 of
times; player B was at bat 250 times, with an on-base percentage of 0.296; player
C was at bat 300 times, with an on-base percentage 0.260. Find (a) the probability
that when either player A, B, or C were at bat, he reached base, (b) the probability
that, given that a player reached base, it was A, B, or C.
</p>
<p>1.9 An experiment consists of rolling two dice simultaneously and independently
of one another. Calculate (a) the probability of the first roll being a 1, given that the
sum of both rolls was 5, (b) the probability of the sum being 5, given that the first
roll was a 1 and (c) the probability of the first roll being a 1 and the sum being 5.
Finally, (d) verify your results with Bayes&rsquo; theorem.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.7 The Total Probability Theorem and Bayes&rsquo; Theorem 15
</p>
<p>1.10 Four coins labeled 1 through 4 are tossed simultaneously and independently of
one another. Calculate (a) the probability of having an ordered combination heads-
tails-heads-tails in the four coins, (b) the probability of having the same ordered
combination given that any two coins are known to have landed heads-up and (c)
the probability of having two coins land heads up given that the sequence heads-
tails-heads-tails has occurred.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 2
Random Variables and Their Distributions
</p>
<p>Abstract The purpose of performing experiments and collecting data is to gain
information on certain quantities of interest called random variables. The exact
value of these quantities cannot be known with absolute precision, but rather we can
constrain the variable to a given range of values, narrower or wider according to the
nature of the variable itself and the type of experiment performed. Random variables
are described by a distribution function, which is the theoretical expectation for the
outcome of experiments aimed to measure it. Other measures of the random variable
are the mean, variance, and higher-order moments.
</p>
<p>2.1 Random Variables
</p>
<p>A random variable is a quantity of interest whose true value is unknown. To gain
information on a random variable we design and conduct experiments. It is inherent
to any experiment that the random variable of interest will never be known exactly.
Instead, the variable will be characterized by a probability distribution function,
which determines what is the probability that a given value of the random variable
occurs. Repeating the measurement typically increases the knowledge we gain of the
distribution of the variable. This is the reason for wanting to measure the quantity
as many times as possible.
</p>
<p>As an example of random variable, consider the gravitational constant G. Despite
the label of &ldquo;constant&rdquo;, we only know it to have a range of possible values in the
approximate interval G D 6:67428 Ë 0:00067 (in the standard S.I. units). This
means that we don&rsquo;t know the true value of G, but we estimate the range of possible
values by means of experiments. The random nature of virtually all quantities lies
primarily in the fact that no quantity is known exactly to us without performing an
experiment and that any experiment is never perfect because of practical or even
theoretical limitations. Among the practical reasons are, for example, limitations in
the precision of the measuring apparatus. Theoretical reasons depend on the nature
of the variable. For example, the measurement of the position and velocity of a
subatomic particle is limited by the Heisenberg uncertainty principle, which forbids
an exact knowledge even in the presence of a perfect measuring apparatus.
</p>
<p>The general method for gaining information on a random variable X starts with
set of measurements xi, ensuring that measurements are performed under the same
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_2
</p>
<p>17</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Random Variables and Their Distributions
</p>
<p>Fig. 2.1 Example of data
collected to measure a
random variable X. The 500
measurements were binned
according to their value to
construct the sample
distribution. The shape of the
distribution depends on the
nature of the experiment and
of the number of
measurements
</p>
<p>experimental conditions. Throughout the book we will reserve uppercase letters for
the name of the variable itself and lowercase letters for the actual measurements.
From these measurements, one obtains a histogram corresponding to the frequency
of occurrence of all values of X (Fig. 2.1). The measurements xi form the sample
distribution of the quantity, which describes the empirical distribution of values
collected in the experiment. On the other hand, random variables are typically
expected to have a theoretical distribution, e.g., Gaussian, Poisson, etc., known
as the parent distribution. The parent distribution represents the belief that there
is an ideal description of a random variable and its form depends on the nature
of the variable itself and the method of measurement. The sample distribution is
expected to become the parent distribution if an infinite number of measurements
are performed, in such a way that the randomness associated with a small number
of measurements is eliminated.
</p>
<p>Example 2.1 In Sect. 3.3 we will show that a discrete variable (e.g., one that can
only take integer values) that describes a counting experiment follows a Poisson
function,
</p>
<p>P.n/ D ï¿½
n
</p>
<p>nÅ 
eï¿½ï¿½
</p>
<p>in which ï¿½ is the mean value of the random variable (for short, its true-yet-unknown
value) and n is the actual value measured for the variable. P.n/ indicates the
probability of measuring the value n, given that the true value is ï¿½. Consider the
experiment of counting the number of photons reaching Earth from a given star;
due to a number of factors, the count may not always be the same every time the
experiment is performed, and if only one experiment is performed, one would obtain
a sample distribution that has a single &ldquo;bar&rdquo; at the location of the measured value
and this sample distribution would not match well a Poisson function. After a small
number of measurements, the distribution may appear similar to that in Fig. 2.1</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Probability Distribution Functions 19
</p>
<p>and the distribution will then become smoother and closer to the parent distribution
as the number of measurements increases. Repeating the experiment therefore will
help in the effort to estimate as precisely as possible the parameterï¿½ that determines
the Poisson distribution. }
</p>
<p>2.2 Probability Distribution Functions
</p>
<p>It is convenient to describe random variables with an analytic function that
determines the probability of the random variable to have a given value. Discrete
random variables are described by a probability mass function f .xi/ , where f .xi/
represents the probability of the variable to have an exact value of xi. Continuous
variables are described by a probability distribution function f .x/, such that f .x/dx is
the probability of the variable to have values in the interval Åx; xCdxï¿½. For simplicity
we will refer to both types of distributions as probability distribution functions
throughout the book.
</p>
<p>Probability distribution functions have the following properties:
</p>
<p>1. They are normalized to 1. For continuous variables this means
</p>
<p>Z C1
</p>
<p>ï¿½1
f .x/dx D 1: (2.1)
</p>
<p>For variables that are defined in a subset of the real numbers, e.g., only values
x ï¿½ 0 or in a finite interval, f .x/ is set to zero outside the domain of definition of
the function. For discrete variables, hereafter the integrals are replaced by a sum
over all values that the function of integration can have.
</p>
<p>2. The probability distribution can never be negative, f .x/ ï¿½ 0. This is a
consequence of the Kolmogorov axiom that requires a probability to be non-
negative.
</p>
<p>3. The function F.x/, called the (cumulative) distribution function,
</p>
<p>F.x/ D
Z x
</p>
<p>ï¿½1
f .ï¿½/dï¿½; (2.2)
</p>
<p>represents the probability that the variable has any value less or equal than x.
F.x/ is a non-decreasing function of x that starts at zero and has its highest value
of one.
</p>
<p>Example 2.2 The exponential random variable follows the probability distribution
function defined by
</p>
<p>f .x/ D ï¿½eï¿½ï¿½x; x ï¿½ 0; (2.3)</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Random Variables and Their Distributions
</p>
<p>Fig. 2.2 The distribution function f .x/ (solid line) and the cumulative distribution function F.x/
(dashed line) for an exponential variable with ï¿½ D 0:5
</p>
<p>whereï¿½ is an adjustable parameter that must be positive. The probability distribution
function is therefore f .x/ D 0 for negative values of the variable. The cumulative
distribution function is given by
</p>
<p>F.x/ D 1ï¿½ eï¿½ï¿½x: (2.4)
</p>
<p>In Fig. 2.2 are drawn the probability distribution function f .x/ and the cumulative
distribution function F.x/ for an exponential variable with ï¿½ D 0:5. }
</p>
<p>2.3 Moments of a Distribution Function
</p>
<p>The probability distribution function f .x/ provides a complete description of the
random variable. It is convenient to find a few quantities that describe the salient
features of the distribution. The moment of order n, ï¿½n, is defined as
</p>
<p>ï¿½n D EÅXnï¿½ ï¿½
Z
</p>
<p>f .x/xndx: (2.5)
</p>
<p>The moment ï¿½n is also represented as EÅXnï¿½, the expectation of the function Xn. It
is possible to demonstrate, although mathematically beyond the scope of this book,
that the knowledge of moments of all orders is sufficient to determine uniquely
the distribution function [42]. This is an important fact, since it shifts the problem
of determining the distribution function to that of determining at least some of its
moments. Moreover, a number of distribution functions only have a few non-zero
moments, and this renders the task even more manageable.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Moments of a Distribution Function 21
</p>
<p>The moments or expectations of a distribution are theoretical quantities that can
be calculated from the probability distribution f .x/. They are parent quantities that
we wish to estimate via measurements. In the following we describe the two main
expectations, the mean and the variance, and the sample quantities that approximate
them, the sample mean and the sample variance. Chapter 5 describes a method to
justify the estimates of parent quantities via sample quantities.
</p>
<p>2.3.1 The Mean and the Sample Mean
</p>
<p>The moment of the first order is also known as the mean or expectation of the
random variable,
</p>
<p>ï¿½ D EÅXï¿½ D
Z C1
</p>
<p>ï¿½1
xf .x/dx: (2.6)
</p>
<p>The expectation is a linear operation and therefore satisfies the property that, e.g.,
</p>
<p>EÅaX C bYï¿½ D aEÅXï¿½C bEÅYï¿½; (2.7)
</p>
<p>where a and b are constants. This is a convenient property to keep in mind when
evaluating expectations of complex functions of a random variable X.
</p>
<p>To estimate the mean of a random variable, consider N measurements xi and
define the sample mean as
</p>
<p>x D 1
N
</p>
<p>NX
</p>
<p>iD1
xi: (2.8)
</p>
<p>To illustrate that the sample mean x defined by (2.8) is equivalent to the mean
ï¿½, consider a discrete variable, for which
</p>
<p>EÅXï¿½ D
MX
</p>
<p>jD1
f .xj/xj; (2.9)
</p>
<p>where f .xj/ is the probability distribution function and we have assumed that
the variable can only have M possible values. According to the classical
interpretation of the probability, the distribution function is given by
</p>
<p>f .xj/ D lim
N!1
</p>
<p>N.xj/
</p>
<p>N
;</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Random Variables and Their Distributions
</p>
<p>in which N.xj/ is the number of occurrence of the value xj. Since ËN.xj/xj is
the value obtained in N measurements, it is equivalent to Ëxi. Therefore the
sample mean will be identical to the parent mean in the limit of an infinite
number of measurements,
</p>
<p>lim
N!1 x D limN!1
</p>
<p>1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
xi D lim
</p>
<p>N!1
1
</p>
<p>N
</p>
<p>MX
</p>
<p>jD1
N.xj/xj D
</p>
<p>MX
</p>
<p>jD1
f .xj/xj D EÅXï¿½:
</p>
<p>A proof that the sample mean provides an unbiased estimate of the mean
will be given in Chap. 5 for Gaussian and Poisson variables.
</p>
<p>The sample mean is therefore a representative value of the random variable that
estimates the parent mean using a finite number of measurements. Other measures of
a random variable include the mode, defined as the value of maximum probability,
and the median, defined as the value that separates the lower 50 % and the upper
50 % of the distribution function. For distributions that are symmetric with respect to
the peak value, as is the case for the Gaussian distribution defined below in Sect. 3.2,
the peak value coincides with the mean, median, and mode. A more detailed analysis
of the various measures of the &ldquo;average&rdquo; value of a variable is described in Chap. 6.
</p>
<p>2.3.2 The Variance and the Sample Variance
</p>
<p>The variance is the expectation of the square of the deviation of X from its mean:
</p>
<p>Var.X/ D EÅ.X ï¿½ ï¿½/2ï¿½ D
Z C1
</p>
<p>ï¿½1
.x ï¿½ ï¿½/2f .x/dx D ï¿½2: (2.10)
</p>
<p>The square root of the variance is referred to as the standard deviation or
standard error ï¿½ and it is a common measure of the average difference of a given
measurement xi from the mean of the random variable. Notice that from the point of
view of physical dimensions of the moments defined by (2.5), moments of the n-th
order have the dimensions of the random variable to the n-th power. For example,
if X is measured in meters, the variance is measured in meters square (m2), thus the
need to use the square root of the variance as a measure of the standard deviation of
the variable from its mean.
</p>
<p>The main reason for defining the average difference of a measurement from
its mean in terms of a moment of the second order is that the expectation of the
deviation X ï¿½ ï¿½ is always zero, as can be immediately seen using the linearity
property of the expectation. The deviation of a random variable is therefore not of
common use in statistics, since its expectation is null.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 A Classic Experiment: J.J. Thomson&rsquo;s Discovery of the Electron 23
</p>
<p>The sample variance is defined as
</p>
<p>s2 D 1
N ï¿½ 1
</p>
<p>NX
</p>
<p>iD1
.xi ï¿½ x/2 (2.11)
</p>
<p>and a proof that this quantity is an unbiased estimate of the parent variance will
be provided in Chap. 5. The presence of a factor of N ï¿½ 1, and not just N, in the
denominator of the sample variance, is caused by the fact that the sample variance
requires also an estimate of the sample mean, since the exact value of the parent
mean is unknown. This result will be explained further in Sect. 5.1.2.
</p>
<p>Using the linear property of the expectation, it is straightforward to show that the
following property applies:
</p>
<p>Var.X/ D EÅX2ï¿½ ï¿½ ï¿½2: (2.12)
</p>
<p>This relationship is very convenient to calculate the variance from the moments of
the first and second order. The deviation and the variance are moments calculated
with respect to the mean, also referred to as central moments.
</p>
<p>Another useful property of the variance, which follows from the fact that the
variance is a moment of the second order, is
</p>
<p>Var.aX/ D a2Var.X/ (2.13)
</p>
<p>where a is a constant.
</p>
<p>2.4 A Classic Experiment: J.J. Thomson&rsquo;s Discovery
of the Electron
</p>
<p>A set of experiments by J.J. Thomson in the late nineteenth century were
aimed at the measurement of the ratio between the mass and charge of a
new lightweight particle, which was later named electron. The experiment
was truly groundbreaking not just for the method used, but also because it
revolutionized our understanding of physics and natural sciences by proving
that the new particle was considerably lighter than the previously known
charge carrier, the proton.
</p>
<p>The experiment described in this book was reported by Thomson in [39].
It consists of measuring the deflection of negatively charged cathode rays by
a magnetic field H in a tube. Thomson wanted to measure the mass m of the
charged particles that constituted these cathode rays. The experiment is based
on the measurement of the following quantities: W is the kinetic energy of the
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Random Variables and Their Distributions
</p>
<p>particles, Q D Ne is the amount of electricity carried by the particles (N is the
number of particles and e the charge of each particle) and I D HR, where R
is the radius of curvature of the path of these rays in a magnetic field H. The
measurements performed by Thomson were used to infer the ratio m=e and
the speed v of the new lightweight particle according to
</p>
<p>v D 2W
QI
I
</p>
<p>m
</p>
<p>e
D I
</p>
<p>2Q
</p>
<p>2W
:
</p>
<p>(2.14)
</p>
<p>For the purpose of the data analysis of this experiment, it is only necessary
to know that W=Q and I are the primary quantities being measured, and
inferences on the secondary quantities of interest are based on (2.14). For the
proton, the mass-to-charge ratio was known to be approximately1	10ï¿½4 g per
electromagnetic (EMU) charge unit, where the EMU charge unit is equivalent
to 10ï¿½10 electrostatic charge units, or ESU (a more common unit of measure
for charge). In Thomson&rsquo;s units, the accepted value of the mass to charge ratio
of the electron is now 5:7	 10ï¿½8. Some of the experimental data collected by
Thomson are reported in Tables 2.1 and 2.2, in which &ldquo;gas&rdquo; refers to the gas
used in the tubes he used for the experiment.
</p>
<p>Some of Thomson&rsquo;s conclusions are reported here:
</p>
<p>(a) &ldquo;It will be seen from these tables that the value of m=e is independent of
the nature of the gas&rdquo;;
</p>
<p>(b) &ldquo;the values of m=e were, however, the same in the two tubes.&rdquo;;
(c) &ldquo;for the first tube, the mean for air is 0:40	10ï¿½7, for hydrogen 0:42	10ï¿½7
</p>
<p>and for carbonic acid 0:4 	 10ï¿½7&rdquo;;
(d) &ldquo;for the second tube, the mean for air is 0:52	10ï¿½7, for hydrogen 0:50	
</p>
<p>10ï¿½7 and for carbonic acid 0:54 	 10ï¿½7&rdquo;.
Using the equations for sample mean and variance explained in Sect. 2.3,
</p>
<p>we are already in a position to measure the sample means and variances in air
as m=e1 D 0:42 and s21 D 0:005 for Tube 1, x2 D 0:52 and s22 D 0:003 for
Tube 2. These statistics can be reported as a measurement of 0:42Ë 0:07 for
Tube 1 and 0:52Ë 0:06 for Tube 2. To make more quantitative statements on
the statistical agreement between the two measurements, we need to know
what is the probability distribution function of the sample mean. The test
to determine whether the two measurements are consistent with each other
will be explained in Sect. 7.5. For now, we simply point out that the fact that
the range of the two measurements overlap, is an indication of the statistical
agreement of the two measurements.
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 A Classic Experiment: J.J. Thomson&rsquo;s Discovery of the Electron 25
</p>
<p>Note: The three measurements marked with a star appear to have value of
v or m=e that are inconsistent with the formulas to calculate them from W=Q
and I. They may be typographical errors in the original publication. The first
appears to be a typo in W=Q (6	1012 should be 6	1011), the corrected value
is assumed throughout this book. The second has an inconsistent value for v
(should be 6:5	109, not 7:5	109), the third has inconsistent values for both v
and m=e, but no correction was applied in these cases to the data in the tables.
</p>
<p>Table 2.1 Data from Thomson&rsquo;s measurements of Tube 1
</p>
<p>Gas W=Q I m=e v
</p>
<p>Tube 1
</p>
<p>Air . . . . . 4:6 ï¿½ 1011 230 0:57ï¿½ 10ï¿½7 4ï¿½ 109
Air . . . . . 1:8 ï¿½ 1012 350 0:34ï¿½ 10ï¿½7 1ï¿½ 1010
Air . . . . . 6:1 ï¿½ 1011 230 0:43ï¿½ 10ï¿½7 5:4 ï¿½ 109
Air . . . . . 2:5 ï¿½ 1012 400 0:32ï¿½ 10ï¿½7 1:2 ï¿½ 1010
Air . . . . . 5:5 ï¿½ 1011 230 0:48ï¿½ 10ï¿½7 4:8ï¿½ 109
Air . . . . . 1ï¿½ 1012 285 0:4ï¿½ 10ï¿½7 7ï¿½ 109
Air . . . . . 1ï¿½ 1012 285 0:4ï¿½ 10ï¿½7 7ï¿½ 109
Hydrogen? . 6ï¿½ 1012 205 0:35ï¿½ 10ï¿½7 6ï¿½ 109
Hydrogen . . 2:1 ï¿½ 1012 460 0:5ï¿½ 10ï¿½7 9:2 ï¿½ 109
Carbonic acid? 8:4 ï¿½ 1011 260 0:4ï¿½ 10ï¿½7 7:5 ï¿½ 109
Carbonic acid 1:47 ï¿½ 1012 340 0:4ï¿½ 10ï¿½7 8:5 ï¿½ 109
Carbonic acid 3:0 ï¿½ 1012 480 0:39ï¿½ 10ï¿½7 1:3 ï¿½ 1010
</p>
<p>See Note for meaning of ?
</p>
<p>Table 2.2 Data from Thomson&rsquo;s measurements of Tube 2
</p>
<p>Gas W=Q I m=e v
</p>
<p>Tube 2
</p>
<p>Air . . . . 2:8ï¿½ 1011 175 0:53ï¿½ 10ï¿½7 3:3 ï¿½ 109
Air? . . . . 2:8ï¿½ 1011 175 0:47ï¿½ 10ï¿½7 4:1ï¿½ 109
Air . . . . 3:5ï¿½ 1011 181 0:47ï¿½ 10ï¿½7 3:8 ï¿½ 109
Hydrogen . 2:8ï¿½ 1011 175 0:53ï¿½ 10ï¿½7 3:3 ï¿½ 109
Air . . . . 2:5ï¿½ 1011 160 0:51ï¿½ 10ï¿½7 3:1 ï¿½ 109
Carbonic acid 2:0ï¿½ 1011 148 0:54ï¿½ 10ï¿½7 2:5ï¿½ 109
Air . . . . 1:8ï¿½ 1011 151 0:63ï¿½ 10ï¿½7 2:3ï¿½ 109
Hydrogen . 2:8ï¿½ 1011 175 0:53ï¿½ 10ï¿½7 3:3 ï¿½ 109
Hydrogen . 4:4ï¿½ 1011 201 0:46ï¿½ 10ï¿½7 4:4ï¿½ 109
Air . . . . 2:5ï¿½ 1011 176 0:61ï¿½ 10ï¿½7 2:8ï¿½ 109
Air . . . . 4:2ï¿½ 1011 200 0:48ï¿½ 10ï¿½7 4:1ï¿½ 109
</p>
<p>See Note for meaning of ?</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Random Variables and Their Distributions
</p>
<p>2.5 Covariance and Correlation Between Random Variables
</p>
<p>It is common to measure more than one random variable in a given experiment. The
variables are often related to one another and it is therefore necessary to define a
measure of how one variable affects the measurement of the others. Consider the
case in which we wish to measure both the length of one side of a square and the
area; it is clear that the two quantities are related in a way that the change of one
quantity affects the other in the same manner, i.e., a positive change of the length
of the side results in a positive change of the area. In this case, the length and the
area will be said to have a positive correlation. In this section we introduce the
mathematical definition of the degree of correlation between variables.
</p>
<p>2.5.1 Joint Distribution and Moments of Two Random
Variables
</p>
<p>When two (or more) variables are measured at the same time via a given experiment,
we are interested in knowing what is the probability of a given pair of measure-
ments for the two variables. This information is provided by the joint probability
distribution function, indicated as h.x; y/, with the meaning that h.x; y/dxdy is the
probability that the two variables X and Y are in a two-dimensional interval of size
dxdy around the value .x; y/. This two-dimensional function can be determined
experimentally via its sample distribution, in the same way as one-dimensional
distributions.
</p>
<p>It is usually convenient to describe one variable at a time, even if the experiment
features more than just one variable. In this case, the expectation of each variable
(for example, X) is defined as
</p>
<p>EÅXï¿½ D
Z C1
</p>
<p>ï¿½1
</p>
<p>Z C1
</p>
<p>ï¿½1
xh.x; y/dxdy D ï¿½x (2.15)
</p>
<p>and the variance is similarly defined as
</p>
<p>EÅ.X ï¿½ ï¿½x/2ï¿½ D
Z C1
</p>
<p>ï¿½1
</p>
<p>Z C1
</p>
<p>ï¿½1
.x ï¿½ ï¿½x/2h.x; y/dxdy D ï¿½2x : (2.16)
</p>
<p>These equations recognize the fact that the other variable, in this case Y, is indeed
part of the experiment, but is considered uninteresting for the calculation at hand.
Therefore the uninteresting variable is integrated over, weighted by its probability
distribution function.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Covariance and Correlation Between Random Variables 27
</p>
<p>The covariance of two random variables is defined as
</p>
<p>Cov.X;Y/ ï¿½ EÅ.X ï¿½ ï¿½x/.Y ï¿½ ï¿½y/ï¿½ D
Z C1
</p>
<p>ï¿½1
</p>
<p>Z C1
</p>
<p>ï¿½1
.x ï¿½ ï¿½x/.y ï¿½ ï¿½y/h.x; y/dxdy D ï¿½2xy:
</p>
<p>(2.17)
</p>
<p>The covariance is the expectation of the product of the deviations of the two
variables. Unlike the deviation of a single variable, whose expectation is always
zero, this quantity will be positive if, on average, a positive deviation of X is
accompanied by a positive deviation of Y, or if two negative deviations are likely
to occur simultaneously, so that the integrand is a positive quantity. If, on the other
hand, the two variables tend to have deviations of opposite sign, the covariance will
be negative. The covariance, like the mean and variance, is a parent quantity that
can be calculated from the theoretical distribution of the random variables.
</p>
<p>The sample covariance for a collection of N pairs of measurements is calcu-
lated as
</p>
<p>s2xy D
1
</p>
<p>N ï¿½ 1
NX
</p>
<p>iD1
.xi ï¿½ x/.yi ï¿½ y/; (2.18)
</p>
<p>using a similar equation to the sample variance.
The correlation coefficient ï¿½ is simply a normalized version of the covariance,
</p>
<p>ï¿½.X;Y/ D Cov.X;Y/
ï¿½xï¿½y
</p>
<p>: (2.19)
</p>
<p>The correlation coefficient is a number between ï¿½1 and C1. When the correlation
is zero, the two variables are said to be uncorrelated. The fact that the correlation
coefficient is normalized to within the values Ë1 derives from (2.10) and the
properties of the joint distribution function.
</p>
<p>The sample correlation coefficient is naturally defined as
</p>
<p>r D s
2
xy
</p>
<p>sxsy
(2.20)
</p>
<p>in which s2x and s
2
y are the sample variances of the two variables.
</p>
<p>The covariance between two random variables is very important in evaluating the
variance in the sum (or any other function) of two random variables, as explained in
detail in Chap. 4. The following examples illustrate the calculation of the covariance
and the sample covariance.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Random Variables and Their Distributions
</p>
<p>Example 2.3 (Variance of Sum of Variables) Consider the random variables X, Y
and the sum Z D X C Y: the variance is given by
</p>
<p>Var.Z/ D
Z Z
</p>
<p>.xC y ï¿½ .ï¿½x C ï¿½y//2h.x; y/dxdy D
</p>
<p>Var.X/C Var.Y/C 2Cov.X;Y/
</p>
<p>which can also be written in the compact form ï¿½2z D ï¿½2x C ï¿½2y C 2ï¿½2xy. This shows
that variances add linearly only if the two random variables are uncorrelated. Failure
to check for correlation will result in errors in the calculation of the variance of the
sum of two random variables. }
Example 2.4 Consider the measurement of the following pairs of variables: (0, 2),
(2, 5), (1, 4), (ï¿½3, ï¿½1). We can calculate the sample covariance by means of the
following equation:
</p>
<p>s2xy D
1
</p>
<p>3
</p>
<p>4X
</p>
<p>iD1
.xi ï¿½ x/.yi ï¿½ y/ D 17
</p>
<p>3
</p>
<p>where x D 0 and y D 2:5. Also, the individual variances are calculated as
</p>
<p>s2x D
1
</p>
<p>3
</p>
<p>4X
</p>
<p>iD1
.xi ï¿½ x/2 D 14
</p>
<p>3
</p>
<p>s2y D
1
</p>
<p>3
</p>
<p>X
.yi ï¿½ y/2 D 21
</p>
<p>3
</p>
<p>which results in the sample correlation coefficient between the two random vari-
ables of
</p>
<p>r D 17p
14 	 21 D 0:99:
</p>
<p>This is in fact an example of nearly perfect correlation between the two variables.
In fact, positive deviations of one variable from the sample mean are accompanied
by positive deviations of the other by nearly the same amount. }
</p>
<p>2.5.2 Statistical Independence of Random Variables
</p>
<p>The independence between events was described and quantified in Chap. 1, where
it was shown that two events are independent only when the probability of their
intersection is the product of the individual probabilities. The concept is extended
here to random variables by defining two random variables as independent if and</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Covariance and Correlation Between Random Variables 29
</p>
<p>only if the joint probability distribution function can be factored in the following
form:
</p>
<p>h.x; y/ D f .x/ ï¿½ g.y/; (2.21)
</p>
<p>where f .x/ and g.y/ are the probability distribution functions of the two random
variables. When two variables are independent, the individual probability distribu-
tion function of each variable is obtained via marginalization of the joint distribution
with respect to the other variable, e.g.,
</p>
<p>f .x/ D
Z C1
</p>
<p>ï¿½1
h.x; y/dy: (2.22)
</p>
<p>It is important to remark that independence between random variables and
uncorrelation are not equivalent properties. Independence, which is a property of
the distribution functions, is a much stronger property than uncorrelation, which is
based on a statement that involves only moments. It can be proven that independence
implies uncorrelation, but not vice versa.
</p>
<p>Proof The fact that independence implies uncorrelation is shown by calculat-
ing the covariance of two independent random variables of joint distribution
function h.x; y/. The covariance is
</p>
<p>ï¿½2xy D
Z C1
</p>
<p>ï¿½1
</p>
<p>Z C1
</p>
<p>ï¿½1
.x ï¿½ ï¿½x/.y ï¿½ ï¿½y/h.x; y/dxdy D
</p>
<p>Z C1
</p>
<p>ï¿½1
.x ï¿½ ï¿½x/f .x/dx
</p>
<p>Z C1
</p>
<p>ï¿½1
.y ï¿½ ï¿½y/g.y/dy D 0;
</p>
<p>since each integral vanishes as the expectation of the deviation of a random
variable. ut
</p>
<p>As a counter-example of the fact that dependent variables can have non-
zero correlation factor, consider the case of a random variable X with a
distribution f .x/ that is symmetric around the origin, and another variable
Y D X2. They cannot be independent since they are functionally related, but
it will be shown that their covariance is zero. Symmetry about zero implies
ï¿½x D 0. The mean of Y is EÅYï¿½ D EÅX2ï¿½ D ï¿½2x since the mean of X is null.
From this, the covariance is given by
</p>
<p>Cov.X;Y/ D EÅX.Y ï¿½ ï¿½2X/ï¿½ D EÅX3 ï¿½ Xï¿½2x ï¿½ D EÅX3ï¿½ D 0
</p>
<p>due to the symmetry of f .x/. Therefore the two variables X and X2 are
uncorrelated, yet they are not independent.
</p>
<p>Example 2.5 (Photon Counting Experiment) A photon-counting experiment con-
sists of measuring the total number of photons in a given time interval and the</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Random Variables and Their Distributions
</p>
<p>number of background events detected by the receiver in the same time interval.
The experiment is repeated six times, by measuring simultaneously the total number
of counts T as .10; 13; 11; 8; 10; 14/ and the number of background counts B as
.2; 3; 2; 1; 1; 3/. We want to estimate the mean number of source photons and its
standard error.
</p>
<p>The random variable we seek to measure is S D Tï¿½B and the mean and variance
of this random variable can be easily shown to be
</p>
<p>ï¿½S D ï¿½T ï¿½ ï¿½B
ï¿½2S D ï¿½2T C ï¿½2B ï¿½ 2ï¿½2TB
</p>
<p>(the derivation is similar to that of Example 2.3). From the data, we measure the
sample means and variances as T D 11:0, B D 2:0, s2T D 4:8, s2B D 0:8 and the
sample covariance as s2TB D C1:6.
</p>
<p>Notice that the correlation coefficient between T and S, as estimated via the
measurements, is then given by corr.T;B/ D 1:6=p4:8 	 0:8 D 0:92, indicating a
strong degree of correlation between the two measurements. The measurements can
be summarized as
</p>
<p>ï¿½S D 11:0 ï¿½ 2:0 D 9:0
ï¿½2S D 4:8C 0:8ï¿½ 2 	 1:6 D 2:4
</p>
<p>and be reported as S D 9:00 Ë 1:55 counts (per time interval). Notice that if the
correlation between the two measurements had been neglected, then one would
(erroneously) report S D 9:00Ë 2:37, e.g., the standard deviation would be largely
overestimated. The correlation between total counts and background counts in this
example has a significant impact in the calculation of the variance of S and needs to
be taken into account. }
</p>
<p>2.6 A Classic Experiment: Pearson&rsquo;s Collection of Data
on Biometric Characteristics
</p>
<p>In 1903 K. Pearson published the analysis of a collection of biometric data on
more than 1000 families in the United Kingdom, with the goal of establishing
how certain characters, such as height, are correlated and inherited [33].
Prof. Pearson is also the inventor of the ï¿½2 test and a central figure in the
development of the modern science of statistics.
</p>
<p>Pearson asked a number of families, composed of at least the father,
mother, and one son or daughter, to perform measurements of height, span of
arms and length of left forearm. This collection of data resulted in a number
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 A Classic Experiment: Pearson&rsquo;s Collection of Data on Biometric. . . 31
</p>
<p>of tables, including some for which Pearson provides the distribution of two
measurements at a time. One such table is that reporting the mother&rsquo;s height
versus the father&rsquo;s height, Table 2.3.
</p>
<p>The data reported in Table 2.3 represent the joint probability distribution of
the two physical characters, binned in one-inch intervals. When a non-integer
count is reported (e.g., a value of 0:25, 0:5 or 0:75), we interpret it as meaning
that the original measurement fell exactly at the boundary between two cells,
although Pearson does not provide an explanation for non-integer values.
</p>
<p>For every column and row it is also reported the sum of all counts. The
bottom row in the table is therefore the distribution of the father&rsquo;s height,
irrespective of the mother&rsquo;s height, likewise the rightmost column is the
distribution of the mother&rsquo;s height, regardless of the father&rsquo;s height. The
process of obtaining a one-dimensional distribution from a multi-dimensional
illustrates the marginalization over certain variables that are not of interest.
In the case of the bottom column, the marginalization of the distribution was
done over the mother&rsquo;s height, to obtain the distribution of father&rsquo;s height.
</p>
<p>From Table 2.3 it is not possible to determine whether there is a correlation
between father&rsquo;s and mother&rsquo;s heights. In fact, according to (2.18), we would
need all 1079 pairs of height measurements originally collected by Pearson
to calculate the covariance. Since Pearson did not report these raw (i.e,
unprocessed) data, we cannot calculate either the covariance or the correlation
coefficient. The measurements reported by Pearson are in a format that goes
under the name of contingency table, consisting of a table with measurements
that are binned into suitable two-dimensional intervals.
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Random variable: A quantity that is not known exactly and is described
by a probability distribution function f .x/.
</p>
<p>ï¿½ Moments of a distribution: Expectations for the random variable or
functions of the random variable, such as the mean ï¿½ D EÅXï¿½ and the
variance ï¿½2 D EÅ.X ï¿½ ï¿½/2ï¿½.
</p>
<p>ï¿½ Sample mean and sample variance: Quantities calculated from the mea-
surements that are intended to approximate the corresponding parent
quantities (mean and variance).
</p>
<p>ï¿½ Joint distribution function: The distribution of probabilities for a pair of
variables.
</p>
<p>ï¿½ Covariance: A measure of the tendency of two variables to follow one
another, Cov.X;Y/ D EÅ.X ï¿½ ï¿½X/.Y ï¿½ ï¿½Y/ï¿½.
</p>
<p>ï¿½ Correlation coefficient: A normalized version of the covariance that takes
values between -1 (perfect anti-correlation) and +1 (perfect correlation).
</p>
<p>ï¿½ Statistically independent variables: Two variables whose joint probability
distribution function can be factored as h.x; y/ D f .x/g.y/.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 Random Variables and Their Distributions
</p>
<p>T
ab
</p>
<p>le
2.
</p>
<p>3
Jo
</p>
<p>in
td
</p>
<p>is
tr
</p>
<p>ib
ut
</p>
<p>io
n
</p>
<p>of
fa
</p>
<p>th
er
</p>
<p>&rsquo;s
he
</p>
<p>ig
ht
</p>
<p>(c
ol
</p>
<p>um
ns
</p>
<p>)
an
</p>
<p>d
m
</p>
<p>ot
he
</p>
<p>r&rsquo;
s
</p>
<p>he
ig
</p>
<p>ht
(r
</p>
<p>ow
s)
</p>
<p>fr
om
</p>
<p>Pe
ar
</p>
<p>so
n&rsquo;
</p>
<p>s
ex
</p>
<p>pe
ri
</p>
<p>m
en
</p>
<p>t,i
n
</p>
<p>in
ch
</p>
<p>es
</p>
<p>Fa
th
</p>
<p>er
&rsquo;s
</p>
<p>he
ig
</p>
<p>ht
</p>
<p>58
&ndash;
</p>
<p>59
&ndash;
</p>
<p>60
&ndash;
</p>
<p>61
&ndash;
</p>
<p>62
&ndash;
</p>
<p>63
&ndash;
</p>
<p>64
&ndash;
</p>
<p>65
&ndash;
</p>
<p>66
&ndash;
</p>
<p>67
&ndash;
</p>
<p>68
&ndash;
</p>
<p>69
&ndash;
</p>
<p>70
&ndash;
</p>
<p>71
&ndash;
</p>
<p>72
&ndash;
</p>
<p>73
&ndash;
</p>
<p>74
&ndash;
</p>
<p>75
&ndash;
</p>
<p>52
&ndash;5
</p>
<p>3
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>1
0
:5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
1
:5
</p>
<p>53
&ndash;5
</p>
<p>4
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
:5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
:5
</p>
<p>54
&ndash;5
</p>
<p>5
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
:2
5
</p>
<p>0
:2
5
</p>
<p>0
0
:5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
1
</p>
<p>55
&ndash;5
</p>
<p>6
0
</p>
<p>0
0
</p>
<p>0
:5
</p>
<p>1
0
</p>
<p>0
0
:2
5
</p>
<p>0
:2
5
</p>
<p>0
0
:5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
2
:5
</p>
<p>56
&ndash;5
</p>
<p>7
0
</p>
<p>0
0
</p>
<p>0
0
:7
5
</p>
<p>1
:2
5
</p>
<p>0
1
</p>
<p>1
:7
5
</p>
<p>1
:7
5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>6
:5
</p>
<p>Mother&rsquo;s height
</p>
<p>57
&ndash;5
</p>
<p>8
0
</p>
<p>0
0
</p>
<p>0
:2
5
</p>
<p>1
1
:2
5
</p>
<p>1
:5
</p>
<p>4
3
:2
5
</p>
<p>2
:5
</p>
<p>3
1
:2
5
</p>
<p>0
:5
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
1
8
:5
</p>
<p>58
&ndash;5
</p>
<p>9
0
</p>
<p>0
:2
5
0
:7
5
1
:2
5
</p>
<p>1
:2
5
</p>
<p>2
:7
5
</p>
<p>4
7
</p>
<p>5
:7
5
</p>
<p>4
:5
</p>
<p>3
:7
5
</p>
<p>1
:2
5
</p>
<p>2
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>3
4
:5
</p>
<p>59
&ndash;6
</p>
<p>0
0
</p>
<p>1
:2
5
1
:2
5
1
</p>
<p>4
4
:5
</p>
<p>7
:7
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>1
6
:7
5
</p>
<p>9
5
:5
</p>
<p>3
:2
5
</p>
<p>1
:2
5
</p>
<p>1
0
:5
</p>
<p>0
0
</p>
<p>8
2
</p>
<p>60
&ndash;6
</p>
<p>1
0
:2
5
0
:2
5
0
:5
</p>
<p>2
4
:2
5
</p>
<p>4
:5
</p>
<p>1
8
</p>
<p>1
6
</p>
<p>2
4
</p>
<p>1
4
:7
5
</p>
<p>2
3
:2
5
</p>
<p>1
2
:7
5
</p>
<p>7
:2
5
</p>
<p>5
:7
5
</p>
<p>4
:2
5
</p>
<p>0
:7
5
0
</p>
<p>0
1
3
8
:5
</p>
<p>61
&ndash;6
</p>
<p>2
0
:2
5
0
:2
5
0
</p>
<p>0
8
</p>
<p>8
:2
5
1
5
</p>
<p>1
7
:2
5
</p>
<p>2
5
</p>
<p>2
0
:7
5
</p>
<p>2
4
</p>
<p>1
4
:2
5
</p>
<p>1
4
:2
5
1
0
</p>
<p>4
0
:7
5
0
:5
</p>
<p>0
1
6
2
:5
</p>
<p>62
&ndash;6
</p>
<p>3
0
</p>
<p>0
:5
</p>
<p>0
:5
</p>
<p>1
:2
5
</p>
<p>4
:7
5
</p>
<p>7
:7
5
1
0
</p>
<p>2
6
</p>
<p>2
1
:2
5
</p>
<p>2
8
</p>
<p>2
8
</p>
<p>2
3
</p>
<p>1
4
:2
5
1
0
:7
5
</p>
<p>4
:5
</p>
<p>2
1
</p>
<p>0
:5
</p>
<p>1
8
4
</p>
<p>63
&ndash;6
</p>
<p>4
0
</p>
<p>0
0
:2
5
2
</p>
<p>3
:5
</p>
<p>4
:5
</p>
<p>9
2
1
</p>
<p>1
5
:7
5
</p>
<p>2
0
:7
5
</p>
<p>1
9
:5
</p>
<p>2
4
</p>
<p>2
2
:5
</p>
<p>1
0
:7
5
</p>
<p>4
2
:2
5
2
:2
5
0
:5
</p>
<p>1
6
2
:5
</p>
<p>64
&ndash;6
</p>
<p>5
0
</p>
<p>0
1
:2
5
0
:7
5
</p>
<p>2
6
</p>
<p>6
:5
</p>
<p>9
:7
5
</p>
<p>1
6
</p>
<p>1
8
:2
5
</p>
<p>2
3
</p>
<p>1
6
:7
5
</p>
<p>1
3
:7
5
</p>
<p>6
:7
5
</p>
<p>4
:7
5
</p>
<p>2
:2
5
0
:2
5
1
:5
</p>
<p>1
2
9
:5
</p>
<p>65
&ndash;6
</p>
<p>6
0
</p>
<p>0
0
</p>
<p>0
:2
5
</p>
<p>1
:5
</p>
<p>1
:5
</p>
<p>3
:2
5
</p>
<p>5
:5
</p>
<p>9
:7
5
</p>
<p>7
1
5
:5
</p>
<p>1
2
:7
5
</p>
<p>1
0
:5
</p>
<p>6
:2
5
</p>
<p>4
:2
5
</p>
<p>1
:7
5
0
:2
5
0
</p>
<p>8
0
</p>
<p>66
&ndash;6
</p>
<p>7
0
</p>
<p>0
0
</p>
<p>0
:2
5
</p>
<p>1
0
:7
5
</p>
<p>0
:5
</p>
<p>3
:5
</p>
<p>5
3
</p>
<p>7
:2
5
</p>
<p>7
:7
5
</p>
<p>7
3
:5
</p>
<p>2
:7
5
</p>
<p>1
:5
</p>
<p>0
:2
5
0
</p>
<p>4
4
</p>
<p>67
&ndash;6
</p>
<p>8
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>1
2
:5
</p>
<p>1
:5
</p>
<p>2
:7
5
</p>
<p>3
:2
5
</p>
<p>2
:7
5
</p>
<p>1
:5
</p>
<p>1
0
:5
</p>
<p>0
:2
5
0
</p>
<p>1
7
</p>
<p>68
&ndash;6
</p>
<p>9
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>1
2
:5
</p>
<p>1
:2
5
</p>
<p>1
:2
5
</p>
<p>0
:5
</p>
<p>1
0
:2
5
0
:2
5
0
</p>
<p>8
</p>
<p>69
&ndash;7
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
:2
5
</p>
<p>2
:2
5
</p>
<p>0
2
</p>
<p>0
0
</p>
<p>0
4
:5
</p>
<p>70
&ndash;7
</p>
<p>1
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
0
</p>
<p>0
1
</p>
<p>0
0
:5
</p>
<p>0
0
</p>
<p>0
1
:5
</p>
<p>0
:5
</p>
<p>2
:5
</p>
<p>4
:5
</p>
<p>9
:5
</p>
<p>3
3
</p>
<p>4
3
</p>
<p>7
5
:5
</p>
<p>1
2
3
:5
</p>
<p>1
4
6
:5
</p>
<p>1
4
0
:5
</p>
<p>1
6
2
:5
</p>
<p>1
2
4
</p>
<p>1
0
2
:5
</p>
<p>5
7
</p>
<p>3
4
</p>
<p>1
2
:5
</p>
<p>5
2
:5
</p>
<p>1
0
7
9</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 A Classic Experiment: Pearson&rsquo;s Collection of Data on Biometric. . . 33
</p>
<p>Problems
</p>
<p>2.1 Consider the exponential distribution
</p>
<p>f .x/ D ï¿½eï¿½ï¿½x
</p>
<p>where ï¿½ ï¿½ 0 and x ï¿½ 0. Show that the distribution is properly normalized, and
calculate the mean, variance and cumulative distribution F.x/.
</p>
<p>2.2 Consider the sample mean as a random variable defined by
</p>
<p>x D 1
N
</p>
<p>NX
</p>
<p>iD1
xi (2.23)
</p>
<p>where xi are identical independent random variables with mean ï¿½ and variance ï¿½2.
Show that the variance of x is equal to ï¿½2=N.
</p>
<p>2.3 J.J. Thomson&rsquo;s experiment aimed at the measurement of the ratio between the
mass and charge of the electron is presented on page 23. Using the datasets for Tube
1 and Tube 2 separately, calculate the mean and variance of the random variables
W=Q and I, and the covariance and correlation coefficient between W=Q and I.
</p>
<p>2.4 Using J.J. Thomson&rsquo;s experiment (page 23), verify the statement that &ldquo;It will
be seen from these tables that the value of m=e is independent of the nature of
the gas&rdquo; used in the experiment. You may do so by calculating the mean and
standard deviation for the measurements in each gas (air, hydrogen, and carbonic
acid) and testing whether the three measurements agree with each other within their
standard deviations.
</p>
<p>2.5 Calculate the sample covariance and correlation coefficient for the following
set of data: .0; 2/; .2; 5/; .1; 4/; .3; 1/.
</p>
<p>2.6 Prove that the following relationship holds,
</p>
<p>Var.X/ D EÅX2ï¿½ ï¿½ ï¿½2
</p>
<p>where ï¿½ is the mean of the random variable X.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 3
Three Fundamental Distributions: Binomial,
Gaussian, and Poisson
</p>
<p>Abstract There are three distributions that play a fundamental role in statistics. The
binomial distribution describes the number of positive outcomes in binary experi-
ments, and it is the &ldquo;mother&rdquo; distribution from which the other two distributions
can be obtained. The Gaussian distribution can be considered as a special case of
the binomial, when the number of tries is sufficiently large. For this reason, the
Gaussian distribution applies to a large number of variables, and it is referred to as
the normal distribution. The Poisson distribution applies to counting experiments,
and it can be obtained as the limit of the binomial distribution when the probability
of success is small.
</p>
<p>3.1 The Binomial Distribution
</p>
<p>Many experiments can be considered as binary, meaning that they can only have
two possible outcomes which we can interpret as success or failure. Even complex
experiments with a larger number of possible outcomes can be described as binary,
when one is simply interested about the occurrence of a specific event A, or its non-
occurrence, A. It is therefore of fundamental importance in statistics to determine
the properties of binary experiments, and the distribution of the number of successes
when the experiment is repeated for a number of times under the same experimental
conditions.
</p>
<p>3.1.1 Derivation of the Binomial Distribution
</p>
<p>Consider a binary experiment characterized by a probability of success p a therefore
a probability of failure q D 1ï¿½p. The probabilities p and q are determined according
to the theory of probability and are assumed to be known for the experiment being
considered. We seek the probability of having n successes in N tries, regardless of
the order in which the successes take place. For example, consider tossing four
coins, and being interested in any two of these coins showing heads up, as an
indication of success of the toss. To obtain this probability, we start by counting
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_3
</p>
<p>35</p>
<p/>
</div>
<div class="page"><p/>
<p>36 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>how many possible outcomes for the experiments are possible, and break down the
derivation into three parts:
</p>
<p>&bull; Probability of a specific sequence of n successes out of N tries. Assume that
successive experiments are independent, e.g., one tosses the same coin many
times, each time independently of each other. The probability of having n
successes and thereforeNï¿½n failures occurring in a specific sequence, is given by
</p>
<p>P.specific sequence of n successes/ D pn 	 qNï¿½n: (3.1)
</p>
<p>This result can be seen by using the property of independence among the N
events, of which the n successes carry a probability p, and the .N ï¿½ n/ failures a
probability q.
</p>
<p>Example 3.1 Considering the case of four coin tosses, the probability of a given
sequence, for example &ldquo;heads-tails-tails-heads,&rdquo; is .1=2/ 	 .1=2/ 	 .1=2/ 	 .1=2/ D
.1=2/4, since p D q D 1=2. Successive tosses are assumed to be independent. }
</p>
<p>&bull; Number of ordered sequences. We start by counting how many ordered sequences
exist that have n successes out of N tries. If there are no successes (n D 0), then
there is only one possible sequence with N failures. If n &gt; 0, each of the N tries
can yield the &ldquo;first&rdquo; success, and therefore there are N possibilities for what try
is the first success. Continuing on to the &ldquo;second&rdquo; success, there are only N ï¿½ 1
possibilities left for what trial will be the second success, and so on. This leads to
the following number of sequences containing n time-ordered successes, that is,
sequences for which we keep track of the order in which the successes occurred:
</p>
<p>Perm.n;N/ D N ï¿½ .N ï¿½ 1/ ï¿½ .N ï¿½ nC 1/ D NÅ 
.N ï¿½ n/Å  : (3.2)
</p>
<p>This is called the number of permutations of n successes out of N tries. This
method of counting sequences can also be imagined as the placement of each
success in a &ldquo;success box&rdquo;: the first place in this box can be filled in N different
ways, the second in (N ï¿½ 1) ways corresponding to the remaining tries, and so
on.
</p>
<p>Example 3.2 Consider the case of n D 2 successes out of N D 4 trials.
According to (3.2), the number of permutations is 4Å =2Å  D 12. We list explicitly
all 12 ordered sequences that give rise to 2 successes out of 4 tries in Table 3.1.
Symbol H1 denotes the &ldquo;first success,&rdquo; and H2 the &ldquo;second success.&rdquo; Consider,
for example, lines 5 and 8: both represent the same situation in which the coin 2
and 3 showed heads up, or success, and they are not really different sequences,
but the separate entries in this table are the result of our method of counting
time-ordered sequences. }</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 The Binomial Distribution 37
</p>
<p>Table 3.1 Permutations (ordered sequences) of 2 successes out of 4 tries
</p>
<p>Number of try Number of try
Sequence 1 2 3 4 Sequence 1 2 3 4
</p>
<p>1 H1 H2 &ndash; &ndash; 7 H2 &ndash; H1 &ndash;
</p>
<p>2 H1 &ndash; H2 &ndash; 8 &ndash; H2 H1 &ndash;
</p>
<p>3 H1 &ndash; &ndash; H2 9 &ndash; &ndash; H1 H2
4 H2 H1 &ndash; &ndash; 10 H2 &ndash; &ndash; H1
5 &ndash; H1 H2 &ndash; 11 &ndash; H2 &ndash; H1
6 &ndash; H1 &ndash; H2 12 &ndash; &ndash; H2 H1
</p>
<p>In reality, we are not interested in the time order in which the n successes
occur, since it is of no consequence whether the first or the Nth, or any other, try
is the &ldquo;first&rdquo; success. We must therefore correct for this artifact in the following.
</p>
<p>&bull; Number of sequences of n successes out of N tries (regardless of order). As it
is clear from the previous example, the number of permutations is not quite
the number we seek, since it is of no consequence which success happened
first. According to (3.2), there are nÅ  ways of ordering n successes among
themselves, or Perm.n; n/ D nÅ . Since all nÅ  permutations give rise to the
same practical situation of n successes, we need to divide the number of (time-
ordered) permutations by nÅ  in order to avoid double-counting of permutations
with successes in the same trial number. It is therefore clear that, regardless of
time order, the number of combinations of n successes out of N trials is
</p>
<p>C.n;N/ D Perm.n;N/
nÅ 
</p>
<p>D NÅ 
.N ï¿½ n/Å nÅ  ï¿½
</p>
<p> 
N
</p>
<p>n
</p>
<p>!
</p>
<p>: (3.3)
</p>
<p>The number of combinations is the number we seek, i.e., the number of possible
sequences of n successes in N tries.
</p>
<p>Example 3.3 Continue to consider the case of 2 successes out of 4 trials. There are
2Å  D 2 ways to order the 2 successes among themselves (either one or the other
is the first success). Therefore the number of combinations of 2 successes out of
4 trials is 6, and not 12. As indicated above, in fact, each sequence had its &ldquo;twin&rdquo;
sequence listed separately, and (3.3) correctly counts only different sequences. }
</p>
<p>According to the results obtained above, what remains to be done is to use the
probability of each sequence (3.1) and multiply it by the number of combinations in
(3.3) to obtain the overall probability of having n successes in N trials:
</p>
<p>P.n/ D
 
N
</p>
<p>n
</p>
<p>!
</p>
<p>pnqNï¿½n n D 0; : : : ;N: (3.4)</p>
<p/>
</div>
<div class="page"><p/>
<p>38 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>This distribution is known as the binomial distribution and it describes the probabil-
ity of n successes in N tries of a binary experiment. It is a discrete distribution that
is defined for non-negative values n 
 N. The factor in (3.3) is in fact the binomial
coefficient and it derives its name from its use in the binomial expansion
</p>
<p>.pC q/N D
NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!
</p>
<p>pnqNï¿½n: (3.5)
</p>
<p>3.1.2 Moments of the Binomial Distribution
</p>
<p>The moment of mth order for a discrete random variable X of distribution P.n/ is
given by
</p>
<p>EÅXmï¿½ D nm D
NX
</p>
<p>nD0
nmP.n/: (3.6)
</p>
<p>We can show that the mean and the second moment of the binomial distribution
are given by
</p>
<p>(
n D pN
n2 D n2 C pqN: (3.7)
</p>
<p>Proof Start with the mean,
</p>
<p>n D
NX
</p>
<p>nD0
P.n/n D
</p>
<p>NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!
</p>
<p>npnqNï¿½n D
NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!ï¿½
</p>
<p>p
@
</p>
<p>@p
</p>
<p>ï¿½
</p>
<p>pnqNï¿½nI
</p>
<p>in which we have introduced a linear operator p
@
</p>
<p>@p
that can be conveniently
</p>
<p>applied to the entire sum,
</p>
<p>n D p @
@p
</p>
<p>"
NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!
</p>
<p>pnqNï¿½n
#
</p>
<p>D p @
@p
.pC q/N D pN.pC q/Nï¿½1 D pN:</p>
<p/>
</div>
<div class="page"><p/>
<p>3.1 The Binomial Distribution 39
</p>
<p>The derivation for the moment n2 is similar:
</p>
<p>n2 D
NX
</p>
<p>nD0
P.n/n2 D
</p>
<p>NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!
</p>
<p>n2pnqNï¿½n D
NX
</p>
<p>nD0
</p>
<p> 
N
</p>
<p>n
</p>
<p>!ï¿½
</p>
<p>p
@
</p>
<p>@p
</p>
<p>ï¿½2
</p>
<p>qNï¿½n
</p>
<p>D
ï¿½
</p>
<p>p
@
</p>
<p>@p
</p>
<p>ï¿½2
</p>
<p>.pC q/N D p @
@p
</p>
<p>ï¿½
pN.pC q/Nï¿½1ï¿½ D
</p>
<p>p
ï¿½
N.pC q/Nï¿½1 C pN.N ï¿½ 1/.pC q/Nï¿½2ï¿½ D
</p>
<p>pN C p2N.N ï¿½ 1/ D pN C .pN/2 ï¿½ p2N D
n2 C p.1ï¿½ p/N D n2 C pqN:
</p>
<p>ut
It follows that the variance of the binomial distribution is given by
</p>
<p>ï¿½2 D EÅ.X ï¿½ n/2ï¿½ D pqN: (3.8)
</p>
<p>Equations (3.7) and (3.8) describe the most important features of the binomial
distribution, shown in Fig. 3.1 for the case of N D 10. The mean is naturally given
by the product of the number of tries N and the probability of success p in each of
the tries. The standard deviation ï¿½ measures the root mean square of the deviation
and it is the measurement of the width of the distribution.
</p>
<p>Example 3.4 (Probability of Overbooking) An airline knows that 5 % of the
persons making reservations will not show up at the gate. On a given flight that
</p>
<p>Fig. 3.1 Binomial distribution with p D q D 0:5 and N D 10. The dotted lines around the mean
mark the Ëï¿½ range</p>
<p/>
</div>
<div class="page"><p/>
<p>40 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>can seat 50 people, 52 tickets have been sold. Calculate the probability that there
will be a seat available for every passenger that will arrive at the gate.
</p>
<p>This is a binary experiment in which p D 0:95 is the probability that a passenger
will show. For that specific flight, N D 52 passenger have the choice of showing
(or not). The probability that there is a seat available for each passenger is therefore
given by P D 1 ï¿½ PN.52/C PN.51/, which is calculated as
</p>
<p>P D 1 ï¿½
 
52
</p>
<p>52
</p>
<p>!
</p>
<p>p52 ï¿½ 1 ï¿½
 
52
</p>
<p>51
</p>
<p>!
</p>
<p>p51 ï¿½ q D 1 ï¿½ .0:95/52 ï¿½ 52 ï¿½ .0:95/51 ï¿½ 0:05 D 0:741:
</p>
<p>Therefore the airline is willing to take a 25.9 % chance of having an overbooked
flight. }
</p>
<p>3.2 The Gaussian Distribution
</p>
<p>The Gaussian distribution, often referred to as the normal distribution, can be
considered as a special case of the binomial distribution in the case of a large number
N of experiments performed. In this section we derive the Gaussian distribution from
the binomial distribution and describe the salient features of the distribution.
</p>
<p>3.2.1 Derivation of the Gaussian Distribution
from the Binomial Distribution
</p>
<p>The binomial distribution of (3.4) acquires a simpler form when N is large. An
alternative analytic expression to the binomial distribution is a great advantage,
given the numerical difficulties associated with the evaluation of the factorial
of large numbers. As was evident from Fig. 3.1, the binomial distribution has a
maximum at value n D Np. In the following we prove that the binomial distribution
can be approximated as
</p>
<p>P.n/ ' 1p
2	Npq
</p>
<p>e
ï¿½ .nï¿½Np/
</p>
<p>2
</p>
<p>2NPq (3.9)
</p>
<p>in the case in which N ï¿½ 1, and for values of the variable that are close to the peak
of the distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 The Gaussian Distribution 41
</p>
<p>Proof Expand the logarithm of the binomial probability as a Taylor series in
the neighborhood of the peak value Qn,
</p>
<p>lnP.n/ D lnP.Qn/C
1X
</p>
<p>kD1
</p>
<p>Bk
kÅ 

nk
</p>
<p>where
n D n ï¿½ Qn is the deviation from the peak value and
</p>
<p>Bk D @ lnP.n/
k
</p>
<p>@kn
</p>
<p>Ë
Ë
Ë
Ë
nDQn
</p>
<p>is the coefficient of the Taylor series expansion. Since, by assumption, Qn is
a point of maximum, the first coefficient is null, @ lnP.n/=@njnDQn D 0. We
neglect terms of order O.
n3/ in the expansion, and the approximation results
in
</p>
<p>lnP.n/ ' lnP.Qn/C 1
2
B2
n
</p>
<p>2;
</p>
<p>where B2 is negative, since n D Qn is a point of maximum. It follows that
</p>
<p>P.n/ ' P.Qn/eï¿½
jB2j
n2
</p>
<p>2 :
</p>
<p>Neglecting higher-order terms in 
n means that the approximation will be
particularly accurate in regions where 
n is small, i.e., near the peak of the
distribution. Away from the peak, the approximation will not hold with the
same precision.
</p>
<p>In the following we show that the unknown terms can be calculated as
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>B2 D ï¿½ 1
Npq
</p>
<p>P.Qn/ D 1p
2	Npq
</p>
<p>:
</p>
<p>First, we calculate the value of jB2j. Start with
</p>
<p>lnP.n/ D ln
ï¿½
</p>
<p>NÅ 
</p>
<p>nÅ .N ï¿½ n/Å p
nqNï¿½n
</p>
<p>ï¿½
</p>
<p>D
</p>
<p>lnNÅ ï¿½ ln nÅ  ï¿½ ln.N ï¿½ n/Å C n ln pC .N ï¿½ n/ ln q:
</p>
<p>At this point we need to start treating n as a continuous variable. This
approximation is acceptable when Np ï¿½ 1, so that values n of the random</p>
<p/>
</div>
<div class="page"><p/>
<p>42 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>variable near the peak of the distribution are large numbers. In this case, we
can approximate the derivative of the logarithm with a difference,
</p>
<p>@ ln nÅ 
</p>
<p>@n
D .ln.nC 1/Å ï¿½ ln nÅ /=1 D ln.nC 1/ ' ln n:
</p>
<p>From this it follows that the first derivative of the probability function, as
expected, is zero at the peak value,
</p>
<p>@ lnP.n/
</p>
<p>@n
</p>
<p>Ë
Ë
Ë
Ë
nDQn
D ï¿½ ln nC ln.N ï¿½ n/C ln p ï¿½ ln qjnDQn
</p>
<p>D ln
ï¿½
N ï¿½ n
n
</p>
<p>p
</p>
<p>q
</p>
<p>ï¿½
</p>
<p>D 0
</p>
<p>so that the familiar result of Qn D p ï¿½N is obtained. This leads to the calculation
of the second derivative,
</p>
<p>B2 D @
2 lnP.n/
</p>
<p>@n2
</p>
<p>Ë
Ë
Ë
Ë
nDQn
D @
@n
</p>
<p>ln
</p>
<p>ï¿½
N ï¿½ n
n
</p>
<p>p
</p>
<p>q
</p>
<p>ï¿½Ë
Ë
Ë
Ë
nDQn
</p>
<p>D @
@n
.ln.N ï¿½ n/ï¿½ ln n/
</p>
<p>Ë
Ë
Ë
Ë
nDQn
D ï¿½ 1
</p>
<p>N ï¿½ n ï¿½
1
</p>
<p>n
</p>
<p>Ë
Ë
Ë
Ë
nDQn
</p>
<p>D ï¿½1Qn ï¿½
1
</p>
<p>N ï¿½ Qn D ï¿½
1
</p>
<p>Np
ï¿½ 1
</p>
<p>N.1 ï¿½ p/ D ï¿½
pC q
Npq
</p>
<p>D ï¿½ 1
Npq
</p>
<p>:
</p>
<p>Finally, the normalization constant P.Qn/ can be calculated making use of
the integral
</p>
<p>Z 1
</p>
<p>ï¿½1
eï¿½ax2dx D
</p>
<p>p
	=a:
</p>
<p>Enforcing the normalization condition of the probability distribution function,
</p>
<p>Z 1
</p>
<p>ï¿½1
P.Qn/eï¿½
</p>
<p>jB2j
n2
2 d
n D P.Qn/
</p>
<p>s
2	
</p>
<p>jB2j D 1
</p>
<p>we find that P.Qn/ D 1=p2	Npq. We are therefore now in a position to obtain
an approximation to the binomial distribution, valid when nï¿½ 1:
</p>
<p>P.n/ D 1p
2	Npq
</p>
<p>e
ï¿½ .nï¿½Np/
</p>
<p>2
</p>
<p>2NPq :
</p>
<p>ut</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 The Gaussian Distribution 43
</p>
<p>Using the fact that the mean of the distribution is ï¿½ D Np, and that the variance
is ï¿½2 D Npq, the approximation takes the form
</p>
<p>P.n/ D 1p
2	ï¿½2
</p>
<p>eï¿½
.nï¿½ï¿½/2
2ï¿½2 (3.10)
</p>
<p>which is the standard form of the Gaussian distribution, in which n is a continuous
variable. Equation (3.10) read as P.n/ being the probability of occurrence of the
value n for a given random variable of mean ï¿½ and variance ï¿½2. The Gaussian
distribution has the familiar &ldquo;bell&rdquo; shape, as shown in Fig. 3.2. When n becomes
a continuous variable, which we will call x, we talk about the probability of
occurrence of the variable in a given range x; x C dx. The Gaussian probability
distribution function is thus written as
</p>
<p>f .x/dx D 1p
2	ï¿½2
</p>
<p>eï¿½
.xï¿½ï¿½/2
2ï¿½2 dx: (3.11)
</p>
<p>A Gaussian of mean ï¿½ and variance ï¿½2 is often referred to as N.ï¿½; ï¿½/. The standard
Gaussian is one with zero mean and unit variance, indicated by N.0; 1/.
</p>
<p>Fig. 3.2 Gaussian distribution with ï¿½ D 50 and ï¿½2 D 12:5, corresponding to a binomial
distribution of p D q D 0:5, and N D 100. The Gaussian distribution is symmetrical around
the mean and therefore the mean, mode, and median coincide. The dotted lines around the mean
mark the Ëï¿½ range</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>3.2.2 Moments and Properties of the Gaussian Distribution
</p>
<p>The parameters ï¿½ and ï¿½2 are, respectively, the mean and variance of the Gaussian
distribution. These results follow from the derivation of the Gaussian distribution
from the binomial, and can be confirmed by direct calculation of expectations from
(3.11). Central moments of odd order are zero, since the Gaussian is symmetric with
respect to the mean.
</p>
<p>Given its wide use in statistics, it is important to quantify the &ldquo;effective width&rdquo;
of the Gaussian distribution around its mean. The half width at half maximum,
or HWHM, is defined as the range of x between the peak and the point where
P.x/ D 0:5P.ï¿½/. It can be easily shown that the HWHM has a size of approximately
1.18ï¿½ , meaning that the half-maximum point is just past one standard deviation of
the mean. By the same token, the full-width at half maximum, or FWHM, is defined
as the range between the two points where P.x/ D 0:5P.ï¿½/. It is twice the HWHM,
or 2.35ï¿½ in size. Tables of the Gaussian distribution are provided in Appendix A.1.
</p>
<p>The range between the points x D ï¿½Ë ï¿½ is a common measure of the effective
range of the random variable. The probability of a Gaussian variable to be in the
range from ï¿½ï¿½ï¿½ to ï¿½Cï¿½ is calculated as the integral of the probability distribution
function between those limits. In general, we define the integral
</p>
<p>A.z/ D
Z ï¿½Czï¿½
</p>
<p>ï¿½ï¿½zï¿½
f .x/dx D 1p
</p>
<p>2	
</p>
<p>Z z
</p>
<p>ï¿½z
eï¿½
</p>
<p>x2
</p>
<p>2 dx (3.12)
</p>
<p>where f .x/ is the Gaussian distribution; this integral is related to the error function,
</p>
<p>erf z D 1p
	
</p>
<p>Z z
</p>
<p>ï¿½z
eï¿½x2dx: (3.13)
</p>
<p>The function A.z/ is tabulated in Appendix A.1 The probability of the variable to
be within one ï¿½ of the mean is A.1/ D 0:683, or 68.3 %. The range of x between
ï¿½ ï¿½ ï¿½ and ï¿½ C ï¿½ therefore corresponds to a 68.3 % interval of probability, and it
is referred to as the 1ï¿½ interval. The correspondence between the 1ï¿½ interval and
the 68.3 % confidence interval applies strictly only to the Gaussian distribution, for
which the value of ï¿½ is defined via the distribution function. It is common practice,
however, to calculate to the 68.3 % interval (sometimes shortened to 68 %) even for
those random variables that do not strictly follow a Gaussian distribution, and refer
to it as the 1ï¿½ interval. The probability associated with characteristic intervals of a
Gaussian variable is also reported in Table 3.2.
</p>
<p>The cumulative distribution of a Gaussian random variable N.0; 1/ is defined by
the following integral:
</p>
<p>B.z/ D
Z z
</p>
<p>ï¿½1
1p
2	
</p>
<p>eï¿½
x2
</p>
<p>2 dxI (3.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 The Poisson Distribution 45
</p>
<p>Table 3.2 Probability
associated with characteristic
intervals of a Gaussian
distribution
</p>
<p>Interval Integrated probability
</p>
<p>ï¿½ï¿½ ï¿½; ï¿½C ï¿½ (1ï¿½ interval) 0.6827, or 68.27 %
ï¿½ï¿½ 2ï¿½; ï¿½C 2ï¿½ (2ï¿½ interval) 0.9545, or 95.45 %
ï¿½ï¿½ 3ï¿½; ï¿½C 3ï¿½ (3ï¿½ interval) 0.9973, or 99.73 %
ï¿½ï¿½ 4ï¿½; ï¿½C 4ï¿½ (4ï¿½ interval) 0.9999, or 99.99 %
</p>
<p>the integral can be calculated as B.z/ D 1=2CA.z/=2 for z &gt; 0 and it is tabulated in
Table A.3. For z &lt; 0, the table can be used to calculate the cumulative distribution
as B.z/ D 1 ï¿½ B.ï¿½z/.
</p>
<p>3.2.3 How to Generate a Gaussian Distribution
from a Standard Normal
</p>
<p>All Gaussian distributions can be obtained from the standard N.0; 1/ via a simple
change of variable. If X is a random variable distributed like N.ï¿½; ï¿½/, and Z a
standard Gaussian N.0; 1/, then the relationship between Z and X is given by
</p>
<p>Z D X ï¿½ ï¿½
ï¿½
</p>
<p>: (3.15)
</p>
<p>The variable Z is also referred to as the z-score associated with the variable X. This
equation means that if we can generate samples from a standard normal, we can also
have samples from any other Gaussian distribution. If we call z a sample from Z,
then
</p>
<p>x D ï¿½ ï¿½ zC ï¿½ (3.16)
</p>
<p>will be a sample drawn from X, according to the equation above. Many program-
ming languages have a built-in function to generate samples from a standard normal,
and this simple process can be used to generate samples from any other Gaussian. A
more general procedure to generate a given distribution from a uniform distribution
will be presented in Sect. 4.8.
</p>
<p>3.3 The Poisson Distribution
</p>
<p>The Poisson distribution describes the probability of occurrence of events in count-
ing experiments, i.e., when the possible outcome is an integer number describing
how many counts have been recorded. The distribution is therefore discrete and can
be derived as a limiting case of the binomial distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>3.3.1 Derivation of the Poisson Distribution
</p>
<p>The binomial distribution has another useful approximation in the case in which
pï¿½ 1, or when the probability of success is small. In this case, the number of
positive outcomes is much smaller than the number of tries, nï¿½ N, and the factorial
function can be approximated as
</p>
<p>NÅ  D N.N ï¿½ 1/ ï¿½ ï¿½ ï¿½ .N ï¿½ nC 1/ ï¿½ .N ï¿½ n/Å  ' Nn.N ï¿½ n/Å :
</p>
<p>We are also interested in finding an approximation for the qNï¿½n term that appears
in the binomial. For this we set
</p>
<p>ln qNï¿½n D ln.1 ï¿½ p/Nï¿½n D .N ï¿½ n/ ln.1 ï¿½ p/ ' ï¿½p.N ï¿½ n/ ' ï¿½pN;
</p>
<p>and therefore we obtain the approximation
</p>
<p>qNï¿½n ' eï¿½pN :
</p>
<p>These two approximations can be used into (3.4) to give
</p>
<p>P.n/ ' N
n.N ï¿½ n/Å 
</p>
<p>nÅ .N ï¿½ n/Å  p
neï¿½pN D .pN/
</p>
<p>n
</p>
<p>nÅ 
eï¿½pN : (3.17)
</p>
<p>Since pN is the mean of the distribution, we can rewrite our approximation as
</p>
<p>P.n/ D ï¿½
n
</p>
<p>nÅ 
eï¿½ï¿½; (3.18)
</p>
<p>known as the Poisson distribution. This function describes the probability of
obtaining n positive outcomes, or counts, when the expected number of outcomes is
ï¿½. It can be immediately seen that the distribution is properly normalized, since
</p>
<p>1X
</p>
<p>nD0
</p>
<p>ï¿½n
</p>
<p>nÅ 
D eï¿½:
</p>
<p>A fundamental feature of this distribution is that it is described by only one
parameter, the mean ï¿½, as opposed to the Gaussian distribution that had two
parameters. This clearly does not mean that the Poisson distribution has no
variance&mdash;in that case, it would not be a random variable!&mdash;but that the variance
can be written as function of the mean, as will be shown in the following.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 The Poisson Distribution 47
</p>
<p>3.3.2 Properties and Interpretation of the Poisson Distribution
</p>
<p>The approximations used in the derivation of (3.18) caused the loss of any reference
to the initial binomial experiment, and only the mean ï¿½ D Np is present. Using
the definition of mean and variance, it is easy to prove that the mean is indeed ï¿½,
and that the variance is also equal to the mean, Var.n/ D ï¿½2 D ï¿½. The fact that the
mean equals the variance can be seen using the values for the binomial,ï¿½ D Np and
ï¿½2 D Npq; since p ï¿½ 1, q ' 1, and ï¿½ ' ï¿½2. As a result, the Poisson distribution
has only one parameter.
</p>
<p>The Poisson distribution can be interpreted as the probability of occurrence of n
events in the case of an experiment that detects individual counts, when the mean of
the counts is ï¿½. This makes the Poisson distribution the primary statistical tool for
all experiments that can be expressed in terms of the counting of a specific variable
associated with the experiment. Typical examples are the counting of photons or the
counting of plants with a given characteristic, etc. When an experiment can be cast
in terms of a counting experiment, even without a specific reference to an underlying
binary experiment, then the Poisson distribution will apply. All reference to the total
number of possible events (N) and the probability of occurrence of each event (p)
was lost because of the approximation used throughout, i.e., p &lt;&lt; 1, and only the
mean ï¿½ remains to describe the primary property of the counting experiment, which
is the mean or expectation for the number of counts.
</p>
<p>As can be seen in Fig. 3.3, the Poisson distribution is not symmetric with respect
of the mean, and the distribution becomes more symmetric for larger values of
the mean. As for all discrete distributions, it is only meaningful to calculate the
probability at a specific point or for a set of points, and not for an interval of points
as in the case of continuous distributions. Moreover, the mean of the distribution
itself can be a non-integer number, and still the outcome of the experiment described
by the Poisson distribution can only take integer values.
</p>
<p>Example 3.5 Consider an astronomical source known to produce photons, which
are usually detected by a given detector in the amount of ï¿½ D 2:5 in a given time
interval. The probability of detecting n D 4 photons in a given time interval is
therefore
</p>
<p>P.4/ D 2:5
4
</p>
<p>4Å 
eï¿½2:5 D 0:134
</p>
<p>The reason for such apparently large probability of obtaining a measurement that
differs from the expected mean is simply due to the statistical nature of the detection
process. }</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>Fig. 3.3 Poisson distribution with ï¿½ D 2, corresponding to a binomial distribution with p D 0:2
and N D 10. The dotted lines represent the mean, the ï¿½ï¿½ ï¿½ and ï¿½C ï¿½ points
</p>
<p>3.3.3 The Poisson Distribution and the Poisson Process
</p>
<p>A more formal justification for the interpretation of the Poisson distribution as the
distribution of counting experiments comes from the Poisson process. Although
a complete treatment of this subject is beyond the scope of this book, a short
description of stochastic processes will serve to strengthen the interpretation of
(3.18), which is one of the foundations of statistics. More details on stochastic
processes can be found, for example, in the textbook by Ross [38].
</p>
<p>A stochastic counting process fN.t/; t &gt; 0g is a sequence of random variables
N.t/, in which t indicates time, and N.t/ is a random variable that indicates
the number of events occurred up to time t. The stochastic process can be
thought of as repeating the experiment of &ldquo;counting the occurrence of a given
event&rdquo; at various times t; N.t/ is the result of the experiment. The Poisson
process with rate ï¿½ is a particular type of stochastic process, with the following
properties:
</p>
<p>1. N.0/ D 0, meaning that at time 0 there are no counts detected.
2. The process has independent increments, meaning that N.t C s/ ï¿½ N.s/ is
</p>
<p>independent of N.t/; this is understood with the events occurring after time
t not being influenced by those occurring prior to it.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 The Poisson Distribution 49
</p>
<p>3. The process has stationary increments, i.e., the distribution of the number
of events in an interval of time s depends only on the length of the time
interval itself.
</p>
<p>4. P.N.h/ D 1/ D ï¿½h C o.h/ in which o.h/ is a function with the property
that
</p>
<p>lim
h!0
</p>
<p>o.h/
</p>
<p>h
D 0:
</p>
<p>5. P.N.h/ ï¿½ 2/ D o.h/. The latter two properties mean that the probability of
obtaining one count depends on the finite value ï¿½, while it is unlikely that
two or more events occur in a short time interval.
</p>
<p>It can be shown that under these hypotheses, the number of events N.t/
recorded in any interval of length t is Poisson distributed,
</p>
<p>PfN.tC s/ ï¿½ N.s/ D ng D .ï¿½t/
n
</p>
<p>nÅ 
eï¿½ï¿½t (3.19)
</p>
<p>This shows that the Poisson distribution is to be interpreted as the distribution
of occurrence of n events during a time interval t, under the hypothesis that
the rate of occurrence of events is ï¿½. This interpretation is identical to the
one provided above, given that ï¿½ D ï¿½t is the mean of the counts in that time
interval.
</p>
<p>3.3.4 An Example on Likelihood and Posterior Probability
of a Poisson Variable
</p>
<p>The estimation of parameters of a random variable, such as the mean of the Poisson
distribution, will be treated in full detail in Chap. 5. Here we present a simple
application that consists of using available measurements to calculate the likelihood
and to make inferences on the unknown value of the parent mean ï¿½ of a Poisson
variable. The following examples illustrate how a single measurement n of a Poisson
variable can be used to constrain the true mean ï¿½, and that care must be exercised
in not confusing the likelihood of a measurement with the posterior probability. We
assume for simplicity that the mean is an integer, although in general it may be any
real number.
</p>
<p>Within the Bayesian framework, a counting experiment can be written in terms
of a dataset B, consisting of the measurement n of the variable, and events Ai,
representing the fact that the parent mean is ï¿½ D i. It follows that the likelihood
can be written as
</p>
<p>P.B=Ai/ D i
n
</p>
<p>nÅ 
eï¿½i:</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>Example 3.6 (Calculation of Data Likelihood) A counting experiment results in
a detection of n D 4 units, and one wants to make a statement as to what is the
probability of such measurement. Using the Poisson distribution, the probability of
detecting 4 counts if, for example, ï¿½ D 0, 1, or 2, is given by the likelihood
</p>
<p>P.B=A012/ D
2X
</p>
<p>ï¿½D0
</p>
<p>ï¿½4
</p>
<p>4Å 
eï¿½ï¿½ D 0C 1
</p>
<p>4Å 
</p>
<p>1
</p>
<p>e
C 2
</p>
<p>4
</p>
<p>4Å 
</p>
<p>1
</p>
<p>e2
D 0:015C 0:091 D 0:106;
</p>
<p>or 10.6 %; this is a likelihood of the data with models that assume a specific value
for the mean. Notice that if the true value of the mean is zero, there is absolutely
no probability of detecting any counts. One can thus conclude that there is slightly
more than a 10 % chance of detecting 4 counts, given that the source truly emits 2
or fewer counts. This is not, however, a statement of possible values of the parent
mean ï¿½. }
</p>
<p>According to Bayes&rsquo; theorem, the posterior distributions are
</p>
<p>P.Ai=B/ D P.B=Ai/P.Ai/
P.B/
</p>
<p>where P.B=Ai/ is the likelihood, corresponding to each of the three terms in the
sum of the example above. In the following example, we determine posterior
probabilities.
</p>
<p>Example 3.7 (Posterior Probability of the Poisson Mean) We want to calculate
the probability of the true mean being less or equal than 2, P.A012=B/, and start
by calculating the likelihoods required to evaluate P.B/. We make an initial and
somewhat arbitrary assumption that the mean should be ï¿½ 
 10, so that only
11 likelihoods must be evaluated. This assumption is dictated simply by practical
considerations, and can also be stated in terms of assuming a subjective prior
knowledge that the mean is somehow known not to exceed 10. We calculate
</p>
<p>P.B/ '
10X
</p>
<p>iD0
</p>
<p>i4
</p>
<p>4Å 
eï¿½i 	 P.Ai/ D 0:979 	 P.Ai/
</p>
<p>Also, assuming uniform priors, we have P.Ai/ D 1=11 and that
</p>
<p>P.A012=B/ D
P.Ai/ 	P2iD0
</p>
<p>i4
</p>
<p>4Å 
eï¿½i
</p>
<p>P.Ai/ 	 0:979 D
1
</p>
<p>0:979
</p>
<p>2X
</p>
<p>iD0
</p>
<p>i4
</p>
<p>4Å 
eï¿½i D 0:108:
</p>
<p>}
The examples presented in this section illustrate the conceptual difference between
the likelihood calculation and the estimate of the posterior, though the two calcula-
tions yielded similar numerical values.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Comparison of Binomial, Gaussian, and Poisson Distributions 51
</p>
<p>3.4 Comparison of Binomial, Gaussian, and Poisson
Distributions
</p>
<p>In this section we provide numerical calculations that compare the binomial
and Gaussian functions, and also discuss under what circumstances the Poisson
distribution can be approximated by a Gaussian of same mean and variance. In fact
practical computations with the Poisson distribution are often hampered by the need
to calculate the factorial of large numbers. In Sect. 3.2 we derived the Gaussian
distribution from the binomial function, using the approximation that Np ï¿½ 1. In
fact we assumed that the function has values n ï¿½ 1 and, since the mean of the
binomial is ï¿½ D Np, the value Np sets the order of magnitude for the values of the
random variable that have non-negligible probability. In the left panel of Fig. 3.4 we
show the binomial distribution with parameters p D q D 0:5, showing that for Np D
5 the approximation is already at the level of 1 % near the peak of the distribution.
</p>
<p>The main limitation of the Poisson distribution (3.18) is the presence of the
factorial function, which becomes very rapidly a large number as function of the
integer n (for example, 20Å  D 2:423 	 1018), and it may lead to overflow problems
in numerical codes. For large values of n, one can use the Stirling approximation to
the factorial function, which retains only the first term of the following expansion:
</p>
<p>nÅ  D p2	n 	 nneï¿½n
ï¿½
</p>
<p>1C 1
12n
C : : :
</p>
<p>ï¿½
</p>
<p>: (3.20)
</p>
<p>Using this approximation for values of n ï¿½ 10, the right panel of Fig. 3.4 shows two
Poisson distributions with mean of, respectively, 3 and 20, and the corresponding
Gaussian distributions with the same mean and of variance equal to the mean, as is
the case for the Poisson distribution. The difference between the Gaussian and the
</p>
<p>Fig. 3.4 (Left) Binomial distributions with p D q D 0:5 and, respectively, N D 2 and N D 10 as
points connected by dashed line. Matching Gaussian distributions with same mean ï¿½ D Np and
variance ï¿½2 D Npq (solid lines). (Right) Gaussian distribution with ï¿½ D ï¿½2 D 3 and ï¿½ D ï¿½2 D
20 (solid lines) and Poisson distributions with same mean as points connected by a dotted line</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>Poisson distributions for a mean of ï¿½ D 20 is at the percent level near the peak of
the distribution. The Poisson distribution retains its characteristic asymmetry and a
heavier tail at large values, and therefore deviations between the two function are
larger away from the mean where, however, the absolute value of the probability
becomes negligible. It can also be shown that for the value of x D ï¿½, the two
distributions have the same value, when the Stirling approximation is used for the
factorial function. A rule of thumb used by many is that for x ï¿½ 20 the Gaussian
approximation to the Poisson distribution is acceptable.
</p>
<p>The approximation of a Poisson distribution with a Gaussian distribution is of
great practical importance. Consider a counting experiment in which N counts are
measured. The parent distribution of the random variable of interest is Poisson
distributed and it is reasonable to assume that the best estimate of its mean is ï¿½ D N
(but see Sect. 5.5.1 for a Bayesian approach that gives a slightly different answer).
For values of N &gt; 20 or so, the standard deviation of the parent Poisson distribution
is therefore ï¿½ D pN. The measurement can be reported at N Ë pN, where the
range of N ËpN corresponds to the ï¿½Ë 1ï¿½ interval for a Gaussian variable.
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Binomial distribution: It describes the probability of occurrence of n
successes in N tries of a binary event,
</p>
<p>P.n/ D
 
N
</p>
<p>n
</p>
<p>!
</p>
<p>pnqNï¿½n
</p>
<p>(mean pN and variance pqN).
ï¿½ Gaussian distribution: It is an approximation of the binomial distribution
</p>
<p>when N is large,
</p>
<p>f .x/dx D 1p
2	ï¿½2
</p>
<p>eï¿½
.xï¿½ï¿½/2
2ï¿½2 dx
</p>
<p>(mean ï¿½ and variance ï¿½2).
ï¿½ Poisson distribution: It is an approximation of the binomial distribution
</p>
<p>when pï¿½ 1 that describes the probability of counting experiments,
</p>
<p>P.n/ D ï¿½
n
</p>
<p>nÅ 
eï¿½ï¿½
</p>
<p>(mean and variance have a value of ï¿½).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Comparison of Binomial, Gaussian, and Poisson Distributions 53
</p>
<p>Problems
</p>
<p>3.1 Consider the Gaussian distribution
</p>
<p>f .x/ D 1p
2	ï¿½2
</p>
<p>eï¿½
.xï¿½ï¿½/2
2ï¿½2 :
</p>
<p>Calculate the mean and variance and show that all odd moments EÅ.X ï¿½ ï¿½/nï¿½ of
order n ï¿½3 are zero.
3.2 Assume that scores from an I.Q. test follow a Gaussian distribution, and that the
scores are standardized in such a way that the mean is ï¿½ D 100, and the standard
deviation is ï¿½ D 15.
(a) Calculate the probability that an I.Q. score is greater or equal than 145.
(b) Calculate the probability that the mean I.Q. score of a sample of 100 persons,
</p>
<p>chosen at random, is equal or larger than 105.
</p>
<p>3.3 A coin is tossed ten times. Find
</p>
<p>(a) The probability of obtaining 5 heads up and 5 tails up;
(b) The probability of having the first 5 tosses show heads up, and the final 5 tosses
</p>
<p>show tails up;
(c) The probability to have at least 7 heads up.
</p>
<p>3.4 In a given course, it is known that 7:3% of students fail.
</p>
<p>(a) What is the expected number of failures in a class of 32 students?
(b) What is the probability that 5 or more students fail?
</p>
<p>3.5 The frequency of twins in European population is about 12 in every 1000
maternities. Calculate the probability that there are no twins in 200 births, using
(a) the binomial distribution, and (b) the Poisson distribution.
</p>
<p>3.6 Given the distribution of a Poisson variable N,
</p>
<p>P.n/ D ï¿½
n
</p>
<p>nÅ 
eï¿½ï¿½
</p>
<p>show that the mean is given by ï¿½ and that the variance is also given by ï¿½.
</p>
<p>3.7 Consider Mendel&rsquo;s experiment of Table 1.1 at page 9 and refer to the &ldquo;Long vs.
short stem&rdquo; data.
</p>
<p>(a) Determine the parent distribution for the number of dominants.
(b) Calculate the uncertainty in the measurement of the number of plants that
</p>
<p>display the dominant character.
(c) Determine the difference between the number of measured plants with the
</p>
<p>dominant character and the expected number, in units of the standard deviation,
to show that this number has an absolute value of less than one.</p>
<p/>
</div>
<div class="page"><p/>
<p>54 3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson
</p>
<p>3.8 For Mendel&rsquo;s experimental data in Table 1.1 at page 9, consider the overall
fraction of plants that display the dominant character, for all seven experiments
combined.
</p>
<p>(a) Determine the parent distribution of the overall fraction X of plants with
dominant character and its expected value.
</p>
<p>(b) Determine the sample mean of the fraction X;
(c) Using the parent variance of X, determine the value
</p>
<p>z D x ï¿½ EÅXï¿½
ï¿½
</p>
<p>which is the standardized difference between the measurement and the mean.
Assuming that the binomial distribution can be approximated by a Gaussian of
same mean and variance, calculate the probability of having a value of z equal
or smaller (in absolute value) to the measured value.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 4
Functions of Random Variables and Error
Propagation
</p>
<p>Abstract Sometimes experiments do not directly measure the quantity of interest,
but rather associated variables that can be related to the one of interest by an analytic
function. It is therefore necessary to establish how we can infer properties of the
interesting variable based on properties of the variables that have been measured
directly. This chapter explains how to determine the probability distribution function
of a variable that is function of other variables of known distribution, and how to
measure its mean and variance, the latter usually referred to as error propagation
formulas. We also establish two fundamental results of the theory of probability, the
central limit theorem and the law of large numbers.
</p>
<p>4.1 Linear Combination of Random Variables
</p>
<p>Experimental variables are often related by a simple linear relationship. The linear
combination of N random variables Xi is a variable Y defined by
</p>
<p>Y D
NX
</p>
<p>iD1
aiXi (4.1)
</p>
<p>where ai are constant coefficients. A typical example of a variable that is a linear
combination of two variables is the signal detected by an instrument, which can be
thought of as the sum of the intrinsic signal from the source plus the background.
The distributions of the background and the source signals will influence the
properties of the total signal detected, and it is therefore important to understand
the statistical properties of this relationship in order to characterize the signal from
the source.
</p>
<p>4.1.1 General Mean and Variance Formulas
</p>
<p>The expectation or mean of the linear combination is EÅYï¿½ DPNiD1 aiEÅXiï¿½ or
</p>
<p>ï¿½y D
NX
</p>
<p>iD1
aiï¿½i; (4.2)
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_4
</p>
<p>55</p>
<p/>
</div>
<div class="page"><p/>
<p>56 4 Functions of Random Variables and Error Propagation
</p>
<p>where ï¿½i is the mean of Xi. This property follows from the linearity of the
expectation operator, and it is equivalent to a weighted mean in which the weights
are given by the coefficients ai.
</p>
<p>In the case of the variance, the situation is more complex:
</p>
<p>VarÅYï¿½ D E
2
</p>
<p>4
</p>
<p> 
NX
</p>
<p>iD1
aiXi ï¿½
</p>
<p>NX
</p>
<p>iD1
aiï¿½i
</p>
<p>!23
</p>
<p>5 D
NX
</p>
<p>iD1
a2i E
</p>
<p>h
.Xi ï¿½ ï¿½i/2
</p>
<p>i
</p>
<p>C 2
NX
</p>
<p>iD1
</p>
<p>NX
</p>
<p>jDiC1
aiajEÅ.Xi ï¿½ ï¿½i/.Xj ï¿½ ï¿½j/ï¿½
</p>
<p>D
NX
</p>
<p>iD1
a2i Var.Xi/C 2
</p>
<p>NX
</p>
<p>iD1
</p>
<p>NX
</p>
<p>jDiC1
aiajCov.Xi;Xj/:
</p>
<p>The result can be summarized in a more compact relationship,
</p>
<p>ï¿½2y D
NX
</p>
<p>iD1
a2i ï¿½
</p>
<p>2
i C 2
</p>
<p>NX
</p>
<p>iD1
</p>
<p>NX
</p>
<p>jDiC1
aiajï¿½
</p>
<p>2
ij : (4.3)
</p>
<p>Equation (4.3) shows that variances add only for variables that are mutually
uncorrelated, or ï¿½2ij D 0, but not in general. The following example illustrates the
importance of a non-zero covariance between two variables, and its effect on the
variance of the sum.
</p>
<p>Example 4.1 (Variance of Anti-correlated Variables) Consider the case of the
measurement of two random variables X and Y that are completely anti-correlated,
Corr.X;Y/ D ï¿½1, with mean and varianceï¿½x D 1,ï¿½y D 1, ï¿½2x D 0:5 and ï¿½2y D 0:5.
</p>
<p>The mean of Z D X C Y is ï¿½ D 1 C 1 D 2 and the variance is ï¿½2 D ï¿½2x C
ï¿½2y ï¿½2Cov.X;Y/ D .ï¿½xï¿½ï¿½y/2 D 0; this means that in this extreme case of complete
anticorrelation the sum of the two random variables is actually not a random variable
any more. If the covariance term had been neglected in (4.3), we would have made
the error of inferring a variance of 1 for the sum. }
</p>
<p>4.1.2 Uncorrelated Variables and the 1=
p
N Factor
</p>
<p>For two or more uncorrelated variables the variances add linearly, according to (4.3).
Uncorrelated variables are common in statistics. For example, consider repeating the
same experiment a number N of times independently, and each time measurements
of a random variable Xi is made. After N experiments, one obtains N measurements
from identically distributed random variables (since they resulted from the same
type of experiment). The variables are independent, and therefore uncorrelated, if</p>
<p/>
</div>
<div class="page"><p/>
<p>4.1 Linear Combination of Random Variables 57
</p>
<p>the experiments were performed in such a way that the outcome of one specific
experiment did not affect the outcome of another.
</p>
<p>With N uncorrelated variables Xi all of equal mean ï¿½ and variance ï¿½2, one is
often interested in calculating the relative uncertainty in the variable
</p>
<p>Y D 1
N
</p>
<p>NX
</p>
<p>iD1
Xi (4.4)
</p>
<p>which describes the sample mean of N measurements. The relative uncertainty is
described by the ratio of the standard deviation and the mean,
</p>
<p>ï¿½y
</p>
<p>ï¿½y
D 1
</p>
<p>N
</p>
<p>p
ï¿½2 C ï¿½ ï¿½ ï¿½ C ï¿½2
</p>
<p>ï¿½
D 1p
</p>
<p>N
	 ï¿½
ï¿½
</p>
<p>(4.5)
</p>
<p>where we used the property that VarÅaXï¿½ D a2VarÅXï¿½ and the fact that both means
and variances add linearly. The result shows that the N measurements reduced the
relative error in the random variable by a factor of 1=
</p>
<p>p
N, as compared with a single
</p>
<p>measurement. This observation is a key factor in statistics, and it is the reason why
one needs to repeat the same experiment many times in order to reduce the relative
statistical error. Equation (4.5) can be recast to show that the variance in the sample
mean is given by
</p>
<p>ï¿½2Y D
ï¿½2
</p>
<p>N
(4.6)
</p>
<p>where ï¿½ is the sample variance, or variance associated with one measurement. The
interpretation is simple: one expects much less variance between two measurements
of the sample mean, than between two individual measurements of the variable,
since the statistical fluctuations of individual measurements average down with
increasing sample size.
</p>
<p>Another important observation is that, in the case of completely correlated
variables, then additional measurements introduces no advantages, i.e., the relative
error does not decrease with the number of measurements. This can be shown with
the aid of (4.3), and is illustrated in the following example.
</p>
<p>Example 4.2 (Variance of Correlated Variables) Consider the two measurements in
Example 4.1, but now with a correlation of 1. In this case, the covariance of the sum
is ï¿½2 D ï¿½2x C ï¿½2y C 2Cov.X;Y/ D .ï¿½x C ï¿½y/2, and therefore the relative error in the
sum is
</p>
<p>ï¿½
</p>
<p>ï¿½
D .ï¿½x C ï¿½y/
</p>
<p>ï¿½x C ï¿½y</p>
<p/>
</div>
<div class="page"><p/>
<p>58 4 Functions of Random Variables and Error Propagation
</p>
<p>which is the same as the relative error of each measurement. Notice that the same
conclusion applies to the average of the two measurements, since the sum and the
average differ only by a constant factor of 1=2. }
</p>
<p>4.2 The Moment Generating Function
</p>
<p>The mean and the variance provide only partial information on the random variable,
and a full description would require the knowledge of all moments. The moment
generating function is a convenient mathematical tool to determine the distribution
function of random variables and its moments. It is also useful to prove the central
limit theorem, one of the key results of statistics, since it establishes the Gaussian
distribution as the normal distribution when a random variable is the sum of a large
number of measurements.
</p>
<p>The moment generating function of a random variable X is defined as
</p>
<p>M.t/ D EÅetXï¿½; (4.7)
</p>
<p>and it has the property that all moments can be derived from it, provided they exist
and are finite. Assuming a continuous random variable of probability distribution
function f .x/, the moment generating function can be written as
</p>
<p>M.t/ D
Z C1
</p>
<p>ï¿½1
etxf .x/dx D
</p>
<p>Z C1
</p>
<p>ï¿½1
</p>
<p>ï¿½
</p>
<p>1C tx
1
C .tx/
</p>
<p>2
</p>
<p>2Å 
C : : :
</p>
<p>ï¿½
</p>
<p>f .x/dx D 1C tï¿½1 C t
2
</p>
<p>2Å 
ï¿½2 C : : :
</p>
<p>and therefore all moments can be obtained as partial derivatives,
</p>
<p>ï¿½r D @
rM.t/
</p>
<p>@tr
</p>
<p>Ë
Ë
Ë
Ë
tD0
</p>
<p>: (4.8)
</p>
<p>The most important property of the moment generating function is that there
is a one-to-one correspondence between the moment generating function and the
probability distribution function, i.e., the moment generating function is a sufficient
description of the random variable. Some distributions do not have a moment
generating function, since some of their moments may be infinite, so in principle
this method cannot be used for all distributions.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 The Moment Generating Function 59
</p>
<p>4.2.1 Properties of the Moment Generating Function
</p>
<p>A full treatment of mathematical properties of the moment generating function can
be found in textbooks on theory of probability, such as [38]. Two properties of the
moment generating function will be useful in the determination of the distribution
function of random variables:
</p>
<p>&bull; If Y D aC bX, where a, b are constants, the moment generating function of Y is
</p>
<p>My.t/ D eatMx.bt/: (4.9)
</p>
<p>Proof This relationship can be proved by the use of the expectation
operator, according to the definition of the moment generating function:
</p>
<p>EÅetY ï¿½ D EÅet.aCbX/ï¿½ D EÅeatebtXï¿½ D eatMx.bt/:
</p>
<p>ut
&bull; If X and Y are independent random variables, with Mx.t/ and My.t/ as moment
</p>
<p>generating functions, then the moment generating function of Z D X C Y is
</p>
<p>Mz.t/ D Mx.t/My.t/: (4.10)
</p>
<p>Proof The relationship is derived immediately by
</p>
<p>EÅetZ ï¿½ D EÅet.XCY/ï¿½ D Mx.t/My.t/:
</p>
<p>ut
</p>
<p>4.2.2 The Moment Generating Function of the Gaussian
and Poisson Distribution
</p>
<p>Important cases to study are the Gaussian distribution of mean ï¿½ and variance ï¿½2
</p>
<p>and the Poisson distribution of mean ï¿½.
</p>
<p>&bull; The moment generating function of the Gaussian is given by
</p>
<p>M.t/ D eï¿½tC 12 ï¿½2t2 : (4.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>60 4 Functions of Random Variables and Error Propagation
</p>
<p>Proof Start with
</p>
<p>M.t/ D 1p
2	ï¿½2
</p>
<p>Z C1
</p>
<p>ï¿½1
etxeï¿½
</p>
<p>.xï¿½ï¿½/2
2ï¿½2 dx:
</p>
<p>The exponent can be written as
</p>
<p>tx ï¿½ 1
2
</p>
<p>x2 C ï¿½2 ï¿½ 2xï¿½
ï¿½2
</p>
<p>D 2ï¿½
2tx ï¿½ x2 ï¿½ ï¿½2 C 2xï¿½
</p>
<p>2ï¿½2
</p>
<p>D ï¿½ .x ï¿½ ï¿½ ï¿½ ï¿½
2t/2
</p>
<p>2ï¿½2
C 2ï¿½ï¿½
</p>
<p>2t
</p>
<p>2ï¿½2
C ï¿½
</p>
<p>2t
</p>
<p>2ï¿½2
ï¿½2t:
</p>
<p>It follows that
</p>
<p>M.t/ D 1p
2	ï¿½2
</p>
<p>Z C1
</p>
<p>ï¿½1
eï¿½te
</p>
<p>ï¿½2t2
</p>
<p>2 eï¿½
.xï¿½ï¿½ï¿½ï¿½2t/2
</p>
<p>2ï¿½2 dx
</p>
<p>D 1p
2	ï¿½2
</p>
<p>eï¿½te
ï¿½2t2
</p>
<p>2
</p>
<p>p
2	ï¿½2 D eï¿½tC
</p>
<p>ï¿½2t2
</p>
<p>2 :
</p>
<p>ut
&bull; The moment generating function of the Poisson distribution is given by
</p>
<p>M.t/ D eï¿½ï¿½eï¿½et : (4.12)
</p>
<p>Proof The moment generating function is obtained by
</p>
<p>M.t/ D EÅetN ï¿½ D
1X
</p>
<p>nD0
ent
ï¿½n
</p>
<p>nÅ 
eï¿½ï¿½ D eï¿½ï¿½
</p>
<p>1X
</p>
<p>nD0
</p>
<p>.ï¿½et/n
</p>
<p>nÅ 
D eï¿½ï¿½eï¿½et :
</p>
<p>ut
Example 4.3 (Sum of Poisson Variables) The moment generating function can be
used to show that the sum of two independent Poisson random variables of mean ï¿½
andï¿½ is a Poisson random variable with mean ï¿½Cï¿½. In fact that mean of the Poisson
appears at the exponent of the moment generating function, and property (4.10),
can be used to prove this result. The fact that the mean of two independent Poisson
distributions will add is not surprising, given that the Poisson distribution relates to
the counting of discrete events. }</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 The Central Limit Theorem 61
</p>
<p>4.3 The Central Limit Theorem
</p>
<p>The Central Limit Theorem is one of statistic&rsquo;s most important results, establishing
that a variable obtained as the sum of a large number of independent variables has a
Gaussian distribution. This result can be stated as:
</p>
<p>Theorem 4.1 (Central Limit Theorem) The sum of a large number of indepen-
dent random variables is approximately distributed as a Gaussian. The mean of
the distribution is the sum of the means of the variables and the variance of the
distribution is the sum of the variances of the variables. This result holds regardless
of the distribution of each individual variable.
</p>
<p>Proof Consider the variable Y as the sum of N variables Xi of mean ï¿½i and
variance ï¿½2i ,
</p>
<p>Y D
NX
</p>
<p>iD1
Xi; (4.13)
</p>
<p>with Mi.t/ the moment generating function of the random variable .Xi ï¿½ ï¿½i/.
Since the random variables are independent, and independence is a stronger
statement than uncorrelation, it follows that the mean of Y is ï¿½ D Pï¿½i, and
that variances likewise add linearly, ï¿½2 D Pï¿½2i . We want to calculate the
moment generating function of the variable Z defined by
</p>
<p>Z D Y ï¿½ ï¿½
ï¿½
D 1
ï¿½
</p>
<p>NX
</p>
<p>iD1
.Xi ï¿½ ï¿½i/:
</p>
<p>The variable Z has a mean of zero and unit variance. We want to show that
Z can be approximated by a standard Gaussian. Using the properties of the
moment generating function, the moment generating function of Z is
</p>
<p>M.t/ D
NY
</p>
<p>iD1
Mi.t=ï¿½/:
</p>
<p>The moment generating function of each variable .Xi ï¿½ ï¿½i/=ï¿½ is
</p>
<p>Mi.t=ï¿½/ D 1C ï¿½.xiï¿½ï¿½i/
t
</p>
<p>ï¿½
C ï¿½
</p>
<p>2
i
</p>
<p>2
</p>
<p>ï¿½ t
</p>
<p>ï¿½
</p>
<p>	2
C ï¿½i;3
</p>
<p>3Å 
</p>
<p>ï¿½ t
</p>
<p>ï¿½
</p>
<p>	3
C : : :
</p>
<p>where ï¿½xiï¿½ï¿½i D 0 is the mean of Xi ï¿½ ï¿½i. The quantities ï¿½2i and ï¿½i;3 are,
respectively, the central moments of the second and third order of Xi.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 4 Functions of Random Variables and Error Propagation
</p>
<p>If a large number of random variables are used, N ï¿½ 1, then ï¿½2 is large, as
it is the sum of variances of the random variables, and we can ignore terms of
order ï¿½ï¿½3. We therefore make the approximation
</p>
<p>lnM.t/ D
X
</p>
<p>lnMi
ï¿½ t
</p>
<p>ï¿½
</p>
<p>	
D
</p>
<p>X
ln
</p>
<p>ï¿½
</p>
<p>1C ï¿½
2
i
</p>
<p>2
</p>
<p>ï¿½ t
</p>
<p>ï¿½
</p>
<p>	2ï¿½
</p>
<p>'
X ï¿½2i
</p>
<p>2
</p>
<p>ï¿½ t
</p>
<p>ï¿½
</p>
<p>	2
D 1
2
t2:
</p>
<p>This results in the approximation of the moment generating function of
.y ï¿½ ï¿½/=ï¿½ as
</p>
<p>) M.t/ ' e t
2
</p>
<p>2 ;
</p>
<p>which shows that Z is approximately distributed as a standard Gaussian
distribution, according to (4.11). Given that the random variable of interest
Y is obtained by a change of variable Z D .Y ï¿½ ï¿½/=ï¿½ , we also know that
ï¿½y D ï¿½ and Var.Y/ D Var.ï¿½Z/ D ï¿½2Var.Z/ D ï¿½2, therefore Y is distributed
as a Gaussian with mean ï¿½ and variance ï¿½2. ut
The central limit theorem establishes that the Gaussian distribution is the limiting
</p>
<p>distribution approached by the sum of random variables, no matter their original
shapes, when the number of variables is large. A particularly illustrative example
is the one presented in the following, in which we perform the sum of a number
of uniform distributions. Although the uniform distribution does not display the
Gaussian-like feature of a centrally peaked distribution, with the increasing number
of variables being summed, the sum rapidly approaches a Gaussian distribution.
</p>
<p>Example 4.4 (Sum of Uniform Random Variables) We show that the sum of N
independent uniform random variables between 0 and 1 tend to a Gaussian with
mean N=2, given that each variable has a mean of 1=2. The calculation that the sum
of N uniform distribution tends to the Gaussian can be done by first calculating the
moment generating function of the uniform distribution, then using the properties
of the moment generating function.
</p>
<p>We can show that the uniform distribution in the range Å0; 1ï¿½ has ï¿½i D 1=2, ï¿½2i D
1=12, and a moment generating function
</p>
<p>Mi.t/ D .e
t ï¿½ 1/
t
I
</p>
<p>the sum of N independent such variables therefore has ï¿½ D N=2 and ï¿½2 D N=12.
To prove that the sum is asymptotically distributed like a Gaussian with this mean
and variance, we must show that
</p>
<p>lim
N!1M.t/ D e
</p>
<p>N
2
tC t2
</p>
<p>2
N
12</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 The Central Limit Theorem 63
</p>
<p>Proof Using the property of the moment generating function of independent
variables, we write
</p>
<p>M.t/ D Mi.t/N D
ï¿½
1
</p>
<p>t
.et ï¿½ 1/
</p>
<p>ï¿½N
</p>
<p>D
ï¿½
1C tC t2=2Å C t3=3Å  : : : : ï¿½ 1
</p>
<p>t
</p>
<p>ï¿½N
</p>
<p>'
ï¿½
</p>
<p>1C t
2
C t
</p>
<p>2
</p>
<p>6
C : : :
</p>
<p>ï¿½N
</p>
<p>:
</p>
<p>Neglect terms of order O.t3/ and higher, and work with logarithms:
</p>
<p>ln.M.t/N/ ' N ln
ï¿½
</p>
<p>1C t
2
C t
</p>
<p>2
</p>
<p>6
</p>
<p>ï¿½
</p>
<p>Use the Taylor series expansion ln.1C x/ ' .x ï¿½ x2=2C : : :/, to obtain
</p>
<p>ln.Mi.t// ' N
 
t
</p>
<p>2
C t
</p>
<p>2
</p>
<p>6
ï¿½ 1
2
</p>
<p>ï¿½
t
</p>
<p>2
C t
</p>
<p>2
</p>
<p>6
</p>
<p>ï¿½2!
</p>
<p>D
</p>
<p>N.t=2C t2=6ï¿½ t2=8C O.t3// ' N.t=2C t2=24/
</p>
<p>in which we continued neglecting terms of order O.t3/. The equation above
shows that the moment generating function can be approximated as
</p>
<p>M.t/ ' eN
ï¿½
t
2
C t2
24
</p>
<p>ï¿½
</p>
<p>(4.14)
</p>
<p>which is in fact the moment generating function of a Gaussian with mean N=2
and variance N=12. ut
In Figure 4.1 we show the simulations of, respectively, 1000 and 100,000 samples
</p>
<p>drawn from N D 100 uniform and independent variables between 0 and 1. The
sample distributions approximate well the limiting Gaussian with ï¿½ D N=2,
ï¿½ D pN=12. The approximation is improved when a larger number of samples
are drawn, also illustrating the fact that the sample distribution approximates the
parent distribution in the limit of a large number of samples collected. }
</p>
<p>Example 4.5 (Sum of Two Uniform Distributions) An analytic way to develop a
practical sense of how the sum of non-Gaussian distributions progressively develops
the peaked Gaussian shape can be illustrated with the sum of just two uniform
distributions. We start with a uniform distribution in the range of ï¿½1 to 1, which
can be shown to have
</p>
<p>M.t/ D 1=.2t/.et ï¿½ eï¿½t/:</p>
<p/>
</div>
<div class="page"><p/>
<p>64 4 Functions of Random Variables and Error Propagation
</p>
<p>Fig. 4.1 Sample distribution functions of the sum of N D 100 independent uniform variables
between 0 and 1, constructed from 1000 simulated measurements (grey histograms) and 100,000
measurements (histogram plot with black outline). The solid curve is the N.ï¿½; ï¿½/ Gaussian, with
ï¿½ D N=2, ï¿½ D pN=12, the limiting distribution according to the Central Limit Theorem
</p>
<p>The sum of two such variables will have a triangular distribution, given by the
analytical form
</p>
<p>f .x/ D
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>1
</p>
<p>2
C x
4
</p>
<p>if ï¿½2 
 x 
 0
1
</p>
<p>2
ï¿½ x
4
</p>
<p>if 0 
 x 
 2:
</p>
<p>This is an intuitive result that can be proven by showing that the moment generating
function of the triangular distribution is equal to M.t/2 (see Problem 4.3). The
calculation follows from the definition of the moment generating function for a
variable of known distribution function. The triangular distribution is the first step
in the development of a peaked, Gaussian-like distribution. }
</p>
<p>4.4 The Distribution of Functions of Random Variables
</p>
<p>The general case of a variable that is a more complex function of other variables can
be studied analytically when certain conditions are met. In this book we present
the method of change of variables which can be conveniently applied to one-
dimensional transformations and a method based on the cumulative distribution
function which can be used for multi-dimensional transformations. Additional
information on this subject can be found, e.g., in the textbook by Ross [38].</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 The Distribution of Functions of Random Variables 65
</p>
<p>4.4.1 The Method of Change of Variables
</p>
<p>A simple method for obtaining the probability distribution function of the dependent
variable Y D Y.X/ is by using the method of change of variables, which applies only
if the function Y.x/ is strictly increasing. In this case the probability distribution of
g.y/ of the dependent variable is related to the distribution f .x/ of the independent
variable via
</p>
<p>g.y/ D f .x/dx
dy
</p>
<p>(4.15)
</p>
<p>In the case of a decreasing function, the same method can be applied but the term
dx=dy must be replaced with the absolute value, jdx=dyj.
Example 4.6 Consider a variable X distributed as a uniform distribution between 0
and 1, and the variable Y D X2. The method automatically provides the information
that the variable Y is distributed as
</p>
<p>g.y/ D 1
2
</p>
<p>1p
y
</p>
<p>with 0 
 y 
 1. You can prove that the distribution is properly normalized in this
domain. }
</p>
<p>The method can be naturally extended to the joint distribution of several random
variables. The multi-variable version of (4.15) is
</p>
<p>g.u; v/ D h.x; y/ jJj (4.16)
</p>
<p>in which
</p>
<p>J D
ï¿½
d.x; y/
</p>
<p>d.u; v/
</p>
<p>ï¿½
</p>
<p>D
</p>
<p>0
</p>
<p>B
@
</p>
<p>dx
</p>
<p>du
</p>
<p>dx
</p>
<p>dv
dy
</p>
<p>du
</p>
<p>dy
</p>
<p>dv
</p>
<p>1
</p>
<p>C
A
</p>
<p>is the Jacobian of the transformation, in this case a 2 by 2 matrix, h.x; y/ is the
joint probability distribution of the independent variables X;Y, and U;V are the
new random variables related to the original ones by a transformation U D u.X;Y/
and V D v.X;Y/.
Example 4.7 (Transformation of Cartesian to Polar Coordinates) Consider two
random variables X,Y distributed as standard Gaussians, and independent of one
another. The joint probability distribution function is
</p>
<p>h.x; y/ D 1
2	
</p>
<p>eï¿½
x2Cy2
2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>66 4 Functions of Random Variables and Error Propagation
</p>
<p>Consider a transformation of variables from Cartesian coordinates x; y to polar
coordinates r; ï¿½ , described by
</p>
<p>(
x D r ï¿½ cos.ï¿½/
y D r ï¿½ sin.ï¿½/
</p>
<p>The Jacobian of the transformation is
</p>
<p>J D
ï¿½
cosï¿½ ï¿½rsinï¿½
sinï¿½ rcosï¿½
</p>
<p>ï¿½
</p>
<p>and its determinant is jJj D r. Notice that to apply the method described by (4.16)
one only needs to know the inverse transformation of .x; y/ as function of .r; ï¿½/. It
follows that the distribution of .r; ï¿½/ is given by
</p>
<p>g.r; ï¿½/ D 1
2	
</p>
<p>reï¿½
r2
</p>
<p>2
</p>
<p>for r ï¿½ 0, 0 
 ï¿½ 
 2	 . The distribution reï¿½ r
2
</p>
<p>2 is called the Rayleigh distribution,
and 1=2	 can be interpreted as a uniform distribution for the angle ï¿½ between 0
and 	 . One important conclusion is that, since g.r; ï¿½/ can be factored out into two
functions that contain separately the two variables r and ï¿½ , the two new variables
are also independent. }
</p>
<p>4.4.2 A Method for Multi-dimensional Functions
</p>
<p>We will consider the case in which the variable Z is a function of two random
variables X and Y, since this is a case of common use in statistics, e.g., X C Y,
or X=Y. We illustrate the methodology with the case of the function Z D X C Y,
when the two variables are independent. The calculation starts with the cumulative
distribution function of the random variable of interest,
</p>
<p>FZ.a/ D P.Z 
 a/ D
Z Z
</p>
<p>xCyï¿½a
f .x/g.y/dxdy
</p>
<p>in which f .x/ and g.y/ are, respectively, the probability distribution functions of
X and Y, and the limits of integration must be chosen so that the sum of the two
variables is less or equal than a. The portion of parameter space such that xC y 
 a
includes all values x 
 a ï¿½ y, for any given value of y, or
</p>
<p>FZ.a/ D
Z C1
</p>
<p>ï¿½1
dy
Z aï¿½y
</p>
<p>1
f .x/g.y/dx D
</p>
<p>Z C1
</p>
<p>ï¿½1
g.y/dyFx.a ï¿½ y/</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 The Distribution of Functions of Random Variables 67
</p>
<p>whereFx is the cumulative distribution for the variableX. It is often more convenient
to express the relationship in terms of the probability distribution function, which is
related to the cumulative distribution function via a derivative,
</p>
<p>fZ.a/ D d
da
</p>
<p>FZ.a/ D
Z 1
</p>
<p>ï¿½1
f .a ï¿½ y/g.y/dy: (4.17)
</p>
<p>This relationship is called the convolution of the distributions f .x/ and g.y/.
</p>
<p>Example 4.8 (Sum of Two Independent Uniform Variables) Calculate the probabil-
ity distribution function of the sum of two independent uniform random variables
between ï¿½1 andC1.
</p>
<p>The probability distribution function of a uniform variable between ï¿½1 and C1
is f .x/ D 1=2, defined for ï¿½1 
 x 
 1. The convolution gives the following integral
</p>
<p>fZ.a/ D
Z C1
</p>
<p>ï¿½1
1
</p>
<p>2
f .a ï¿½ y/dy:
</p>
<p>The distribution function of the sum Z can have values ï¿½2 
 a 
 2, and the
convolution must be divided into two integrals, since f .aï¿½y/ is only defined between
ï¿½1 andC1. We obtain
</p>
<p>fZ.a/ D 1
4
	
( R aC1
</p>
<p>ï¿½1 dy if ï¿½2 
 a 
 0R 1
aï¿½1 dy if 0 
 a 
 2:
</p>
<p>This results in
</p>
<p>fZ.a/ D 1
4
	
(
.aC 2/ if ï¿½2 
 a 
 0
.2 ï¿½ a/ if 0 
 a 
 2
</p>
<p>which is the expected triangular distribution between ï¿½2 andC2. }
Another useful application is for the case of Z D X=Y, where X and Y are
again independent variables. We begin with the cumulative distribution,
</p>
<p>FZ.z/ D P.Z &lt; z/ D P.X=Y &lt; z/ D P.X &lt; zY/:
</p>
<p>For a given value y of the random variable Y, this probability equals FX.zy/;
since Y has a probability fY.y/dy to be in the range between y and yC dy, we
obtain
</p>
<p>FZ.z/ D
Z
</p>
<p>FX.zy/fY.y/dy:</p>
<p/>
</div>
<div class="page"><p/>
<p>68 4 Functions of Random Variables and Error Propagation
</p>
<p>Following the same method as for the derivation of the distribution of X C Y,
we must take the derivative of FZ.z/ with respect to z to obtain:
</p>
<p>fZ.z/ D
Z
</p>
<p>fX.zy/yfY.y/dy: (4.18)
</p>
<p>This is the integral than must be solved to obtain the distribution of X=Y.
</p>
<p>4.5 The Law of Large Numbers
</p>
<p>Consider N random variables Xi that are identically distributed, and ï¿½ is their
common mean. The Strong Law of Large Numbers states that, under suitable
conditions on the variance of the random variables, the sum of the N variables tends
to the mean ï¿½, which is a deterministic number and not a random variable. This
result can be stated as
</p>
<p>lim
n!1
</p>
<p>X1 C : : :C XN
N
</p>
<p>D ï¿½; (4.19)
</p>
<p>and it is, together with the Central Limit Theorem, one of the most important results
of the theory of probability, and of great importance for statistics. Equation (4.19)
is a very strong statement because it shows that, asymptotically, the sum of random
variables becomes a constant equal to the sample mean of the N variables, or N
measurements. Although no indication is given towards establishing how large N
should be in order to achieve this goal, it is nonetheless an important result that will
be used in determining the asymptotic behavior of random variables. Additional
mathematical properties of this law can be found in books of theory of probability,
such as [38] or [26].
</p>
<p>Instead of providing a formal proof of this law, we want to focus on an important
consequence. Given a function y.x/, we would like to estimate its expected value
EÅy.X/ï¿½ from the N measurements of the variables Xi. According to the law of large
numbers, we can say that
</p>
<p>lim
n!1
</p>
<p>y.X1/C : : :C y.XN/
N
</p>
<p>D EÅy.X/ï¿½: (4.20)
</p>
<p>Equation (4.20) states that a large number of measurements of the variables Xi can
be used to measure the expectation of EÅy.X/ï¿½, entirely bypassing the probability
distribution function of the function y.X/. This property is used in the following
section.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.6 The Mean of Functions of Random Variables 69
</p>
<p>4.6 The Mean of Functions of Random Variables
</p>
<p>For a function of random variables it is often necessary or convenient to develop
methods to estimate the mean and the variance without having full knowledge of its
probability distribution function.
</p>
<p>For functions of a single variable Y D y.X/
</p>
<p>EÅy.X/ï¿½ D
Z
</p>
<p>y.x/f .x/dx (4.21)
</p>
<p>where f .x/ is the distribution function of X. This is in fact a very intuitive result,
stating that the distribution function of X is weighted by the function of interest, and
it makes it straightforward to compute expectation values of variables without first
having to calculate their full distribution. According to the law of large numbers,
this expectation can be estimated from N measurements xi as per (4.20),
</p>
<p>y.x/ D y.x1/C : : :C y.xn/
N
</p>
<p>: (4.22)
</p>
<p>An important point is that the mean of the function is not equal to the function of the
mean, y.x/ &curren; y.x/, as will be illustrated in the following example. Equation (4.22)
says that we must have access to the individual measurements of the variable X, if
we want to make inferences on the mean of a function of X. If, for example, we only
had the mean x, we cannot measure u.x/. This point is relevant when one has limited
access to the data, e.g., when the experimenter does not report all information on the
measurements performed.
</p>
<p>Example 4.9 (Mean of Square of a Uniform Variable) Consider the case of a
uniform variable U in the range 0&ndash;1, with mean 1=2. If we want to evaluate the
parent mean of X D U2, we calculate
</p>
<p>ï¿½ D
Z 1
</p>
<p>0
</p>
<p>u2du D 1=3:
</p>
<p>It is important to see that the mean of U2 is not just the square of the mean of U,
and therefore the means do not transform following the same analytic expression as
the random variables. You can convince yourself of this fact by assuming to draw
five &ldquo;fair&rdquo; samples from a uniform distribution, 0:1; 0:3; 0:5; 0:7 and 0:9&mdash;they can
be considered as a dataset of measurements. Clearly their mean is 1=2, but the mean
of their squares is 1=3 and not 1=4, in agreement with the theoretical calculation of the
parent mean. }
</p>
<p>Another example where the mean of the function does not equal to the function
of the mean is reported in Problem 4.5, in which you can show that using the means
of I and W=Q do not give the mean of m=e for the Thomson experiment to measure
the mass to charge ratio of the electron. The problem provides a multi-dimensional</p>
<p/>
</div>
<div class="page"><p/>
<p>70 4 Functions of Random Variables and Error Propagation
</p>
<p>extension to (4.22), since the variable m=e is a function of two variables that have
been measured in pairs.
</p>
<p>4.7 The Variance of Functions of Random Variables
and Error Propagation Formulas
</p>
<p>A random variable Z that is a function of other variables can have its variance
estimated directly if the measurements of the independent variables are available,
similar to the case of the estimation of the mean. Considering, for example, the
case of a function Z D z.U/ that depends on just one variable, for which we have
N measurements u1, . . . , uN available. With the mean estimated from (4.22), the
variance can accordingly be estimated as
</p>
<p>s2u D
.z.u1/ï¿½ z/2 C : : :C .z.uN/ï¿½ z/2
</p>
<p>N ï¿½ 1 ; (4.23)
</p>
<p>as one would normally do, treating the numbers z.u1/, . . . , z.uN/ as samples from
the dependent variable. This method can naturally be extended to more than one
variable, as illustrated in the following example. When the measurements of the
independent variables are available, this method is the straightforward way to
estimate the variance of the function of random variables.
</p>
<p>Example 4.10 Using the Thomson experiment described on page 23, consider the
data collected for Tube 1, consisting of 11 measurements of W=Q and I, from which
the variable of interest v is calculated as
</p>
<p>v D 2W=Q
I
</p>
<p>From the reported data, one obtains 11 measurements of v, from which the mean
and standard deviation can be immediately calculated as v D 7:9 	 109, and sv D
2:8 	 109. }
</p>
<p>There are a number of instances in which one does not have access to the original
measurements of the independent variable or variables, required for an accurate
estimate of the variance according to (4.23). In this care, an approximate method
to estimate the variance must be used instead. This method takes the name of
error propagation. Consider a random variable Z that is a function of a number
of variables, Z D z.U;V; : : :/. A method to approximate the variance of Z in terms
of the variance of the independent variables U, V , etc. starts by expanding Z in a</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 The Variance of Functions of Random Variables and Error Propagation. . . 71
</p>
<p>Taylor series about the means of the independent variables, to obtain
</p>
<p>z.u; v; : : :/ D z.ï¿½u; ï¿½v; : : :/C .u ï¿½ ï¿½u/ @z
@u
</p>
<p>Ë
Ë
Ë
Ë
ï¿½u
</p>
<p>C .v ï¿½ ï¿½v/ @z
@v
</p>
<p>Ë
Ë
Ë
Ë
ï¿½v
</p>
<p>C : : :C O.u ï¿½ ï¿½u/2 C O.v ï¿½ ï¿½v/2 C : : :
</p>
<p>Neglecting terms of the second order, the expectation of Z would be given
by EÅZï¿½ D z.ï¿½u; ï¿½v; : : :/, i.e., the mean of X would be approximated as ï¿½X D
z.ï¿½u; ï¿½v; : : :/. This is true only if the function is linear, and we have shown in
Sect. 4.6 that this approximation may not be sufficiently accurate in the case of
nonlinear functions such as U2. This approximation for the mean is used to estimate
the variance of Z, for which we retain only terms of first order in the Taylor
expansion:
</p>
<p>EÅ.Z ï¿½ EÅZï¿½/2ï¿½ ' E
2
</p>
<p>4
</p>
<p> 
</p>
<p>.u ï¿½ ï¿½u/ @z
@u
</p>
<p>Ë
Ë
Ë
Ë
ï¿½u
</p>
<p>C .v ï¿½ ï¿½v/ @z
@v
</p>
<p>Ë
Ë
Ë
Ë
ï¿½v
</p>
<p>C : : :
!2
3
</p>
<p>5
</p>
<p>' E
2
</p>
<p>4
</p>
<p> 
</p>
<p>.u ï¿½ ï¿½u/ @z
@u
</p>
<p>Ë
Ë
Ë
Ë
ï¿½u
</p>
<p>!2  
</p>
<p>.v ï¿½ ï¿½v/ @z
@v
</p>
<p>Ë
Ë
Ë
Ë
ï¿½v
</p>
<p>!2
</p>
<p>C 2.uï¿½ ï¿½u/ @z
@u
</p>
<p>Ë
Ë
Ë
Ë
ï¿½u
</p>
<p>ï¿½ .v ï¿½ ï¿½v/ @z
@v
</p>
<p>Ë
Ë
Ë
Ë
ï¿½v
</p>
<p>C : : :
3
</p>
<p>5 :
</p>
<p>This formula can be rewritten as
</p>
<p>ï¿½2X ' ï¿½2u
@f
</p>
<p>@u
</p>
<p>Ë
Ë
Ë
Ë
</p>
<p>2
</p>
<p>ï¿½u
</p>
<p>C ï¿½2v
@f
</p>
<p>@v
</p>
<p>Ë
Ë
Ë
Ë
</p>
<p>2
</p>
<p>ï¿½v
</p>
<p>C 2 ï¿½ ï¿½2uv
@f
</p>
<p>@u
</p>
<p>Ë
Ë
Ë
Ë
ï¿½u
</p>
<p>@f
</p>
<p>@v
</p>
<p>Ë
Ë
Ë
Ë
ï¿½v
</p>
<p>C : : : (4.24)
</p>
<p>which is usually referred to as the error propagation formula, and can be used
for any number of independent variables. This result makes it possible to estimate
the variance of a function of variable, knowing simply the variance of each of the
independent variables and their covariances. The formula is especially useful for all
cases in which the measured variables are independent, and all that is known is their
mean and standard deviation (but not the individual measurements used to determine
the mean and variance). This method must be considered as an approximation when
there is only incomplete information about the measurements. Neglecting terms of
the second order in the Taylor expansion can in fact lead to large errors, especially
when the function has strong nonlinearities. In the following we provide a few
specific formulas for functions that are of common use.</p>
<p/>
</div>
<div class="page"><p/>
<p>72 4 Functions of Random Variables and Error Propagation
</p>
<p>4.7.1 Sum of a Constant
</p>
<p>Consider the case in which a constant a is added to the variable U,
</p>
<p>Z D U C a
</p>
<p>where a is a deterministic constant which can have either sign. It is clear that
@z=@a D 0, @z=@u D 1, and therefore the addition of a constant has no effect on
the uncertainty of X,
</p>
<p>ï¿½2z D ï¿½2u : (4.25)
</p>
<p>The addition or subtraction of a constant only changes the mean of the variable
by the same amount, but leaves its standard deviation unchanged.
</p>
<p>4.7.2 Weighted Sum of Two Variables
</p>
<p>The variance of the weighted sum of two variables,
</p>
<p>Z D aU C bV
</p>
<p>where a, b are constants of either sign, can be calculated using @z=@u D a,
@z=@v D b. We obtain
</p>
<p>ï¿½2z D a2ï¿½2u C b2ï¿½2v C 2abï¿½2uv: (4.26)
</p>
<p>The special case in which the two variables U, V are uncorrelated leads to the
weighted sum of the variances.
</p>
<p>Example 4.11 Consider a decaying radioactive source which is found to emit N1 D
50 counts and N2 D 35 counts in two time intervals of same duration, during which
B D 20 background counts are recorded. This is an idealized situation in which we
have directly available the measurement of the background counts. In the majority
of real-life experiments one simply measures the sum of signal plus background,
and in those cases additional considerations must be used. We want to calculate the
background subtracted source counts in the two time intervals and estimate their
signal-to-noise ratio, defined as S=N D ï¿½=ï¿½ . The inverse of the signal-to-noise
ratio is the relative error of the variable.
</p>
<p>Each random variableN1,N2, andB obeys the Poisson distribution, since it comes
from a counting process. Therefore, we can estimate the following parent means and</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 The Variance of Functions of Random Variables and Error Propagation. . . 73
</p>
<p>variances from the sample measurements,
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½1 D 50 ï¿½1 D
p
50 D 7:1
</p>
<p>ï¿½2 D 35 ï¿½2 D
p
35 D 5:9
</p>
<p>ï¿½B D 20 ï¿½B D
p
20 D 4:5
</p>
<p>Since the source counts are given by S1 D N1 ï¿½ B and S2 D N2 ï¿½ B, we can now
use the approximate variance formulas assuming that the variables are uncorrelated,
ï¿½S1 D
</p>
<p>p
50C 20 D 8:4 and ï¿½S2 D
</p>
<p>p
35C 20 D 7:4. The two measurements
</p>
<p>of the source counts would be reported as S1 D 30 Ë 8:4 and S2 D 15 Ë 7:4,
from which the signal-to-noise ratios are given, respectively, as ï¿½S1=ï¿½S1 D 3:6 and
ï¿½S2=ï¿½S2 D 2:0. }
</p>
<p>4.7.3 Product and Division of Two Random Variables
</p>
<p>Consider the product of two random variables U, V , optionally also with a constant
factor a of either sign,
</p>
<p>Z D aUV: (4.27)
</p>
<p>The partial derivatives are @z=@u D av, @z=@v D au, leading to the approximate
variance of
</p>
<p>ï¿½2z D a2v2ï¿½2u C a2u2ï¿½2v C 2auvï¿½2uv:
</p>
<p>This can be rewritten as
</p>
<p>ï¿½2z
</p>
<p>z2
D ï¿½
</p>
<p>2
u
</p>
<p>u2
C ï¿½
</p>
<p>2
v
</p>
<p>v2
C 2ï¿½
</p>
<p>2
uv
</p>
<p>uv
: (4.28)
</p>
<p>Similarly, the division between two random variables,
</p>
<p>Z D aU
V
; (4.29)
</p>
<p>leads to
</p>
<p>ï¿½2z
</p>
<p>z2
D ï¿½
</p>
<p>2
u
</p>
<p>u2
C ï¿½
</p>
<p>2
v
</p>
<p>v2
ï¿½ 2ï¿½
</p>
<p>2
uv
</p>
<p>uv
: (4.30)</p>
<p/>
</div>
<div class="page"><p/>
<p>74 4 Functions of Random Variables and Error Propagation
</p>
<p>Notice the equations for product and division differ by just one sign, meaning
that a positive covariance between the variables leads to a reduction in the standard
deviation for the division, and an increase in the standard deviation for the product.
</p>
<p>Example 4.12 Using the Thomson experiment of page 23, consider the data for
Tube 1, and assume that the only number available are the mean and standard
deviation of W=Q and I. From these two numbers we want to estimate the mean and
variance of v. The measurement of the two variables are W=Q D 13:3Ë 8:5	 1011
and I D 312:9 Ë 93:4, from which the mean of v would have to be estimated as
v D 8:5 	 109&mdash;compare with the value of 7:9 	 109 obtained from the individual
measurements.
</p>
<p>The estimate of the variance requires also a knowledge of the covariance between
the two variables W=Q and I. In the absence of any information, we will assume that
the two variables are uncorrelated, and use the error propagation formula to obtain
</p>
<p>ï¿½v ' 2 	 13:3 	 10
11
</p>
<p>312:9
	
 ï¿½
</p>
<p>8:5
</p>
<p>13:3
</p>
<p>ï¿½2
</p>
<p>C
ï¿½
93:4
</p>
<p>312:9
</p>
<p>ï¿½2
!1=2
</p>
<p>D 6 	 109;
</p>
<p>which is a factor of 2 larger than estimated directly from the data (see Exam-
ple 4.10). Part of the discrepancy is to be attributed to the neglect of the covariance
between the measurement, which can be found to be positive, and therefore
would reduce the variance of v according to (4.30). Using this approximate
method, we would estimate the measurement as v D 8:5 Ë 6 	 109, instead of
7:9Ë 2:8 	 109. }
</p>
<p>4.7.4 Power of a Random Variable
</p>
<p>A random variable may be raised to a constant power, and optionally multiplied by
a constant,
</p>
<p>Z D aUb (4.31)
</p>
<p>where a and b are constants of either sign. In this case, @z=@u D abubï¿½1 and the
error propagation results in
</p>
<p>ï¿½z
</p>
<p>z
D jbjï¿½u
</p>
<p>u
: (4.32)
</p>
<p>This results states that the relative error in Z is b times the relative error in U.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 The Variance of Functions of Random Variables and Error Propagation. . . 75
</p>
<p>4.7.5 Exponential of a Random Variable
</p>
<p>Consider the function
</p>
<p>Z D aebU; (4.33)
</p>
<p>where a and b are constants of either sign. The partial derivative is @z=@u D abebu,
and we obtain
</p>
<p>ï¿½z
</p>
<p>z
D jbjï¿½u: (4.34)
</p>
<p>4.7.6 Logarithm of a Random Variable
</p>
<p>For the function
</p>
<p>Z D a ln.bU/; (4.35)
</p>
<p>where a is a constant of either sign, and b &gt; 0. The partial derivative is @z=@u D
a=U, leading to
</p>
<p>ï¿½z D jajï¿½u
u
: (4.36)
</p>
<p>A similar result applies for a base-10 logarithm,
</p>
<p>Z D a log.bU/; (4.37)
</p>
<p>where a is a constant of either sign, and b &gt; 0. The partial derivative is @z=@u D
a=.U ln.10//, leading to
</p>
<p>ï¿½z D jaj ï¿½u
u ln.10/
</p>
<p>: (4.38)
</p>
<p>Similar error propagation formulas can be obtained for virtually any analytic
function for which derivatives can be calculated. Some common formulas are
reported for convenience in Table 4.1, where the terms z, u, and v refer to the random
variables evaluated at their estimated mean value.
</p>
<p>Example 4.13 With reference to Example 4.11, we want to give a quantitative
answer to the following question: what is the probability that during the second
time interval the radioactive source was actually detected? In principle a fluctuation
of the number of background counts could give rise to all detected counts.</p>
<p/>
</div>
<div class="page"><p/>
<p>76 4 Functions of Random Variables and Error Propagation
</p>
<p>Table 4.1 Common error propagation formulas
</p>
<p>Function Error propagation formula Notes
</p>
<p>Z D U C a ï¿½2z D ï¿½2u a is a constant
Z D aU C bV ï¿½2z D a2ï¿½2u C b2ï¿½2b C 2abï¿½2uv a, b are constants
Z D aUV ï¿½
</p>
<p>2
z
</p>
<p>z2
D ï¿½
</p>
<p>2
u
</p>
<p>u2
C ï¿½
</p>
<p>2
v
</p>
<p>v2
C 2ï¿½
</p>
<p>2
uv
</p>
<p>uv
a is a constant
</p>
<p>Z D aU
V
</p>
<p>ï¿½2z
</p>
<p>z2
D ï¿½
</p>
<p>2
u
</p>
<p>u2
C ï¿½
</p>
<p>2
v
</p>
<p>v2
ï¿½ 2ï¿½
</p>
<p>2
uv
</p>
<p>uv
a is a constant
</p>
<p>Z D aUb ï¿½ z
z
</p>
<p>D bï¿½u
u
</p>
<p>a, b are constants
</p>
<p>Z D aebU ï¿½ z
z
</p>
<p>D jbjï¿½u a, b are constants
</p>
<p>Z D a ln.bU/ ï¿½z D jajï¿½ u
u
</p>
<p>a, b are constants, b &gt; 0
</p>
<p>Z D a log.bU/ ï¿½z D jaj ï¿½ u
u ln.10/
</p>
<p>a, b are constants, b &gt; 0
</p>
<p>A solution to this question can be provided by stating the problem in a Bayesian
way:
</p>
<p>P(detection)D P(S2 &gt; 0/data)
</p>
<p>where the phrase &ldquo;data&rdquo; refers also to the available measurement of the background
where S2 D N2 ï¿½ B is the number of source counts. This could be elaborated by
stating that the data were used to estimate a mean of 15 and a standard deviation
of 7.4 for S2, and therefore we want to calculate the probability to exceed zero for
such random variable. We can use the Central Limit Theorem to say that the sum
of two random variables&mdash;each approximately distributed as a Gaussian since the
number of counts is sufficiently large&mdash;is Gaussian, and the probability of a positive
detection of the radioactive source therefore becomes equivalent to the probabil-
ity of a Gaussian-distributed variable to have values larger than approximately
ï¿½ ï¿½ 2ï¿½ . According to Table A.3, this probability is approximately 97.7 %. We can
therefore conclude that source were detected in the second time period with such
confidence. }
</p>
<p>4.8 The Quantile Function and Simulation of Random
Variables
</p>
<p>In data analysis one often needs to simulate a random variable, that is, drawing
random samples from a parent distribution. The simplest such case is the generation
of a random number between two limits, which is equivalent to drawing samples
from a uniform distribution. In particular, several Monte Carlo methods including
the Markov chain Monte Carlo method discussed in Chap. 16 will require random</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 The Quantile Function and Simulation of Random Variables 77
</p>
<p>variables with different distributions. Most computer languages and programs do
have available a uniform distribution, and thus it is useful to learn how to simulate
any distribution based on the availability of a simulator for a uniform variable.
</p>
<p>Given a variable X with a distribution f .x/ and a cumulative distribution function
F.x/, we start by defining the quantile function Fï¿½1.p/ as
</p>
<p>Fï¿½1.p/ D minfx"R; p 
 F.x/g (4.39)
</p>
<p>with the meaning that x is the minimum value of the variable at which the cumulative
distribution function reaches the value 0 
 p 
 1. The word &ldquo;minimum&rdquo; in the
definition of the quantile function is necessary to account for those distributions that
have steps&mdash;or discontinuities&mdash;in their cumulative distribution, but in the more
common case of a strictly increasing cumulative distribution, the quantile function
is simply defined by the relationship p D F.x/. This equation can be solved for x, to
obtain the quantile function x D Fï¿½1.p/.
Example 4.14 (Quantile Function of a Uniform Distribution) For a uniform vari-
able in the range 0&ndash;1, the quantile function has a particularly simple form. In fact,
F.x/ D x, and the quantile function defined by the equation p D F.x/ yields x D p,
and therefore
</p>
<p>x D Fï¿½1.p/ D p: (4.40)
</p>
<p>Therefore the analytical form of both the cumulative distribution and the quantile
function is identical for the uniform variable in 0&ndash;1, meaning that, e.g., the value
0.75 of the random variable is the p D 0:75, or 75 % quantile of the distribution. }
</p>
<p>The basic property of the quantile function can be stated mathematically as
</p>
<p>p 
 F.x/, x 
 Fï¿½1.p/ (4.41)
</p>
<p>meaning that the value of Fï¿½1.p/ is the value x at which the probability of having
X ï¿½ x is p.
Example 4.15 (Quantile Function of an Exponential Distribution) Consider a ran-
dom variable distributed like an exponential,
</p>
<p>f .x/ D ï¿½eï¿½ï¿½x;
</p>
<p>with x ï¿½ 0. Its cumulative distribution function is
</p>
<p>F.x/ D 1ï¿½ eï¿½ï¿½x:
</p>
<p>The quantile function is obtained from,
</p>
<p>p D F.x/ D 1 ï¿½ eï¿½ï¿½x;</p>
<p/>
</div>
<div class="page"><p/>
<p>78 4 Functions of Random Variables and Error Propagation
</p>
<p>Fig. 4.2 Distribution function f .x/, cumulative distribution F.x/, and quantile function Fï¿½1.p/ of
an exponential variable with ï¿½ D 1=2
</p>
<p>leading to x D ln.1ï¿½ p/=.ï¿½ï¿½/, and therefore the quantile function is
</p>
<p>x D Fï¿½1.p/ D ln.1 ï¿½ p/ï¿½ï¿½ :
</p>
<p>Figure 4.2 shows the cumulative distribution and the quantile function for the
exponential distribution. }
</p>
<p>4.8.1 General Method to Simulate a Variable
</p>
<p>The method to simulate a random variable is summarized in the following equation,
</p>
<p>X D Fï¿½1.U/; (4.42)
</p>
<p>which states that any random variable X can be expressed in terms of the uniform
variable U between 0 and 1, F is the cumulative distribution of the variable X,
and Fï¿½1 is the quantile function. If a closed analytic form for F is available for
that distribution, this equation results in a simple method to simulate the random
variable.
Proof We have already seen that for the uniform variable the quantile
function is Fï¿½1.U/ D U, i.e., it is the uniform random variable itself. The
proof therefore simply consists of showing that, assuming (4.42), then the
cumulative distribution of X is indeed F.X/, or P.X 
 x/ D F.x/. This can be
shown by writing
</p>
<p>P.X 
 x/ D P.Fï¿½1.U/ 
 x/ D P.U 
 F.x// D F.x/;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 The Quantile Function and Simulation of Random Variables 79
</p>
<p>in which the second equality follows from the definition of the quantile
function, and the last equality follows from the fact that P.U 
 u/ D u,
for u a number between 0 and 1, for a uniform variable. ut
</p>
<p>Example 4.16 (Simulation of an Exponential Variable) Consider a random variable
distributed like an exponential, f .x/ D ï¿½eï¿½ï¿½x, x ï¿½ 0. Given the calculations
developed in the example above, the exponential variable can be simulated as
</p>
<p>X D ln.1 ï¿½U/ï¿½ï¿½ :
</p>
<p>Notice that, although this relationship is between random variables, its practical
use is to draw random samples u from U, and a random sample x from X is obtained
by simply using the equation
</p>
<p>x D ln.1 ï¿½ u/ï¿½ï¿½ :
</p>
<p>Therefore, for a large sample of values u, the above equation returns a random
sample of values for the exponential variable X. }
Example 4.17 (Simulation of the Square of Uniform Variable) It can be proven
that the simulation of the square of a uniform random variable Y D U2 is indeed
achieved by squaring samples from a uniform distribution, a very intuitive result.
</p>
<p>In fact, we start with the distribution of Y as g.y/ D 1=2 yï¿½1=2. Since its
cumulative distribution is given by G.y/ D py, the quantile function is defined
by p D py, or y D p2 and therefore the quantile function for U2 is
</p>
<p>y D Gï¿½1.p/ D p2:
</p>
<p>This result, according to (4.42), defines U2, or the square of a uniform distribution,
as the function that needs to be simulated to draw fair samples from Y. }
</p>
<p>4.8.2 Simulation of a Gaussian Variable
</p>
<p>This method of simulation of random variables relies on the knowledge of F.x/ and
the fact that such a function is analytic and invertible. In the case of the Gaussian
distribution, the cumulative distribution function is a special function,
</p>
<p>F.x/ D 1
2	
</p>
<p>Z x
</p>
<p>ï¿½1
eï¿½
</p>
<p>x2
</p>
<p>2 dx
</p>
<p>which cannot be inverted analytically. Therefore, this method cannot be applied.
This complication must be overcome, given the importance of Gaussian distribution</p>
<p/>
</div>
<div class="page"><p/>
<p>80 4 Functions of Random Variables and Error Propagation
</p>
<p>in probability and statistics. Fortunately, a relatively simple method is available
that permits the simulation of two Gaussian distributions from two uniform random
variables.
</p>
<p>In Sect. 4.4 we showed that the transformation from Cartesian to polar coordi-
nates results in two random variables R; ï¿½ that are distributed, respectively, like a
Rayleigh and a uniform distribution:
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>h.r/ D reï¿½ r
2
</p>
<p>2 r ï¿½ 0
i.ï¿½/ D 1
</p>
<p>2	
0 
 ï¿½ 
 2	:
</p>
<p>(4.43)
</p>
<p>Since these two distributions have an analytic form for their cumulative distribu-
tions, R and ï¿½ can be easily simulated. We can then use the transformation given
by (4.7) to simulate a pair of independent standard Gaussians. We start with the
Rayleigh distribution, for which the cumulative distribution function is
</p>
<p>H.r/ D 1 ï¿½ eï¿½ r
2
</p>
<p>2 :
</p>
<p>The quantile function is given by
</p>
<p>p D 1 ï¿½ eï¿½ r
2
</p>
<p>2 ;
</p>
<p>and from this we obtain
</p>
<p>r Dpï¿½2 ln.1 ï¿½ p/ D Hï¿½1.p/
</p>
<p>and therefore R D pï¿½2 ln.1 ï¿½ U/ simulates a Rayleigh distribution, given the
uniform variable U. For the uniform variable ï¿½, it is clear that the cumulative
distribution is given by
</p>
<p>I.ï¿½/ D
(
ï¿½=.2	/ 0 
 ï¿½ 
 2	
0 otherwiseI
</p>
<p>the quantile function is ï¿½ D 2	p D Iï¿½1.p/, and therefore ï¿½ D 2	V simulates a
uniform distribution between 0 and 2	 , with V the uniform distribution between 0
and 1.
</p>
<p>Therefore, with the use of two uniform distributions U, V , we can use R and ï¿½
to simulate a Rayleigh and a uniform angular distribution
</p>
<p>(
R Dpï¿½2 ln.1ï¿½ U/
ï¿½ D 2	V: (4.44)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 The Quantile Function and Simulation of Random Variables 81
</p>
<p>Then, using the Cartesian-Polar coordinate transformation, we arrive at the formulas
needed to simulate a pair of Gaussians X and Y:
</p>
<p>(
X D R cos.ï¿½/ Dpï¿½2 ln.1 ï¿½ U/ ï¿½ cos.2	V/
Y D R sin.ï¿½/ D pï¿½2 ln.1 ï¿½U/ ï¿½ sin.2	V/ (4.45)
</p>
<p>Equations (4.45) can be easily implemented by having available two simultaneous
and independent uniform variables between 0 and 1.
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Linear combination of variables: The formulas for the mean and variance
of the linear combination of variables are
</p>
<p>(
ï¿½ DP aiï¿½i
ï¿½2 DPNiD1 a2i ï¿½2i C 2
</p>
<p>PN
iD1
</p>
<p>PN
jDiC1 aiajï¿½2ij
</p>
<p>ï¿½ Variance of uncorrelated variables: When variables are uncorrelated
the variances add linearly. The variance of the mean of N independent
measurements is ï¿½2Y D ï¿½2=N.
</p>
<p>ï¿½ Moment generating function: It is a mathematical function that enables the
calculation of moments of a distribution, M.t/ D EÅetXï¿½.
</p>
<p>ï¿½ Central Limit theorem: The sum of a large number of independent
variables is distributed like a Gaussian of mean equal to the sum of the
means and variance equal to the sum of the variances.
</p>
<p>ï¿½ Method of change of variables: A method to obtain the distribution
function of a variable Y that is a function of another variable X, g.y/ D
f .x/dx=dy.
</p>
<p>ï¿½ Law of Large Numbers: The sum of a large number of random variables
with mean ï¿½ tends to a constant number equal to ï¿½.
</p>
<p>ï¿½ Error propagation formula: It is an approximation for the variance of
a function of random variables. For a function x D f .u; v/ of two
uncorrelated variables U and V , the variance of X is given by
</p>
<p>ï¿½2x D ï¿½2u
@f
</p>
<p>@u
</p>
<p>Ë
Ë
Ë
Ë
</p>
<p>2
</p>
<p>C ï¿½2v
@f
</p>
<p>@v
</p>
<p>Ë
Ë
Ë
Ë
</p>
<p>2
</p>
<p>ï¿½ Quantile function: It is the function x D Fï¿½1.p/ used to find the value x of
a variable that corresponds to a given quantile p.
</p>
<p>ï¿½ Simulation of a Gaussian: Two Gaussians can be obtained from two
uniform random variables U;V via
</p>
<p>(
X D pï¿½2 ln.1 ï¿½U/ cos.2	V/
Y D pï¿½2 ln.1 ï¿½U/ sin.2	V/</p>
<p/>
</div>
<div class="page"><p/>
<p>82 4 Functions of Random Variables and Error Propagation
</p>
<p>Problems
</p>
<p>4.1 Consider the data from Thomson&rsquo;s experiment of Tube 1, from page 23.
</p>
<p>(a) Calculate the mean and standard deviation of the measurements of v.
(b) Use the results from Problem 2.3, in which the mean and standard deviation of
</p>
<p>W=Q and I were calculated, to calculate the approximate values of mean and
standard deviation of v using the relevant error propagation formula, assuming
no correlation between the two measurements.
</p>
<p>This problem illustrates that the error propagation formulas may give different
results than direct measurement of the mean and variance of a variable, when the
individual measurements are available.
</p>
<p>4.2 Calculate the mean, variance, and moment generating function M.t/ for a
uniform random variable in the range 0&ndash;1.
</p>
<p>4.3 Consider two uniform independent random variables X, Y in the range ï¿½1
to 1.
</p>
<p>(a) Determine the distribution function, mean and variance, and the moment
generating function of the variables.
</p>
<p>(b) We speculate that the sum of the two random variables is distributed like a
&ldquo;triangular&rdquo; distribution between the range ï¿½2 to 2, with distribution function
</p>
<p>f .x/ D
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>1
</p>
<p>2
C x
4
</p>
<p>if ï¿½2 
 x 
 0
1
</p>
<p>2
ï¿½ x
4
</p>
<p>if 0 
 x 
 2
</p>
<p>Using the moment generation function, prove that the variable Z D X C Y is
distributed like the triangular distribution above.
</p>
<p>4.4 Using a computer language of your choice, simulate the sum of N D 100
uniform variables in the range 0&ndash;1, and show that the sampling distribution of the
sum of the variables is approximately described by a Gaussian distribution with
mean equal to the mean of the N uniform variables and variance equal to the sum of
the variances. Use 1,000 and 100,000 samples for each variable.
</p>
<p>4.5 Consider the J.J. Thomson experiment of page 23.
</p>
<p>(a) Calculate the sample mean and the standard deviation of m=e for Tube 1.
(b) Calculate the approximate mean and standard deviation of m=e from the mean
</p>
<p>and standard deviation of W=Q and I, according to the equation
</p>
<p>m
</p>
<p>e
D I
</p>
<p>2
</p>
<p>2
</p>
<p>Q
</p>
<p>W
I
</p>
<p>Assume that W=Q and I are uncorrelated.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.8 The Quantile Function and Simulation of Random Variables 83
</p>
<p>4.6 Use the data provided in Example 4.11. Calculate the probability of a positive
detection of source counts S in the first time period (where there are N1 D 50
total counts and B D 20 background counts), and the probability that the source
emitted ï¿½10 source counts. You will need to assume that the measured variable can
be approximated by a Gaussian distribution.
</p>
<p>4.7 Consider the data in the Thomson experiment for Tube 1 and the fact that the
variables W=Q and I are related to the variable v via the relationship
</p>
<p>v D 2W
QI
:
</p>
<p>Calculate the sample mean and variance of v from the direct measurements of this
variable, and then using the measurements of W=Q and I and the error propagation
formulas. By comparison of the two estimates of the variance, determine if there is
a positive or negative correlation between W=Q and I.
</p>
<p>4.8 Provide a general expression for the error propagation formula when three
independent random variables are present, to generalize (4.24) that is valid for two
variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 5
Maximum Likelihood and Other Methods
to Estimate Variables
</p>
<p>Abstract In this chapter we study the problem of estimating parameters of the
distribution function of a random variable when N observations of the variable
are available. We discuss methods that establish what sample quantities must be
calculated to estimate the corresponding parent quantities. This establishes a firm
theoretical framework that justifies the definition of the sample variance as an
unbiased estimator of the parent variance, and the sample mean as an estimator
of the parent mean. One of these methods, the maximum likelihood method, will
later be used in more complex applications that involve the fit of two-dimensional
data and the estimation of fit parameters. The concepts introduced in this chapter
constitute the core of the statistical techniques for the analysis of scientific data.
</p>
<p>5.1 The Maximum Likelihood Method for Gaussian
Variables
</p>
<p>Consider a random variableX distributed like a Gaussian. The probability of making
a measurement between xi and xi C dx is given by
</p>
<p>f .xi/dx D 1p
2	ï¿½2
</p>
<p>eï¿½
.xiï¿½ï¿½/2
2ï¿½2 dx:
</p>
<p>This probability describes the likelihood of collecting the data point xi given that
the distribution has a fixed value of ï¿½ and ï¿½ . Assume now that N measurements of
the random variable have been made. The goal is to estimate the most likely values
of the true&mdash;yet unknown&mdash;values of ï¿½ and ï¿½ , the two parameters that determine
the distribution of the random variable. The method of analysis that follows this
principle is called the maximum likelihood method. The method is based on the
postulate that the values of the unknown parameters are those that yield a maximum
probability of observing the measured data. Assuming that the measurements are
made independently of one another, the quantity
</p>
<p>P D
NY
</p>
<p>iD1
P.xi/ D
</p>
<p>NY
</p>
<p>iD1
</p>
<p>1p
2	ï¿½2
</p>
<p>e
ï¿½
.xi ï¿½ ï¿½/2
2ï¿½2 (5.1)
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_5
</p>
<p>85</p>
<p/>
</div>
<div class="page"><p/>
<p>86 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>is the probability of making N independent measurements in intervals of unit length
around the values xi, which can be viewed as the probability of measuring the dataset
composed of the given N measurements.
</p>
<p>The method of maximum likelihood consists therefore of finding the parameters
of the distribution that maximize the probability in (5.1). This is simply achieved
by finding the point at which the first derivative of the probability P with respect to
the relevant parameter of interest vanishes, to find the extremum of the function. It
can be easily proven that the second derivative with respect to the two parameters is
negative at the point of extremum, and therefore this is a point of maximum for the
likelihood function.
</p>
<p>5.1.1 Estimate of the Mean
</p>
<p>To find the maximum-likelihood estimate of the mean of the Gaussian distribution
we proceed with the calculation of the first derivative of lnP, instead of P, with
respect to the mean ï¿½. Given that the logarithm is a monotonic function of the
argument, maximization of lnP is equivalent to that of P, and the logarithm has the
advantage of ease of computation. We obtain
</p>
<p>@
</p>
<p>@ï¿½
</p>
<p>NX
</p>
<p>iD1
</p>
<p>.xi ï¿½ ï¿½/2
2ï¿½2
</p>
<p>D 0:
</p>
<p>The solution is the maximum-likelihood estimator of the mean, which we define as
ï¿½ML, and is given by
</p>
<p>ï¿½ML D 1
N
</p>
<p>NX
</p>
<p>iD1
xi D x: (5.2)
</p>
<p>This result was to be expected: the maximum likelihood method shows that the
&ldquo;best&rdquo; estimate of the mean is simply the sample average of the measurements.
</p>
<p>The quantity ï¿½ML is a quantity that, despite the Greek letter normally reserved
for parent quantities, is a function of the measurements. Although it appears obvious
that the sample average is the correct estimator of the true mean, it is necessary to
prove this statement by calculating its expectation. It is clear that the expectation of
the sample average is in fact
</p>
<p>EÅNxï¿½ D 1
N
EÅx1 C : : :C xN ï¿½ D ï¿½;
</p>
<p>This calculation is used to conclude that the sample mean is an unbiased estimator
of the true mean.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Maximum Likelihood Method for Gaussian Variables 87
</p>
<p>5.1.2 Estimate of the Variance
</p>
<p>Following the same method used to estimate the mean, we can also take the first
derivative of lnP with respect to ï¿½2, to obtain
</p>
<p>@
</p>
<p>@ï¿½2
</p>
<p>ï¿½
</p>
<p>N ln
1p
2	ï¿½2
</p>
<p>ï¿½
</p>
<p>C @
@ï¿½2
</p>
<p>ï¿½X
ï¿½1
2
</p>
<p>.xi ï¿½ ï¿½/2
ï¿½2
</p>
<p>ï¿½
</p>
<p>D 0
</p>
<p>from which we obtain
</p>
<p>N
</p>
<p>ï¿½
</p>
<p>ï¿½ 1
2
</p>
<p>1
</p>
<p>ï¿½2
</p>
<p>ï¿½
</p>
<p>C
X
ï¿½1
2
.xi ï¿½ ï¿½/2
</p>
<p>ï¿½
</p>
<p>ï¿½ 1
ï¿½4
</p>
<p>ï¿½
</p>
<p>D 0
</p>
<p>and finally the result that the maximum likelihood estimator of the variance is
</p>
<p>ï¿½2ML D
1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
.xi ï¿½ ï¿½/2: (5.3)
</p>
<p>It is necessary to notice that in the maximum likelihood estimate of the variance
we have implicitly assumed that the mean ï¿½ was known, while in reality we can
only estimate it as the sample mean, from the same data used also to estimate the
variance. To account for the fact that ï¿½ is not known, we replace it with x in (5.3),
and call
</p>
<p>s2ML D
1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
.xi ï¿½ x/2 (5.4)
</p>
<p>the maximum likelihood sample variance estimator, which differs from the sample
variance defined in (2.11) by a factor of .N ï¿½ 1/=N. The fact that x replaced ï¿½ in its
definition leads to the following expectation:
</p>
<p>EÅs2MLï¿½ D
N ï¿½ 1
N
</p>
<p>ï¿½2ML: (5.5)
</p>
<p>Proof Calculation of the expectation is obtained as
</p>
<p>EÅs2MLï¿½ D EÅ
1
</p>
<p>N
</p>
<p>X
.xi ï¿½ Nx/2ï¿½ D 1
</p>
<p>N
EÅ
X
</p>
<p>.xi ï¿½ ï¿½C ï¿½ ï¿½ Nx/2ï¿½
</p>
<p>D 1
N
EÅ
X
</p>
<p>.xi ï¿½ ï¿½/2 C
X
</p>
<p>.ï¿½ï¿½ Nx/2 C 2.ï¿½ï¿½ Nx/
X
</p>
<p>.xi ï¿½ ï¿½/ï¿½</p>
<p/>
</div>
<div class="page"><p/>
<p>88 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>The term EÅ
P
.ï¿½ ï¿½ Nx/2ï¿½ is the variance of the sample mean, which we
</p>
<p>know from Sect. 4.1 to be equal to ï¿½2=N. The last term in the equation isP
.xi ï¿½ ï¿½/ D N.Nx ï¿½ ï¿½/, therefore:
</p>
<p>EÅs2MLï¿½ D
1
</p>
<p>N
</p>
<p>ï¿½
EÅ
X
</p>
<p>.xi ï¿½ ï¿½/2ï¿½C NEÅ.ï¿½ ï¿½ Nx/2ï¿½C 2NEÅ.ï¿½ ï¿½ Nx/.Nx ï¿½ ï¿½/ï¿½
	
</p>
<p>D 1
N
.Nï¿½2 C Nï¿½2=N ï¿½ 2NEÅ.ï¿½ ï¿½ Nx/2ï¿½/
</p>
<p>leading to the result that
</p>
<p>EÅs2MLï¿½ D
1
</p>
<p>N
</p>
<p>

Nï¿½2 C Nï¿½2=N ï¿½ 2Nï¿½2=Nï¿½ D N ï¿½ 1
</p>
<p>N
ï¿½2:
</p>
<p>In this proof we used the notation ï¿½2 D ï¿½2ML ut
This result is at first somewhat surprising, since there is an extra factor .Nï¿½1/=N
</p>
<p>that makes EÅs2MLï¿½ different from the maximum likelihood estimator of ï¿½
2. This
</p>
<p>is actually due to the fact that, in estimating the variance, the mean needed to be
estimated as well and was not known beforehand. The unbiased estimator of the
variance is therefore
</p>
<p>s2 D s2ML 	
N
</p>
<p>N ï¿½ 1 D
1
</p>
<p>N ï¿½ 1
NX
</p>
<p>iD1
.xi ï¿½ x/2 (5.6)
</p>
<p>for which we have shown that EÅs2ï¿½ D ï¿½2. This is the reason for the definition of
the sample variance according to (5.6), and not (5.4).
</p>
<p>It is important to pay attention to the fact that (5.6) defines a statistic for which we
could also find, in addition to its expectation, also its variance, similar to what was
done for the sample mean. In Chap. 7 we will study how to determine the probability
distribution function of certain statistics of common use, including the distribution
of the sample variance s2.
</p>
<p>Example 5.1 We have already made use of the sample mean and the sample
variance as estimators for the parent quantities in the analysis of the data from
Thomson&rsquo;s experiment (page 23). The estimates we obtained are unbiased if the
assumptions of the maximum likelihood method are satisfied, namely that I and
W=Q are Gaussian distributed. }
</p>
<p>5.1.3 Estimate of Mean for Non-uniform Uncertainties
</p>
<p>In the previous sections we assumed a set of measurements xi of the same random
variable, i.e., the parent mean ï¿½ and variance ï¿½2 were the same. It is often the case</p>
<p/>
</div>
<div class="page"><p/>
<p>5.1 The Maximum Likelihood Method for Gaussian Variables 89
</p>
<p>with real datasets that observations are made from variables with the same mean,
but with different variance. This could be the case when certain measurements are
more precise than others, and therefore they feature the same mean (since they are
drawn from the same process), but the standard error varies with the precision of the
instrument, or because some measurements were performed for a longer period of
time. In this case, each measurement xi is assigned a different standard deviation ï¿½i,
which represents the precision with which that measurement was made.
</p>
<p>Example 5.2 A detector is used to measure the rate of arrival of a certain species of
particles. One measurement consists of 100 counts in 10 s, another of 180 particles
in 20 s, and one of 33 particles in 3 s. The measured count rates would be reported
as, respectively, 10.0, 9.0, and 11.0 counts per second. Given that this is a counting
experiment, the Poisson distribution applies to each of the measurements. Moreover,
since the number of counts is sufficiently large, it is reasonable to approximate the
Poisson distribution with a Gaussian, with variance equal to the mean. Therefore
the variance of the counts is 100, 180, and 33, and the variance of the count rate
can be calculated by the property that VarÅX=tï¿½ D VarÅXï¿½=t2, where t is the known
time of each measurement. It follows that the standard deviation ï¿½ of the count
rates is, respectively, 1.0, 0.67, and 1.91 for the three measurements. The three
measurements would be reported as 10:0Ë 1:0, 9:0Ë 0:67, and 11:0Ë 1:91, with
the last measurement being clearly of lower precision because of the shorter period
of observation. }
</p>
<p>Our goal is therefore now focused on the maximum likelihood estimate of the
parent mean ï¿½, which is the same for all measurements. This is achieved by using
(5.1) in which the parent mean of each measurement is ï¿½, and the parent variance
of each measurement is ï¿½2i . Following the same procedure as in the case of equal
standard deviations for each measurement, we start with the probability P of making
the N measurements,
</p>
<p>P D
NY
</p>
<p>iD1
P.xi/ D
</p>
<p>NY
</p>
<p>iD1
</p>
<p>1
q
</p>
<p>2	ï¿½2i
</p>
<p>e
ï¿½ .xiï¿½ï¿½/
</p>
<p>2
</p>
<p>2ï¿½2i : (5.7)
</p>
<p>Setting the derivative of lnP with respect to ï¿½&mdash;the common mean to all
measurements&mdash;equal to zero, we obtain that the maximum likelihood estimates is
</p>
<p>ï¿½ML D
PN
</p>
<p>iD1.xi=ï¿½2i /
PN
</p>
<p>iD1.1=ï¿½2i /
(5.8)
</p>
<p>This is the weighted mean of the measurements, where the weights are the inverse
of the variance of each measurement, 1=ï¿½2i .</p>
<p/>
</div>
<div class="page"><p/>
<p>90 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>The variance in this weighted sample mean can be calculated using the expecta-
tion of the weighted mean, by assuming that the ï¿½2i are constant numbers:
</p>
<p>Var.ï¿½ML/ D 1ï¿½PN
iD1.1=ï¿½2i /
</p>
<p>	2 	
NX
</p>
<p>iD1
</p>
<p>Var.xi/
</p>
<p>ï¿½4i
D
</p>
<p>PN
iD1.1=ï¿½2i /
</p>
<p>ï¿½PN
iD1.1=ï¿½2i /
</p>
<p>	2 ;
</p>
<p>which results in
</p>
<p>ï¿½2ï¿½ D
1
</p>
<p>PN
iD1.1=ï¿½2i /
</p>
<p>: (5.9)
</p>
<p>The variance of the weighted mean on (5.9) becomes the usual ï¿½2=N if all variances
ï¿½2i are identical.
</p>
<p>Example 5.3 Continuing Example 5.2 of the count rate of particle arrivals, we use
(5.8) and (5.9) to calculate a weighted mean and standard deviation of 9.44 and
0.53. Since the interest is just in the overall mean of the rate, the more direct means
to obtain this number is by counting a total of 313 counts in 33 s, for an overall
measurement of the count rate of 9:48 Ë 0:54, which is virtually identical to that
obtained using the weighted mean and its variance. }
</p>
<p>It is common, as in the example above, to assume that the parent variance ï¿½2i is
equal to the value estimated from the measurements themselves. This approximation
is necessary, unless the actual precision of the measurement is known beforehand
by some other means, for example because the apparatus used for the experiment
has been calibrated by prior measurements.
</p>
<p>5.2 The Maximum Likelihood Method for Other
Distributions
</p>
<p>The method of maximum likelihood can also be applied when the measurements do
not follow a Gaussian distribution.
</p>
<p>A typical case is that of N measurements ni, i D 1; : : : ;N, from a Poisson
variableN of parameter ï¿½, applicable to all situations in which the measurements are
derived from a counting experiment. In this case, the maximum likelihood method
can be used to estimate ï¿½, which is the mean of the random variable, and the only
parameter of the Poisson distribution.
</p>
<p>The Poisson distribution is discrete in nature and the probability of making N
independent measurements is simply given by
</p>
<p>P D
NY
</p>
<p>iD1
</p>
<p>ï¿½ni
</p>
<p>niÅ 
eï¿½ï¿½:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Method of Moments 91
</p>
<p>It is convenient to work with logarithms,
</p>
<p>lnP D
NX
</p>
<p>iD1
ln eï¿½ï¿½ C
</p>
<p>NX
</p>
<p>iD1
ln
1
</p>
<p>niÅ 
C
</p>
<p>NX
</p>
<p>iD1
lnï¿½ni D A ï¿½ Nï¿½C lnï¿½
</p>
<p>NX
</p>
<p>iD1
ni
</p>
<p>in which A is a term that doesn&rsquo;t depend on ï¿½. The condition that the probability
must be maximum requires @P=@ï¿½ D 0. This condition results in
</p>
<p>1
</p>
<p>ï¿½
</p>
<p>NX
</p>
<p>iD1
xi ï¿½ N D 0;
</p>
<p>and therefore we obtain that the maximum likelihood estimator of the ï¿½ parameter
of the Poisson distribution is
</p>
<p>ï¿½ML D 1
N
</p>
<p>NX
</p>
<p>iD1
ni:
</p>
<p>This result was to be expected, since ï¿½ is the mean of the Poisson distribution, and
the linear average of N measurements is an unbiased estimate of the mean of a
random variable, according to the Law of Large Numbers.
</p>
<p>The maximum likelihood method can in general be used for any type of
distribution, although often the calculations can be mathematically challenging if
the distribution is not a Gaussian.
</p>
<p>5.3 Method of Moments
</p>
<p>The method of moments takes a more practical approach to the estimate of the
parameters of a distribution function. Consider a random variable X for which we
have N measurements and whose probability distribution function f(x) depends on
M unknown parameters, for example ï¿½1 D ï¿½ and ï¿½2 D ï¿½2 for a Gaussian (M D 2),
or ï¿½1 D ï¿½ for an exponential (M D 1), etc. The idea is to develop a method that
yields as many equations as there are free parameters and solve for the parameters
of the distribution. The method starts with the determination of arbitrary functions
aj.x/; j D 1; : : :M, that make the distribution function integrable:
</p>
<p>EÅaj.X/ï¿½ D
Z 1
</p>
<p>ï¿½1
aj.x/f .x/dx D gj.ï¿½/ (5.10)
</p>
<p>where gj.ï¿½/ is an analytic function of the parameters of the distribution. Although
we have assumed that the random variable is continuous, the method can also be
applied to discrete distributions. According to the law of large numbers, the left-</p>
<p/>
</div>
<div class="page"><p/>
<p>92 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>hand side of (5.10) can be approximated by the sample mean of the function of the
N measurements, and therefore we obtain a linear system of M equations:
</p>
<p>1
</p>
<p>N
.aj.x1/C : : :C aj.xN// D gj.ï¿½/ (5.11)
</p>
<p>which can be solved for the parameters ï¿½ as function of the N measurements xi.
As an illustration of the method, consider the case in which the parent distribution
</p>
<p>is a Gaussian of parameters ï¿½, ï¿½2. First, we need to decide which functions a1.x/
and a2.x/ to choose. A simple and logical choice is to use a1.x/ D x and a2.x/ D x2;
this choice is what gives the name of &ldquo;moments,&rdquo; since the right-hand side of (5.10)
will be, respectively, the first and second order moment. Therefore we obtain the
two equations
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>EÅa1.X/ï¿½ D 1
N
.X1 C : : :C XN/ D ï¿½
</p>
<p>EÅa2.X/ï¿½ D 1
N
.X21 C : : :C X2N/ D ï¿½2 C ï¿½2:
</p>
<p>(5.12)
</p>
<p>The estimator for mean and variance are therefore
8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½MM D 1
N
.X1 C : : :C XN/
</p>
<p>ï¿½2MM D
1
</p>
<p>N
.X21 C : : :C X2N/ ï¿½
</p>
<p>ï¿½
1
</p>
<p>N
.X1 C : : :C XN/
</p>
<p>ï¿½2
</p>
<p>D 1
N
</p>
<p>P
.xi ï¿½ ï¿½MM/2
</p>
<p>(5.13)
</p>
<p>which, in this case, are identical to the estimates obtained from the likelihood
method. This method is often easier computationally than the method of maximum
likelihood, since it does not require the maximization of a function, but just a careful
choice of the integrating functions aj.x/. Also, notice that in this application we
did not make explicit use of the assumption that the distribution is a Gaussian,
since the same results will apply to any distribution function with mean ï¿½ and
variance ï¿½2. Equation (5.13) can therefore be used in a variety of situations in which
the distribution function has parameters that are related to the mean and variance,
even if they are not identical to them, as in the case of the Gaussian. The method
of moments therefore returns unbiased estimates for the mean and variance of every
distribution in the case of a large number of measurements.
</p>
<p>Example 5.4 Consider the five measurements presented in Example 4.9: 0.1, 0.3,
0.5, 0.7, and 0.9, and assume that they are known to be drawn from a uniform
distribution between 0 and a. The method of moments can be used to estimate the
parameter a of the distribution from the measurements. The probability distribution
function is f .x/ D 1=a between 0 and a, and null otherwise. Using the integrating
function a1.x/ D x, the method of moments proceeds with the calculation of the</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Quantiles and Confidence Intervals 93
</p>
<p>first moment of the distribution,
</p>
<p>EÅXï¿½ D
Z a
</p>
<p>0
</p>
<p>xf .x/dx D a
2
</p>
<p>Therefore, using (5.12), we can estimate the only parameter a of the distribution
function as
</p>
<p>a D 2 	 1
N
</p>
<p>NX
</p>
<p>iD1
xi
</p>
<p>where N D 5, for the result of a D 1. The result confirms that the five measurements
are compatible with a parent mean of 1=2. }
</p>
<p>5.4 Quantiles and Confidence Intervals
</p>
<p>The parameters of the distribution function can be used to determine the range
of values that include a given probability, for example, 68.3 %, or 90 %, or 99 %,
etc. This range, called confidence interval, can be conveniently described by the
cumulative distribution function F.x/.
</p>
<p>Define the Ë-quantile xË, where Ë is a number between 0 and 1, as the value of
the variable such that x 
 xË with probability Ë:
</p>
<p>Ë quantile xË W P.x 
 xË D Ë/ or F.xË/ D Ë: (5.14)
</p>
<p>For example, consider the cumulative distribution shown in Fig. 5.1 (right panel):
the Ë D 0:05 quantile is the number of the variable x where the lower horizontal
dashed line intersects the cumulative distribution F.x/, xË ' 0:2, and the Ë D 0:95
quantile is the number of the variable x where the upper dashed line intersects F.x/,
xË ' 6. Therefore the range xË to xË , or 0.2&ndash;6, corresponds to the .Ë ï¿½ Ë/ D 90%
confidence interval, i.e., there is 90 % probability that a measurement of the variable
falls in that range. These confidence intervals are called central because they are
centered at the mean (or median) of the distribution, and are the most commonly
used type of confidence intervals.
</p>
<p>Confidence intervals can be constructed at any confidence level desired, depend-
ing on applications and on the value of probability that the analyzer wishes to
include in that interval. It is common to use 68 % confidence intervals because
this is the probability between Ëï¿½ of the mean for a Gaussian variable (see
Sect. 5.4.1). Normally a confidence interval or limit at a significance lower than
68 % is not considered interesting, since there is a significant probability that the
random variable will be outside of this range.
</p>
<p>One-sided confidence intervals that extends down to ï¿½1, or to the lowest value
allowed for that random variable, is called an upper limit, and intervals that extend</p>
<p/>
</div>
<div class="page"><p/>
<p>94 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>Fig. 5.1 (Left) Distribution function of an exponential variable with central 68 and 90 % confi-
dence interval marked by, respectively, dot-dashed and dotted lines. (Right) The confidence interval
are obtained as the intersection of dot-dashed and dotted lines with the cumulative distribution
(solid line)
</p>
<p>to C1, or to the highest allowed value, is called a lower limit. A lower limit
describes a situation in which a large number is detected, for example counts from
a Poisson experiment, and we want to describe how small the value of the variable
can be, and still be consistent with the data. An upper limit is used for a situation
in which a small number is detected, to describe how high can the variable be and
still be consistent with the data. Lower and upper limits depend on the value of the
probability that we want to use; for example, using a value for Ë that is closer to 0
results in a lower limit that progressively becomes ï¿½lo D ï¿½1 (or lowest allowed
value), which is not a very interesting statement. If Ë is progressively closer to 1,
the upper limit will tend to ï¿½up D1.
</p>
<p>5.4.1 Confidence Intervals for a Gaussian Variable
</p>
<p>When the variable is described by a Gaussian distribution function we can use
integral tables (Table A.2) to determine confidence intervals that enclose a given
probability. It is usually meaningful to have central confidence intervals, i.e.,
intervals centered at the mean of the distribution and extending by equal amounts on
either side of the mean. For central confidence intervals, the relationship between
the probability p enclosed by a given interval (say p D 0:9 or 90 % confidence
interval) and the size 
x D 2.z 	 ï¿½/ of the interval is given by
</p>
<p>p D
Z ï¿½Czï¿½
</p>
<p>ï¿½ï¿½zï¿½
f .x/dx; (5.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Quantiles and Confidence Intervals 95
</p>
<p>where f .x/ is a Gaussian of mean ï¿½ and variance ï¿½2. The number z represents the
number of standard deviations allowed by the interval in each direction (positive
and negative relative to the mean). The most common central confidence intervals
for a Gaussian distribution are reported in Table 5.1. For example, for a mean ï¿½
and variance ï¿½2 and the interval from ï¿½ ï¿½ ï¿½ to ï¿½ C ï¿½ is a 68.3 % confidence
interval, the interval from ï¿½ ï¿½ 1:65ï¿½ to ï¿½ C 1:65ï¿½ is a 90 % confidence interval.
In principle, one could have confidence intervals that are not centered on the mean
of the distribution&mdash;such intervals would still be valid confidence intervals. It can
be shown that central confidence intervals are the smallest possible, for a given
confidence level.
</p>
<p>Example 5.5 Using the data for the J.J. Thomson experiment on the measurement
of the electron&rsquo;s mass-to-charge ratio, we can calculate the 90 % confidence interval
on m=e for Tube 1 and Tube 2. For Tube 1, we estimated the mean as ï¿½1 D 0:42
and the standard error as ï¿½1 D 0:07, and for Tube 2 ï¿½2 D 0:53 and ï¿½ D 0:08.
Since the random variable is assumed to be Gaussian, the 90 % confidence interval
corresponds to the range between ï¿½ ï¿½ 1:65ï¿½ and ï¿½ C 1:65ï¿½ ; therefore for the
Thomson measurements of Tube 1 and Tube 2, the 90 % central confidence intervals
are, respectively, 0.30&ndash;0.54 and 0.40&ndash;0.66. }
</p>
<p>Upper and lower limits can be easily calculated using the estimates of ï¿½ and
ï¿½ for a Gaussian variable. They are obtained numerically from the following
relationships,
</p>
<p>p D
Z ï¿½up
</p>
<p>ï¿½1
f .x/dx D F.ï¿½up/ upper limit ï¿½up
</p>
<p>p D
Z 1
</p>
<p>ï¿½lo
</p>
<p>f .x/dx D 1 ï¿½ F.ï¿½lo/ lower limit ï¿½lo
(5.16)
</p>
<p>making use of Tables A.2 and A.3. The quantites F.ï¿½up/ and F.ï¿½lo/ are the values
of the cumulative distribution of the Gaussian, showing that ï¿½up is the p-quantile
and ï¿½lo is the .1 ï¿½ p/-quantile of the distribution. Useful upper and lower limits
for the Gaussian distribution are reported in Table 5.2.Upper limits are typically of
interest when the measurements result in a low value of the mean of the variable.
In this case we usually want to know how high the variable can be and still be
</p>
<p>Table 5.1 Common confidence intervals for a Gaussian distribution
</p>
<p>Interval Range Enclosed probability (%)
</p>
<p>50 % confidence interval ï¿½ï¿½ 0:68ï¿½; ï¿½C 0:68ï¿½ 50
1-ï¿½ interval ï¿½ï¿½ ï¿½; ï¿½C ï¿½ 68:3
90 % confidence interval ï¿½ï¿½ 1:65ï¿½; ï¿½C 1:65ï¿½ 90
2-ï¿½ interval ï¿½ï¿½ 2ï¿½; ï¿½C 2ï¿½ 95:5
3-ï¿½ interval ï¿½ï¿½ 3ï¿½; ï¿½C 3ï¿½ 99:7</p>
<p/>
</div>
<div class="page"><p/>
<p>96 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>Table 5.2 Common upper and lower limits for a Gaussian distribution
</p>
<p>Enclosed Enclosed
</p>
<p>Upper limit Range probability (%) Lower limit Range probability (%)
</p>
<p>50 % confidence ï¿½ ï¿½ 50 50 % confidence ï¿½ ï¿½ 50
90 % confidence ï¿½ ï¿½C 1:28ï¿½ 90 90 % confidence ï¿½ ï¿½ï¿½ 1:28ï¿½ 90
95 % confidence ï¿½ ï¿½C 1:65ï¿½ 95 95 % confidence ï¿½ ï¿½ï¿½ 1:65ï¿½ 95
99 % confidence ï¿½ ï¿½C 2:33ï¿½ 99 99 % confidence ï¿½ ï¿½ï¿½ 2:33ï¿½ 99
1-ï¿½ ï¿½ ï¿½C ï¿½ 84.1 1-ï¿½ ï¿½ ï¿½ï¿½ ï¿½ 84.1
2-ï¿½ ï¿½ ï¿½C 2ï¿½ 97.7 2-ï¿½ ï¿½ ï¿½ï¿½ 2ï¿½ 97.7
3-ï¿½ ï¿½ ï¿½C 3ï¿½ 99.9 3-ï¿½ ï¿½ ï¿½ï¿½ 3ï¿½ 99.9
</p>
<p>consistent with the measurement, at a given confidence level. For example, in the
case of the measurement of Tube 1 for the Thomson experiment, the variable m=e
was measured to be 0:42 Ë 0:07. In this case, it is interesting to ask the question
of how high can m=e be and still be consistent with the measurement at a given
confidence level.
</p>
<p>Example 5.6 Using the data for the J.J. Thomson experiment on the measurement
of the electron&rsquo;s mass-to-charge ratio, we can calculate the 90 % upper limits to m=e
for Tube 1 and Tube 2. For Tube 1, we estimated the mean as ï¿½1 D 0:42 and the
standard error as ï¿½1 D 0:07, and for Tube 2 ï¿½2 D 0:53 and ï¿½ D 0:08.
</p>
<p>To determine the upper limit m=eUL;90 of the ratio, we calculate the probability
of occurrence of m=e 
 m=eUL;90:
</p>
<p>P.m=e 
 m=eUL;90/ D 0:90
</p>
<p>Since the random variable is assumed to be Gaussian, the value x ' ï¿½ C 1:28ï¿½
corresponds to the 90 percentile of the distribution (see Table 5.2). The two 90 %
upper limits are, respectively, 0.51 and 0.63. }
</p>
<p>A common application of upper limits is when an experiment has failed to
detect the variable of interest. In this case we have a non-detection and we want
to place upper limits based on the measurements made. This problem is addressed
by considering the parent distribution of the variable that we did not detect, for
example a Gaussian of zero mean and given variance. We determine the upper limit
as the value of the variable that exceeds the mean by 1, 2 or 3 ï¿½ , corresponding
to the probability levels shown in Table 5.2. A 3-ï¿½ upper limit, for example, is the
value of the variable that has only a 0.1 % chance of being observed based on the
parent distribution for the non-detection, and therefore we are 99.9 % confident that
the true value of the variable is lower than this upper limit.
</p>
<p>Example 5.7 (Gaussian Upper Limit to the Non-detection of a Source) A measure-
ment of n D 8 counts in a given time interval is made in the presence of a source
of unknown intensity. The instrument used for the measurement has a background
level with a mean of 9:8Ë0:4 counts, as estimated from an independent experiment</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Quantiles and Confidence Intervals 97
</p>
<p>of long duration. Given that the measurement is below the expected background
level, it is evident that there is no positive detection of the source. The hypothesis
that the source has zero emission can be described by a distribution function with
a mean of approximately 9.8 counts and, since this is a counting experiment, the
probability distribution of counts should be Poisson. We are willing to approximate
the distribution with a Gaussian of same mean, and variance equal to the mean, or
ï¿½ ' 3:1, to describe the distribution of counts one expects from an experiment of
the given duration as the one that yielded n D 8 counts.
</p>
<p>A 99 % upper limit to the number of counts that can be recorded by this
instrument, in the given time interval, can be calculated according to Table 5.2 as
</p>
<p>ï¿½C 2:33ï¿½ D 9:8C 2:33 	 3:1 ' 17:
</p>
<p>This means that we are 99 % confident that the true value of the source plus
background counts is less than 17. A complementary way to interpret this number
is that the experimenter can be 99 % sure that the measurement cannot be due to
just the background if there is a detection of ï¿½17 total counts. A conservative
analyst might also want to include the possibility that the Gaussian distribution
has a slightly higher mean, since the level of the background is not known exactly,
and conservatively assume that perhaps 18 counts are required to establish that the
source does have a positive level of emission. After subtraction of the assumed
background level, we can conclude that the 99 % upper limit to the source&rsquo;s true
emission level in the time interval is 8.2 counts. This example was adapted from the
analysis of an astronomical source that resulted in a non-detection [4]. }
</p>
<p>5.4.2 Confidence Intervals for the Mean of a Poisson Variable
</p>
<p>The Poisson distribution does not have the simple analytical properties of the
Gaussian distribution. For this distribution it is convenient to follow a different
method to determine its confidence intervals.
</p>
<p>Consider the case of a single measurement of a Poisson variable of unknown
mean ï¿½ for which nobs was recorded. We want to make inferences on the parent
mean based on this information. Also, we assume that the measurement includes
a uniform and known background ï¿½B. The measurement is therefore drawn from a
random variable
</p>
<p>X D NS C NB (5.17)
</p>
<p>in which NB D ï¿½B is assumed to be a constant, i.e., the background is known exactly
(this generalization can be bypassed by simply setting ï¿½B D 0). The probability</p>
<p/>
</div>
<div class="page"><p/>
<p>98 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>distribution function of X (the total source plus background counts) is
</p>
<p>f .n/ D .ï¿½C ï¿½B/
n
</p>
<p>nÅ 
eï¿½.ï¿½Cï¿½B/; (5.18)
</p>
<p>where n is an integer number describing possible values of X. Equation (5.18) is true
even if the background is not known exactly, since the sum of two Poisson variables
is also a Poisson variable with mean equal to the sum of the means. It is evident that,
given the only measurement available, the estimate of the source mean is
</p>
<p>Oï¿½ D nobs ï¿½ ï¿½B:
</p>
<p>This estimate is the starting point to determine a confidence interval for the parent
mean. We define the lower limit ï¿½lo as the value of the source mean that results in
the observation of n ï¿½ nobs with a probability Ë:
</p>
<p>Ë D
1X
</p>
<p>nDnobs
</p>
<p>.ï¿½lo C ï¿½B/n
nÅ 
</p>
<p>eï¿½.ï¿½loC&#13;B/ D 1 ï¿½
nobsï¿½1X
</p>
<p>nD0
</p>
<p>.ï¿½lo C ï¿½B/n
nÅ 
</p>
<p>eï¿½.ï¿½loCï¿½B/: (5.19)
</p>
<p>The mean ï¿½lo corresponds to the situation shown in the left panel of Fig. 5.2:
assuming that the actual mean is as low as ï¿½lo, there is only a small probability
Ë (say 5 %) to make a measurement above or equal to what was actually measured.
Thus, we can say that there is only a very small chance (Ë) that the actual mean
could have been as low (or lower) than ï¿½lo. The quantity ï¿½lo is the lower limit with
confidence .1 ï¿½ Ë/, i.e., we are .1ï¿½ Ë/, say 95 %, confident that the mean is higher
than this value.
</p>
<p>Fig. 5.2 This illustration of the upper and lower limits to the measurement of a Poisson mean
assumes a measurement of nobs D 3. On the left, the lower limit to the parent mean is such that
there is a probability of Ë to measure nobs or higher (hatched area); on the right, the upper limit
leaves a probability of .1ï¿½ Ë/ that a measurement is nobs or lower</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Quantiles and Confidence Intervals 99
</p>
<p>By the same logic we also define ï¿½up as the parent value of the source mean that
results in the observation of n 
 nobs with a probability .1 ï¿½ Ë/, or
</p>
<p>1 ï¿½ Ë D
nobsX
</p>
<p>nD0
</p>
<p>.ï¿½up C ï¿½B/n
nÅ 
</p>
<p>eï¿½.ï¿½upCï¿½B/: (5.20)
</p>
<p>This is illustrated in the right panel of Fig. 5.2, where the number .1ï¿½Ë/ is intended
as a small number, of same magnitude as Ë. Assuming that the mean is as high as
ï¿½up, there is a small probability of 1ï¿½Ë to make a measurement equal or lower than
the actual measurement. Therefore we say that there is only a small probability that
the true mean could be as high or higher than ï¿½up. The number ï¿½up is the upper limit
with confidence Ë, that is, we are Ë (say 95 %) confident that the mean is lower than
this value.
</p>
<p>If we combine the two limits, the probability that the true mean is above ï¿½up or
below ï¿½lo is just .1ï¿½Ë/CË, say 10 %, and therefore the interval ï¿½lo to ï¿½up includes
a probability of
</p>
<p>P.ï¿½lo 
 ï¿½ 
 ï¿½up/ D 1 ï¿½ .1 ï¿½ Ë/ ï¿½ Ë D Ë ï¿½ Ë;
</p>
<p>i.e., this is a .Ë ï¿½ Ë/, say 90 %, confidence interval.
The upper and lower limits defined by (5.19) and (5.20) can be approximated
analytically using a relationship that relates the Poisson sum with an analytic
distribution function:
</p>
<p>nobsï¿½1X
</p>
<p>xD0
</p>
<p>eï¿½ï¿½ï¿½x
</p>
<p>xÅ 
D 1 ï¿½ Pï¿½2.ï¿½2; &#13;/ (5.21)
</p>
<p>where Pï¿½2.ï¿½
2; &#13;/ is the cumulative distribution of the ï¿½2 probability distribu-
</p>
<p>tion function defined in Sect. 7.2, with parameters ï¿½2 D 2ï¿½ and &#13; D 2nobs,
</p>
<p>Pï¿½2.ï¿½
2; &#13;/ D
</p>
<p>Z ï¿½2
</p>
<p>ï¿½1
fï¿½2 .x; &#13;/dx:
</p>
<p>The approximation is due to Gehrels [16], and makes use of mathematical relation-
ships that can be found in the handbook of Abramowitz and Stegun [1]. The result
is that the upper and lower limits can be simply approximated once we specify
the number of counts nobs and the probability level of the upper or lower limit.
The probability level is described by the number S, which is the equivalent number
of Gaussian ï¿½ that corresponds to the confidence level chosen (for example, 84 %</p>
<p/>
</div>
<div class="page"><p/>
<p>100 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>confidence interval corresponds to S D 1, etc. see Table 5.3)
8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½up D nobs C S
2 C 3
4
C S
</p>
<p>r
</p>
<p>nobs C 3
4
</p>
<p>ï¿½lo D nobs
ï¿½
</p>
<p>1ï¿½ 1
9nobs
</p>
<p>ï¿½ S
3
p
nobs
</p>
<p>ï¿½3
</p>
<p>:
</p>
<p>(5.22)
</p>
<p>The S parameter is also a quantile of a standard Gaussian distribution, enclosing a
probability as illustrated in Table 5.3.
Proof Use of (5.21) into (5.19) and (5.20) gives a relationship between the
function Pï¿½2 and the probability levels Ë and Ë,
</p>
<p>(
Pï¿½2 .2ï¿½lo; 2nobs/ D Ë
Pï¿½2 .2ï¿½up; 2nobs C 2/ D Ë:
</p>
<p>(5.23)
</p>
<p>We use the simplest approximation for the function Pï¿½2 described in [16], one
that is guaranteed to give limits that are accurate within 10 % of the true values.
The approximation makes use of the following definitions: for any probability
a 
 1, ya is the a-quantile of a standard normal distribution, or G.ya/ D a,
</p>
<p>G.ya/ D 1p
2	
</p>
<p>Z ya
</p>
<p>ï¿½1
eï¿½t2=2dt:
</p>
<p>If Pï¿½2.ï¿½
2
a; &#13;/ D a, then the simplest approximation between ï¿½2a and ya given
</p>
<p>by Gehrels [16] is
</p>
<p>ï¿½2a '
1
</p>
<p>2
</p>
<p>ï¿½
ya C
</p>
<p>p
2&#13; ï¿½ 1
</p>
<p>	2
: (5.24)
</p>
<p>Consider the upper limit in (5.23). We can solve for ï¿½up by using (5.24) with
2ï¿½up D ï¿½2a, &#13; D 2nobs C 2 and S D ya, since ya is the a-quantile of a standard
</p>
<p>Table 5.3 Poisson parameters S and corresponding probabilities
</p>
<p>Upper or lower limit Range Probability (%) Poisson S parameter
</p>
<p>90 % confidence ï¿½ ï¿½C 1:28ï¿½ 90 1:28
95 % confidence ï¿½ ï¿½C 1:65ï¿½ 95 1:65
99 % confidence ï¿½ ï¿½C 2:33ï¿½ 99 2:33
1-ï¿½ ï¿½ ï¿½C ï¿½ 84.1 1:0
2-ï¿½ ï¿½ ï¿½C 2ï¿½ 97.7 2:0
3-ï¿½ ï¿½ ï¿½C 3ï¿½ 99.9 3:0</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Quantiles and Confidence Intervals 101
</p>
<p>normal distribution, thus equivalent to S. It follows that
</p>
<p>2ï¿½up D 1
2
</p>
<p>ï¿½
SC
</p>
<p>p
4nobs C 3
</p>
<p>	2
</p>
<p>and from this the top part of (5.22) after a simple algebraic manipulation.
A similar result applies for the lower limit. ut
Equation (5.22) is tabulated in Tables A.5 and A.6 for several interesting values
</p>
<p>of nobs and S. A few cases of common use are also shown in Table 5.4.
</p>
<p>Example 5.8 (Poisson Upper Limit to Non-detection with No Background) An
interesting situation that can be solved analytically is that corresponding to the
situation in which there was a complete non-detection of a source, nobs D 0.
Naturally, it is not meaningful to look for a lower limit to the Poisson mean, but it is
quite interesting to solve (5.20) in search for an upper limit with a given confidence
Ë. In this case of n D 0 the equation simplifies to
</p>
<p>1 ï¿½ Ë D eï¿½.ï¿½upCï¿½B/ ) ï¿½up D ï¿½ï¿½B ï¿½ lnË:
</p>
<p>For Ë D 0:84 and zero background (ï¿½B D 0) this corresponds to an upper limit of
ï¿½up D ï¿½ ln 0:16 D 1:83. This example can also be used to test the accuracy of the
approximation given by (5.22). Using nobs D 0, we obtain
</p>
<p>ï¿½up D 1
4
.1Cp3/2 D 1:87
</p>
<p>which is in fact just 2 % higher than the exact result. An example of upper limits in
the presence of a non-zero background is presented in Problem 5.8. }
</p>
<p>Table 5.4 Selected Upper and Lower limits for a Poisson variable using the Gehrels approxima-
tion (see Tables A.5 and A.6 for a complete list of values)
</p>
<p>Poisson parameter S or confidence level
</p>
<p>S D 1 S D 2 S D 3
nobs (1-ï¿½ , or 84.1 %) (2-ï¿½ , or 97.7 %) (3-ï¿½ , or 99.9 %)
</p>
<p>Upper limit
</p>
<p>0 1:87 3:48 5:60
</p>
<p>1 3:32 5:40 7:97
</p>
<p>ï¿½ ï¿½ ï¿½
Lower limit
</p>
<p>ï¿½ ï¿½ ï¿½
9 6:06 4:04 2:52
</p>
<p>10 6:90 4:71 3:04
</p>
<p>ï¿½ ï¿½ ï¿½</p>
<p/>
</div>
<div class="page"><p/>
<p>102 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>5.5 Bayesian Methods for the Poisson Mean
</p>
<p>The Bayesian method consists of determining the posterior probability P.ï¿½=obs/,
having calculated the likelihood as
</p>
<p>P.nobs=ï¿½/ D .ï¿½C ï¿½B/
nobs
</p>
<p>nobsÅ 
eï¿½.ï¿½Cï¿½B/: (5.25)
</p>
<p>We use Bayes&rsquo; theorem,
</p>
<p>P.ï¿½=obs/ D P.nobs=ï¿½/	.ï¿½/R1
0
</p>
<p>P.nobs=ï¿½0/	.ï¿½0/; dï¿½0
(5.26)
</p>
<p>in which we needed to introduce a prior probability distribution 	.ï¿½/ in order to
calculate the posterior probability. The use of a prior distribution is what constitutes
the Bayesian approach. The simplest assumption is that of a uniform prior, 	.ï¿½/ D
C, over an arbitrarily large range of ï¿½ ï¿½ 0, but other choices are possible according
to the information available on the Poisson mean prior to the measurements. In this
section we derive the Bayesian expectation of the Poisson mean and upper and lower
limits and describe the differences with the classical method.
</p>
<p>5.5.1 Bayesian Expectation of the Poisson Mean
</p>
<p>The posterior distribution of the Poisson mean ï¿½ (5.26) can be used to calculate the
Bayesian expectation for the mean can be calculated as the integral of (5.26) over
the entire range allowed to the mean,
</p>
<p>EÅï¿½=obsï¿½ D
R1
0 P.ï¿½=obs/ï¿½dï¿½R1
0
</p>
<p>P.ï¿½=obs/dï¿½
: (5.27)
</p>
<p>The answer will in general depend on the choice of the prior distribution 	.ï¿½/.
Assuming a constant prior, the expectation becomes
</p>
<p>EÅï¿½=obsï¿½ D
Z 1
</p>
<p>0
</p>
<p>eï¿½ï¿½ï¿½nobsC1
</p>
<p>nÅ 
dï¿½ D nobs C 1; (5.28)
</p>
<p>where we made use of the integral
</p>
<p>Z 1
</p>
<p>0
</p>
<p>eï¿½ï¿½ï¿½ndï¿½ D nÅ </p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Bayesian Methods for the Poisson Mean 103
</p>
<p>The interesting result is therefore that a measurement of nobs counts implies a
Bayesian expectation of EÅï¿½ï¿½ D nobs C 1, i.e., one count more than the observation.
Therefore even a non-detection results in an expectation for the mean of the parent
distribution of 1, and not 0. This somewhat surprising result can be understood by
considering the fact that even a parent mean of 1 results in a likelihood of 1=e (i.e., a
relatively large number) of obtaining zero counts as a result of a random fluctuations.
Moreover, the Poisson distribution is skewed, with a heavier tail at large values of
ï¿½. This calculation is due to Emslie [12].
</p>
<p>5.5.2 Bayesian Upper and Lower Limits for a Poisson Variable
</p>
<p>Using a uniform prior, we use the Bayesian approach (5.26) to calculate the upper
limit to the source mean with confidence Ë (say, 95 %). This is obtained by
integrating (5.26) from the lower limit of 0 to the upper limit ï¿½up,
</p>
<p>Ë D
R ï¿½up
0 P.nobs=ï¿½/dï¿½R1
0
</p>
<p>P.nobs=ï¿½/dï¿½
D
R ï¿½up
0 .ï¿½C ï¿½B/nobseï¿½.ï¿½Cï¿½B/dï¿½R1
0
.ï¿½C ï¿½B/nobseï¿½.ï¿½Cï¿½B/dï¿½
</p>
<p>(5.29)
</p>
<p>Similarly, the lower limit can be estimated according to
</p>
<p>Ë D
R ï¿½lo
0
</p>
<p>P.nobs=ï¿½/dï¿½S
R1
0
</p>
<p>P.nobs=ï¿½/dï¿½
D
R ï¿½lo
0
.ï¿½C ï¿½B/nobseï¿½.ï¿½Cï¿½B/dï¿½
</p>
<p>R1
0
.ï¿½C ï¿½B/nobseï¿½.ï¿½Cï¿½B/dï¿½
</p>
<p>(5.30)
</p>
<p>where Ë is a small probability, say 5 %. Since nobs is always an integer, these
integrals can be evaluated analytically.
</p>
<p>The difference between the classical upper limits described by (5.19) and (5.20)
and the Bayesian limits of (5.29) and (5.30) is summarized by the different variable
of integration (or summation) in the relevant equations. For the classical limits we
use the Poisson probability to make nobs measurements for a true mean of ï¿½. We then
estimate the upper or lower limits as the values of the mean that gives a probability
of, respectively, 1ï¿½Ë and Ë, to observe n 
 nobs events. In this case, the probability
is evaluated as a sum over the number of counts, for a fixed value of the parent mean.
</p>
<p>In the case of the Bayesian limits, on the other hand, we first calculate the
posterior distribution of ï¿½, and then require that the range between 0 and the limits
ï¿½up and ï¿½lo includes, respectively, a 1ï¿½Ë and Ë probability, evaluated as an integral
over the mean for a fixed value of the detected counts. In general, the two methods
will give different results.
</p>
<p>Example 5.9 (BayesianUpper Limit to a Non-detection) The case of non-detection,
nobs D 0, is especially simple and interesting, since the background drops out of the
equation, resulting in Ë D 1 ï¿½ eï¿½ï¿½up, which gives
</p>
<p>ï¿½up D ï¿½ ln.1 ï¿½ Ë/ (case of nobs D 0, Bayesian upper limit) (5.31)</p>
<p/>
</div>
<div class="page"><p/>
<p>104 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>The Bayesian upper limit is therefore equal to the classical limit, when there is no
background. When there is background, the two estimate will differ.1 }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Maximum Likelihood (ML) method: A method to estimate parameters of a
distribution under the assumption that the best-fit parameters maximize the
likelihood of the measurements.
</p>
<p>ï¿½ ML estimates of mean and variance: For a Gaussian variable, the unbiased
ML estimates are
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>ï¿½ML D x
s2 D 1
</p>
<p>N ï¿½ 1
P
.xi ï¿½ x/2
</p>
<p>ï¿½ Estimates of mean with non-uniform uncertainties: They are given by
8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½ML D
P
</p>
<p>xi=ï¿½2iP
1=ï¿½2i
</p>
<p>ï¿½2ï¿½ D
1
</p>
<p>P
1=ï¿½2i
</p>
<p>ï¿½ Confidence intervals: Range of the variable that contains a given proba-
bility of occurrence (e.g., Ë1ï¿½ range contains 68 % of probability for a
Gaussian variable).
</p>
<p>ï¿½ Upper and lower limits: An upper (lower) limit is the value below (above)
which there is a given probability (e.g., 90 %) to observe the variable.
</p>
<p>Problems
</p>
<p>5.1 Using the definition of weighted sample mean as in (5.8), derive its variance
and show that it is given by (5.9).
</p>
<p>5.2 Using the data from Mendel&rsquo;s experiment (Table 1.1), calculate the standard
deviation in the measurement of each of the seven fractions of dominants, and the
weighted mean and standard deviation of the seven fractions.
</p>
<p>Compare your result from a direct calculation of the overall fraction of domi-
nants, obtained by grouping all dominants from the seven experiments together.
</p>
<p>1Additional considerations on the measurements of the mean of a Poisson variable, and the case of
upper and lower limits, can be found in [10].</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Bayesian Methods for the Poisson Mean 105
</p>
<p>5.3 The Mendel experiment of Table 1.1 can be described as n number n of
measurements of ni, the number of plants that display the dominant character, out
of a total of Ni plants. The experiment is described by a binomial distribution with
probability p D 0:75 for the plant to display the dominant character.
</p>
<p>Using the properties of the binomial distribution, show analytically that the
weighted average of the measurements of the fraction fi D ni=Ni is equal to the
value calculated directly as
</p>
<p>ï¿½ D
Pn
</p>
<p>iD1 niPn
iD1 Ni
</p>
<p>5.4 Consider a decaying radioactive source observed in a time interval of duration
T D 15 s; N is the number of total counts, and B is the number of background counts
(assumed to be measured independently of the total counts):
</p>
<p>(
N D 19 counts
B D 14 counts :
</p>
<p>The goal is to determine the probability of detection of source counts S D N ï¿½B
in the time interval T.
</p>
<p>(a) Calculate this probability directly via:
</p>
<p>Prob(detection)D Prob(S &gt; 0/data)
</p>
<p>in which S is treated as a random variable, with Gaussian distribution of mean
and variance calculated according to the error propagation formulas. Justify why
the Gaussian approximation may be appropriate for the variable S.
</p>
<p>(b) Use the same method as in (a), but assuming that the background B is known
without error (e.g., as if it was observed for such along time interval that its
error becomes negligible).
</p>
<p>(c) Assume that the background is a variable with mean of 14 counts in a 15 s
interval, and that it can be observed for an interval of time T ï¿½ 15 s. Find what
interval of time T makes the error ï¿½B15 of the background over a time interval
of 15-s have a value ï¿½B15=B15 D 0:01, e.g., negligible.
</p>
<p>5.5 For the Thomson experiment of Table 2.1 (tube 1) and Table 2.2 (tube 2),
calculate:
</p>
<p>(a) The 90 % central confidence intervals for the variable v;
(b) The 90 % upper and lower limits, assuming that the variable is Gaussian.
</p>
<p>5.6 Consider a Poisson variable X of mean ï¿½.
</p>
<p>(a) We want to set 90 % confidence upper limits to the value of the parent mean
ï¿½, assuming that one measurement of the variable yielded the result of N D 1.
Following the classical approach, find the equation that determines the exact</p>
<p/>
</div>
<div class="page"><p/>
<p>106 5 Maximum Likelihood and Other Methods to Estimate Variables
</p>
<p>90 % upper limit to the mean ï¿½up. Recall that the classical 90 % confidence
upper limit is defined as the value of the Poisson mean that yields a P.X 

N/ D Ë, where 1 ï¿½ Ë D 0:9.
</p>
<p>(b) Using the Bayesian approach, which consists of defining the 1ï¿½Ë D 0:9 upper
limit via
</p>
<p>Ë D
R ï¿½up
0 P.nobs=ï¿½/dï¿½R1
0 P.nobs=ï¿½/dï¿½
</p>
<p>(5.32)
</p>
<p>where nobs D N; find the equation that determines the 90 % upper limit to the
mean ï¿½up.
</p>
<p>5.7 The data provided in Table 2.3 from Pearson&rsquo;s experiment on biometric data
describes the cumulative distribution function of heights from a sample of 1,079
couples. Calculate the 2ï¿½ upper limit to the fraction of couples in which both mother
and father are taller than 68 in.
</p>
<p>5.8 Use the data presented in Example 5.7, in which there is a non-detection of a
source in the presence of a background of ï¿½B ' 9:8. Determine the Poisson upper
limit to the source count at the 99 % confidence level and compare this upper limit
with that obtained in the case of a zero background level.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 6
Mean, Median, and Average Values of Variables
</p>
<p>Abstract The data analyst often faces the question of what is the &ldquo;best&rdquo; value to
report from N measurements of a random variable. In this chapter we investigate
the use of the linear average, the weighted average, the median and a logarithmic
average that may be applicable when the variable has a log-normal distribution.
The latter may be useful when a variable has errors that are proportional to their
measurements, avoiding the inherent bias arising in the weighted average from
measurements with small values and small errors. We also introduce a relative-error
weighted average that can be used as an approximation for the logarithmic mean for
log-normal distributions.
</p>
<p>6.1 Linear and Weighted Average
</p>
<p>In the previous chapter (see Sect. 5.1.3) we have shown that the weighted mean is
the most likely value of the mean of the random variable. Therefore, the weighted
mean is a commonly accepted quantity to report as the best estimate for the value
of a measured quantity. If the measurements have the same standard deviation, then
the weighted mean becomes the linear average; in general, the linear and weighted
means differ unless all measurement errors are identical.
</p>
<p>The difference between linear average and weighted mean can be illustrated
with an example. Consider the N D 25 measurements shown in Table 6.1,
which reports the measurement of the energy of certain astronomical sources
made at a given radius [5]. This dataset is illustrative of the general situation
of the measurement of a quantity (in this example, the ratio between the two
measurements) in the presence of different measurement error. The weighted mean
is 0:90Ë 0:02, while the linear average is 1.01 (see Problem 6.1). The difference is
clearly due to the presence of a few measurements with a low value of the ratio that
carry higher weight because of the small measurement error (for example, source
15).
</p>
<p>Which of the two values is more representative? This question can be addressed
by making the following observations. The measurement error reported in the table
reflects the presence of such sources of uncertainty as Poisson fluctuations in the
detection of photons from the celestial sources. The same type of uncertainty would
also apply to other experiments, in particular those based on the counting of events.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_6
</p>
<p>107</p>
<p/>
</div>
<div class="page"><p/>
<p>108 6 Mean, Median, and Average Values of Variables
</p>
<p>Table 6.1 Dataset with measurement of energy for N D 25 different sources and their ratio
Energy
</p>
<p>Source Radius Method #1 Method #2 Ratio
</p>
<p>1 221:1Ë11:012:3 8:30Ë0:760:88 9:67Ë1:141:12 0:86Ë0:080:07
2 268:5Ë22:120:7 4:92Ë0:770:70 4:19Ë0:820:70 1:17Ë0:160:15
3 138:4Ë12:711:9 3:03Ë0:530:49 2:61Ë0:590:49 1:16Ë0:200:18
4 714:3Ë23:534:5 49:61Ë3:153:19 60:62Ë4:846:13 0:82Ë0:060:05
5 182:3Ë18:515:1 2:75Ë0:490:43 3:30Ë0:810:61 0:83Ë0:140:14
6 72:1Ë5:55:7 1:01Ë0:230:20 0:86Ë0:140:13 1:17Ë0:240:21
7 120:3Ë8:67:5 5:04Ë0:660:57 3:80Ë0:720:57 1:33Ë0:160:15
8 196:2Ë15:115:5 5:18Ë0:730:70 6:00Ë1:171:11 0:86Ë0:140:11
9 265:7Ë8:78:6 12:17Ë1:221:17 10:56Ë0:930:95 1:14Ë0:130:10
10 200:0Ë9:610:7 7:74Ë0:570:58 6:26Ë0:780:83 1:24Ë0:140:11
11 78:8Ë5:65:1 1:08Ë0:160:15 0:73Ë0:110:10 1:49Ë0:260:24
12 454:4Ë20:320:3 17:10Ë2:642:03 23:12Ë2:362:32 0:75Ë0:070:06
13 109:4Ë8:38:3 3:31Ë0:340:34 3:06Ë0:540:52 1:09Ë0:180:15
14 156:5Ë11:510:2 2:36Ë0:610:58 2:31Ë0:360:31 1:02Ë0:260:23
15 218:0Ë6:65:9 14:02Ë0:750:75 21:59Ë1:821:82 0:65Ë0:040:04
16 370:7Ë7:68:0 31:41Ë1:561:56 29:67Ë1:561:57 1:06Ë0:060:06
17 189:1Ë16:415:4 2:15Ë0:450:39 2:52Ë0:570:51 0:86Ë0:220:18
18 150:5Ë4:24:6 3:39Ë0:570:50 4:75Ë0:440:46 0:72Ë0:110:11
19 326:7Ë12:19:9 15:73Ë1:431:30 18:03Ë1:541:26 0:87Ë0:060:06
20 189:1Ë9:99:1 5:04Ë0:650:55 4:61Ë0:610:50 1:09Ë0:120:12
21 147:7Ë8:011:1 2:53Ë0:290:30 2:76Ë0:370:48 0:93Ë0:120:10
22 504:6Ë12:511:2 44:97Ë2:992:74 43:93Ë3:082:59 1:02Ë0:050:05
23 170:5Ë8:68:1 3:89Ë0:300:29 3:93Ë0:490:42 0:98Ë0:100:09
24 297:6Ë13:113:6 10:78Ë1:041:02 10:48Ë1:341:22 1:04Ë0:100:11
25 256:2Ë13:414:4 7:27Ë0:810:77 7:37Ë0:970:95 0:99Ë0:090:09
</p>
<p>This type of uncertainty is usually referred to as statistical error. Many experiments
and measurements are also subject to other sources of uncertainty that may not be
explicitly reported in the dataset. For example, the measurement of events recorded
by a detector is affected by the calibration of the detector, and a systematic offset
in the calibration would affect the numbers recorded. In the case of the data of
Table 6.1, the uncertainty due to the calibration of the detector is likely to affect by
the same amount of all measurements, regardless of the precision indicated by the
statistical error. This type of uncertainty is typically referred to as systematic error,
and the inclusion of such additional source of uncertainty would modify the value
of the weighted mean. As an example of this effect, if we add an error of Ë0:1 to
all values of the ratio of Table 6.1, the weighted mean becomes 0:95 Ë 0:04 (see
Problem 6.2). It is clear that the addition of a constant error for each measurement
causes a de-weighting of datapoints with small statistical errors, and in the limit of
a large systematic error the weighted mean becomes the linear average. Therefore,</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Logarithmic Average and Fractionalor Multiplicative Errors 109
</p>
<p>the linear average can be used when the data analyst wants to weigh equally all
datapoints, regardless of the precision indicated by the statistical errors. Systematic
errors are discussed in more detail in Chap. 11.
</p>
<p>6.2 The Median
</p>
<p>Another quantity that can be calculated from the N measurements is the median,
defined in Sect. 2.3.1 as the value of the variable that is greater than 50 % of
the measurements, and also lower than 50 % of the measurements. In the case of
the measurement of the ratios in Table 6.1, this is simply obtained by ordering
the 25 measurements in ascending order, and using the 13th measurement as an
approximation for the median. The value obtained in this case is 1.02, quite close
to the value of the linear average, since both statistics do not take into account the
measurement errors.
</p>
<p>One useful feature of the median is that it is not very sensitive to &ldquo;outliers&rdquo; in the
distribution. For example, if one of the measurements was erroneously reported as
0:07Ë 0:01 (instead of 0:72Ë 0:11, such as source 18 in the Table), both linear and
weighted averages would be affected by the error, but the median would not. The
median may therefore be an appropriate value to report in cases where the analyst
suspects the presence of outliers in the dataset.
</p>
<p>6.3 The Logarithmic Average and Fractional
or Multiplicative Errors
</p>
<p>The quantity &ldquo;Ratio&rdquo; in Table 6.1 can be used to illustrate a type of variables that
may require a special attention when calculating their averages. Consider a variable
whose errors are proportional to their measured values. In this case, a weighted
average will be skewed towards lower values because of the smaller errors in those
measurements. The question we want address is whether it is appropriate to use
a weighted average of these measurements or whether one should use a different
approach.
</p>
<p>To illustrate this situation, let&rsquo;s use two measurements such as x1 D 1:2Ë 0:24
and x2 D 0:80Ë 0:16. Both measurements have a relative error of 20 %, the linear
average is 1.00 and the weighted average is 0.923. The base-10 logarithm of these
measurements are log x1 D 0:0792 and log x2 D ï¿½0:0969, with the same error. In
fact, using the error propagation method (Sect. 4.7.6), the error in the logarithm is
proportional to the fractional error according to
</p>
<p>ï¿½log x D ï¿½x
x
</p>
<p>1
</p>
<p>ln 10
: (6.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>110 6 Mean, Median, and Average Values of Variables
</p>
<p>For our measurements, this equation gives a value of ï¿½log x D 0:087 for both
measurements. The weighted average of these logarithms is therefore the linear
average log x D ï¿½0:0088, leading to an average of x D 0:980. This value is much
closer to the linear average of 1.00 than to the weighted average.
</p>
<p>Errors that are exactly proportional to the measurement, or
</p>
<p>ï¿½x D xï¿½r (6.2)
</p>
<p>may be called fractional or multiplicative errors. The quantity ï¿½r is the relative error
and it remains constant for purely multiplicative errors. In most cases, including that
of Table 6.1, the relative error ï¿½x=x varies among the measurements, and therefore
(6.2) applies only as an approximation. In the following we investigate when it is in
fact advisable to use the logarithm of measurements, instead of the measurements
themselves, to obtain a more accurate determination of the mean of a variable that
has multiplicative errors.
</p>
<p>6.3.1 The Weighted Logarithmic Average
</p>
<p>The maximum likelihood method applied to the logarithm of measurements of a
variable X can be used to estimate the mean and the error of logX. The weighted
logarithmic average of N measurements xi is defined as
</p>
<p>log x D
</p>
<p>PN
iD1
</p>
<p>log xi
ï¿½2log xi
</p>
<p>PN
iD1
</p>
<p>1
</p>
<p>ï¿½2log xi
</p>
<p>(6.3)
</p>
<p>where ï¿½2log xi is the variance of the logarithm of the measurements, which can be
obtained from (6.1). The uncertainty in the weighted logarithmic average is given
by
</p>
<p>ï¿½2log x D
1
</p>
<p>PN
iD1
</p>
<p>1
</p>
<p>ï¿½2log xi
</p>
<p>: (6.4)
</p>
<p>The use of this logarithmic average is justified when the variable X has a log-
normal distribution, i.e., when logX has a Gaussian distribution, rather than the
variable X itself. An example of a log-normal variable is illustrated in Fig. 6.1.
In this case, the maximum likelihood method estimator of the mean of logX is
the logarithmic mean of (6.3). Clearly, a variable can only be log-normal when
the variable has positive values, such as the ratio of two positive quantities. The</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Logarithmic Average and Fractionalor Multiplicative Errors 111
</p>
<p>Fig. 6.1 Log-normal distribution with mean ï¿½ D 0 and standard deviation ï¿½ D 0:3 (black line)
and linear plot of the same distribution (red line). A heavier right-hand tail in the linear plot may
be indicative of a log-normal distribution
</p>
<p>determination of the log-normal shape can be made if one has available random
samples from its distribution.
</p>
<p>In the limit of measurements with the same fractional error and small deviations
from the mean ï¿½, the weighted logarithmic average is equivalent to the linear
average.
</p>
<p>Proof This can be shown by proving that
</p>
<p>log x D log x
</p>
<p>where x is the ordinary linear average. Notice that log x in (6.3) is a base-10
logarithm. In this proof we make use the base-e logarithm (ln x), the two are
related by
</p>
<p>log x D ln x= ln 10:
</p>
<p>Consider N measurements xi in the neighborhood of the meanï¿½ of the random
variable, xi D ï¿½C
xi. A Taylor series expansion yields
</p>
<p>ln xi D lnï¿½.1C 
xi
ï¿½
/ D lnï¿½C 
xi
</p>
<p>ï¿½
ï¿½ .
xi=ï¿½/
</p>
<p>2
</p>
<p>2
C : : :</p>
<p/>
</div>
<div class="page"><p/>
<p>112 6 Mean, Median, and Average Values of Variables
</p>
<p>If the deviation is 
xi ï¿½ ï¿½, one can neglect terms of the second order and
higher. The average of the logarithms of the N measurements can thus be
approximated as
</p>
<p>log x D 1
N
</p>
<p>NX
</p>
<p>iD1
log xi ' 1
</p>
<p>ln 10
</p>
<p> 
</p>
<p>lnï¿½C 1
N
</p>
<p>NX
</p>
<p>iD1
</p>
<p>
xi
ï¿½
</p>
<p>!
</p>
<p>:
</p>
<p>On the other hand, the logarithm of the mean x is
</p>
<p>log x D log 1
N
</p>
<p>NX
</p>
<p>iD1
xi D log
</p>
<p> 
1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
ï¿½.1C 
xi
</p>
<p>ï¿½
/
</p>
<p>!
</p>
<p>:
</p>
<p>This leads to
</p>
<p>log x D 1
ln 10
</p>
<p> 
</p>
<p>lnï¿½C ln
 
</p>
<p>1C 1
N
</p>
<p>NX
</p>
<p>iD1
</p>
<p>
xi
ï¿½
</p>
<p>!!
</p>
<p>'
</p>
<p>1
</p>
<p>ln 10
</p>
<p> 
</p>
<p>lnï¿½C 1
N
</p>
<p>NX
</p>
<p>iD1
</p>
<p>
xi
ï¿½
</p>
<p>!
</p>
<p>D log x
</p>
<p>where we retained only the first-order term in the Taylor series expansion of
the logarithm since
</p>
<p>P

xi=ï¿½ï¿½ N. ut
</p>
<p>As discussed earlier in this section, the logarithmic average is an appropriate
quantity for log-normal distributed variables. The results of this section show
that this average is closer to the linear average of the measurements than the
standard weighted average, when measurement errors are positively correlated to
the measurements themselves.
</p>
<p>Example 6.1 The data of Table 6.1 can be used to calculate the logarithmic average
of the column &ldquo;Ratio&rdquo; according to (6.3) and (6.4) as log x D ï¿½0:023 Ë 0:018.
These quantities can be converted easily to linear quantities taking into account the
error propagation formula ï¿½log x D ï¿½=.x ln 10/, to obtain a value of 0:95Ë 0:04.
</p>
<p>Notice how the logarithmic mean has a value that is somewhat between that of
the linear average x D 1:01 and the traditional weighted average of 0:90Ë 0:02. It
should not be surprising that the logarithmic mean is not exactly equal to the linear
average. In fact, the measurements of Table 6.1 have different relative errors. Only
in the case of identical relative errors for all measurements we expect that the two
averages have the same value. }</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Logarithmic Average and Fractionalor Multiplicative Errors 113
</p>
<p>6.3.2 The Relative-Error Weighted Average
</p>
<p>Although transforming measurements to their logarithms is a simple procedure,
we also want to investigate another type of average that deals directly with the
measurements without the need to calculate their logarithms.
</p>
<p>We introduce the relative-error weighted average as
</p>
<p>xRE D
PN
</p>
<p>iD1 xi=.ï¿½i=xi/2
PN
</p>
<p>iD1 1=.ï¿½i=xi/2
: (6.5)
</p>
<p>The only difference with the weighted mean defined in Sect. 5.1.3 is the use of
the extra factor of xi in the error term, so that ï¿½i=xi is the relative error of each
measurement.
</p>
<p>The reason to introduce this new average is that, for log-normal variables, this
relative-error weighted mean is equivalent to the logarithmic mean of (6.3). This can
be proven by showing that ln x D ln xRE.
Proof Start with the logarithm of the relative-error weighted average,
</p>
<p>ln xRE D ln
 PN
</p>
<p>iD1 xi=.ï¿½i=xi/2
PN
</p>
<p>iD1 1=.ï¿½i=xi/2
</p>
<p>!
</p>
<p>D ln
 PN
</p>
<p>iD1 xi=ï¿½2log xi
PN
</p>
<p>iD1 1=ï¿½2log xi
</p>
<p>!
</p>
<p>:
</p>
<p>From this, expand the measurement term xi D ï¿½C
xi, where ï¿½ is the parent
mean of the variable X,
</p>
<p>ln
</p>
<p> 
</p>
<p>ï¿½C
PN
</p>
<p>iD1 
xi=ï¿½2log xi
PN
</p>
<p>iD1 1=ï¿½2log xi
</p>
<p>!
</p>
<p>D lnï¿½C ln
 
</p>
<p>1C
PN
</p>
<p>iD1 
xi=.ï¿½ï¿½2log xi/
PN
</p>
<p>iD1 1=ï¿½2log xi
</p>
<p>!
</p>
<p>:
</p>
<p>If 
xi ï¿½ ï¿½, then
</p>
<p>ln xRE D lnï¿½C
PN
</p>
<p>iD1 
xi=.ï¿½ï¿½2log xi/
PN
</p>
<p>iD1 1=ï¿½2log xi
</p>
<p>leading to
</p>
<p>log xRE D logï¿½C 1
ln 10
</p>
<p>PN
iD1 
xi=.ï¿½ï¿½2log xi/
PN
</p>
<p>iD1 1=ï¿½2log xi
:
</p>
<p>The logarithmic average can also be expanded making use of
</p>
<p>NX
</p>
<p>iD1
</p>
<p>log xi
ï¿½2log xi
</p>
<p>D
NX
</p>
<p>iD1
</p>
<p>logï¿½C log.1C
xi/=ï¿½
ï¿½2log xi
</p>
<p>'
NX
</p>
<p>iD1
</p>
<p> 
logï¿½
</p>
<p>ï¿½2log xi
C 
xi=ï¿½
ï¿½2log xi ln 10
</p>
<p>!
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>114 6 Mean, Median, and Average Values of Variables
</p>
<p>This leads to
</p>
<p>log x D logï¿½C 1
ln 10
</p>
<p>PN
iD1 
xi=.ï¿½ï¿½2log xi/
PN
</p>
<p>iD1 1=ï¿½2log xi
D log xRE:
</p>
<p>ut
The use of the relative-error weighted average should be viewed as an ad hoc method
to obtain an average value that is consistent with the logarithmic average, especially
in the limit measurements with equal relative errors. The statistical uncertainty in
this error-weighted average can be simply assigned as the error in the traditional
weighted average (5.8). In fact, the statistical error should be determined by the
&ldquo;physical&rdquo; uncertainties in the measurements, as is the case for the variance in (5.8).
It would be tempting to use the inverse of the denominator of (6.5) as the variance;
however, the result would be biased by our somewhat arbitrary choice of weighing
the measurements by the relative errors, instead of the error themselves.
</p>
<p>Example 6.2 Continuing with the values of &ldquo;Ratio&rdquo; in Table 6.1, the error-weighted
average is calculated as xRE D 0:96. The error in the traditional weighted average
was 0.02, therefore we may report the result as 0:96Ë 0:02. Comparison with the
values of 0:95 Ë 0:04 for the logarithmic average shows the general agreement
between these two values.
</p>
<p>}
Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Linear average: The mean x of N measurements.
ï¿½ Median: The 50 % quantile, or the number below and above which there
</p>
<p>are 50 % of the variable&rsquo;s values.
ï¿½ Logarithmic average: In some cases (e.g., when errors are proportional to
</p>
<p>the measured values) it is meaningful to calulate the weighted average of
the logarithm of the variable,
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>log x D
P
</p>
<p>log xi=ï¿½2logxiP
1=ï¿½2logxi
</p>
<p>ï¿½2log x D
1
</p>
<p>ï¿½2logxi
</p>
<p>where ï¿½logxi D ï¿½i=.xi ln 2/.
ï¿½ Relative-error weighted average: An approximation of the logarithmic
</p>
<p>average that does not require logarithms,
</p>
<p>xRE D
P
</p>
<p>xi=.ï¿½i=xi/2
P
1=.ï¿½i=xi/2
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Logarithmic Average and Fractionalor Multiplicative Errors 115
</p>
<p>Problems
</p>
<p>6.1 Calculate the linear average and the weighted mean of the quantity &ldquo;Ratio&rdquo; in
Table 6.1.
</p>
<p>6.2 Consider the 25 measurements of &ldquo;Ratio&rdquo; in Table 6.1. Assume that an
additional uncertainty of Ë0.1 is to be added linearly to the statistical error of
each measurement reported in the table. Show that the addition of this source of
uncertainty results in a weighted mean of 0:95Ë 0:04.
6.3 Given two measurements x1 and x2 with values in the neighborhood of 1.0,
show that the logarithm of the average of the measurements is approximately equal
to the average of the logarithms of the measurements.
</p>
<p>6.4 Given two measurements x1 and x2 with values in the neighborhood of a
positive number A, show that the logarithm of the average of the measurements
is approximately equal to the average of the logarithms of the measurements.
</p>
<p>6.5 For the data in Table 6.1, calculate the linear average, weighted average
and median of each quantity (Radius, Energy Method 1, Energy Method 2 and
Ratio). You may assume that the error of each measurements is the average of the
asymmetric errors of each measurement reported in the table.
</p>
<p>6.6 Table 6.1 contains the measurement of the thermal energy of certain sources
using two independent methods labeled as method #1 and method #2. For each
source, the measurement is made at a given radius, which varies from source
to source. The error bars indicate the 68 %, or 1ï¿½ , confidence intervals; the fact
that most are asymmetric indicate that the measurements do not follow exactly a
Gaussian distribution. Calculate the weighted mean of the ratios between the two
measurements and its standard deviation, assuming that the errors are Gaussian and
equal to the average of the asymmetric errors, as it is often done in this type of
situation.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 7
Hypothesis Testing and Statistics
</p>
<p>Abstract Every quantity that is estimated from the data, such as the mean or
the variance of a Gaussian variable, is subject to statistical fluctuations of the
measurements. For this reason they are referred to as a statistics. If a different
sample of measurements is collected, statistical fluctuations will certainly give rise
to a different set of measurements, even if the experiments are performed under the
same conditions. The use of different data samples to measure the same statistic
results in the determination of the sampling distribution of the statistic, to describe
what is the expected range of values for that quantity. In this chapter we derive the
distribution of a few fundamental statistics that play a central role in data analysis,
such as the ï¿½2 statistic. The distribution of each statistic can be used for a variety of
tests, including the acceptance or rejection of the fit to a model.
</p>
<p>7.1 Statistics and Hypothesis Testing
</p>
<p>In this book we have already studied several quantities that are estimated from the
data, such as the sample mean and the sample variance. These quantities are subject
to random statistical fluctuations that occur during the measurement and collection
process and they are often referred to as random variables or statistics. For example,
a familiar statistic is the sample mean of a variable X. Under the hypothesis that the
variable X follows a Gaussian distribution of mean ï¿½ and variance ï¿½2, the sample
mean of N measurements is Gaussian-distributed with mean ï¿½ and variance equal
to ï¿½2=N (see Sect. 4.1.2). This means that different samples of size N will in general
give rise to different sample means and that ones expects a variance of order ï¿½2=N
among the various samples. This knowledge lets us establish whether a given sample
mean is consistent with this theoretical expectation.
</p>
<p>Hypothesis testing is the process that establishes whether the measurement
of a given statistic, such as the sample mean, is consistent with its theoretical
distribution. Before describing this process in detail, we illustrate with the following
example the type of statistical statement that can be made from a given measurement
and the knowledge of its parent distribution.
</p>
<p>Example 7.1 Consider the case of the measurement of the ratio m=e from Tube 1
of Thomson&rsquo;s experiment, and arbitrarily assume (this assumption will be relaxed
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_7
</p>
<p>117</p>
<p/>
</div>
<div class="page"><p/>
<p>118 7 Hypothesis Testing and Statistics
</p>
<p>in more realistic applications) that the parent mean is known to be equal to ï¿½ D
0:475, and that the parent variance is ï¿½ D 0:075. We want to make quantitative
statements regarding the possibility that the measurements are drawn from the
parent distribution.
</p>
<p>For example, we can make the following statements concerning the measurement
m=e D 0:42: since m=e D ï¿½ ï¿½ 0:73ï¿½ , there is a probability of 24 % that a
measurement of 0.42 or lower is recorded. This statement addresses the fact that,
despite the measurement fell short of the parent mean, there is still a significant
(24 %) chance that any given measurement will be that low, or even lower. We can
also make this statement: the measurement is within the 1 ï¿½ ï¿½ central confidence
interval, which encompasses 68 % of the probability. This statement looks at the
distance of the measurement from the mean, regardless of its sign.
</p>
<p>Before we can say: the measurement is consistent with the parent distribution,
we need to quantify the meaning of the word consistent. }
</p>
<p>The process of hypothesis testing requires a considerable amount of care in the
definition the hypothesis to test and in drawing conclusions. The method can be
divided into the following four steps.
</p>
<p>1. Begin with the definition of a hypothesis to test. For the measurements of a
variable X, a possible hypothesis is that the measurements are consistent with
a parent mean of ï¿½ D 0 and a variance of ï¿½2 D 1. For a fit of a dataset to a linear
model (Chap. 8) we may want to test whether the linear model is a constant, i.e.,
whether the parent value of the slope coefficient is b D 0. This initial step in
the process identifies a so-called null hypothesis that we want to test with the
available data.
</p>
<p>2. The next step is to determine the statistic to use for the null hypothesis. In the
example of the measurements of a variable X, the statistic we can calculate from
the data is the sample mean. For the fit to the linear model, we will learn that the
ï¿½2min is the statistic to use for a Gaussian dataset. The choice of statistic means that
we are in a position to use the theoretical distribution function for that statistic to
tell whether the actual measurements are consistent with its expected distribution,
according to the null hypothesis.
</p>
<p>3. Next we need to determine a probability or confidence level for the agreement
between the statistic and its expected distribution under the null hypothesis. This
level of confidence p, say p D 0:9 or 90 %, defines a range of values for the
statistics that are consistent with its expected distribution. We will refer to this
range as the acceptable region for the statistic. For example, a standard Gaussian
of zero mean and unit variance has 90 % of its values in the range from ï¿½1:65
to C1:65. For a confidence level of p D 0:9, the analyst would require that the
measurement must fall within this range. The choice of probability p is somewhat
arbitrary: some analysts may choose 90 %, some may require 99.99 %, some may
even be satisfied with 68 %, which is the probability associated with Ë1ï¿½ for
a Gaussian distribution. Values of the statistics outside of the acceptable range
define the rejection region. For the standard Gaussian, the rejection region at
p D 0:9 consists of values ï¿½ 1:65 and values 
 ï¿½1:65, i.e., the rejection region</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Statistics and Hypothesis Testing 119
</p>
<p>is two-sided, as obtained from
</p>
<p>PfjSj ï¿½ Sg D 1 ï¿½
Z S
</p>
<p>ï¿½S
f .s/ds D 1 ï¿½ p (7.1)
</p>
<p>where f .s/ is the probability distribution of the statistic (in this example the
standard Gaussian) and S is the critical value of the statistic at the level of
confidence p. For two-sided rejection regions such as this, where large values
of the absolute value of the statistic S are not acceptable, the null hypothesis can
be summarized as
</p>
<p>H0 D {The statistic has values jSj 
 S}
</p>
<p>Here we have assumed that the acceptable region is centered at 0, but other
choices are also possible.
</p>
<p>In other cases of interest, such as for the ï¿½2 distribution, the rejection region
is one-sided. The critical value at confidence level p for the statistic can be found
from
</p>
<p>PfS ï¿½ Sg D
Z 1
</p>
<p>s
f .s/ds D 1 ï¿½ p (7.2)
</p>
<p>where f .s/ is the probability distribution function of the statistic S. For one-sided
rejection regions where large values of the statistic are not acceptable the null
hypothesis can now be summarized as
</p>
<p>H0 D {The statistic has values S 
 S}:
</p>
<p>Clearly p and NS are related: the larger the value of the probability p, the larger the
value of NS, according to (7.1) and (7.2). Larger values of p, such as p D 0:9999,
increase the size of the acceptable region and reduce the size of the rejection
region.
</p>
<p>In principle, other choices for the acceptable and rejection regions are
possible, such as multiple intervals or intervals that are not centered at zero. The
corresponding critical value(s) of the statistic can be calculated using expression
similar to the two reported above. The majority of cases for the rejection region
are, however, either a one-sided interval extending to infinity or a two-sided
region centered at zero.
</p>
<p>4. Finally we are in a position to make a quantitative and definitive statement
regarding the null hypothesis. Since we have partitioned the range of the statistic
into an acceptable region and a rejection region, only two cases are possible:
</p>
<p>&bull; Case 1: The measured value of the statistic S falls into the rejection region.
This means that the distribution function of the statistic of interest, under the
null hypothesis, does not allow the measured value at the confidence level p.</p>
<p/>
</div>
<div class="page"><p/>
<p>120 7 Hypothesis Testing and Statistics
</p>
<p>In this case the null hypothesis must be rejected at the stated confidence level
p. The rejection of the null hypothesis means that the data should be tested for
alternative hypotheses and the procedure can be repeated.
</p>
<p>&bull; Case 2: The measured value of the statistic S is within the acceptable region.
This means that there is a reasonable probability that the measured value of the
statistic is consistent with the null hypothesis. In that case the null hypothesis
cannot be rejected, i.e., the null hypothesis could be true. In this case one can
state that the null hypothesis or the underlying model is consistent with the
data. Sometimes this situation can be referred to as the null hypothesis being
acceptable. This is, however, not the same as stating that the null hypothesis
is the correct hypothesis and that the null hypothesis is accepted. In fact, there
could be other hypotheses that could be acceptable and one cannot be certain
that the null hypothesis tested represents the parent model for the data.
</p>
<p>Example 7.2 Consider N D 5 independent measurements of a random variable
X, namely xi D .10; 12; 15; 11; 13/. We would like to test the hypothesis that
the measurements are drawn from a Gaussian random variable with ï¿½ D 13 and
ï¿½2 D 2/.
</p>
<p>Next we need to determine the test statistic that we want to use. Since there are
N independent measurements of the same variable, we can consider the sum of all
measurements as the statistic of interest,
</p>
<p>Y D
5X
</p>
<p>iD1
Xi;
</p>
<p>which is distributed like a Gaussian N.N ï¿½ ï¿½;N ï¿½ ï¿½2/ D N.65; 10/. We could have
chosen the average of the measurements instead. It can be proven that the results of
the hypothesis testing are equivalent for the two statistics.
</p>
<p>The next step requires the choice of a confidence level for our hypothesis.
Assume that we are comfortable with a value of p D 95 % level. This means that
the rejection region includes values that areË1:96ï¿½ (orË 6.2 units) away from the
parent mean of ï¿½ D 65, as shown by the cross-hatched are in Fig. 7.1.
</p>
<p>Next, we calculate the value of the statistic as Y D 61, and realize that the
measured value does not fall within the region of rejection. We conclude that the
data are consistent with the hypothesis that the measurements are drawn from the
parent Gaussian at the 95 % probability level (or 1:96ï¿½ level).
</p>
<p>Assume next that another analyst is satisfied with a p D 68% probability, instead
of 95 %. This means that the region of rejection will be Ë1:0ï¿½ D 1:0 ï¿½ p10 D 3:2
away from the mean. In this case, the rejection region becomes the hatched area in
Fig. 7.1, and the measured value of the test statistic Y falls in the rejection region. In
this case, we conclude that the hypothesis must be rejected at the 68 % probability
level (or at the 1ï¿½ level). }
</p>
<p>The example above illustrates the importance of the choice of the confidence
level p&mdash;the same null hypothesis can be acceptable or must be rejected depending</p>
<p/>
</div>
<div class="page"><p/>
<p>7.1 Statistics and Hypothesis Testing 121
</p>
<p>Fig. 7.1 Rejection regions at p D 0:95 and p D 0:68 confidence level for the test of the Gaussian
origin of measurements xi D .10; 12; 15; 11; 13/. The null hypothesis is that the sum of the
measurements are drawn from a random variable Y ï¿½ N.ï¿½ D 65; ï¿½2 D 10/
</p>
<p>on its value. To avoid this ambiguity, some analysts prefer to take a post-facto
approach to the choice of p. In this example, the measured value of the sample mean
corresponds to an absolute value of the deviation of 1:26ï¿½ from the parent mean.
Such deviation corresponds to a probability of approximately 79 % to exceed the
parent mean. It is therefore possible to report this result with the statement that the
data are consistent with the parent model at the 79 % confidence level. In general,
for a two-dimensional rejection region, the measurement Sdata corresponds to a level
of confidence p via
</p>
<p>PfS ï¿½ jSdatajg D 1 ï¿½
Z Sdata
</p>
<p>ï¿½Sdata
f .s/ds D 1 ï¿½ p; (7.3)
</p>
<p>where f .s/ is the probability distribution of the test statistic under the null hypothesis
(an equivalent expression applies to a one-sided rejection region). This equation can
be used to make the statement that the measurement of Sdata is consistent with the
model at the p confidence level.
</p>
<p>It is necessary to discuss further the meaning of the word &ldquo;acceptable&rdquo; with
regard to the null hypothesis. The fact that the measurements were within 1-ï¿½ of
a given mean does not imply that the parent distribution of the null hypothesis is the
correct one; in fact, there could be other parent distributions that are equally well
&ldquo;acceptable.&rdquo; Therefore, any null hypothesis can only be conclusively disproved (if
the measurements were beyond, say, 3- or 5-ï¿½ of the parent mean, depending on the
choice of probability p), but never conclusively proven to be the correct one, since
this would imply exhausting and discarding all possible alternative hypotheses. The
process of hypothesis testing is therefore slanted towards trying to disprove the null
hypothesis, possibly in favor of alternative hypotheses. The rejection of the null</p>
<p/>
</div>
<div class="page"><p/>
<p>122 7 Hypothesis Testing and Statistics
</p>
<p>hypothesis is the only outcome of the hypothesis testing process that is conclusive,
in that it requires to discard the hypothesis.
</p>
<p>7.2 The ï¿½2 Distribution
</p>
<p>Consider N random variables Xi, each distributed like a Gaussian with mean ï¿½i,
variance ï¿½2i , and independent of one other. For each variable Xi, the associated z-
score
</p>
<p>Zi D Xi ï¿½ ï¿½i
ï¿½i
</p>
<p>is a standard Gaussian of zero mean and unit variance. We are interested in finding
the distribution function of the random variable given by the sum of the square of
all the deviations,
</p>
<p>Z D
NX
</p>
<p>iD1
Z2i : (7.4)
</p>
<p>This quantity will be called a ï¿½2-distributed variable.
The reason for our interest in this distribution will become apparent from the
</p>
<p>use of the maximum likelihood method in fitting two-dimensional data (Chap. 8). In
fact, the sum of the squares of the deviations of the measurements from their mean,
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>ï¿½
xi ï¿½ ï¿½i
ï¿½i
</p>
<p>ï¿½2
</p>
<p>;
</p>
<p>represents a measure of how well the measurements follow the expected values ï¿½i.
</p>
<p>7.2.1 The Probability Distribution Function
</p>
<p>The theoretical distribution of Z is obtained by making use of the Gaussian
distribution for its components. To derive the distribution function of Z, we
first prove that the moment generating function of the square of each Gaussian
Zi is given by
</p>
<p>MZ2i .t/ D
r
</p>
<p>1
</p>
<p>1 ï¿½ 2t : (7.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The ï¿½2 Distribution 123
</p>
<p>This result enables the comparison with the moment generating function of
another distribution and the determination of the distribution function of Z.
</p>
<p>Proof The moment generating function of the square of a standard Gaussian
Zi is given by
</p>
<p>MZ2i .t/ D EÅe
Z2i tï¿½ D
</p>
<p>Z C1
</p>
<p>ï¿½1
ex
2t 1p
</p>
<p>2	
eï¿½
</p>
<p>x2
</p>
<p>2 dx D 1p
2	
</p>
<p>Z C1
</p>
<p>ï¿½1
eï¿½x
</p>
<p>2
ï¿½
1
2
ï¿½t
	
</p>
<p>dx
</p>
<p>We use the fact that
R C1
ï¿½1 e
</p>
<p>ï¿½y2dy D p	 ; thus, change variable y2 D
x2.1=2ï¿½ t/, and use 2xdx.1=2ï¿½ t/ D 2ydy:
</p>
<p>dx D y
x
</p>
<p>dy
</p>
<p>.1=2ï¿½ t/ D
p
1=2ï¿½ t
1=2ï¿½ t dy D
</p>
<p>dy
p
1=2ï¿½ t :
</p>
<p>This results in the following moment generating function for Y2:
</p>
<p>MZ2i .t/ D
Z C1
</p>
<p>ï¿½1
eï¿½y2p
	
</p>
<p>dy
p
2.1=2ï¿½ t/ D
</p>
<p>r
1
</p>
<p>1 ï¿½ 2t : (7.6)
</p>
<p>ut
We make use of the property that MxCy.t/ D Mx.t/ ï¿½My.t/ for independent
</p>
<p>variables (4.10). Since the variables Xi are independent of one another, so are
the variables Z2i . Therefore, the moment generating function of Z is given by
</p>
<p>MZ.t/ D
ï¿½
MZ2i .t/
</p>
<p>	N D
 r
</p>
<p>1
</p>
<p>1 ï¿½ 2t
</p>
<p>!N=2
</p>
<p>:
</p>
<p>To connect this result with the distribution function for Z, we need to introduce
the gamma distribution:
</p>
<p>fï¿½ .r; Ë/ D Ë.Ëx/
rï¿½1eï¿½Ëx
</p>
<p>ï¿½ .r/
(7.7)
</p>
<p>where Ë, r are positive numbers, and x ï¿½ 0. Its name derives from the
following relationship with the Gamma function:
</p>
<p>ï¿½ .r/ D
Z 1
</p>
<p>0
</p>
<p>eï¿½xxrï¿½1dx: (7.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>124 7 Hypothesis Testing and Statistics
</p>
<p>For integer arguments, ï¿½ .n/ D .n ï¿½ 1/Å  It can be shown that the mean of
the gamma distribution is ï¿½ D r=Ë, and the variance is ï¿½2 D r=Ë2. From
property (7.8), it is also clear that the gamma distribution in (7.7) is properly
normalized.
</p>
<p>Next, we show that the moment generating function of a gamma distribu-
tion is a generalization of the moment generating function of the square of a
standard normal distribution,
</p>
<p>Mg.t/ D 1ï¿½
1 ï¿½ t
</p>
<p>Ë
</p>
<p>	r : (7.9)
</p>
<p>Proof The moment generating function of a gamma distribution is calculated
as
</p>
<p>Mg.t/ D EÅetGï¿½ D
Z 1
</p>
<p>0
</p>
<p>etzfï¿½ .r; Ë/dz D
Z 1
</p>
<p>0
</p>
<p>Ër
</p>
<p>ï¿½ .r/
zrï¿½1eï¿½z.Ëï¿½t/dz
</p>
<p>D Ë
r
</p>
<p>ï¿½ .r/
</p>
<p>Z 1
</p>
<p>0
</p>
<p>.Ë ï¿½ t/.Ë ï¿½ t/rï¿½1zrï¿½1eï¿½z.Ëï¿½t/dz:
</p>
<p>The change of variable x D z.Ë ï¿½ t/, dx D dz.Ë ï¿½ t/ enables us to use the
normalization property of the gamma distribution,
</p>
<p>Mg.t/ D Ë
r
</p>
<p>.Ë ï¿½ t/r
Z 1
</p>
<p>0
</p>
<p>xrï¿½1
</p>
<p>ï¿½ .r/
eï¿½xdx D Ë
</p>
<p>r
</p>
<p>.Ë ï¿½ t/r D
1
</p>
<p>ï¿½
1 ï¿½ t
</p>
<p>Ë
</p>
<p>	r : (7.10)
</p>
<p>ut
The results shown in (7.5) and (7.9) prove that the moment generating functions
</p>
<p>for the Z and gamma distributions are related to one another. This relationship can be
used to conclude that the random variable Z is distributed like a gamma distribution
with parameters r D N=2 and Ë D 1=2. The random variable Z is usually referred
to as a ï¿½2 variable with N degrees of freedom, and has a probability distribution
function
</p>
<p>fï¿½2.z;N/ D fZ.z/ D
ï¿½
1
</p>
<p>2
</p>
<p>ï¿½N=2
1
</p>
<p>ï¿½ .N=2/
eï¿½z=2zN=2ï¿½1: (7.11)
</p>
<p>An example of ï¿½2 distribution is shown in Fig. 7.2. The distribution is unimodal,
although not symmetric with respect to the mean.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 The ï¿½2 Distribution 125
</p>
<p>Fig. 7.2 The Z statistic is a ï¿½2 distribution with 5 degrees of freedom. The hatched area is the
68 % rejection region, and the cross-hatched area the 95 % region
</p>
<p>7.2.2 Moments and Other Properties
</p>
<p>Since the mean and variance of a gamma distribution with parameters r, Ë, are
</p>
<p>ï¿½ D r
Ë
</p>
<p>and ï¿½2 D r
Ë2
</p>
<p>, the ï¿½2 distribution has the following moments:
</p>
<p>(
ï¿½ D N
ï¿½2 D 2N: (7.12)
</p>
<p>This result shows that the expectation of a ï¿½2 variable is equal to the number of
degrees of freedom. It is common to use the reduced ï¿½2 square variable defined by
</p>
<p>ï¿½2red D
ï¿½2
</p>
<p>N
: (7.13)
</p>
<p>The mean or expectation of the reduced ï¿½2 and the variance are therefore given by
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>ï¿½ D 1
ï¿½2 D 2
</p>
<p>N
:
</p>
<p>(reduced ï¿½2) (7.14)
</p>
<p>As a result, the ratio between the standard deviation and the mean for the reduced
ï¿½2, a measure of the spread of the distribution, decreases with the number of degrees</p>
<p/>
</div>
<div class="page"><p/>
<p>126 7 Hypothesis Testing and Statistics
</p>
<p>of freedom,
</p>
<p>ï¿½
</p>
<p>ï¿½
D
r
2
</p>
<p>N
:
</p>
<p>As the numbers of degrees of freedom increase, the values of the reduced ï¿½2 are
more closely distributed around 1.
</p>
<p>As derived earlier, the moment generating function of the ï¿½2 distribution
</p>
<p>Mï¿½2.t/ D EÅetZ ï¿½ D
1
</p>
<p>.1 ï¿½ 2t/N=2 : (7.15)
</p>
<p>This form of the moment generating function highlights the property that, if two
independent ï¿½2 distributions have, respectively, N and M degrees of freedom, then
the sum of the two variables will also be a ï¿½2 variable, and it will haveNCM degrees
of freedom. In fact, the generating function of the sum of independent variables is
the product of the two functions, and the exponents in (7.15) will add.
</p>
<p>7.2.3 Hypothesis Testing
</p>
<p>The null hypothesis for a ï¿½2 distribution is that all measurements are consistent
with the parent Gaussians. Under this hypothesis, we have derived the probability
distribution function fï¿½2.z;N/, where N is the number of degrees of freedom of the
distribution. If the N measurements are consistent with their parent distributions,
one expects a value of approximately ï¿½2 ' N, i.e., each of the N measurements
contributes approximately a value of one to the ï¿½2. Large values of ï¿½2 clearly
indicate that some of the measurements are not consistent with the parent Gaussian,
i.e., some of the measurements xi differ by several standard deviations from the
expected mean, either in defect or in excess. Likewise, values of ï¿½2 ï¿½ N are
also not expected. Consider, for example, the extreme case of N measurements all
identical to the parent mean, resulting in ï¿½2 D 0. Statistical fluctuations of the
random variables make it extremely unlikely that all N measurements match the
mean. Clearly such an extreme case of perfect agreement between the data and the
parent model is suspicious and the data should be checked for possible errors in the
collection or analysis.
</p>
<p>Despite the fact that very small value of ï¿½2 is unlikely, it is customary to test for
the agreement between a measurement of ï¿½2 and its theoretical distribution using a
one-sided rejection region consisting of values of ï¿½2 exceeding a critical value. This
means that the acceptable region is for values of ï¿½2 that are between zero and the
critical value. Critical values of the ï¿½2 distribution for a confidence level p can be</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 The Sampling Distribution of the Variance 127
</p>
<p>calculated via
</p>
<p>P.Z ï¿½ ï¿½2crit/ D
Z 1
</p>
<p>ï¿½2crit
</p>
<p>fï¿½2 .z; n/dz D 1 ï¿½ p (7.16)
</p>
<p>and are tabulated in Table A.7.
</p>
<p>Example 7.3 Assume the N D 5measurements of a variable X, .10; 12; 15; 11; 13/,
presented in Example 7.2. We want to test the hypothesis that these were inde-
pendent measurements of a Gaussian variable X of mean ï¿½ D 13 and variance
ï¿½2 D 2. Under this assumption, we could use the ï¿½2 statistic to try and falsify the
null hypothesis that the data are drawn from the given Gaussian. The procedure for
a quantitative answer to this hypothesis is that of deciding a level of probability p,
then to calculate the value of the statistic,
</p>
<p>ï¿½2 D 1=2 ï¿½ ..10ï¿½ 13/2C .12ï¿½ 13/2C .15ï¿½ 13/2C .11ï¿½ 13/2C .13ï¿½ 13/2/ D 9:
</p>
<p>In Fig. 7.2 we show the rejection regions for a probability p D 0:95 and p D
0:68, which are determined according to the tabulation of the integral of the ï¿½2
</p>
<p>distribution with N D 5 degrees of freedom: ï¿½2crit D 6:1 marks the beginning of
the 70 % rejection region, and ï¿½2crit D 11:1 that of the 95 % rejection region. The
hypothesis is therefore rejected at the 68 % probability level, but cannot be rejected
at the 95 % confidence level.
</p>
<p>Moreover, we calculate from Table A.7
</p>
<p>P.ï¿½2 ï¿½ 9/ D
Z 1
</p>
<p>9
</p>
<p>fï¿½2.z; 5/dz ' 0:10:
</p>
<p>We therefore conclude that there is a 10 % probability of observing such value of ï¿½2,
or higher, under the hypothesis that the measurements were made from a Gaussian
distribution of such mean and variance (see Fig. 7.2). Notice that the results obtained
using the ï¿½2 distribution are similar to those obtained with the test that made use of
the sum of the five measurements. }
</p>
<p>7.3 The Sampling Distribution of the Variance
</p>
<p>The distribution function of the sample variance, or sampling distribution of the
variance, is useful to compare a given measurement of the sample variance s2 with
the parent variance ï¿½2. We consider N measurements of X that are distributed like
a Gaussian of mean ï¿½, variance ï¿½2 and independent of each other. The variable S2
</p>
<p>defined by
</p>
<p>S2 D .N ï¿½ 1/s2 D
NX
</p>
<p>iD1
.Xi ï¿½ NX/2 (7.17)</p>
<p/>
</div>
<div class="page"><p/>
<p>128 7 Hypothesis Testing and Statistics
</p>
<p>is proportional to the sample variance s2. We seek a distribution function for S2=ï¿½2
</p>
<p>that enables a comparison of the measured sample variance with the parent variance
ï¿½2.
</p>
<p>In determining the sampling distribution of the variance we do not want to
assume that the mean of the parent Gaussian is known, as we did in the previous
section for the determination of the ï¿½2 distribution. This is important, since in a
typical experiment we do not know a priori the parent mean of the distribution, but
we can easily calculate the sample mean. One complication in the use of (7.17)
is therefore that NX is itself a random variable, and not an exactly known quantity.
This fact must be taken into account when calculating the expectation of S2. A
measurement of S2 is equal to
</p>
<p>S2 D
NX
</p>
<p>iD1
.xi ï¿½ ï¿½C ï¿½ï¿½ Nx/2 D
</p>
<p>NX
</p>
<p>iD1
.xi ï¿½ ï¿½/2 ï¿½ N.ï¿½ ï¿½ Nx/2: (7.18)
</p>
<p>Dividing both terms by ï¿½2, we obtain the following result:
</p>
<p>PN
iD1.xi ï¿½ ï¿½/2
</p>
<p>ï¿½2
D S
</p>
<p>2
</p>
<p>ï¿½2
C .Nx ï¿½ ï¿½/
</p>
<p>2
</p>
<p>ï¿½2=N
: (7.19)
</p>
<p>According to the result in Sect. 7.2, the left-hand side term is distributed like a
ï¿½2 variable with N degrees of freedom, since the parent mean ï¿½ and variance ï¿½2
</p>
<p>appear in the sum of squares. For the same reason, the second term in the right-
hand side is also distributed like a ï¿½2 variable with 1 degree of freedom, since we
have already determined that the sample mean X is distributed like a Gaussian with
mean ï¿½ and with variance ï¿½2=N. Although it may not be apparent at first sight, it
can be proven that the two terms on the right-hand side are two independent random
variables. If we can establish the independence between these two variables, then it
must be true that the first variable in the right-hand side, S2=ï¿½2, is also distributed
like a ï¿½2 distribution with N ï¿½ 1 degrees of freedom. This follows from the fact that
the sum of two independent ï¿½2 variables is also a ï¿½2 variable featuring the sum of
the degrees of freedom of the two variables, as shown in Sect. 7.2.
</p>
<p>Proof The proof of the independence between S2=ï¿½2 and .
NXï¿½ï¿½/2
ï¿½2ï¿½
</p>
<p>, and the fact
</p>
<p>that both are distributed like ï¿½2 distributions with, respectively, N ï¿½ 1 and 1
degrees of freedom, can be obtained by making a suitable change of variables
from the original N standard normal variables that appear in the left-hand side
of (7.19),
</p>
<p>Zi D Xi ï¿½ ï¿½
ï¿½
</p>
<p>;</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 The Sampling Distribution of the Variance 129
</p>
<p>to a new set of N variables Yi. The desired transformation is one that has the
property
</p>
<p>Z21 C : : :C Z2N D Y21 C : : :C Y2N :
</p>
<p>This is called an orthonormal (linear) transformation, and in matrix form it
can be expressed by a transformation matrix A, of dimensions N 	 N, such
that a row vector z D .Z1; : : : ;ZN/ is transformed into another vector y
by way of the product y D zA. For such a transformation, the dot product
between two vectors is expressed as yyT D zAATzT . Since for an orthonormal
transformation the relationship AAT D I holds, where I is the N 	 N identity
matrix, then the dot product remains constant upon this transformation. An
orthonormal transformation, expressed in extended form as
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>Y1 D a1Z1 C : : :C aNZN
Y2 D b1Z1 C : : :C bNZN
: : :
</p>
<p>is obtained when, for each row vector,
P
</p>
<p>a2i D 1; and, for any pair of row
vectors,
</p>
<p>P
aibi D 0, so that the Yi&rsquo;s are independent of one another.
</p>
<p>Any such orthonormal transformation, when applied to N independent
variables that are standard Gaussians, Zi ï¿½ N.0; 1/, as is the case in this
application, is such that the transformed variables Yi are also independent
standard Gaussians. In fact, the joint probability distribution function of the
Zi&rsquo;s can be written as
</p>
<p>f .z/ D 1
.2	/N=2
</p>
<p>eï¿½
z21C:::Cz2N
</p>
<p>2 I
</p>
<p>and, since the transformed variables have the same dot product, z21C: : :Cz2N D
y21 C : : : C y2N , the N variables Yi have the same joint distribution function,
proving that they are also independent standard Gaussians.
</p>
<p>We want to use these general properties of orthonormal transformations to
find a transformation that will enable a proof of the independence between
S2=ï¿½2 and .X ï¿½ ï¿½/2=ï¿½2ï¿½. The first variable is defined by the following linear
combination,
</p>
<p>Y1 D Z1p
N
C : : :C ZNp
</p>
<p>N</p>
<p/>
</div>
<div class="page"><p/>
<p>130 7 Hypothesis Testing and Statistics
</p>
<p>in such a way that the following relationships hold:
</p>
<p>8
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
:
</p>
<p>Y21 D
. NX ï¿½ ï¿½/2
ï¿½2=N
</p>
<p>NP
</p>
<p>iD1
Z2i D
</p>
<p>1
</p>
<p>ï¿½2
</p>
<p>NP
</p>
<p>iD1
.Xi ï¿½ ï¿½/2; or
</p>
<p>NP
</p>
<p>iD1
Z2i D Y21 C
</p>
<p>NP
</p>
<p>iD2
Y2i :
</p>
<p>The other N ï¿½ 1 variables Y2; : : : ;YN can be chosen arbitrarily, provided they
satisfy the requirements of orthonormality. Since
</p>
<p>NP
</p>
<p>iD1
Z2i ï¿½Y21 D S2=ï¿½2, we can
</p>
<p>conclude that
</p>
<p>S2
</p>
<p>ï¿½2
D
</p>
<p>NX
</p>
<p>iD2
Y2i
</p>
<p>proving that S2=ï¿½2 is distributed like a ï¿½2 distribution with N ï¿½ 1 degrees of
freedom, as the sum of squares on Nï¿½ 1 independent standard Gaussians, and
that S2=ï¿½2 is independent of the sampling distribution of the mean, Y21 , since
the variables Yi are independent of each other. This proof is due to Bulmer [7],
who used a derivation done earlier by Helmert [20]. ut
We are therefore able to conclude that the ratio S2=ï¿½2 is distributed like a ï¿½2
</p>
<p>variable with N ï¿½ 1 degrees of freedom,
</p>
<p>S2
</p>
<p>ï¿½2
ï¿½ ï¿½2.N ï¿½ 1/: (7.20)
</p>
<p>The difference between the ï¿½2 distribution (7.11) and the distribution of the
sample variance (7.20) is that in the latter case the mean of the parent distribution is
not assumed to be known, but it is calculated from the data. This is in fact the more
common situation, and therefore when N measurements are obtained, the quantity
</p>
<p>S2
</p>
<p>ï¿½2
D
</p>
<p>NX
</p>
<p>iD1
</p>
<p>ï¿½
xi ï¿½ Nx
ï¿½
</p>
<p>ï¿½2
</p>
<p>is distributed like a ï¿½2 distribution with just N ï¿½ 1 degrees of freedom, not N. This
reduction in the number of degrees of freedom can be expressed by saying that one
degree of freedom is being used to estimate the mean.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 The F Statistic 131
</p>
<p>Example 7.4 Assume N D 10 measurements of a given quantity (10, 12, 15, 11,
13, 16, 12, 10, 18, 13). We want to answer the following question: Are these
measurements consistent with being drawn from the same Gaussian random variable
with ï¿½2 D 2? If the measurements are in fact derived from the same variable, then
the probability of measuring the actual value of s2 for the sample variance will be
consistent with its theoretical distribution that was just derived in (7.20).
</p>
<p>The value of the sample variance is obtained by Nx D 13 as S2 D 62. Therefore,
the measurement s2=ï¿½2 D 62=2 D 36 must be compared with the ï¿½2 distribution
with N ï¿½ 1 D 9 degrees of freedom. The measurement is equivalent to a reduced ï¿½2
value of 4, which is inconsistent with a ï¿½2 distribution with 9 degrees of freedom
at more than the 99 % confidence level. We therefore conclude that the hypothesis
must be rejected with this confidence level.
</p>
<p>It is necessary to point out that, in this calculation, we assumed that the parent
variance was known. In the following section we will provide another test that can be
used to compare two measurements of the variance that does not require knowledge
of the parent variance. That is in fact the more common experimental situation and
it requires a detailed study. }
</p>
<p>7.4 The F Statistic
</p>
<p>The distribution of the sample variance discussed above in Sect. 7.3 shows that if
the actual variance ï¿½ is not known, then it is impossible to make a quantitative
comparison of the sample variance with the parent distribution. Alternatively, one
can compare two different measurements of the variance, and ask the associated
question of whether the ratio between the two measurements is reasonable. In this
case the parent variance ï¿½2 drop out of the equation and the parent variance is not
required to compare two measurements of the sample variance.
</p>
<p>For this purpose, consider two independent random variables Z1 and Z2, respec-
tively, distributed like a ï¿½2 distribution with f1 and f2 degrees of freedom. We define
the random variable F as
</p>
<p>F D Z1=f1
Z2=f2
</p>
<p>: (7.21)
</p>
<p>The variable F is equivalent to the ratio of two reduced ï¿½2, and therefore is expected
to have values close to unity.</p>
<p/>
</div>
<div class="page"><p/>
<p>132 7 Hypothesis Testing and Statistics
</p>
<p>7.4.1 The Probability Distribution Function
</p>
<p>We show that the probability distribution function of the random variable F is
given by
</p>
<p>fF.z/ D
ï¿½
</p>
<p>ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f1
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½
f1
f2
</p>
<p>ï¿½ f1
2 z
</p>
<p>f1
2
ï¿½1
</p>
<p>ï¿½
</p>
<p>1C z f1
f2
</p>
<p>ï¿½ f1Cf2
2
</p>
<p>: (7.22)
</p>
<p>Proof The proof makes use of the methods described in Sects. 4.4.1 and 4.4.2.
First we derive the distribution functions of the numerator and denominator of
(7.21), and then we calculate the distribution function for the ratio of two
variables with known distribution.
</p>
<p>Given that Z1 ï¿½ ï¿½2. f1/ and Z2 ï¿½ ï¿½2. f2/, the distribution functions of
X0 D Z1=f1 and Y 0 D Z2=f2 are found using change of variables; for X0,
</p>
<p>fX0.x
0/ D f .z/ dz
</p>
<p>dx0
D f .z/f1;
</p>
<p>where f .z/ is the distribution of Z1. This results in
</p>
<p>fX0.x
0/ D z
</p>
<p>f1=2ï¿½1eï¿½z=2
</p>
<p>ï¿½ . f1=2/2f1=2
f1 D .x
</p>
<p>0f1/f1=2ï¿½1eï¿½.x
0f1/=2
</p>
<p>ï¿½ . f1=2/2f1=2
f1I
</p>
<p>same transformation applies to Y 0. Now we can use (4.18),
</p>
<p>fF.z/ D
Z 1
</p>
<p>0
</p>
<p>fX0.zï¿½/ï¿½fY0.ï¿½/dï¿½
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>.zï¿½f1/f1=2ï¿½1eï¿½.zï¿½f1/=2
</p>
<p>ï¿½ . f1=2/2f1=2
f1ï¿½
.ï¿½f2/f2=2ï¿½1eï¿½.ï¿½f2/=2
</p>
<p>ï¿½ . f2=2/2f2=2
dï¿½
</p>
<p>D f1f2z
f1=2ï¿½1f f1=2ï¿½11 f
</p>
<p>f2=2ï¿½1
2
</p>
<p>ï¿½ . f1=2/2. f1Cf2/=2ï¿½ . f2=2/
</p>
<p>Z 1
</p>
<p>0
</p>
<p>ï¿½ f1=2ï¿½1Cf2=2ï¿½1C1eï¿½1=2ï¿½.zf1Cf2/dï¿½
</p>
<p>D z
f1=2ï¿½1f f1=21 f
</p>
<p>f2=2
2
</p>
<p>ï¿½ . f1=2/ï¿½ . f2=2/2. f1Cf2/=2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>ï¿½. f1Cf2/=2ï¿½1eï¿½1=2ï¿½.zf1Cf2/dï¿½</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 The F Statistic 133
</p>
<p>After another change of variables, t D ï¿½.zf1 C f2/=2, dt D dï¿½.zf1 C f2/=2,
the integral becomes
</p>
<p>Z 1
</p>
<p>0
</p>
<p>ï¿½
2t
</p>
<p>zf1 C f2
ï¿½. f1Cf2/=2ï¿½1
</p>
<p>eï¿½t
dt
</p>
<p>ï¿½
zf1 C f2
2
</p>
<p>ï¿½
</p>
<p>D 2
. f1Cf2/=2
</p>
<p>.zf1 C f2/1C. f1Cf2/=2ï¿½1
Z 1
</p>
<p>0
</p>
<p>t. f1Cf2/=2ï¿½1eï¿½tdt
</p>
<p>D 2
. f1Cf2/=2
</p>
<p>.zf1 C f2/. f1Cf2/=2 ï¿½
ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>:
</p>
<p>Therefore the distribution of Z is given by
</p>
<p>fF.z/ D z
f1=2ï¿½1f f1=21 f
</p>
<p>f2=2
2
</p>
<p>ï¿½ . f1=2/ï¿½ . f2=2/2. f1Cf2/=2
</p>
<p>2. f1Cf2/=2ï¿½
ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>.zf1 C f2/. f1Cf2/=2
</p>
<p>D
f f1=21 f
</p>
<p>f2=2
2 ï¿½
</p>
<p>ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½ . f1=2/ï¿½ . f2=2/
</p>
<p>zf1=2ï¿½1
</p>
<p>.1C z f1
f2
/. f1Cf2/=2f . f1Cf2/=22
</p>
<p>D
ï¿½
f1
f2
</p>
<p>ï¿½f1=2 ï¿½
</p>
<p>ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f1
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f2
2
</p>
<p>ï¿½
zf1=2ï¿½1
</p>
<p>ï¿½
</p>
<p>1C z f1
f2
</p>
<p>ï¿½. f1Cf2/=2 :
</p>
<p>ut
The distribution of F is known as the F distribution. It is named after Fisher [13],
</p>
<p>who was the first to study it.
</p>
<p>7.4.2 Moments and Other Properties
</p>
<p>The mean and higher-order moments of the F distribution can be calculated by
making use of the Beta function,
</p>
<p>B.x; y/ D
Z 1
</p>
<p>0
</p>
<p>f xï¿½1
</p>
<p>.1C t/xCy dt D
ï¿½ .x/ï¿½ .y/
</p>
<p>ï¿½ .xC y/ ; (7.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>134 7 Hypothesis Testing and Statistics
</p>
<p>to find that
8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½ D f2
f2 ï¿½ 2. f2 &gt; 2/
</p>
<p>ï¿½2 D 2f
2
2 . f1 C f2 ï¿½ 2/
</p>
<p>f1. f2 ï¿½ 2/2. f2 ï¿½ 4/. f2 &gt; 4/
(7.24)
</p>
<p>The mean is approximately 1, provided that f2 is not too small.
It is possible to find an approximation to the F distribution when either f1 or f2 is
</p>
<p>a large number:
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>lim
f2!1
</p>
<p>fF.z; f1; f2/ D fï¿½2 .x; f1/ where x D f1z
lim
</p>
<p>f1!1
fF.z; f1; f2/ D fï¿½2 .x; f2/ where x D f2=z:
</p>
<p>(7.25)
</p>
<p>The approximation, discussed, for example, in [1], is very convenient, since it
overcomes the problems with the evaluation of the Gamma function for large
numbers.
</p>
<p>7.4.3 Hypothesis Testing
</p>
<p>The F statistic is a ratio
</p>
<p>F D ï¿½
2
1=f1
ï¿½22=f2
</p>
<p>(7.26)
</p>
<p>between two independent ï¿½2 measurements of, respectively, f1 and f2 degrees of
freedom. A typical application of the F test is the comparison of two ï¿½2 statistics
from independent datasets using the parent Gaussians as models for the data. The
null hypothesis is that both sets of measurements follow the respective Gaussian
distribution. In this case, the measured ratio F will follow the F distribution.
This implies that the measured value of F should not be too large under the null
hypothesis that both measurements follow the parent models.
</p>
<p>It is customary to do hypothesis testing of an F distribution using a one-sided
rejection region above a critical value. The critical value at confidence level p is
calculated via
</p>
<p>P.F &gt; Fcrit/ D
Z 1
</p>
<p>Fcrit
</p>
<p>fF.z/dz D 1 ï¿½ p: (7.27)
</p>
<p>Critical values are tabulated in Table A.8 for the case of fixed f1 D 1, and Tables A.9,
A.10, A.11, A.12, A.13, A.14, and A.15 for various values of p, and as function of
f1 and f2. The values of Fcrit calculated from (7.27) indicate how high a value of the</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 The F Statistic 135
</p>
<p>F statistic can be, and still be consistent with the hypothesis that the two quantities
at the numerator and denominator are ï¿½2-distributed variables.
</p>
<p>The approximations for the F distribution in (7.25) can be used to calculate
critical values when one of the degrees of freedom is very large. For example, the
critical value of F at 90 % confidence, p D 0:90, for f1 D 100 and f2 ! 1 (e.g.,
Table A.13) is calculated from Table A.7 as F = 1.185. Note that Table A.7 reports
the value of the reduced ï¿½2, or z in the notation of the top equation in (7.25).
</p>
<p>Example 7.5 Consider the data set composed of the ten measurements
</p>
<p>.10; 12; 15; 11; 13; 16; 12; 10; 18; 13/.
</p>
<p>We assume that the measurements follow a Gaussian distribution of mean of
ï¿½ D 13 and variance ï¿½2. The goal is to compare the calculation of the ï¿½2 of the
first five measurements with the last five to address whether both subsets are equally
likely to be described by the same Gaussian.
</p>
<p>We obtain ï¿½21 D 18=ï¿½2 and ï¿½22 D 44=ï¿½2, respectively, for the first and the
second set of five measurements. Both variables, under the null hypothesis that the
measurements follow the reference Gaussian, are distributed like ï¿½2 with 5 degrees
of freedom (since both mean and variance are assumed to be known). We therefore
can calculate an F statistic of F D 44=18 D 2:44. For simplicity, we have placed
the initial five measurements at the denominator.
</p>
<p>In the process of calculating the F statistic, the variances ï¿½2 cancel, and therefore
the null hypothesis is that of a mean of ï¿½ D 13 and same variance for both sets,
regardless of its value. In Fig. 7.3 we plot the F distribution for f1 D 5 and f2 D 5
</p>
<p>Fig. 7.3 Solid curve is the F distribution with &#13;1 D 5, &#13;2 D 5 degrees of freedom; the hatched
area is the 75 % rejection region, and the cross-hatched area is the 90 % rejection region. For
comparison, the F distribution with &#13;1 D 4, &#13;2 D 4 degrees of freedom is shown as the dashed
line, and the two rejection regions are outlined in green and red, respectively. The rejection region
for the F distribution with &#13;1 D 4, &#13;2 D 4 degrees of freedom is shifted to higher values, relative
to that with &#13;1 D 5, &#13;2 D 5 degrees of freedom, because of its heavier tail</p>
<p/>
</div>
<div class="page"><p/>
<p>136 7 Hypothesis Testing and Statistics
</p>
<p>as the solid line, and its 75 and 90 % rejection regions, marked, respectively, by
the critical values F D 1:89 and 3.45, as hatched and cross-hatched areas. The
measurements are therefore consistent with the null hypothesis at the 90 % level,
but the null hypothesis must be discarded at the 75 % confidence level. Clearly the
first set of five numbers follows the parent Gaussian more closely than the second
set. Yet, there is a reasonable chance (ï¿½ 10%) that both sets follow the Gaussian.
</p>
<p>If the parent variance was given, say ï¿½2 D 4, we could have tested both subsets
independently for the hypothesis that they follow a Gaussian of mean ï¿½ D 13 and
variance ï¿½2 D 4 using the ï¿½2 distribution. The two measurements are ï¿½21 D 4:5 and
ï¿½22 D 11 for 5 degrees of freedom. Assuming a confidence level of p D 0:9, the
critical value of the ï¿½2 distribution is ï¿½2crit D 9:2. At this confidence level, we would
reject the null hypothesis for the second measurement. }
</p>
<p>The ratio between two measurements of the sample variance follows the F
distribution. For two independent sets of, respectively, N and M measurements, the
sample variances s21 and s
</p>
<p>2
2 are related to the parent variances ï¿½
</p>
<p>2
1 and ï¿½
</p>
<p>2
2 of the
</p>
<p>Gaussian models via
</p>
<p>F D Z1=f1
Z2=f2
</p>
<p>D
S21
ï¿½21 f1
S22
ï¿½22 f2
</p>
<p>; (7.28)
</p>
<p>where
</p>
<p>(
S21 D .N ï¿½ 1/s21 D
</p>
<p>PN
iD1.xi ï¿½ x/2
</p>
<p>S22 D .M ï¿½ 1/s22 D
PM
</p>
<p>jD1.yj ï¿½ y/2:
(7.29)
</p>
<p>The quantities Z1 D S21=ï¿½21 and Z2 D S22=ï¿½22 are ï¿½2-distributed variables with,
respectively, f1 D N ï¿½ 1 and f2 D M ï¿½ 1 degrees of freedom. The statistic F can be
used to test whether both measurements of the variance are equally likely to have
come from the respective models.
</p>
<p>The interesting case is clearly when the two variances are equal, ï¿½21 D ï¿½22 , so that
the value of the variance drops out of the equation and the F statistic becomes
</p>
<p>F D S
2
1=f1
</p>
<p>S22=f2
(ï¿½21 D ï¿½22 ): (7.30)
</p>
<p>In this case, the null hypothesis becomes that the two samples are Gaussian
distributed, regardless of values for the mean and the variance. The statistic therefore
measure if the variances or variability of the data in the two measurements are
consistent with one another or if one measurement has a sample variance that is
significantly larger than the other. If the value of F exceeds the critical value, then
the null hypothesis must be rejected and the conclusion is that the measurement with
the largest value of Z=f , which is placed at the numerator, is not as likely to have</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 The Sampling Distribution of the Mean and the Student&rsquo;s t Distribution 137
</p>
<p>come from the parent model as the other set. This type of analysis will have specific
applications to model fitting in Chap. 13.
</p>
<p>Example 7.6 Using the same data as in the previous example, we can calculate the
sample variance using the sample mean for each of the two 5-measurement sets. We
calculate a sample mean of x1 D 12:2 and x2 D 13:8, for a value of S21 D 14:8 and
S22 D 40:8, for a ratio of F D 2:76. Given that the sample mean was estimated from
the data, the null hypothesis is that both sets are drawn from the same Gaussian
distribution, without specification of the value of either variance or mean, and each
measurement of S2=ï¿½2 is distributed now like a ï¿½2 variable with just 4 degrees of
freedom (and not 5). The value of the F statistic must therefore be compared with
an F distribution with f1 D 4 and f2 D 4 degrees of freedom, reported in Fig. 7.3
as a dashed line. The 75 and 90 % rejection regions, marked, respectively, by the
critical values F D 2:06 and 4.1, are outlined in green and red, respectively. The
measurements are therefore consistent at the 90 % confidence level, but not at the
75 % level.
</p>
<p>We conclude that there is at least a 10 % probability that the two measurements
of the variance are consistent with one another. At the p D 0:9 level we therefore
cannot reject the null hypothesis. }
</p>
<p>7.5 The Sampling Distribution of the Mean
and the Student&rsquo;s t Distribution
</p>
<p>In many experimental situations we want to compare the sample mean obtained
from the data to a parent mean based on theoretical considerations. Other times
we want to compare two sample means to one another. The question we answer
in this section is how the sample mean is expected to vary when estimated from
independent samples of size N.
</p>
<p>7.5.1 Comparison of Sample Mean with Parent Mean
</p>
<p>For measurements of a Gaussian variable of mean ï¿½ and variance ï¿½2, the sample
mean Nx is distributed as a Gaussian of meanï¿½ and variance ï¿½2=N. Therefore, if both
the mean and the variance of the parent distribution are known, the sample mean NX
is such that
</p>
<p>NX ï¿½ ï¿½
ï¿½=
p
N
ï¿½ N.0; 1/:</p>
<p/>
</div>
<div class="page"><p/>
<p>138 7 Hypothesis Testing and Statistics
</p>
<p>A simple comparison between the z-score of the sample mean to the N.0; 1/
Gaussian therefore addresses the consistency between the measurement and the
model.
</p>
<p>Example 7.7 Continue with the example of the five measurements of a random
variable .10; 12; 15; 11; 13/, assumed to be distributed like a Gaussian of ï¿½ D 13
and ï¿½2 D 2. Assuming knowledge of the parent mean and variance, the z-score of
the sample mean is
</p>
<p>Nx ï¿½ ï¿½
ï¿½ï¿½
D 12:2 ï¿½ 13p
</p>
<p>2=5
D ï¿½1:27:
</p>
<p>According to Table A.2, there is a probability of about 20 % to exceed the absolute
value of this measurement according to the parent distribution N.0; 1/. Therefore
the null hypothesis that the measurements are distributed like a Gaussian of ï¿½ D 13
and ï¿½2 D 2 cannot be rejected at the 90 % confidence level. Notice that this is the
same probability as obtained by using the sum of the five measurements, instead
of the average. This was to be expected, since the mean differs from the sum by a
constant value, and therefore the two statistics are equivalent. }
</p>
<p>A more common situation is when the meanï¿½ of the parent distribution is known
but the parent variance is unknown. In those cases the parent variance can only be
estimated from the data themselves via the sample variance s2 and one needs to
allow for such uncertainty when estimating the distribution of the sample mean.
This additional uncertainty leads to a deviation of the distribution function from the
simple Gaussian shape. We therefore seek to find the distribution of
</p>
<p>T D Nx ï¿½ ï¿½
s=
p
n
</p>
<p>(7.31)
</p>
<p>in which we define the sample variance in such a way that it is an unbiased estimator
of the parent variance,
</p>
<p>s2 D 1
N ï¿½ 1
</p>
<p>X
.xi ï¿½ Nx/2 D S
</p>
<p>2
</p>
<p>N ï¿½ 1:
</p>
<p>The variable T can be written as
</p>
<p>T D Nx ï¿½ ï¿½
s=
p
N
D
ï¿½ Nx ï¿½ ï¿½
ï¿½=
p
N
</p>
<p>ï¿½
</p>
<p>=.s=ï¿½/ D
ï¿½ Nx ï¿½ ï¿½
ï¿½=
p
N
</p>
<p>ï¿½
</p>
<p>=
</p>
<p>ï¿½
S2
</p>
<p>.N ï¿½ 1/ï¿½2
ï¿½1=2
</p>
<p>; (7.32)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 The Sampling Distribution of the Mean and the Student&rsquo;s t Distribution 139
</p>
<p>in which S2 is the sum of the squares of the deviations from the sample mean. As
shown in previous sections,
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>Nx ï¿½ ï¿½
ï¿½=
p
n
ï¿½ N.0; 1/
</p>
<p>S2
</p>
<p>ï¿½2
ï¿½ ï¿½2.N ï¿½ 1/:
</p>
<p>We therefore need to determine the distribution function of the ratio of these two
variables. We will show that a random variable T defined by the ratio
</p>
<p>T D Xp
Z=f
</p>
<p>; (7.33)
</p>
<p>in which X ï¿½ N.0; 1/ and Z ï¿½ ï¿½2. f / (a ï¿½2 distribution with f degrees of freedom)
is said to be distributed like a t distribution with f degrees of freedom:
</p>
<p>fT.t/ D 1p
f	
</p>
<p>ï¿½ .. f C 1/=2/
ï¿½ . f=2/
</p>
<p>	
ï¿½
</p>
<p>1C t
2
</p>
<p>f
</p>
<p>ï¿½ï¿½
f C 1
2
</p>
<p>: (7.34)
</p>
<p>Proof The proof of (7.34) follows the same method as that of the F distribu-
tion. First, we can derive the distribution function of Y D pZ=f using the
usual method of change of variables,
</p>
<p>g.y/ D h.z/dz
dy
D h.z/2pfZ
</p>
<p>where
</p>
<p>h.z/ D z
f=2ï¿½1eï¿½z=2
</p>
<p>2f=2ï¿½ . f=2/
:
</p>
<p>Therefore the distribution of Y is given by substituting z D fy2 into the first
equation,
</p>
<p>g.y/ D f
. fï¿½1/=2yfï¿½1eï¿½fy2=2
</p>
<p>p
f
</p>
<p>2f=2ï¿½1ï¿½ . f=2/
: (7.35)
</p>
<p>The distribution function of the numerator of (7.33) is simply
</p>
<p>f.x/ D 1p
2	
</p>
<p>eï¿½x2=2;</p>
<p/>
</div>
<div class="page"><p/>
<p>140 7 Hypothesis Testing and Statistics
</p>
<p>and therefore the distribution of T is given by applying (4.18),
</p>
<p>fT.t/ D
Z 1
</p>
<p>0
</p>
<p>1p
2	
</p>
<p>eï¿½.ty/2=2y
f . fï¿½1/=2yfï¿½1eï¿½fy2=2
</p>
<p>p
f
</p>
<p>2f=2ï¿½1ï¿½ . f=2/
dy: (7.36)
</p>
<p>The integral can be shown to be equal to (7.34) following a few steps of
integration as in the case of the F distribution. ut
This distribution is symmetric and has a mean of zero, and it goes under the name
</p>
<p>of Student&rsquo;s t distribution. This distribution was studied first by Gosset in 1908 [18],
who published a paper on the subject under the pseudonym of &ldquo;Student.&rdquo;
</p>
<p>The random variable T defined in (7.31) therefore is distributed like a t variable
with N ï¿½ 1 degrees of freedom. It is important to notice the difference between
the sample distribution of the mean in the case in which the variance is known,
which is N.0; 1/, and the t distribution. In particular, the latter depends on the
number of measurements, while the former does not. One expects that, in the
limit of a large number of measurements, the t distribution tends to the standard
normal (see Problem 7.10). The t distribution has in fact broader wings than the
standard Gaussian, and in the limit of an infinite number of degrees of freedom,
the two distributions are identical; an example of the comparison between the
two distributions is shown in Fig. 7.4. The t distribution has heavier tails than the
Gaussian distribution, indicative of the additional uncertainty associated with the
fact that the variance is estimated from the data and not known a priori.
</p>
<p>Fig. 7.4 Student&rsquo;s t
distribution with f D 4
degrees of freedom. The
dashed curve is the N.0; 1/
Gaussian, to which the
t-distribution tends for a large
number of degrees of
freedom. The hatched area is
the 68 % rejection region
(compare to the Ë1ï¿½ region
for the N.0; 1/ distribution)
and the cross-hatched area is
the 95 % region (compare to
Ë1:95ï¿½ for the N.0; 1/
distribution)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 The Sampling Distribution of the Mean and the Student&rsquo;s t Distribution 141
</p>
<p>7.5.1.1 Hypothesis Testing
</p>
<p>Hypothesis testing with the t distribution typically uses a two-sided rejection region.
After obtaining a measurement of the t variable from a given dataset, we are usually
interested in knowing how far the measurement can be from the expected mean of 0
and still be consistent with the parent distribution. The critical value for a confidence
level p is calculated via
</p>
<p>P.jtj 
 Tcrit/ D
Z Tcrit
</p>
<p>ï¿½Tcrit
fT.t/dt D p (7.37)
</p>
<p>and it is a function of the number of degrees of freedom for the t distribution.
Tables A.16, A.17, A.18, A.19, A.20, A.21, and A.22 report the value of p as
function of the critical value Tcrit for selected degrees of freedom, and Table A.23
compares the t distribution with the standard Gaussian.
</p>
<p>Example 7.8 Assume now that the five measurements .10; 12; 15; 11; 13/ are dis-
tributed like a Gaussian of ï¿½ D 13, but without reference to a parent variance. In
this case we consider the t statistic and start by calculating the sample variance:
</p>
<p>s2 D 1
4
</p>
<p>X
.xi ï¿½ Nx/2 D 3:7:
</p>
<p>With this we can now calculate the t statistic,
</p>
<p>t D Nx ï¿½ ï¿½
s=
p
5
D 12:2 ï¿½ 13
1:92=
</p>
<p>p
5
D ï¿½0:93:
</p>
<p>This value of t corresponds to a probability of approximately ï¿½40 % to exceed
the absolute value of this measurement, using the t distribution with 4 degrees of
freedom of Table A.23. It is clear that the estimation of the variance from the data
has added a source of uncertainty in the comparison of the measurement with the
parent distribution. }
</p>
<p>7.5.2 Comparison of Two Sample Means and Hypothesis
Testing
</p>
<p>The same distribution function is also applicable to the comparison between two
sample means x1 and x2, derived from samples of size N1 and N2, respectively. In</p>
<p/>
</div>
<div class="page"><p/>
<p>142 7 Hypothesis Testing and Statistics
</p>
<p>this case, we define the following statistic:
</p>
<p>T D x1 ï¿½ x2
s
p
1=N1 C 1=N2
</p>
<p>: (7.38)
</p>
<p>where
</p>
<p>8
ËÌ
ËÌ
ËÌ
ËÌ
&lt;Ì
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
:Ì
</p>
<p>S2 D S21 C S22
s2 D S
</p>
<p>2
</p>
<p>N1 C N2 ï¿½ 2
S21 D
</p>
<p>N1P
</p>
<p>iD1
.xi ï¿½ x1/
</p>
<p>S22 D
N2P
</p>
<p>jD1
.xj ï¿½ x2/:
</p>
<p>We show that this statistic is distributed like a T distribution with f D N1 C N2 ï¿½ 2
degrees of freedom, and therefore we can use the same distribution also for testing
the agreement between two sample means.
Proof Under the hypothesis that all measurements are drawn from the same
parent distribution, X ï¿½ N.ï¿½; ï¿½/, we know that
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>x1 ï¿½ ï¿½
ï¿½=
p
N1
ï¿½ N.0; 1/
</p>
<p>x2 ï¿½ ï¿½
ï¿½=
p
N2
ï¿½ N.0; 1/
</p>
<p>and, from (7.20)
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>S21
ï¿½2
ï¿½ ï¿½2.N1 ï¿½ 1/
</p>
<p>S22
ï¿½2
ï¿½ ï¿½2.N2 ï¿½ 1/
</p>
<p>:
</p>
<p>First, we find the distribution function for the variable .x1 ï¿½ ï¿½/=ï¿½ ï¿½ .x2 ï¿½
ï¿½/=ï¿½ . Assuming that the measurements are independent, then the variable is
a Gaussian with zero mean, with variances added in quadrature, therefore
</p>
<p>X D
ï¿½
x1 ï¿½ ï¿½
ï¿½
ï¿½ x2 ï¿½ ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>=
</p>
<p>s
1
</p>
<p>N1
C 1
</p>
<p>N2
ï¿½ N.0; 1/:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 The Sampling Distribution of the Mean and the Student&rsquo;s t Distribution 143
</p>
<p>Next, since independent ï¿½2 variables are also distributed like a ï¿½2 distribu-
tion with a number of degrees of freedom equal to the sum of the individual
degrees of freedom,
</p>
<p>Z D S
2
1
</p>
<p>ï¿½2
C S
</p>
<p>2
2
</p>
<p>ï¿½2
ï¿½ ï¿½2.N1 C N2 ï¿½ 2/:
</p>
<p>We also know the distribution of
p
Z=f from (7.35), with f D N1 C N2 ï¿½ 2
</p>
<p>the number of degrees of freedom for both datasets combined. As a result, the
variable T can be written as
</p>
<p>T D Xp
Z=f
</p>
<p>(7.39)
</p>
<p>in a form that is identical to the T function for comparison of sample mean
with the parent mean, and therefore we can conclude that the random variable
defined in (7.38) is in fact a T variable with f D N1 C N2 ï¿½ 2 degrees of
freedom. ut
</p>
<p>Example 7.9 Using the ten measurements .10; 12; 15; 11; 13; 16; 12; 10; 18; 13/,
we have already calculated the sample mean of the first and second half of the
measurements as x1 D 12:2 and x2 D 13:8, and the sample variances as S21 D 14:8
and S22 D 40:8. This results in a measurement of the t distribution for the comparison
between two means of
</p>
<p>t D x1 ï¿½ x2q
s21 C s22
</p>
<p>p
1=N1 C 1=N2
</p>
<p>D ï¿½0:97: (7.40)
</p>
<p>This number is to be compared with a t distribution with 8 degrees of freedom,
and we conclude that the measurement is consistent, at any reasonable level of
confidence, with the parent distribution. In this case, we are making a statement
regarding the fact that the two sets of measurements may have the same mean, but
without committing to a specific value. }</p>
<p/>
</div>
<div class="page"><p/>
<p>144 7 Hypothesis Testing and Statistics
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Hypothesis Testing: A four-step process that consists of (1) defining a
null hypothesis to test, (2) determine the relevant statistic (e.g., ï¿½2), (3)
a confidence level (e.g., 90 %), and (4) whether the null hypothesis is
discarded or not.
</p>
<p>ï¿½ ï¿½2 distribution: The theoretical distribution of the sum of the squares of
independent z-scores,
</p>
<p>ï¿½2 D
Xï¿½xi ï¿½ ï¿½i
</p>
<p>ï¿½i
</p>
<p>ï¿½2
</p>
<p>:
</p>
<p>(mean N and variance 2N).
ï¿½ Sampling distribution of variance: Distribution of sample variance s2 D
</p>
<p>S2=.N ï¿½ 1/,
</p>
<p>S2=ï¿½2 ï¿½ ï¿½2.N ï¿½ 1/
</p>
<p>ï¿½ F Statistic: Distribution of the ratio of independent ï¿½2 variables
</p>
<p>F D ï¿½
2
1=f1
ï¿½22=f2
</p>
<p>(mean f2=. f2 ï¿½ 2/ for f2 &gt; 2) also used to test for additional model
components.
</p>
<p>ï¿½ Student&rsquo;s t distribution: Distribution for the variable
</p>
<p>T D x ï¿½ ï¿½
s=
p
n
;
</p>
<p>useful to compare the sample mean to the parent mean when the variance
is estimated from the data.
</p>
<p>Problems
</p>
<p>7.1 Five students score 70, 75, 65, 70, and 65 on a test. Determine whether the
scores are compatible with the following hypotheses:
</p>
<p>(a) The mean is ï¿½ D 75;
(b) the mean is ï¿½ D 75 and the standard deviation is ï¿½ D 5.
</p>
<p>Test both hypotheses at the 95 % or 68 % confidence levels, assuming that the
scores are Gaussian distributed.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 The Sampling Distribution of the Mean and the Student&rsquo;s t Distribution 145
</p>
<p>7.2 Prove that the mean and variance of theF distribution are given by the following
relationships,
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½ D f2
f2 ï¿½ 2
</p>
<p>ï¿½2 D 2f
2
2 . f1 C f2 ï¿½ 2/
</p>
<p>f1. f2 ï¿½ 2/2. f2 ï¿½ 4/ ;
</p>
<p>where f1 and f2 are the degrees of freedom of the variables at the numerator and
denominator, respectively.
</p>
<p>7.3 Using the same data as Problem (7.1), test whether the sample variance is
consistent with a parent variance of ï¿½2 D 25, at the 95 % level.
7.4 Using the J.J. Thomson experiment data of page 23, measure the ratio of
the sample variances of the m=e measurements in Air for Tube 1 and Tube 2.
Determine if the null hypothesis that the two measurements are drawn from the
same distribution can be rejected at the 90 % confidence level. State all assumptions
required to use the F distribution.
</p>
<p>7.5 Consider a dataset .10; 12; 15; 11; 13; 16; 12; 10; 18; 13/, and calculate the ratio
of the sample variance of the first two measurements with that of the last eight. In
particular, determine at what confidence level for the null hypothesis both subsets
are consistent with the same variance.
</p>
<p>7.6 Six measurements of the length of a wooden block gave the following
measurements: 20.3, 20.4, 19.8, 20.4, 19.9, and 20.7 cm.
</p>
<p>(a) Estimate the mean and the standard error of the length of the block;
(b) Assume that the block is known to be of length ï¿½ D 20 cm. Establish if the
</p>
<p>measurements are consistent with the known length of the block, at the 90 %
probability level.
</p>
<p>7.7 Consider Mendel&rsquo;s experimental data in Table 1.1 shown at page 9.
</p>
<p>(a) Consider the data that pertain to the case of &ldquo;Long vs. short stem.&rdquo; Write
an expression for the probability of making that measurement, assuming
Mendel&rsquo;s hypothesis of independent assortment. You do not need to evaluate
the expression.
</p>
<p>(b) Using the distribution function that pertains to that measurement, determine the
mean and variance of the parent distribution. Using the Gaussian approximation
for this distribution, determine if the null hypothesis that the measurement is
drawn from the parent distribution is compatible with the data at the 68 %
confidence level.</p>
<p/>
</div>
<div class="page"><p/>
<p>146 7 Hypothesis Testing and Statistics
</p>
<p>7.8 Consider Mendel&rsquo;s experimental data in Table 1.1 shown at page 9. Considering
all seven measurements, calculate the probability that the mean fraction of dominant
characters agrees with the expectation of 0.75. For this purpose, you may use the t
statistic.
</p>
<p>7.9 Starting with (7.36), complete the derivation of (7.34).
</p>
<p>7.10 Show that the t distribution,
</p>
<p>fT.t/ D 1p
f	
</p>
<p>ï¿½ .. f C 1/=2/
ï¿½ . f=2/
</p>
<p>	
ï¿½
</p>
<p>1C t
2
</p>
<p>f
</p>
<p>ï¿½ï¿½ 1
2
. fC1/
</p>
<p>becomes a standard Gaussian in the limit of large f . You can make use of the
asymptotic expansion of the Gamma function (A.17).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 8
Maximum Likelihood Methods for Two-Variable
Datasets
</p>
<p>Abstract One of the most common tasks in the analysis of scientific data is
to establish a relationship between two quantities. Many experiments feature the
measurement of a quantity of interest as function of another control quantity that
is varied as the experiment is performed. In this chapter we use the maximum
likelihood method to determine whether a certain relationship between the two
quantities is consistent with the available measurements and the best-fit parameters
of the relationship. The method has a simple analytic solution for a linear function
but can also be applied to more complex analytic functions.
</p>
<p>8.1 Measurement of Pairs of Variables
</p>
<p>A general problem in data analysis is to establish a relationship y D y.x/ between
two random variables X and Y for which we have available a set of N measurements
.xi; yi/. The random variable X is considered to be the independent variable and
it will be treated as having uncertainties that are much smaller than those in the
dependent variable, i.e., ï¿½x ï¿½ ï¿½y. This may not always be the case and there are
some instances in which both errors need to be considered. The case of datasets
with errors in both variables is presented in Chap. 12.
</p>
<p>The starting point of the analysis of a two-dimensional dataset is an analytic
form for y.x/, e.g., y.x/ D a C bx. The function f .x/ has a given number of
adjustable parameters ak, k D 1; : : : ;m that are to be constrained according to the
measurements. When the independent variable X is assumed to be known exactly,
then the two-variable data set can be described as a sequence of random variables
Y.Xi/. For these variables we typically have a measurement of the standard error
such that the two-variable data are of the form
</p>
<p>.xi; yi Ë ï¿½i/ i D 1; : : : ;N:
</p>
<p>An example of this situation may be a dataset in which the size of an object is
measured at different time intervals. In this example the time of measurement
ti is the independent variable, assumed to be known exactly, and ri Ë ï¿½i is the
measurement of the size at that time interval. Although we call y.xi/ D ri Ë ï¿½i a
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_8
</p>
<p>147</p>
<p/>
</div>
<div class="page"><p/>
<p>148 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>&ldquo;measurement,&rdquo; it really may itself be obtained from a number of measurements
from which one infers the mean and the variance of that random variable, as
described in the earlier chapters. It is therefore reasonable to expect that the
measurement provides also an estimate of the standard error.
</p>
<p>Before describing the mathematical properties of the method used to estimate the
best-fit parameters we need to understand the framework for the analysis. Consider
as an example the case of a linear function between X and Y illustrated in Fig. 8.1.
The main assumption of the method is that the function y D y.x/ is the correct
description of the relationship between the two variables. This means that each
random variable y.xi/ is a Gaussian with the following parameters:
</p>
<p>ï¿½
ï¿½i D y.xi/ the parent mean is determined by y.x/
</p>
<p>ï¿½2i variance is estimated from the data.
(8.1)
</p>
<p>Notice how this framework is somewhat of a hybrid: the parent mean is determined
by the parent model y.x/ while the variance is estimated from the data. It should
not be viewed as a surprise that the model y D y.x/ typically cannot determine by
itself the variance of the variable. In fact, we know that the variance depends on the
quality of the measurements made and therefore it is reasonable to expect that ï¿½i
is estimated from the data themselves. In Sect. 8.2 we will use the assumption that
Y has a Gaussian distribution, but this need not be the only possibility. In fact, in
Sect. 8.8 we will show how data can be fit in alternative cases, such as when the
variable has a Poisson distribution.
</p>
<p>Fig. 8.1 In the fit of two-variable data to a linear function, measurements of the dependent variable
Y are made for few selected points of the variable X (in this example x1 D 1; x2 D 3; x3 D 5 and
x4 D 7). Each datapoint is marked by the circle with error bars. The independent variable X is
assumed to be known exactly and the size of the error bar determines the value of the variance
of y.xi)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Maximum Likelihood Method for Gaussian Data 149
</p>
<p>8.2 Maximum Likelihood Method for Gaussian Data
</p>
<p>In many cases the variables Y.Xi/ have a Gaussian distribution, as illustrated in
Fig. 8.1. The data are represented by points with an error bar and the model for each
data point is a Gaussian centered at the value of the parent model y.xi/. The model
y.x/ can be any function and, as described in the previous section, the standard
deviation ï¿½i is estimated from the data themselves.
</p>
<p>The goal of fitting data to a model is twofold: to determine whether the model
y.x/ is an accurate representation of the data and, at the same time, to determine
what values of the adjustable parameters are compatible with the data. The two
goals are necessarily addressed together. The starting point is the calculation of the
likelihood L of the data with the model as
</p>
<p>L D P.data/model/ D
NY
</p>
<p>iD1
</p>
<p>1
q
</p>
<p>2	ï¿½2i
</p>
<p>e
ï¿½
.yi ï¿½ y.xi//2
</p>
<p>2ï¿½2i D
</p>
<p>0
</p>
<p>B
@
</p>
<p>NY
</p>
<p>iD1
</p>
<p>1
q
</p>
<p>2	ï¿½2i
</p>
<p>1
</p>
<p>C
A e
</p>
<p>ï¿½PNiD1
.yi ï¿½ y.xi//2
</p>
<p>2ï¿½2i (8.2)
</p>
<p>In the previous equation we have assumed that the measurements yi Ë ï¿½i are inde-
pendent of one other, so that the Gaussian probabilities can be simply multiplied.
Independence between measurements is a critical assumption in the use of the
maximum likelihood method.
</p>
<p>The core of the maximum likelihood method is the requirement that the unknown
parameters ak of the model y D y.x/ are those that maximize the likelihood of the
data. This is the same logic used in the estimate of parameters for a single variable
presented in Chap. 5. The method of maximum likelihood results in the condition
that the following function has to be minimized:
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>ï¿½
yi ï¿½ y.xi/
</p>
<p>ï¿½i
</p>
<p>ï¿½2
</p>
<p>(8.3)
</p>
<p>In fact, the factor in (8.2) containing the product of the sample variances is constant
with respect to the adjustable parameters and maximization of the likelihood is
obtained by minimization of the exponential term.
</p>
<p>Equation (8.3) defines the goodness of fit statistic ï¿½2min, which bears its name
from the fact that it is distributed like a ï¿½2 variable. The number of degrees of
freedom associated with this variable depends on the number of free parameters
of the model y.x/, as will be explained in detail in Chap. 10. The simplest case
is that of a model that has no free parameters. In that case, we know already that</p>
<p/>
</div>
<div class="page"><p/>
<p>150 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>the minimum ï¿½2 has exactly N degrees of freedom. Given the form of (8.3), the
maximum likelihood method, when applied to Gaussian distribution, is also known
as the least squares method.
</p>
<p>8.3 Least-Squares Fit to a Straight Line,
or Linear Regression
</p>
<p>When the fitting function is
</p>
<p>y.x/ D aC bx (8.4)
</p>
<p>the problem of minimizing the ï¿½2 defined in (8.3) can be solved analytically. The
conditions of minimum ï¿½2 are written as partial derivatives with respect to the two
unknown parameters:
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>@
</p>
<p>@a
ï¿½2 D ï¿½2P 1
</p>
<p>ï¿½2i
.yi ï¿½ a ï¿½ bxi/ D 0
</p>
<p>@
</p>
<p>@b
ï¿½2 D ï¿½2P xi
</p>
<p>ï¿½2i
.yi ï¿½ a ï¿½ bxi/ D 0
</p>
<p>(8.5)
</p>
<p>)
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>P yi
ï¿½2i
D aP 1
</p>
<p>ï¿½2i
C bP xi
</p>
<p>ï¿½2i
</p>
<p>P xiyi
ï¿½2i
D aP xi
</p>
<p>ï¿½2i
C bP x
</p>
<p>2
i
</p>
<p>ï¿½2i
</p>
<p>(8.6)
</p>
<p>which is a system of two equations in two unknowns. The solution is
</p>
<p>8
ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
:
</p>
<p>a D 1


</p>
<p>Ë
Ë
Ë
Ë
Ë
Ë
Ë
Ë
</p>
<p>P yi
ï¿½2i
</p>
<p>P xi
ï¿½2i
</p>
<p>P xiyi
ï¿½2i
</p>
<p>P x2i
ï¿½2i
</p>
<p>Ë
Ë
Ë
Ë
Ë
Ë
Ë
Ë
</p>
<p>I
</p>
<p>b D 1


</p>
<p>Ë
Ë
Ë
Ë
Ë
Ë
Ë
</p>
<p>P 1
</p>
<p>ï¿½2i
</p>
<p>P yi
ï¿½2iP xi
</p>
<p>ï¿½2i
</p>
<p>P xiyi
ï¿½2i
</p>
<p>Ë
Ë
Ë
Ë
Ë
Ë
Ë
</p>
<p>:
</p>
<p>(8.7)
</p>
<p>where
</p>
<p>
 D
X 1
</p>
<p>ï¿½2i
</p>
<p>X x2i
ï¿½2i
ï¿½
X xi
</p>
<p>ï¿½2i
</p>
<p>X xi
ï¿½2i
: (8.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Multiple Linear Regression 151
</p>
<p>Equation (8.7) provides the solution for the best-fit parameters of the linear model.
The determination of the parameters of the linear model is known as linear
regression.
</p>
<p>When all errors are identical, ï¿½i D ï¿½ , it is easy to show that the best-fit
parameters estimated by the least-squares method are equivalent to
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>b D Cov.X;Y/
Var.X/
</p>
<p>a D E.Y/ï¿½ bE.X/
(8.9)
</p>
<p>[see Problem (8.9)]. This means that, in the absence of correlation between the
two variables, the best-fit slope will be zero and the value of a is simply the linear
average of the measurements.
</p>
<p>8.4 Multiple Linear Regression
</p>
<p>The method outlined above in Sect. 8.3 can be generalized to a fitting function of
the form
</p>
<p>y.x/ D
mX
</p>
<p>kD1
akfk.x/: (8.10)
</p>
<p>Equation (8.10) describes a function that is linear in the m parameters. In this
case one speaks of multiple linear regression, or simply multiple regression. The
functions fk.x/ can have any analytical form. The linear regression described in the
previous section has only two such function, f1.x/ D 1 and f2.x/ D x. A common
case is when the functions are polynomials,
</p>
<p>fk.x/ D xk: (8.11)
</p>
<p>The important feature to notice is that the functions fk.x/ do not depend on the
parameters ak.
</p>
<p>We want to find an analytic solution to the minimization of the ï¿½2 with the
fitting function in the form of (8.10). As we have seen, this includes the simple
linear regression as a special case. In the process of ï¿½2 minimization we will also
determine the variance and the covariances on the fitted parameters ak, since no
fitting is complete without an estimate of the errors and of the correlation between
the coefficients. As a special case we will therefore also find the variances and
covariance between the fit parameters a and b for the linear regression.</p>
<p/>
</div>
<div class="page"><p/>
<p>152 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>8.4.1 Best-Fit Parameters for Multiple Regression
</p>
<p>Minimization of ï¿½2 with respect to the m parameters ak is obtaining by taking partial
derivatives over the m unknown parameters ak.
</p>
<p>This yields the following m equations:
</p>
<p>@
</p>
<p>@al
</p>
<p>NX
</p>
<p>iD1
</p>
<p>ï¿½
.yi ï¿½PmkD1 akfk.xi//2
</p>
<p>ï¿½2i
</p>
<p>ï¿½
</p>
<p>D 0
</p>
<p>or
</p>
<p>ï¿½2
NX
</p>
<p>iD1
</p>
<p>ï¿½
yi ï¿½PmkD1 akfk.xi/
</p>
<p>ï¿½2i
</p>
<p>ï¿½
</p>
<p>fl.xi/ D 0:
</p>
<p>These equations can be written as
</p>
<p>NX
</p>
<p>iD1
</p>
<p>fl.xi/
</p>
<p>ï¿½2i
</p>
<p> 
</p>
<p>yi ï¿½
mX
</p>
<p>kD1
akfk.xi/
</p>
<p>!
</p>
<p>D 0 (8.12)
</p>
<p>leading to
</p>
<p>NX
</p>
<p>iD1
</p>
<p>fl.xi/yi
ï¿½2i
</p>
<p>D
mX
</p>
<p>kD1
ak
</p>
<p>NX
</p>
<p>iD1
</p>
<p>fk.xi/fl.xi/
</p>
<p>ï¿½2i
l D 1; : : : ;m: (8.13)
</p>
<p>Equation (8.13) are m coupled equations in the parameters ak, which can be
solved using matrix algebra, as described below. Notice that the term fl.xi/ is
the lth model component (thus the index l is not summed over), and the index
i runs from 1 to N, where N is the number of data points.
</p>
<p>The best-fit parameters are therefore obtained by defining the row vectors Ë and
a and the m 	 m symmetric matrix A as
</p>
<p>8
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
:
</p>
<p>Ë D .Ë1; : : : ; Ëm/ in which Ëk D
NP
</p>
<p>iD1
fk.xi/yi=ï¿½2i
</p>
<p>a D .a1; : : : ; am/ (model parameters)
</p>
<p>Alk D
NP
</p>
<p>iD1
fl.xi/fk.xi/
</p>
<p>ï¿½2i
(l; k component of the m	m matrix A)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Multiple Linear Regression 153
</p>
<p>With these definitions, (8.13) can be rewritten in matrix form as
</p>
<p>Ë D aA; (8.14)
</p>
<p>and therefore the task of estimating the best-fit parameters is that of inverting the
matrix A, which can be done numerically. The m best-fit parameters ak are placed
in a row vector a (of dimensions 1 	 m) and are given by
</p>
<p>a D ËAï¿½1: (8.15)
</p>
<p>The 1	m row vector Ë and the m	m matrix A can be calculated from the data and
the fit functions fk.x/.
</p>
<p>8.4.2 Parameter Errors and Covariances for Multiple
Regression
</p>
<p>To calculate errors in the best-fit parameters, we treat parameters ak as functions of
the measurements, ak D ak.yi/. Therefore we can use the error propagation method
to calculate variances and covariances between parameters as:
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½2ak D
NP
</p>
<p>iD1
</p>
<p>ï¿½
@ak
@yi
</p>
<p>ï¿½2
</p>
<p>ï¿½2i
</p>
<p>ï¿½2al aj D
NP
</p>
<p>iD1
@al
@yi
</p>
<p>@aj
@yi
ï¿½2i :
</p>
<p>(8.16)
</p>
<p>We have used the fact that the error in each measurement yi is given by ï¿½i and that
the measurements are independent.
</p>
<p>We show that the variance ï¿½2al aj is given by the l; j term of the inverse of the
matrix A, which we define as the error matrix
</p>
<p>" D Aï¿½1: (8.17)
</p>
<p>The error matrix " is a symmetric matrix, of which the diagonal terms contain the
variances of the fitted parameters and the off-diagonal terms contain the covariances.
</p>
<p>Proof Use the matrix equation a D Ë" to write
</p>
<p>al D
mX
</p>
<p>kD1
Ëk"kl D
</p>
<p>mX
</p>
<p>kD1
"kl
</p>
<p>NX
</p>
<p>iD1
</p>
<p>yifk.xi/
</p>
<p>ï¿½2i
) @al
</p>
<p>@yi
D
</p>
<p>mX
</p>
<p>kD1
"kl
</p>
<p>fk.xi/
</p>
<p>ï¿½2i
:</p>
<p/>
</div>
<div class="page"><p/>
<p>154 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>The equation above can be used into (8.16) to show that
</p>
<p>ï¿½2al aj D
NX
</p>
<p>iD1
</p>
<p>2
</p>
<p>4ï¿½2i
</p>
<p>mX
</p>
<p>kD1
</p>
<p>ï¿½
</p>
<p>"jk
fk.xi/
</p>
<p>ï¿½2i
</p>
<p>ï¿½
</p>
<p>	
mX
</p>
<p>pD1
</p>
<p>ï¿½
</p>
<p>"lp
fp.xi/
</p>
<p>ï¿½2i
</p>
<p>ï¿½
3
</p>
<p>5
</p>
<p>in which the indices k and p indicate the m model parameters, and the index i
is used for the sum over the N measurements.
</p>
<p>) ï¿½2al aj D
mX
</p>
<p>kD1
"jk
</p>
<p>mX
</p>
<p>pD1
"lp
</p>
<p>nX
</p>
<p>iD1
</p>
<p>fk.xi/fp.xi/
</p>
<p>ï¿½2i
D
</p>
<p>mX
</p>
<p>kD1
"jk
</p>
<p>mX
</p>
<p>pD1
"lpApk:
</p>
<p>Now recall that A is the inverse of ", and therefore the expression above can
be simplified to
</p>
<p>ï¿½2al aj D
X
</p>
<p>k
</p>
<p>"jk1kl D "jl: (8.18)
</p>
<p>ut
</p>
<p>8.4.3 Errors and Covariance for Linear Regression
</p>
<p>The results of Sect. 8.4.2 apply also to the case of linear regression as a special
case. We therefore use these results to estimate the errors in the linear regression
parameters a and b and their covariance. In this case, the functions fl.xi/ are given,
respectively, by f1.x/ D 1 and f2.x/ D x and therefore the matrix A is a 2 	 2
symmetric matrix with the following elements:
</p>
<p>8
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
:
</p>
<p>A11 D
NP
</p>
<p>iD1
1=ï¿½2i
</p>
<p>A12 D A21 D
NP
</p>
<p>iD1
xi=ï¿½2i
</p>
<p>A22 D
NP
</p>
<p>iD1
x2i =ï¿½
</p>
<p>2
i :
</p>
<p>(8.19)
</p>
<p>The inverse matrix A ï¿½1 D " is given by
8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>"11 D A22=

"12 D "21 D ï¿½A12=

"22 D A11=

</p>
<p>(8.20)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Special Cases: Identical Errors or No Errors Available 155
</p>
<p>in which 
 is the determinant of A. Using (8.14) we calculate Ë:
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>Ë1 DP yi=ï¿½2i
Ë2 DP yixi=ï¿½2i
</p>
<p>(8.21)
</p>
<p>and thus proceed to calculating the best-fit parameters and their errors. The best-fit
parameters, already found in Sect. 8.3, are given by
</p>
<p>.a; b/ D .Ë1; Ë2/
ï¿½
"11 "12
"21 "22
</p>
<p>ï¿½
</p>
<p>which give the same results as previously found in (8.7). We are now in a position
to estimate the errors in the best-fit parameters:
</p>
<p>8
ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
:
</p>
<p>ï¿½2a D "11 D
1
</p>
<p>

</p>
<p>NP
</p>
<p>iD1
x2i =ï¿½
</p>
<p>2
i
</p>
<p>ï¿½2b D "22 D
1
</p>
<p>

</p>
<p>NP
</p>
<p>iD1
1=ï¿½2i
</p>
<p>ï¿½2ab D "12 D ï¿½
1
</p>
<p>

</p>
<p>NP
</p>
<p>iD1
xi=ï¿½2i :
</p>
<p>(8.22)
</p>
<p>The importance of (8.22) is that the errors in the parameters a and b and their
covariance can be computed analytically from the N measurements. This simple
solution make the linear regression very simple to implement.
</p>
<p>8.5 Special Cases: Identical Errors or No Errors Available
</p>
<p>It is common to have a dataset where all measurements have the same error. When
all errors in the dependent variable are identical (ï¿½i D ï¿½) (8.7) and (8.22) for the
linear regression are simplified to
</p>
<p>8
ËÌ
ËÌ
ËÌ
ËÌ
&lt;Ì
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
:Ì
</p>
<p>a D 1


</p>
<p>1
</p>
<p>ï¿½4
.
</p>
<p>NP
</p>
<p>iD1
yi
</p>
<p>NP
</p>
<p>iD1
x2i ï¿½
</p>
<p>NP
</p>
<p>iD1
xi
</p>
<p>NP
</p>
<p>iD1
xiyi/
</p>
<p>b D 1


</p>
<p>1
</p>
<p>ï¿½4
.N
</p>
<p>NP
</p>
<p>iD1
xiyi ï¿½
</p>
<p>NP
</p>
<p>iD1
yi
</p>
<p>NP
</p>
<p>iD1
xi/
</p>
<p>
 D 1
ï¿½4
.N
</p>
<p>NP
</p>
<p>iD1
x2i ï¿½ .
</p>
<p>NP
</p>
<p>iD1
xi/2/
</p>
<p>(8.23)</p>
<p/>
</div>
<div class="page"><p/>
<p>156 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>and
8
ËÌ
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
:
</p>
<p>ï¿½2a D
1
</p>
<p>

</p>
<p>1
</p>
<p>ï¿½2
</p>
<p>NP
</p>
<p>iD1
x2i
</p>
<p>ï¿½2b D
1
</p>
<p>

</p>
<p>N
</p>
<p>ï¿½2
</p>
<p>ï¿½2ab D ï¿½
1
</p>
<p>

</p>
<p>1
</p>
<p>ï¿½2
</p>
<p>NP
</p>
<p>iD1
xi:
</p>
<p>(8.24)
</p>
<p>The important feature is that the best-fit parameters are independent of the value ï¿½
of the error.
</p>
<p>For dataset that do not have errors available it is often reasonable to assume that
all datapoints have the same error and calculate the best-fit parameters without the
need to specify the value of ï¿½ . The variances, which depend on the error, cannot
however be estimated. The absence of errors therefore limits the applicability of
the linear regression method. It is in general not possible to reconstruct the errors
ï¿½i a posteriori. In fact, the errors are the result of the experimental procedure that
led to the measurement of the variables. A typical example is the case in which
each of the variables y.xi/ was measured via repeated experiments which led to
the measurement of y.xi/ as the mean of the measurements and its error as the
square root of the sample variance. In the absence of the &ldquo;raw&rdquo; data that permit
the calculation of the sample variance, it is simply not possible to determine the
error in ï¿½i.
</p>
<p>Another possibility to use a dataset that does not report the errors in the
measurements is based on the assumption that the fitting function y D f .x/ is the
correct description for the data. Under this assumption, one can estimate the errors,
assumed to be identical for all variables in the dataset, via a model sample variance
defined as
</p>
<p>ï¿½2 D 1
N ï¿½m
</p>
<p>NX
</p>
<p>iD1
.yi ï¿½ Oyi/2 (8.25)
</p>
<p>where Oyi is the value of the fitting function f .xi/ evaluated with the best-fit
parameters, which must be first obtained by a fit assuming identical errors. The
underlying assumption behind the use of (8.25) is to treat each measurement yi as
drawn from a parent distribution f .xi/, i D 1; : : :N, e.g., assuming that the model
is the correct description for the data. In the case of a linear regression, m D 2,
since two parameters (a and b) are estimated from the data. It will become clear
in Sect. 10.1 that this procedure comes at the expenses of the ability to determine
whether the dataset is in fact well fit by the function y D f .x/, since that is the
working assumption.
</p>
<p>In the case of no errors reported, it may not be clear which variable is to be
treated as independent. We have shown in (8.9) that, when no errors are reported,</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 A Classic Experiment: Edwin Hubble&rsquo;s Discovery of the Expansion of the. . . 157
</p>
<p>the best-fit parameters can be written as
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>b D Cov.X;Y/
Var.X/
</p>
<p>a D E.Y/ ï¿½ bE.X/:
This equation clearly shows that the best-fit linear regression model is dependent on
the choice of which between x and y is considered the independent variable. In fact,
if y is regarded as the independent variable, and the data fit to the model
</p>
<p>x D a0 C b0y (8.26)
the least-squares method gives the best-fit slope of
</p>
<p>b0 D Cov.X;Y/
Var.Y/
</p>
<p>:
</p>
<p>When the model is rewritten in the usual form
</p>
<p>y D aX=Y C bX=Yx
in which the notation X/Y means &ldquo;X given Y,&rdquo; the best-fit model parameters are
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>bX=Y D ï¿½ 1
b0
D Var.Y/
</p>
<p>Cov.X;Y/
aX=Y D E.Y/ï¿½ bX=YE.X/
</p>
<p>and therefore the two linear models assuming x or y as independent variable will
be different from one another. It is up to the data analyst to determine which of
the two variables is to be considered as independent when there is a dataset of
.xi; yi/measurements with no errors reported in either variable. Normally the issue is
resolved by knowing how the experiment was performed, e.g., which variable had to
be assumed or calculated first in order to calculate or measure the second. Additional
considerations for the fit of two-variable datasets are presented in Chap. 12.
</p>
<p>8.6 A Classic Experiment: Edwin Hubble&rsquo;s Discovery
of the Expansion of the Universe
</p>
<p>In the early twentieth century astronomers were debating whether &ldquo;nebulae,&rdquo;
now known to be external galaxies, were in fact part of our own Galaxy, and
there was no notion of the Big Bang and the expansion of the universe. Edwin
Hubble pioneered the revolution via a seemingly simple observation that a
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>158 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>number of &ldquo;nebulae&rdquo; moved away from the Earth with a velocity v that is
proportional to their distance d, known as Hubble&rsquo;s law
</p>
<p>v D H0d: (8.27)
</p>
<p>The quantity H0 is the Hubble constant, typically measured in the units of
km sï¿½1 Mpcï¿½1, where Mpc indicates a distance of 106 parsec. The data used
by Hubble [21] is summarized in Table 8.1.
</p>
<p>The quantity m is the apparent magnitude, related to the distance via the
following relationship,
</p>
<p>log d D m ï¿½M C 5
5
</p>
<p>(8.28)
</p>
<p>where M D ï¿½13:8 is the absolute magnitude, also measured by Hubble as
part of the same experiment, and considered as a constant for the purpose of
this dataset, and d is measured in parsecs.
</p>
<p>The first part of the experiment consisted in fitting the .v;m/ dataset to a
relationship that is linear in log v,
</p>
<p>log v D aC b ï¿½ m (8.29)
</p>
<p>where a and b are the adjustable parameters of the linear regression. Instead
of performing the linear regression described in Sects. 8.3 and 8.4.3, Hubble
reported two different fit results, one in which he determined also the error
in a,
</p>
<p>log v D .0:202Ë 0:007/ ï¿½ mC 0:472 (8.30)
</p>
<p>and one in which he fixed a D 0:2, and determined the error in b:
</p>
<p>log v D 0:2 ï¿½ mC 0:507Ë 0:012: (8.31)
</p>
<p>Using (8.31) into (8.28), Hubble determined the following relationship
between velocity and distance,
</p>
<p>log
v
</p>
<p>d
D 0:2M ï¿½ 0:493 D ï¿½3:253 (8.32)
</p>
<p>and this results in the measurement of his name-sake constant, H0 D v=d D
10ï¿½3:253 D 558 	 10ï¿½6 km sï¿½1 pcï¿½1, or 558 km sï¿½1 Mpcï¿½1.
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>8.6 A Classic Experiment: Edwin Hubble&rsquo;s Discovery of the Expansion of the. . . 159
</p>
<p>Table 8.1 Data from E. Hubble&rsquo;s measurements
</p>
<p>Name of nebula Mean velocity km sï¿½1 Number of velocities Mean m
</p>
<p>Virgo 890 7 12:5
</p>
<p>Pegasus 3810 5 15:5
</p>
<p>Pisces 4630 4 15:4
</p>
<p>Cancer 4820 2 16:0
</p>
<p>Perseus 5230 4 16:4
</p>
<p>Coma 7500 3 17:0
</p>
<p>Ursa Major 11;800 1 18:0
</p>
<p>Leo 19;600 1 19:0
</p>
<p>(No name) 2350 16 13:8
</p>
<p>(No name) 630 21 11:6
</p>
<p>Fig. 8.2 Best-fit linear
regression model for the data
in Table 8.1
</p>
<p>Example 8.1 The data from Hubble&rsquo;s experiment are a typical example of a dataset
in which no errors were reported. A linear fit can be initially performed by assuming
equal errors, and the best-fit line is reported in red in Fig. 8.2. Using (8.25), the
common errors in the dependent variables logv.xi/ are found to be ï¿½ D 0:06, the
best-fit parameters of the models are a D 0:55 Ë 0:13, b D 0:197 Ë 0:0085, and
the covariance is ï¿½2ab D ï¿½1:12 	 10ï¿½3, for a correlation coefficient of ï¿½0.99. The
uncertainties and the covariance are measured using the method of (8.23). The best-
fit line is shown in Fig. 8.2 as a solid line. }</p>
<p/>
</div>
<div class="page"><p/>
<p>160 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>8.7 Maximum Likelihood Method for Non-linear Functions
</p>
<p>The method described in Sect. 8.4 assumes that the model is linear in the fitting
parameters ak. This requirement is, however, not necessary to apply the maximum
likelihood criterion. We can assume that the relationship y D f .x/ has any analytic
form and still apply the maximum likelihood criterion for the N measurements [see
(8.3)]. The best-fit parameters are still those that minimize the ï¿½2 statistic. In fact, all
considerations leading to (8.3) do not require a specific form for the fitting function
y D f .x/. The assumption that must still be satisfied is that each variable yi is
Gaussian distributed, in order to obtain the likelihood in the form of (8.2).
</p>
<p>The only complication for nonlinear functions is that an analytic solution for
the best-fit values and the errors is in general no longer available. This is often not
a real limitation, since numerical methods to minimize the ï¿½2 are available. The
most straightforward way to achieve a minimization of the ï¿½2 as function of all
parameters is to construct an m dimensional grid of all possible parameter values,
evaluate the ï¿½2 at each point, and then find the global minimum. The parameter
values corresponding to this minimum can be regarded as the best estimate of the
model parameters. The direct grid-search method becomes rapidly unfeasible as the
number of free parameters increases. In fact, the full grid consists of nm points,
where n is the number of discrete points into which each parameter is investigated.
One typically wants a large number of n, so that parameter space is investigated with
the necessary resolution, and the time to evaluate the entire space depends on how
efficiently a calculation of the likelihood can be obtained. Among the methods that
can be used to bypass the calculation of the entire grid, one of the most efficient and
popular is the Markov chain Monte Carlo technique, which is discussed in detail in
Chap. 16.
</p>
<p>To find the uncertainties in the parameters using the grid search method requires
a knowledge of the expected variation of the ï¿½2 around the minimum. This problem
will be explained in the next chapter. The Markov chain Monte Carlo also technique
provides estimates of the parameter errors and their covariance.
</p>
<p>8.8 Linear Regression with Poisson Data
</p>
<p>The two main assumptions made so far in the maximum likelihood method are
that the random variables y.xi/ are Gaussian and the variance of these variables are
estimated from the data as the measured variance ï¿½2i . In the following we discuss
how the maximum likelihood method can be applied to data without making the
assumption of a Gaussian distribution. One case of great practical interest is when
variables have Poisson distribution, which is the case in many counting experiments.
For simplicity we focus on the case of linear regression, although all considerations
can be extended to any type of fitting function.
</p>
<p>When y.xi/ is assumed to be Poisson distributed, the dataset takes the form of
.xi; yi/, in which the values yi are intended as integers resulting from a counting</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Linear Regression with Poisson Data 161
</p>
<p>experiment. In this case, the value y.xi/ D aC bxi is considered as the parent mean
for a given choice of parameters a and b,
</p>
<p>ï¿½i D y.xi/ D aC bxi: (8.33)
</p>
<p>The likelihood is calculated using the Poisson distribution and, under the
hypothesis of independent measurements, it is
</p>
<p>L D
NY
</p>
<p>iD1
</p>
<p>y.xi/yieï¿½y.xi/
</p>
<p>yiÅ 
: (8.34)
</p>
<p>Once we remove the Gaussian assumption, there is no ï¿½2 function to minimize, but
the whole likelihood must be taken into account. It is convenient to minimize the
logarithm of the likelihood,
</p>
<p>lnL D
NX
</p>
<p>iD1
yi ln y.xi/ï¿½
</p>
<p>NX
</p>
<p>iD1
y.xi/C A (8.35)
</p>
<p>where A D ï¿½P ln yiÅ  does not depend on the model parameters but only on the
fixed values of the datapoints. Minimization of the logarithm of the likelihood is
equivalent to a minimization of the likelihood, since the logarithm is a monotonic
function of its argument. The principle of maximum likelihood requires that
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>@
</p>
<p>@a
lnL D 0
</p>
<p>@
</p>
<p>@b
lnL D 0
</p>
<p>)
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>N DP yi
aC bxi
</p>
<p>P
xi DP xiyi
</p>
<p>aC bxi :
(8.36)
</p>
<p>The fact that the minimization was done with respect to lnL instead of ï¿½2 is
a significant difference relative to the case of Gaussian data. For Poisson data we
define the fit statistic C as
</p>
<p>C D ï¿½2 lnL C B; (8.37)
</p>
<p>where B is a constant term. This is called the Cash statistic, after a paper by Cash in
1979 [9]. This statistic will be discussed in detail in Sect. 10.2 and it will be shown
to have the property of being distributed like a ï¿½2 distribution with N ï¿½ m degrees
of freedom in the limit of large N. This result is extremely important, as it allows
to proceed with the Poisson fitting in exactly the same way as in the more common
Gaussian case in order to determine the goodness of fit.
</p>
<p>There are many cases in which a Poisson dataset can be approximated with a
Gaussian dataset, and therefore use ï¿½2 as fit statistic. When the number of counts
in each measurement yi is approximately larger than 10 or so (see Sect. 3.4), the
Poisson distribution is accurately described by a Gaussian of same mean and</p>
<p/>
</div>
<div class="page"><p/>
<p>162 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>variance. When the number of counts is lower, one method to turn a Poisson dataset
into a Gaussian one is to bin the data into fewer variables of larger count rates. There
are, however, many situations in which such binning is not desirable, especially
when the dependent variable y has particular behaviors for certain values of the
independent variable x. In those cases, binning of the data smears those features,
which we would like to retain in the datasets. In those cases, the best option is to
use the Poisson fitting method described in this section, and use C as the fit statistic
instead.
</p>
<p>Example 8.2 Consider a set of N D 4 measurements (3,5,4,2) to be fit to a constant
model, y D a. In this case, (8.36) become
</p>
<p>a D 1
N
</p>
<p>NX
</p>
<p>iD1
yi
</p>
<p>which means that the maximum likelihood estimator of a constant model, for a
Poisson dataset, is the average of the measurements. The maximum likelihood best-
fit parameter is therefore a D 3:5. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ ML fit to two-dimensional data: A method to find best-fit parameters of
a model fit to x; y data assuming that one variable (typically x) is the
independent variable.
</p>
<p>ï¿½ Linear regression: ML fit to a linear model, best-fit parameters when all
errors are identical are
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>b D Cov.X;Y/
Var.X/
</p>
<p>a D EÅYï¿½ ï¿½ bEÅXï¿½
</p>
<p>(assuming x as independent variable).
ï¿½ Multiple linear regression: An extension of the linear regression to models
</p>
<p>of the type
</p>
<p>y D
X
</p>
<p>akfk.x/:
</p>
<p>ï¿½ Model sample variance: When errors in the dependent variable (y) are not
known, they can be estimated via the model sample variance
</p>
<p>ï¿½2 D 1
N ï¿½m
</p>
<p>X
.yi ï¿½ Oyi/2
</p>
<p>where m is the number of model parameters.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 Linear Regression with Poisson Data 163
</p>
<p>Problems
</p>
<p>8.1 Consider the data from Hubble&rsquo;s experiment in Table 8.1.
</p>
<p>(a) Determine the best-fit values of the fit to a linear model for .m; log v/ assuming
that the dependent variables have a common value for the error.
</p>
<p>(b) Using the best-fit model determined above, estimate the error from the data and
the best-fit model, and then estimate the errors in the parameters a and b, and
the correlation coefficient between a and b.
</p>
<p>(c) Calculate the minimum ï¿½2 of the linear fit, using the common error as estimated
in part (a).
</p>
<p>8.2 Consider the following two-dimensional data, in which X is the independent
variable, and Y is the dependent variable assumed to be derived from a photon-
counting experiment:
</p>
<p>xi yi
0:0 25
</p>
<p>1:0 36
</p>
<p>2:0 47
</p>
<p>3:0 64
</p>
<p>4:0 81
</p>
<p>(a) Determine the errors associated with the dependent variables Yi.
(b) Find the best-fit parameters a, b of the linear regression curve
</p>
<p>y.x/ D aC bxI
</p>
<p>also compute the errors in the best-fit parameters and the correlation coefficient
between them;
</p>
<p>(c) Calculate the minimum ï¿½2 of the fit, and the corresponding probability to
exceed this value.
</p>
<p>8.3 Consider the following Gaussian dataset in which the dependent variables are
assumed to have the same unknown standard deviation ï¿½ ,
</p>
<p>xi yi
0:0 0:0
</p>
<p>1:0 1:5
</p>
<p>2:0 1:5
</p>
<p>3:0 2:5
</p>
<p>4:0 4:5
</p>
<p>5:0 5:0</p>
<p/>
</div>
<div class="page"><p/>
<p>164 8 Maximum Likelihood Methods for Two-Variable Datasets
</p>
<p>The data are to be fit to a linear model.
</p>
<p>(a) Using the maximum likelihood method, find the analytic relationships betweenP
xi,
P
</p>
<p>yi,
P
</p>
<p>xiyi,
P
</p>
<p>x2i , and the model parameters a and b.
(b) Show that the best-fit values of the model parameters are a D 0 and b D 1.
8.4 In the case of a maximum likelihood fit to a 2-dimensional dataset with
equal errors in the dependent variable, show that the conditions for having best-fit
parameters a D 0 and b D 1 are
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>NP
</p>
<p>iD1
yi D
</p>
<p>NP
</p>
<p>iD1
xi
</p>
<p>NP
</p>
<p>iD1
x2i D
</p>
<p>NP
</p>
<p>iD1
xiyi:
</p>
<p>(8.38)
</p>
<p>8.5 Show that the best-fit parameter b of a linear fit to a Gaussian dataset is
insensitive to a change of all datapoints by the same amount 
x, or by the same
amount 
y. You can show that this property applies in the case of equal errors in
the dependent variable, although the same result applies also for the case of different
errors.
</p>
<p>8.6 The background rate in a measuring apparatus is assumed to be constant with
time. N measurements of the background are taken, of which N=2 result in a value
of yC
, and N=2 in a value yï¿½
. Determine the sample variance of the background
rate.
</p>
<p>8.7 Find an analytic solution for the best-fit parameters of a linear model to the
following Poisson dataset:
</p>
<p>x y
</p>
<p>ï¿½2 ï¿½1
ï¿½1 0
0 1
</p>
<p>1 0
</p>
<p>2 2
</p>
<p>8.8 Use the data provided in Table 6.1 to calculate the best-fit parameters a and b
for the fit to the radius vs. pressure ratio data, and the minimum ï¿½2. For the fit, you
can assume that the radius is known exactly, and that the standard deviation of the
pressure ratio is obtained as a linear average of the positive and negative errors.
</p>
<p>8.9 Show that, when all measurement errors are identical, the least squares
estimators of the linear parameters a and b are given by b D Cov.X;Y/=Var.X/
and a D E.Y/ ï¿½ bE.X/.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 9
Multi-Variable Regression
</p>
<p>Abstract In many situations a variable of interest depends on several other
variables. Such multi-variable data is common across the sciences and in many other
fields such as economics and business. Multi-variable analysis can be performed in
a simple and effective way when the relationship that links the variable of interest to
the other quantities is linear. In this chapter we study the method of multi-variable
regression and show how it is related to the multiple regression described in Chap. 8
which applies to the traditional two-variable dataset. This chapter also presents
methods for hypothesis testing on the multi-variable regression and its parameters.
</p>
<p>9.1 Multi-Variable Datasets
</p>
<p>Two-dimensional dataset studied so far include an independent variable (X) and a
dependent variable (Y) and the data take the form of a collection of (xi, yi Ë ï¿½i),
where i D 1; : : : ;N and N indicates the total number of measurements. In Chap. 8
we have developed a method to fit such two-dimensional data. In that case, the linear
regression formula takes the form of y.x/ D aCbx, where a and b are the parameters
of the linear regression.
</p>
<p>Datasets that have measurements for three or more variables are referred to
as multi-variable datasets. An example of multi-variable dataset is presented in
Sect. 9.2, which reports measurements of different characteristics of irises per-
formed by Fisher and Anderson in 1936 [14]. Each of those measurement comprises
four quantities: the sepal length, sepal width, petal length, and petal width of 50
irises. For several multi-variable datasets such as that of Fisher and Anderson it
is often unclear which variable is the dependent one. It typically depends on what
question we want to address with the data: if we want to determine the sepal length
of an iris flower based on the sepal width, petal length, and petal width, then
the sepal length becomes the dependent variable and the remaining three are the
independent variables.
</p>
<p>Using multi-variable datasets to predict or forecast the behavior of one quantity
based on several other variables is a fundamental topic in data analysis. It is
common throughout the sciences and especially used in such fields as economics
or behavioral sciences, where a number of possible factors can be used to predict
one quantity of interest. An example is to predict the score on a college-admission
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_9
</p>
<p>165</p>
<p/>
</div>
<div class="page"><p/>
<p>166 9 Multi-Variable Regression
</p>
<p>test based on factors such as the grade-point average during the sophomore and the
junior year, a measure of the motivation of the student and their economic status.
Another example is to predict the price of a stock based, e.g., on the overall index of
the stock exchange, a consumer&rsquo;s index for goods in the relevant class and the rate
of treasury bonds. To address any such questions clearly requires a multi-variable
dataset that has several measurements for all quantities of interest.
</p>
<p>In this chapter we develop a method to determine the relationship between one of
the quantities of a multi-dimensional datasets based on the others, assuming a linear
relationship among the variables. This method will also let us study whether one
or more of the quantities are in fact not useful in predicting the variable of interest.
For example, we may find that the treasury bond rates are irrelevant in predicting
the stock value of a given corporation and therefore we can focus only on those
variables that are useful in predicting its stock price.
</p>
<p>9.2 A Classic Experiment: The R.A. Fisher and
E. Anderson Measurements of Iris Characteristics
</p>
<p>R.A. Fisher is one of the fathers of modern statistics. In 1936 he published the
paper The Use of Multiple Measurements in Taxonomic Problems reporting
measurements of several characteristics of three species of the iris plant [14].
</p>
<p>Figure 9.1 reproduces the original measurements, performed by E. Ander-
son, of the petal length and the sepal length of 150 iris plants of the species
Iris setosa, Iris versicolor, and Iris virginica. The measurements are in milli-
meters (mm). Fisher&rsquo;s aim was to find a linear combination of the four
characteristics that would be best suited to identify one species from the
others. It is already clear from the data in Fig. 9.1 that one of the quantites
(e.g., the sepal length) may be used as a discriminator among the three species.
R.A. Fisher used this dataset to find a linear combination of the four quantities
that would improve the classification of irises.
</p>
<p>The dataset is a classic example of a multi-variate dataset, in which several
variables are measured simultaneously and independently. In addition to
Fisher&rsquo;s original purpose, these data can also be used to determine whether
one of the characteristics, e.g., the sepal length, can be efficiently predicted
based on any (or all) of the other characteristics. For example, one could
expect that the length of the sepal (which is part of the calyx of the flower) is
related linearly to its width, or to the length of the petal. Assuming a linear
relationship among the variables, we set
</p>
<p>SL D aC bSW C cPLC dPW (9.1)
</p>
<p>where a, b, c, and d are coefficients that we can estimate from the data using
the method described in Sect. 9.3.
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 A Classic Experiment: The R.A. Fisher and E. Anderson Measurements. . . 167
</p>
<p>Throughout this chapter we use these data to study the linear regression of
(9.1) for the species Iris setosa. We will find that the most important variable
needed to predict the sepal length is the sepal width, while the measurements
of characteristics of petals are not very important in predicting the sepal
length.
</p>
<p>5.1
4.9
4.7
4.6
5.0
5.4
4.6
5.0
4.4
4.9
5.4
4.8
4.8
4.3
5.8
5.7
5.4
5.1
5.7
5.1
5.4
5.1
</p>
<p>5.1
4.8
</p>
<p>4.6
</p>
<p>5.0
5.0
5.2
5.2
4.7
4.8
5.4
5.2
5.5
4.9
5.0
5.5
4.9
4.4
5.1
5.0
</p>
<p>5.1
5.0
</p>
<p>4.5
4.4
</p>
<p>4.8
5.1
4.6
5.3
5.0
</p>
<p>3.5
3.0
3.2
3.1
3.6
3.9
3.4
3.4
2.9
3.1
3.7
3.4
3.0
3.0
4.0
4.4
3.9
3.5
3.8
3.8
3.4
3.7
3.6
3.3
3.4
3.0
3.4
3.5
3.4
3.2
3.1
3.4
4.1
4.2
3.1
3.2
3.5
3.6
3.0
</p>
<p>3.5
3.4
</p>
<p>2.3
3.2
3.5
</p>
<p>3.0
3.8
</p>
<p>3.8
</p>
<p>3.2
3.7
3.3
</p>
<p>1.4
</p>
<p>Iris setosa
</p>
<p>Sepal
length
</p>
<p>Petal
length
</p>
<p>Petal
width
</p>
<p>0.2 7.0 3.2
3.2
</p>
<p>3.3
2.4
2.9
2.7
</p>
<p>2.7
</p>
<p>2.0
3.0
</p>
<p>3.0
</p>
<p>2.2
</p>
<p>2.2
2.5
3.2
2.8
2.5
2.8
</p>
<p>2.8
</p>
<p>2.9
</p>
<p>2.9
2.6
2.4
2.4
2.7
2.7
</p>
<p>2.7
</p>
<p>2.9
2.9
</p>
<p>2.5
</p>
<p>2.5
2.8
</p>
<p>2.6
</p>
<p>2.6
</p>
<p>3.0
</p>
<p>3.0
</p>
<p>3.0
</p>
<p>3.0
</p>
<p>3.0
</p>
<p>3.0
</p>
<p>3.4
3.1
2.3
</p>
<p>2.3
</p>
<p>2.9
2.9
</p>
<p>3.1
</p>
<p>3.1
</p>
<p>2.3
2.8
2.8
</p>
<p>6.4
6.9
5.5
</p>
<p>5.7
</p>
<p>5.2
5.0
</p>
<p>6.0
6.1
</p>
<p>5.9
</p>
<p>5.6
</p>
<p>5.6
</p>
<p>5.6
5.9
</p>
<p>5.8
6.2
</p>
<p>6.1
</p>
<p>6.1
6.3
</p>
<p>6.4
6.6
6.8
6.7
6.0
5.7
5.5
5.5
5.8
6.0
5.4
6.0
6.7
6.3
5.6
5.5
5.5
6.1
5.8
5.0
5.6
5.7
5.7
6.2
5.1
5.7
</p>
<p>6.7
</p>
<p>6.5
</p>
<p>6.6
</p>
<p>6.3
4.9
</p>
<p>0.2
0.2
0.2
0.2
</p>
<p>0.2
0.2
</p>
<p>0.2
0.2
</p>
<p>0.2
0.4
0.4
0.3
0.3
0.3
0.2
</p>
<p>0.2
</p>
<p>0.2
0.2
</p>
<p>0.2
0.2
0.2
0.2
</p>
<p>0.2
0.2
0.2
0.2
</p>
<p>0.2
0.2
</p>
<p>0.2
</p>
<p>0.2
0.2
0.2
0.2
</p>
<p>0.6
0.4
</p>
<p>0.3
0.3
</p>
<p>0.3
</p>
<p>0.5
</p>
<p>0.4
</p>
<p>0.4
</p>
<p>0.4
0.1
</p>
<p>0.1
</p>
<p>0.1
</p>
<p>0.1
0.1
</p>
<p>0.4
0.3
</p>
<p>Sepal
width
</p>
<p>Sepal
length
</p>
<p>Petal
length
</p>
<p>Petal
width
</p>
<p>Sepal
width
</p>
<p>Sepal
length
</p>
<p>Petal
length
</p>
<p>Petal
width
</p>
<p>Sepal
width
</p>
<p>Iris versicolor Iris virginica
</p>
<p>1.4
1.3
1.5
1.4
1.7
1.4
1.5
1.4
1.5
1.5
1.6
1.4
1.1
1.2
1.5
1.3
1.4
1.7
1.5
1.7
1.5
1.0
1.7
1.9
1.6
1.6
1.5
1.4
1.6
1.6
1.5
1.5
1.4
1.5
1.2
1.3
1.4
1.3
1.5
1.3
1.3
1.3
1.6
1.9
1.4
1.6
1.4
1.5
1.4
</p>
<p>4.7 1.4 6.3 3.3
2.7
3.0
2.9
3.0
3.0
2.5
2.9
2.5
3.6
3.2
2.7
3.0
2.5
2.8
3.2
3.0
3.8
2.6
2.2
3.2
2.8
2.8
2.7
3.3
3.2
2.8
3.0
2.8
3.0
3.8
3.8
2.8
2.8
2.6
3.0
3.4
3.1
3.0
3.1
3.1
3.1
2.7
3.2
3.3
3.0
2.5
3.0
3.4
3.0
</p>
<p>5.8
7.1
6.3
6.5
7.6
4.9
7.3
6.7
7.2
6.5
6.4
6.8
5.7
5.8
6.4
6.5
7.7
7.7
6.0
6.9
5.6
7.7
6.3
6.7
7.2
6.2
6.1
6.4
7.2
7.4
7.9
6.4
6.3
6.1
7.7
6.3
6.4
6.0
6.9
6.7
6.9
5.8
6.8
6.7
6.7
6.3
6.5
6.2
5.9
</p>
<p>1.5
1.5
1.3
1.5
1.3
1.6
1.0
1.3
1.4
1.0
1.5
1.0
1.4
1.3
1.4
1.5
1.0
1.5
1.1
1.8
1.3
1.5
1.2
1.3
1.4
1.4
1.7
1.5
1.0
1.1
1.0
1.2
1.6
1.5
1.6
1.5
1.3
1.3
1.3
1.2
1.4
1.2
1.0
1.3
1.2
1.3
1.3
1.1
1.3
</p>
<p>4.5
4.9
4.0
</p>
<p>4.0
</p>
<p>4.4
4.5
</p>
<p>4.5
</p>
<p>4.8
4.0
4.9
4.7
4.3
4.4
4.8
5.0
4.5
3.5
3.8
3.7
3.9
5.1
4.5
4.5
4.7
4.4
4.1
4.0
4.4
4.6
4.0
3.3
4.2
4.2
4.2
4.3
3.0
4.1
</p>
<p>4.1
</p>
<p>4.6
</p>
<p>4.6
</p>
<p>4.5
4.7
</p>
<p>4.7
</p>
<p>3.3
</p>
<p>3.9
3.5
</p>
<p>3.6
</p>
<p>3.9
</p>
<p>4.2
</p>
<p>6.0
5.1
5.9
5.6
5.8
6.6
4.5
6.3
5.8
6.1
5.1
5.3
5.5
5.0
5.1
5.3
5.5
6.7
6.9
5.0
5.7
4.9
6.7
4.9
5.7
6.0
4.8
4.9
5.6
5.8
6.1
6.4
5.6
5.1
5.6
6.1
5.6
5.5
4.8
5.4
5.6
5.1
5.1
5.9
5.7
5.2
5.0
5.2
5.4
5.1
</p>
<p>2.5
1.9
2.1
1.8
2.2
2.1
1.7
1.8
1.8
2.5
2.0
1.9
2.1
2.0
2.4
2.3
1.8
2.2
2.3
1.5
2.3
2.0
2.0
1.8
2.1
1.8
1.8
1.8
2.1
1.6
1.9
2.0
2.2
1.5
1.4
2.3
2.4
1.8
1.8
2.1
2.4
2.3
1.9
2.3
2.5
2.3
1.9
2.0
2.3
1.8
</p>
<p>Fig. 9.1 Measurements of three iris species from the 1936 R.A. Fisher paper [14]. Measure-
ments are in mm</p>
<p/>
</div>
<div class="page"><p/>
<p>168 9 Multi-Variable Regression
</p>
<p>9.3 The Multi-Variable Linear Regression
</p>
<p>Consider a dataset of N measurements of mC 1 variables which we call Y, X1, : : :,
Xm. We can use the index i to indicate the measurement, i D 1; : : : ;N, and the index
k for the variables Xk, k D 1; : : : ;m. Each set of measurements is therefore indicated
as .yi Ë ï¿½i; x1i; : : : ; xmi/.
</p>
<p>We write the variable Y as a linear function of the m variables Xi,
</p>
<p>y.x/ D a0 C a1x1 C ï¿½ ï¿½ ï¿½ C amxm D a0 C
mX
</p>
<p>kD1
akxk: (9.2)
</p>
<p>The goal is to find the values for the m C 1 coefficients ak, k D 0; : : : ;m that
minimize the ï¿½2 function
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>ï¿½
yi ï¿½ y.xi/
</p>
<p>ï¿½i
</p>
<p>ï¿½2
</p>
<p>: (9.3)
</p>
<p>The quantity y.xi/ D a0 C a1x1i C ï¿½ ï¿½ ï¿½ C amxmi is the value of y.x/ calculated for
the i-th set of measurements of the Xk&rsquo;s. The coefficient a0 is an overall offset,
equivalent to the constant a for the two-dimensional linear regression function y D
aC bx.
</p>
<p>This form for the ï¿½2 function is the same as that used for the multiple linear
regression of Sect. 8.4. The only change is that the measurements xki take the place
of the functions fk.xi/. The quantity ï¿½i is interpreted as the error in the variable Y,
which is the dependent quantity in this regression. As in the case of the two-variable
dataset, we ignore the errors in the variables Xk (see Chap. 12 for an extension of
the two-variable dataset regression with errors in both variables). When the multi-
variable dataset has no errors, or if we choose to ignore the errors in the Y variable
as well, we can omit the ï¿½i term in (9.3). This corresponds to assuming a uniform
error for all measurements.
</p>
<p>The similarity in form between the ï¿½2 functions to minimize for the present
multi-variable linear regression and the multiple regression of Sect. 8.4 means that
we have already at hand a solution for the coefficients of the regression and their
errors. We need to make the following substitutions:
</p>
<p>(
f1.x/ D 1 ï¿½ x0 .thus x0i&rsquo;s are not needed/
fkC1.x/ D xk; k D 1; : : : ;m:
</p>
<p>(9.4)
</p>
<p>and use the solution from Sect. 8.4 with mC 1 terms. The best-fit parameters ak can
be found via the matrix equation
</p>
<p>a D ËAï¿½1; (9.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 The Multi-Variable Linear Regression 169
</p>
<p>where the row vectors Ë and a and the .mC 1/ 	 .mC 1/ symmetric matrix A are
given by
</p>
<p>8
ËÌ
ËÌ
&lt;Ì
</p>
<p>ËÌ
ËÌ
:Ì
</p>
<p>Ë D .Ë0; Ë1; : : : ; Ëm/ in which Ëk D
NP
</p>
<p>iD1
xkiyi=ï¿½2i
</p>
<p>a D .a0; a1; : : : ; am/
Alk D
</p>
<p>NP
</p>
<p>iD1
xlixki
ï¿½2i
</p>
<p>(l; k component of A).
</p>
<p>The errors and covariances among parameters are likewise given by the error matrix
ï¿½ D Aï¿½1. Assuming a constant value for the variance ï¿½2 (i.e., uniform measurement
errors), the matrix A and the vector Ë can be written in extended form as
</p>
<p>A D 1
ï¿½2
</p>
<p>2
</p>
<p>6
6
4
</p>
<p>N
P
</p>
<p>x1i : : :
P
</p>
<p>xmiP
x1i
</p>
<p>P
x21i : : :
</p>
<p>P
x1ixmi
</p>
<p>: : :
P
</p>
<p>xmi
P
</p>
<p>xmix1i : : :
P
</p>
<p>x2mi
</p>
<p>3
</p>
<p>7
7
5 (9.6)
</p>
<p>Ë D 1
ï¿½2
.
X
</p>
<p>yi;
X
</p>
<p>x1iyi; : : : ;
X
</p>
<p>xmiyi/ (9.7)
</p>
<p>where all sums are over the N measurements. An estimate for the variance ï¿½2 is
given by
</p>
<p>s2 D 1
N ï¿½ m ï¿½ 1
</p>
<p>NX
</p>
<p>iD1
.yi ï¿½ Oyi/2 (9.8)
</p>
<p>where Oyi D a0 C a1x1i C ï¿½ ï¿½ ï¿½ C amxmi is calculated for the best-fit values of the
coefficients ak.
</p>
<p>An alternative notation for finding the coefficients ak makes use of the following
definitions:
</p>
<p>y D
</p>
<p>2
</p>
<p>6
6
4
</p>
<p>y1
y2
: : :
</p>
<p>yN
</p>
<p>3
</p>
<p>7
7
5 IX D
</p>
<p>2
</p>
<p>6
6
4
</p>
<p>1 x11 : : : x1m
1 x21 : : : x2m
: : :
</p>
<p>1 xN1 : : : xNm
</p>
<p>3
</p>
<p>7
7
5 and a D
</p>
<p>2
</p>
<p>6
6
4
</p>
<p>a0
a1
: : :
</p>
<p>am
</p>
<p>3
</p>
<p>7
7
5 (9.9)
</p>
<p>where X is called the design matrix and we have arranged the Y measurements and
the vector of coefficients in column vectors. With this notation, the least-squares
approach gives the following solution for the coefficients [41]:
</p>
<p>a D .XTX/ï¿½1XTY (9.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>170 9 Multi-Variable Regression
</p>
<p>It is easy to show that (9.5) and (9.10) are equivalent (see Problem 9.3). Using this
notation, the error matrix is given by
</p>
<p>ï¿½ D s2.XTX/ï¿½1: (9.11)
</p>
<p>We therefore have two equivalent methods to calculate the coefficients of the
multiple regression and their errors. The latter form (9.9) may be convenient if the
data are already tabulated according to the form of matrix A and therefore a can be
found using the matrix algebra of (9.10). The drawback is that the design matrix can
be of very large size, N 	 .m C 1/, where N is the number of measurements. The
form of (9.5) is more compact, since the matrix A is .m C 1/ 	 .m C 1/, and the
summation over the N measurements must be performed beforehand to obtain A.
</p>
<p>9.4 Tests for Significance of the Multiple Regression
Coefficients
</p>
<p>The multi-variable linear regression model of (9.2) is specified by the m C 1
coefficients ak. After determining their best-fit values and errors, it is necessary to
establish whether the model is an accurate representation of the data and whether
there are any independent variables Xi that do not provide significant contribution
to the prediction of the Y variable. Both tasks can be performed using hypothesis
testing on the relevant statistic. We discuss these tests of significance using the
Fisher&rsquo;s data of Sect. 9.2
</p>
<p>9.4.1 T-Test for the Significance of Model Components
</p>
<p>It is necessary to test the significance of each of the mC 1 parameters of the multi-
variable linear regression. The null hypothesis is that their true value is zero, i.e., the
corresponding variable is not needed in the model. For this purpose, we show that
the ratio of the parameter&rsquo;s best-fit value ak and its standard deviation sk,
</p>
<p>tk D ak
sk
</p>
<p>(9.12)
</p>
<p>is distributed like a Student&rsquo;s t distribution with N ï¿½m ï¿½ 1 degrees of freedom.
Proof Following the derivation provided in Sect. 7.5.1 for the sample mean,
we can write
</p>
<p>tk D .ak ï¿½ ï¿½k/=ï¿½k
sk=ï¿½k
</p>
<p>(9.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Tests for Significance of the Multiple Regression Coefficients 171
</p>
<p>whereï¿½k D 0 is the null hypothesis and ï¿½2k is the unknown parent variance for
the parameter. Recall that the sample variance of the parameter s2k is obtained
as a product of the diagonal term in the error matrix and the estimate of the
data variance s2. Accordingly we set
</p>
<p>.N ï¿½m ï¿½ 1/s2k
ï¿½2k
</p>
<p>ï¿½
P
.yi ï¿½ Oyi/2
ï¿½2k
</p>
<p>ï¿½ ï¿½2.N ï¿½m ï¿½ 1/; (9.14)
</p>
<p>i.e., the denominator of tk can be written as a function of a variable that is ï¿½2-
distributed. It is also clear that, under the null hypothesis, ï¿½k D 0 is the parent
value of ak, and therefore the numerator of tk is distributed like a standard
normal distribution.
</p>
<p>It follows that tk is distributed like a t distribution,
</p>
<p>tk ï¿½ N.0; 1/p
ï¿½2.N ï¿½m ï¿½ 1/=.N ï¿½ m ï¿½ 1/ ï¿½ t.N ï¿½ m ï¿½ 1/ (9.15)
</p>
<p>according to the definition of the t distribution of (7.33). ut
To test for the significance of coefficient ak we therefore use the critical value for
the t distribution for the appropriate number of degrees of freedom and the desired
confidence level.
</p>
<p>Example 9.1 (Multi-Variable Linear Regression on Iris setosa Data) The data of
Fig. 9.1 for the Iris setosa species are fit to the linear model of (9.1), where the sepal
length is used as the Y variable and the remaining three variables are the independent
variables. Using (9.5) and the inverse of matrix A for the errors, we find the results
shown in Table 9.1, including the t scores for the four parameters of the multiple
regression.
</p>
<p>For each parameter is reported the probability to exceed the absolute value of
the measured t according to a t distribution with f D 46 degrees of freedom, where
f D N ï¿½ m ï¿½ 1 with N D 50 measurements and m D 3 independent variable. It
is clear that the parameters a2 and a3, corresponding to the petal length and width,
are not significant because of the large probability p to exceed their value under the
null hypothesis. Accordingly, it would be meaningful to repeat the linear regression
using only the sepal width as an estimator for the sepal length. }
</p>
<p>Table 9.1 Multiple
regression parameter for the
Iris setosa data
</p>
<p>Parameter Best-fit value Error t score p value
</p>
<p>a0 2:352 0:393 5:99 &lt; 0:001
</p>
<p>a1 0:655 0:092 7:08 &lt; 0:001
</p>
<p>a2 0:238 0:208 1:14 0:26
</p>
<p>a3 0:252 0:347 0:73 0:47</p>
<p/>
</div>
<div class="page"><p/>
<p>172 9 Multi-Variable Regression
</p>
<p>9.4.2 F-Test for Goodness of Fit
</p>
<p>The purpose of the multi-variable linear model is to provide a fit to the data
that is more accurate than a simple constant predictor, i.e., the average of the Y
measurements. In other words, we want to establish whether any of the parameters
a1, : : :, am provides a significant improvement over the constant model with a1 D
a2 D : : : D am D 0.
</p>
<p>For this purpose we write the total variance of the data as follows:
</p>
<p>NX
</p>
<p>iD1
.yi ï¿½ Ny/2 D
</p>
<p>NX
</p>
<p>iD1
.yi ï¿½ Oyi/2 C
</p>
<p>NX
</p>
<p>iD1
.Oyi ï¿½ Ny/2 (9.16)
</p>
<p>where Oyi D y.xi/ is evaluated for the best-fit values of the parameters ak. This
equation can be shown to hold because the following property applies,
</p>
<p>NX
</p>
<p>iD1
.yi ï¿½ Oyi/.Oyi ï¿½ Ny/ D 0 (9.17)
</p>
<p>(see Problem 9.7). The parent variance ï¿½2 of the data is unknown and it is not
required for this test. We therefore ignore it for the considerations that follow by
setting ï¿½2 D 1. The three terms in (9.16) are interpreted as follows. The left-hand
side term is the total variance of the data and it is distributed like
</p>
<p>S2 D
NX
</p>
<p>iD1
.yi ï¿½ Ny/2 ï¿½ ï¿½2.N ï¿½ 1/: (9.18)
</p>
<p>The total variance S2 can be interpreted as the variance obtained using a model
with a1 D : : : D am D 0, i.e., a constant model equal to the average of the Y
measurements.
</p>
<p>The first term on the right-hand side is the residual variance after the data are fit
to the linear model and it follows the usual ï¿½2 distribution
</p>
<p>S2r D
NX
</p>
<p>iD1
.yi ï¿½ Oyi/2 ï¿½ ï¿½2.N ï¿½ m ï¿½ 1/ (9.19)
</p>
<p>because of the mC 1 parameters used in the fit. This is the usual variance obtained
using the full model in which at least some of the ak parameters are not equal to
zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Tests for Significance of the Multiple Regression Coefficients 173
</p>
<p>Finally, the second term on the right-hand side can be interpreted as the variance
explained by the best-fit model and it is distributed like
</p>
<p>S2e D
NX
</p>
<p>iD1
.Oyi ï¿½ Ny/2 ï¿½ ï¿½2.m/: (9.20)
</p>
<p>The distribution of the last term can be explained by the independence between the
two variables on the right-hand side of the equation and the distribution of the left-
hand side term, following a derivation similar to that of Sect. 7.3. Such derivation is
not discussed in this book.
</p>
<p>The variances described above can be used to define the variable
</p>
<p>F D S
2
e=m
</p>
<p>S2r=.N ï¿½ m ï¿½ 1/
D
</p>
<p>PN
iD1.Oyi ï¿½ Ny/2=m
</p>
<p>PN
iD1.yi ï¿½ Oyi/2=.N ï¿½ m ï¿½ 1/
</p>
<p>; (9.21)
</p>
<p>which is distributed as an F variable with m, Nï¿½mï¿½1 degrees of freedom under the
null hypothesis that a1 D : : : D am D 0. The meaning of this variable is the ratio
between the variance explained by the fit and the residual variance, each normalized
by the respective degrees of freedom. A large value of this ratio is desirable, since it
means that the model does a good job at explaining the variability of the data.
</p>
<p>The measurement of F that results from the fit of a dataset to the multi-variable
linear model can therefore be used to test the null hypothesis. If the measurement
exceeds the critical value of the F distribution for the desired confidence level, the
null hypothesis must be rejected and the linear model is considered acceptable.
</p>
<p>Example 9.2 (F-Test of Iris setosa Data) The variances for the Iris setosa data are
shown in Table 9.2. The variable F is
</p>
<p>F D S
2
e=3
</p>
<p>S2r=46
D 3:50=2
2:59=46
</p>
<p>D 20:76: (9.22)
</p>
<p>The 99 % (p D 0:01) critical value for an F distribution with 3, 46 degrees of
freedom is 4.24. Therefore the null hypothesis that the linear model does not provide
a significant improvement must be rejected. In practice, this means that the linear
</p>
<p>Table 9.2 Variances and F-test results for the Iris setosa data
</p>
<p>Variances Value d.o.f F-test Value p value
</p>
<p>S2 6:09 N ï¿½ 1 D49
S2r 2:59 N ï¿½ mï¿½ 1=46
S2e 3:50 m D 3
</p>
<p>S2e=m
</p>
<p>S2r =.N ï¿½ mï¿½ 1/ 20:76 1:2ï¿½ 10
ï¿½8</p>
<p/>
</div>
<div class="page"><p/>
<p>174 9 Multi-Variable Regression
</p>
<p>model is warranted. The probability to exceed the measured value of 20.7 for the
test statistic is 1:2 	 10ï¿½8, i.e., very small. }
</p>
<p>9.4.3 The Coefficient of Determination
</p>
<p>The ratio of the explained variance S2e to the total variance S
2, defined as
</p>
<p>R2 D S
2
e
</p>
<p>S2
D
PN
</p>
<p>iD1.Oyi ï¿½ Ny/2
PN
</p>
<p>iD1.yi ï¿½ Ny/2
(9.23)
</p>
<p>is a common measure of the ability of the linear model to describe the data. This
ratio is called the coefficient of (multiple) determination and it is 0 
 R2 
 1. A
value close to 1 indicates that the model describes the data with little additional
variance left unexplained.
</p>
<p>It is possible to relate the coefficient R2 to the F-test variable defined in (9.21)
and obtain an equivalent test for the multi-variable regression based on R2 instead
of the F variable (see, e.g., [41] and [29]). Since the two quantities are related, it
is sufficient to test the overall multiple regression using the F test provided in the
previous section. The advantage of reporting explicitly a value for R2 is that we
can identify in a simple way the amount of variance that remains in the data after
performing the multiple regression.
</p>
<p>Example 9.3 (R2 Value for the Iris setosa Data) We can use the data in Table 9.2
to calculate a coefficient of multiple determination R2 D 0:575. This number
means that 57.5 % of the total data variance is explained by the best-fit regression
model. }
</p>
<p>In the case of the simple linear regression with just one independent variable,
y D aC bx, the coefficient of determination is the same as the coefficient of linear
correlation r defined earlier in (2.19) (see Problem 9.4). In this case it is possible to
test the significance of the linear model using either the correlation coefficient r or
the F test. The two tests will be equivalent.
</p>
<p>Example 9.4 (Linear Fit to the Iris setosa Data Using a Single Independent
Variable) In a previous example we have shown that the coefficients of multiple
regression for the variables Petal Length and Petal Width were not statistically
significant, according to the t test.
</p>
<p>Excluding these two columns of data, a fit to the function y D a C bx, where
Y is the Sepal Length and X the Sepal Width, can be shown to return the values
a D 2:64Ë 0:31 and b D 0:69Ë 0:09 with a correlation coefficient of r D 0:7425
or a value of F D 58:99 for 1, 49 degrees of freedom (see Problem 9.5). The value
of r2 D 0:551 is very similar to that obtained from the full fit using the additional
two variables. The fact that the reduction in r2 is minimal between the m D 3 and</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Tests for Significance of the Multiple Regression Coefficients 175
</p>
<p>the m D 1 case is an indication that the Sepal Length can be predicted with nearly
the same precision using just the Sepal Width as an indicator. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Multi-variable dataset Simultaneous measurements of several (&gt; 2)
variables, usually without reference to a specific independent variable.
</p>
<p>ï¿½ Multi-variable linear regression: Extension of the (multiple) linear regres-
sion to the case of multi-variable data. Best-fit coefficients are given by the
matrix equation
</p>
<p>a D .XTX/ï¿½1XTY:
</p>
<p>ï¿½ Coefficient of determination: The ratio between the explained variance and
total variance R2 D S2e=S2 
 1.
</p>
<p>Problems
</p>
<p>9.1 Calculate the best-fit parameters and uncertainties for the multi-variable regres-
sion of the Iris setosa data of Fig. 9.1.
</p>
<p>9.2 Use an F test to determine whether the multi-variable regression of the Iris
setosa data is justified or not.
</p>
<p>9.3 Prove that (9.5) and (9.10) are equivalent. Take into consideration that in (9.5)
the vectors a and Ë are row vectors. You may re-write (9.5) using column vectors.
</p>
<p>9.4 Prove that the coefficient of determination R2 for the simple linear regression
y D aC bx is equivalent to the sample correlation coefficient of (2.20).
9.5 Fit the Iris setosa data using the function y D a C bx, where Y is the Sepal
Length and X the Sepal Width. For this fit, you will ignore the data associated with
the petal. Determine the best-fit parameters of the linear model and their errors.
</p>
<p>9.6 Using the results of Problem 9.5, determine whether there is sufficient evidence
for the use of the simple y D aC bx model for the data. Use a confidence level of
99 % to draw your conclusions.
</p>
<p>9.7 Prove (9.17).</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 10
Goodness of Fit and Parameter Uncertainty
</p>
<p>Abstract After calculating the best-fit values of model parameters, it is necessary
to determine whether the model is actually a correct description of the data, even
when we use the best possible values for the free parameters. In fact, only when
the model is acceptable are best-fit parameters meaningful. The acceptability of a
model is typically addressed via the distribution of the ï¿½2 statistic or, in the case of
Poisson data, of the Cash statistic. A related problem is the estimate of uncertainty
in the best-fit parameters. This chapter describes how to derive confidence intervals
for fit parameters, in the general case of Gaussian distributions that require ï¿½2
</p>
<p>minimization, and for the case of Poisson data requiring the Cash statistic. We
also study whether a linear relationship between two variables is warranted at all,
providing a statistical test based on the linear correlation coefficient. This is a
question that should be asked of a two-variable dataset prior to any attempt to fit
with a linear or more sophisticated model.
</p>
<p>10.1 Goodness of Fit for the ï¿½2min Fit Statistic
</p>
<p>For both linear and nonlinear Gaussian fits, one needs to establish if the set of best-
fit parameters that minimize ï¿½2 are acceptable, i.e., if the fit was successful. For this
purpose, we need to perform a hypothesis testing based on the minimum of the ï¿½2
</p>
<p>statistic that was obtained for the given model. According to its definition,
</p>
<p>ï¿½2min D
NX
</p>
<p>iD1
</p>
<p>.yi ï¿½ Oyi/2
ï¿½2i
</p>
<p>(10.1)
</p>
<p>in which Oyi D y.xi/jbest-fit is the model calculated with the best-fit parameters. It
is tempting to say that the ï¿½2min statistic is distributed like a ï¿½
</p>
<p>2 random variable
(Sect. 7.2), since it is the sum of N several random variables, each assumed to be
distributed like a standard normal. If the function y.x/ has no free parameters, this is
certainly the case, and it would be also clear that ï¿½2 will have N degrees of freedom.
</p>
<p>The complication is that the fit function has m free parameters that were adjusted
in such a way as to minimize the ï¿½2. This has two implications on the ï¿½2min statistic:
the free parameters will reduce the value of ï¿½2 with respect to the case in which no
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_10
</p>
<p>177</p>
<p/>
</div>
<div class="page"><p/>
<p>178 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>free parameters were present, and, more importantly, the fit function y.x/ introduces
a dependence among the N random variables in the sum. Given that the ï¿½2min is no
longer the sum of N independent terms, we cannot conclude that ï¿½2min ï¿½ ï¿½2.N/.
</p>
<p>It can be shown that ï¿½2min is in fact still distributed as a ï¿½
2 variable, but with
</p>
<p>f D N ï¿½ m (10.2)
</p>
<p>degrees of freedom. This result applies to any type of function f .x/, under the
assumptions that the m parameters are independent of one another, as is normally
the case for &ldquo;meaningful&rdquo; fit functions. The general proof of this statement is rather
elaborate, and can be found in the textbook by Cramer [11]. Here we limit ourselves
to provide a proof for a specific case in which f .x/ D a, meaning a one-parameter
fit function that is a constant, to illustrate the reduction of degrees of freedom from
N to N ï¿½ 1 when there is just one free parameter that can be used to minimize ï¿½2.
Proof When performing a maximum likelihood fit to the function y.x/ D a,
we have shown that the best-fit parameter is estimated as
</p>
<p>a D x D 1
N
</p>
<p>NX
</p>
<p>iD1
xi;
</p>
<p>under the assumption that all measurements are drawn from the same distri-
bution N.ï¿½; ï¿½/ (see Sect. 5.1). Therefore, we can write
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>.xi ï¿½ ï¿½/2
ï¿½2
</p>
<p>D .x ï¿½ ï¿½/
2
</p>
<p>ï¿½2=N
C
</p>
<p>NX
</p>
<p>iD1
</p>
<p>.xi ï¿½ x/2
ï¿½2
</p>
<p>D .x ï¿½ ï¿½/
2
</p>
<p>ï¿½2=N
C ï¿½2min:
</p>
<p>This equation is identical to the relationship used to derive the sampling
distribution of the variance, (7.19), and therefore we can directly conclude
that ï¿½2min ï¿½ ï¿½2.Nï¿½ 1/ and that ï¿½2min and ï¿½2 are independent random variables.
Both properties will be essential for the calculation of confidence intervals on
fit parameters. ut
Now that the distribution function of the fit statistic ï¿½2min is known, we can use
</p>
<p>the hypothesis testing methods of Sect. 7.2.3 to determine whether a value of the
statistic is acceptable or not. The null hypothesis that the data are well fit by, or
compatible with, the model, can be rejected at a confidence level p according to a
one-tailed test defined by
</p>
<p>1 ï¿½ p D
Z 1
</p>
<p>ï¿½2crit
</p>
<p>fï¿½2 . f ; x/dx D P.ï¿½2. f / ï¿½ ï¿½2crit/: (10.3)
</p>
<p>The value ï¿½2crit calculated from (10.3) for a specified value of p defines the rejection
region ï¿½2min ï¿½ ï¿½2crit. The data analyst must chose a value of p, say p D 0:9, and</p>
<p/>
</div>
<div class="page"><p/>
<p>10.1 Goodness of Fit for the ï¿½2min Fit Statistic 179
</p>
<p>calculate the critical value ï¿½2crit that satisfies (10.3), using Table A.7. If the ï¿½
2
</p>
<p>value measured from the data is higher than what calculated from (10.3), then the
hypothesis should be rejected at the p, say 90 %, confidence level. On the other hand,
if theï¿½2 value measured from the data is lower than this critical value, the hypothesis
should not be rejected, and the fit considered as consistent with the model or, more
precisely, not rejected, at that confidence level.
</p>
<p>Example 10.1 In Fig. 10.1 it is shown a linear fit using data from Table 6.1. The
quantity Energy 1 is used as the independent variable, and its errors are neglected.
The quantity Energy 2 is the dependent variable, and errors are calculated as
the average of the positive and negative error bars. The best-fit linear model is
represented as the dotted line, for a fit statistic of ï¿½2min D 60:5 for 23 degrees of
freedom. The value of the fit statistic is too large, and the linear model must be
discarded (see Appendix A.3 for critical values of the ï¿½2 distribution).
</p>
<p>Despite failing the ï¿½2min test, the best-fit model appears to be a reasonable match
to the data. The large value of the test statistic are clearly caused by a few datapoints
with small error bars, but there appears to be no systematic deviation from the linear
model. One reason for the poor fit statistic could be that errors in the independent
variables were neglected. In Chap. 12 we explain an alternative fitting method that
takes into account errors in both variables. Another possibility for the poor fit is
that there are other sources of error that are not accounted. This additional errors
are often referred to as systematic errors. In Chap. 11 we address the presence of
systematic errors and how one can handle the presence of such errors in the fit. }
</p>
<p>Fig. 10.1 Linear fit to the
data of Table 6.1. We
assumed that the independent
variable is Energy 1, errors
for this variable were
neglected in the fit. Note the
logarithmic scale for both
axes</p>
<p/>
</div>
<div class="page"><p/>
<p>180 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>10.2 Goodness of Fit for the Cash C Statistic
</p>
<p>In the case of a Poisson dataset (Sect. 8.8) the procedure to determine whether the
best-fit model is acceptable is identical to that for Gaussian data, provided that the
ï¿½2 fit statistic is replaced by the Cash statistic C, defined by
</p>
<p>C D ï¿½2 lnL ï¿½ B;
where
</p>
<p>B D 2
NX
</p>
<p>iD1
yi ï¿½ 2
</p>
<p>NX
</p>
<p>iD1
yi ln yi C 2
</p>
<p>NX
</p>
<p>iD1
ln yiÅ :
</p>
<p>We now prove that C is approximately distributed like a ï¿½2 distribution with
N ï¿½ m/ degrees of freedom. This is an important result that lets us use the Cash
statistic C in the same way as the ï¿½2 statistic.
</p>
<p>Proof We start with
</p>
<p>ï¿½2 lnL D ï¿½2
 
</p>
<p>NX
</p>
<p>iD1
yi ln y.xi/ï¿½
</p>
<p>NX
</p>
<p>iD1
y.xi/ï¿½
</p>
<p>NX
</p>
<p>iD1
ln yiÅ 
</p>
<p>!
</p>
<p>:
</p>
<p>and rewrite as
</p>
<p>ï¿½2 lnL D 2
NX
</p>
<p>iD1
</p>
<p>ï¿½
</p>
<p>y.xi/ï¿½ yi ln y.xi/
yi
ï¿½ yi ln yi C ln yiÅ 
</p>
<p>ï¿½
</p>
<p>:
</p>
<p>In order to find an expression that asymptotically relates C to ï¿½2, define d D
yiï¿½y.xi/ as the &ldquo;average&rdquo; deviation of the measurement from the parent mean.
It is reasonable to expect that
</p>
<p>d
</p>
<p>yi
' 1p
</p>
<p>yi
) y.xi/
</p>
<p>yi
D yi ï¿½ d
</p>
<p>yi
D 1 ï¿½ d
</p>
<p>yi
</p>
<p>where yi is the number of counts in that specific bin. It follows that
</p>
<p>ï¿½2 lnL D 2
NX
</p>
<p>iD1
</p>
<p>ï¿½
</p>
<p>y.xi/ ï¿½ yi ln.1 ï¿½ d
yi
/ ï¿½ yi ln yi C ln yiÅ 
</p>
<p>ï¿½
</p>
<p>' 2
NX
</p>
<p>iD1
</p>
<p> 
</p>
<p>y.xi/ï¿½ yi
 
</p>
<p>ï¿½ d
yi
ï¿½ 1
2
</p>
<p>ï¿½
d
</p>
<p>yi
</p>
<p>ï¿½2
!
</p>
<p>ï¿½ yi ln yi C ln yiÅ 
!
</p>
<p>D 2
NX
</p>
<p>iD1
</p>
<p>ï¿½
</p>
<p>y.xi/C .yi ï¿½ y.xi//C 1
2
</p>
<p>.yi ï¿½ y.xi//2
yi
</p>
<p>ï¿½ yi ln yi C ln yiÅ 
ï¿½</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Confidence Intervals of Parameters for Gaussian Data 181
</p>
<p>The quadratic term can now be written in such a way that the denominator
carries the term y.xi/ D ï¿½2:
</p>
<p>.yi ï¿½ y.xi//2
yi
</p>
<p>D .yi ï¿½ y.xi//
2
</p>
<p>dC y.xi/ D
.yi ï¿½ y.xi//2
</p>
<p>y.xi/
</p>
<p>ï¿½
d
</p>
<p>y.xi/
C y.xi/
</p>
<p>y.xi/
</p>
<p>ï¿½ï¿½1
</p>
<p>' .yi ï¿½ y.xi//
2
</p>
<p>y.xi/
</p>
<p>ï¿½
</p>
<p>1 ï¿½ d
y.xi/
</p>
<p>ï¿½
</p>
<p>:
</p>
<p>We therefore conclude that
</p>
<p>ï¿½2 lnL D
NX
</p>
<p>iD1
</p>
<p>.yiï¿½y.xi//2
y.xi/
</p>
<p>ï¿½
</p>
<p>1ï¿½ d
y.xi/
</p>
<p>ï¿½
</p>
<p>C
 
</p>
<p>2
</p>
<p>NX
</p>
<p>iD1
yiï¿½2
</p>
<p>NX
</p>
<p>iD1
yi ln yiC2
</p>
<p>NX
</p>
<p>iD1
ln yiÅ 
</p>
<p>!
</p>
<p>;
</p>
<p>showing that, within the multiplicative terms .1 ï¿½ d=y.xi//, the variable C D
ï¿½2 lnL ï¿½ B has a ï¿½2 distribution with N ï¿½ m degrees of freedom. ut
For the purpose of finding the best-fit parameters via minimization of the fit
</p>
<p>statistic, the constant term B is irrelevant. However, in order to determine the
goodness of fit and confidence intervals, it is important to work with a statistic that
is distributed as a ï¿½2 variable. Therefore the Cash statistic is defined as
</p>
<p>C D ï¿½2
NX
</p>
<p>iD1
yi ln
</p>
<p>y.xi/
</p>
<p>yi
C 2
</p>
<p>NX
</p>
<p>iD1
.y.xi/ ï¿½ yi/: (10.4)
</p>
<p>Example 10.2 Consider an ideal set of N D 10 identical measurements , yi D 1.
For a fit to a constant model, y D a, it is clear that the best-fit model parameter must
be a D 1. Using the Cash statistic as redefined by (10.4), we find that C D 0, since
the data and the model match exactly. A similar result would be obtained if we had
assumed a Gaussian dataset of yi D 1 and ï¿½i D 1, for which ï¿½2 D 0. }
</p>
<p>10.3 Confidence Intervals of Parameters for Gaussian Data
</p>
<p>In this section we develop a method to calculate confidence intervals on model
parameters assuming a Gaussian dataset. The results will also be applicable to
Poisson data, provided that the ï¿½2 statistic is replaced with the Cash C statistic
(see Sect. 10.4).</p>
<p/>
</div>
<div class="page"><p/>
<p>182 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>Under the assumption that a given model with m parameters is the correct
description of the data, the fit statistic ï¿½2 calculated with these fixed true values,
</p>
<p>ï¿½2true D
NX
</p>
<p>iD1
</p>
<p>ï¿½
yi ï¿½ y.xi/jtrue
</p>
<p>ï¿½2i
</p>
<p>ï¿½2
</p>
<p>(10.5)
</p>
<p>is distributed as ï¿½2.N/, i.e., we expect random variations of the measurement of
ï¿½2true according to a ï¿½
</p>
<p>2 distribution with N degrees of freedom. This is so because the
true parameters are fixed and no minimization of the ï¿½2 function can be performed.
The quantity ï¿½2true is clearly only a mathematical construct, since the true values
of the parameters are unknown. One does not expect that ï¿½2true D 0, meaning a
perfect match between the data and the model. In fact, even if the model was correct,
statistical fluctuations will result in random deviations from the parent model.
</p>
<p>On the other hand, when finding the best-fit parameters ai, we calculate the
statistic:
</p>
<p>ï¿½2min D
NX
</p>
<p>iD1
</p>
<p>ï¿½
yi ï¿½ Oyi
ï¿½i
</p>
<p>ï¿½2
</p>
<p>(10.6)
</p>
<p>which minimizes ï¿½2 with respect to all possible free parameters. In this case, we
know that ï¿½2min ï¿½ ï¿½2.N ï¿½ m/ from the discussion in Sect. 10.1. It is also clear that
the values of the best-fit parameters are not identical to the true parameters, again
for the presence of random fluctuations of the datapoints.
</p>
<p>After finding ï¿½2min, any change in the parameters (say, from ak to a
0
k) will yield a
</p>
<p>larger value of the test statistic, ï¿½2 &gt; ï¿½2min. We want to test whether the new set of
parameters a0k can be the true (yet unknown) values of the parameters, e.g., whether
the corresponding ï¿½2 can be considered ï¿½2true. For this purpose we construct a new
statistic:
</p>
<p>
ï¿½2 ï¿½ ï¿½2 ï¿½ ï¿½2min (10.7)
</p>
<p>where ï¿½2 is obtained for a given set of model parameters and, by definition, 
ï¿½2 is
always positive. The hypothesis we want to test is that ï¿½2 is distributed like ï¿½2true,
i.e., the ï¿½2 calculated using a new set of parameters is consistent with ï¿½2true. Since
ï¿½2true and ï¿½
</p>
<p>2
min are independent (see Sect. 10.1), we conclude that
</p>
<p>
ï¿½2 ï¿½ ï¿½2.m/ (10.8)
</p>
<p>when ï¿½2 ï¿½ ï¿½2true. Equation (10.8) provides a quantitative way to determine
how much ï¿½2 can increase, relative to ï¿½2min, and still the value of ï¿½
</p>
<p>2 remaining
consistent with ï¿½2true. The method to use (10.8) for confidence intervals on the model
parameters is described below.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Confidence Intervals of Parameters for Gaussian Data 183
</p>
<p>10.3.1 Confidence Interval on All Parameters
</p>
<p>Equation (10.8) provides a quantitative method to estimate the confidence interval
on the m best-fit parameters. The value of 
ï¿½2 is expected to follow the ï¿½2.m/
distribution. This means that one can tolerate deviations from the best-fit values of
the parameters leading to an increase in ï¿½2, provided such increase is consistent
with the critical value of the respective 
ï¿½2 distribution. For example, in the case
of a model with m D 2 free parameters, one can expect a change 
ï¿½2 ï¿½ 4:6 for
p D 0:9 confidence, or for a model with m D 1 parameter a change
ï¿½2 ï¿½ 2:7 (see
Table A.7).
</p>
<p>The method to determine the confidence interval on the parameters starts with
the value of ï¿½2min. From this, one constructs an m-dimensional volume bounded by
the surface of 
ï¿½2 D ï¿½2 ï¿½ ï¿½2min 
 ï¿½2crit, where ï¿½2crit is the value that corresponds to
a given confidence level p for m degrees of freedom, as tabulated in Table A.7. The
surface of this m-dimensional volume marks the boundaries of the rejection region
at the p level (say p = 90 %) for the m parameters, i.e., the parameters can vary
within this volume and still remain an acceptable fit to the data at that confidence
level. In practice, a surface at fixed 
ï¿½2 D ï¿½2crit can be calculated by a grid
of points around the values that correspond to ï¿½2min. This calculation can become
computationally intensive as the number of parameters m increases. An alternative
method to estimate confidence intervals on fit parameters that makes use of Monte
Carlo Markov chains (see Chap. 16) will overcome this limitation.
</p>
<p>Example 10.3 Consider the case of a linear fit to the data of Table 10.1. According
to the data in Table 10.1, one can calculate the best-fit estimates of the parameters
as a D 23:54 Ë 4:25 and b D 13:48 Ë 2:16, using (8.7) and (8.22). The best-fit
line is shown in Fig. 10.2. There is no guarantee that these values are in fact the true
values: they are only the best estimates based on the maximum likelihood method.
For these best-fit values of the coefficients, the fit statistic is ï¿½2min D 0:53, for f D 3
degrees of freedom, corresponding to a probability p D 0:09 (i.e., a probability
P.ï¿½2.3/ ï¿½ 0:53 D 0:91). The fit cannot be rejected at any reasonable confidence
level, since the probability to exceed the measured ï¿½2min is so high.
</p>
<p>We now sample the parameter space, and determine variations in the fit statistic
ï¿½2 around the minimum value. The result is shown in Fig. 10.3, in which the
contours mark the ï¿½2min C 1:0, ï¿½2min C 2:3 and ï¿½2min C 4:6 boundaries. In this
</p>
<p>Table 10.1 Data used to
illustrate the linear
regression, and the estimate
of confidence intervals on fit
parameters
</p>
<p>xi yi ï¿½i
(Indep. variable) (Dependent variable)
</p>
<p>0 25 5
</p>
<p>1 36 6
</p>
<p>2 47 6:85
</p>
<p>3 64 8
</p>
<p>4 81 9</p>
<p/>
</div>
<div class="page"><p/>
<p>184 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>Fig. 10.2 Best-fit linear
model (dashed line) to the
data of Table 10.1. The
ï¿½min D 0:53 indicates a very
good fit which cannot be
rejected at any reasonable
confidence level
</p>
<p>xi yi Ïi
</p>
<p>(indep. variable) (dependent variable)
0 25 5
1 36 6
2 47 6.85
3 64 8
4 81 9
</p>
<p>application, m D 2, a value of 
ï¿½2 D 4:6 or larger is expected 10 % of the time.
Accordingly, the 
ï¿½2 D 4:6 contour marks the 90 % confidence surface: the true
values of a and b are within this area 90 % of the time, if the null hypothesis that
the model is an accurate description of the data is correct. This area is therefore the
90 % confidence area for the two fitted parameters. }
</p>
<p>10.3.2 Confidence Intervals on Reduced Number
of Parameters
</p>
<p>In the case of a large number m of free parameters, it is customary to report the
uncertainty on each of the fitted parameters or, in general, on just a subset of l &lt; m
parameters considered to be of interest. In this case, the l parameters a1; : : : ; al are
said to be the interesting parameters, and the remaining m ï¿½ l parameters are said
to be uninteresting. This can be thought of as reducing the number of parameters
of the model from m to l, often in such a way that only one interesting parameter
is investigated at a time (l D 1). This is a situation that is of practical importance
for several reason. First, it is not convenient to display surfaces in more than two or
three dimensions. Also, sometimes there are parameters that are truly uninteresting</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Confidence Intervals of Parameters for Gaussian Data 185
</p>
<p>Fig. 10.3 Contours of 
ï¿½2 D 1.0, 2.3 and 4.6 (from smaller to larger areas). For the example of
m D 2 free parameters, the contours mark the area within which the true parameters a and b are
expected to be with, respectively, 25, 67, and 90 % probability
</p>
<p>to the interpretation of the data, although necessary for its analysis. One case of this
is the presence of a measurement background, which must be taken into account
for a proper analysis of the data, but it is of no interest in the interpretation of the
results.
</p>
<p>New considerations must be applied to ï¿½2true and its parent distribution in this
situation. We find ï¿½2min in the usual way, that is, by fitting all parameters and
adjusting them until the minimum ï¿½2 is found. Therefore ï¿½2min continues to be
distributed like ï¿½2.N ï¿½ m/. For ï¿½2true, we want to ignore the presence of the
uninteresting parameters. We do so by assuming that the l interesting parameters are
fixed at the true values and marginalize over the mï¿½ l uninteresting parameters. This
process of marginalization means that we let the uninteresting parameters adjust
themselves to the values that yield the lowest value of ï¿½2. This process ensures that
ï¿½2true / ï¿½2.N ï¿½ .m ï¿½ l//. Notice that the marginalization does not mean fixing the
values of the uninteresting parameters to their best-fit values.
</p>
<p>In summary, the change in ï¿½2 that can be tolerated will therefore be 
ï¿½2 D
ï¿½2true ï¿½ ï¿½2min, in which ï¿½2true / ï¿½2.N ï¿½ .mï¿½ l// and ï¿½2min / ï¿½2.N ï¿½m/.Since the two
ï¿½2 distributions are independent of one another, it follows that
</p>
<p>
ï¿½2 ï¿½ ï¿½2.l/ (10.9)
</p>
<p>where l is the number of interesting parameters. The process of finding confidence
intervals for a reduced number of parameters is illustrated in the following example
of a model with m D 2 free parameters, for which we also find confidence intervals
for one interesting parameters at a time.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>Example 10.4 Consider the case in Fig. 10.3, and assume that the interesting
parameter is a. The ï¿½2min for each value of a is done by searching the minimum
ï¿½2 long a vertical line (i.e., for a fixed value of a). The best-fit value of a is already
known, marked by a cross in Fig. 10.3. When seeking the 68 % confidence interval
for the interesting parameter a, the limiting values of a are those on either side
of the best-fit value that result in a minimum ï¿½2 value of ï¿½2min C 1:0 (where ï¿½2min
is the global minimum). Therefore, the 68 % confidence interval for a is found by
projecting the ï¿½2minC1:0 contour along the a axis. That is to say, we find the smallest
and largest values of a along the ï¿½2minC 1:0 contour, which is the innermost contour
in Fig. 10.3. Likewise the projection of the same contour along the b axis gives the
68 % confidence interval on b, when considered as the only interesting parameter.
</p>
<p>On the other hand, the 2-dimensional 68 % confidence surface on a, b was given
by the ï¿½2C 2:3 contour. It is important not to confuse those two confidence ranges,
both at the same level of confidence of 68 %. The reason for the difference (ï¿½2min C
1:0 for one interesting parameter vs. ï¿½2 C 2:3 for two interesting parameters) is the
numbers of degrees of freedom of the respective
ï¿½2. }
</p>
<p>This procedure for estimation of intervals on a reduced number of parameters
was not well understood until the work of Lampton and colleagues in 1976 [27]. It
is now widely accepted as the correct method to estimate errors in a subset of the
model parameters.
</p>
<p>10.4 Confidence Intervals of Parameters for Poisson Data
</p>
<p>The fit to Poisson data was described in Sect. 8.8. Since the Cash statistic C follows
approximately the ï¿½2 distribution in the limit of a large number of datapoints, then
the statistic
</p>
<p>
C D Ctrue ï¿½ Cmin (10.10)
</p>
<p>has the same statistical properties as the
ï¿½2 distribution. Parameter estimation with
Poisson statistic therefore follows the same rules and procedures as with the ï¿½2
</p>
<p>statistic.
</p>
<p>Example 10.5 Consider an ideal dataset of N identical measurement yi D 1. We
want to fit the data to a constant model y D a, and construct a 1-ï¿½ confidence
interval on the fit parameter a using both the Poisson fit statistic C, and the Gaussian
fit statistic ï¿½2. In the case of the Poisson statistic, we assume that the measurements
are derived from a counting experiment, that is, a count of 1 was recorded in each
case. In the case of Gaussian variables, we assume uncertainties of ï¿½i D 1.
</p>
<p>In the case of Poisson data, we use the Cash statistic defined in (10.4). The best-
fit value of the model is clearly a D 1, and we want to find the value Ä± corresponding</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Linear Correlation Coefficient 187
</p>
<p>to a change in C by a value of 1 with respect to the minimum value Cmin,
</p>
<p>
C D 1) ï¿½2
X
</p>
<p>1 ln
.1C Ä±/
1
</p>
<p>C 2
X
.1C Ä± ï¿½ 1/ D 1
</p>
<p>Using the approximation ln.1C Ä±/ ' .Ä± ï¿½ Ä±2=2/, we find that
</p>
<p>ï¿½20Ä±C 10Ä±2 C 20Ä± D 1) Ä± D
r
1
</p>
<p>10
:
</p>
<p>This shows that the 68 % confidence range is between 1 ï¿½p1=10 and 1Cp1=10,
or 1Ëp1=10.
</p>
<p>Using Gaussian errors, we calculate
ï¿½2 D 1, leading to 10Ä±2 D 1, and the same
result as in the case of the Poisson dataset. }
</p>
<p>10.5 The Linear Correlation Coefficient
</p>
<p>We want to define a quantity that describes whether there is a linear relationship
between two random variables X and Y. This quantity is based on the slopes of two
linear fits of X and Y, using each in turn as the independent variable. Call b the slope
of the regression y D aC bx (where X is the independent variable) and b0 the slope
of the regression x D a0Cb0y (where Y is the independent variable) and assume that
there are N measurements of the two variables. The linear correlation coefficient r
is defined as the product of the slopes of the two fits via
</p>
<p>r2 D bb0 D .N
P
</p>
<p>xiyi ï¿½P xiP yi/2
ï¿½
N
P
</p>
<p>x2i ï¿½ .
P
</p>
<p>xi/
2
	 ï¿½
</p>
<p>N
P
</p>
<p>y2i ï¿½ .
P
</p>
<p>yi/
2
	 (10.11)
</p>
<p>in which we have used the results of (8.23). It is easy to show that this expression
can be rewritten as
</p>
<p>r2 D .
P
.xi ï¿½ x/.yi ï¿½ y//2
</p>
<p>P
.xi ï¿½ x/2P.yi ï¿½ y/2 (10.12)
</p>
<p>and therefore r is the sample correlation coefficient as defined in (2.20).
Consider as an example the data from Pearson&rsquo;s experiment at page 30. The
</p>
<p>measurement of mother&rsquo;s and father&rsquo;s height are likely to have the same uncertainty,
since one expects that both women and men followed a similar procedure for the
measurement. Therefore no precedence should be given to either when assigning
the tag of &ldquo;independent&rdquo; variable. Instead, one can proceed with two separate fits:
one in which the father&rsquo;s height (X) is considered as the independent variable, or the
regression of Y on X (dashed line), and the other in which the mother&rsquo;s height (Y) is</p>
<p/>
</div>
<div class="page"><p/>
<p>188 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>Fig. 10.4 Linear regressions
based on the data collected by
Pearson, Table 2.3 at page 30.
Larger circles indicate a
higher number of occurrence
for that bin
</p>
<p>the independent variable, or linear regression of X on Y (dot-dash line). The two fits
are reported in Fig. 10.4, obtained by maximum likelihood method assuming equal
errors for the dependent variables.
</p>
<p>If the two variables X and Y are uncorrelated, then the two best-fit slopes b and
b0 are expected to be zero. In fact, as one variable varies through its range, the
other is not expected to either decrease (negative correlation) or increase (positive
correlation), resulting in null best-fit slopes for the two fits. We therefore expect
the sample distribution of r to have zero mean, under the null hypothesis of lack
of correlation between X and Y. If there is a true linear correlation between the
two variables, i.e., y D a C bx is satisfied with b &curren; 0, then it is also true that
x D a0 C b0x D ï¿½a=bC 1=by. In this case one therefore expects bb0 D r2 D 1.
</p>
<p>10.5.1 The Probability Distribution Function
</p>
<p>A quantitative test for the correlation between two random variables requires the
distribution function fr.r/. We show that the probability distribution of r, under the
hypothesis that the two variables X and Y are uncorrelated, is given by
</p>
<p>fr.r/ D 1p
	
</p>
<p>ï¿½
</p>
<p>ï¿½
f C 1
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f
</p>
<p>2
</p>
<p>ï¿½
</p>
<p>ï¿½
1
</p>
<p>1 ï¿½ r2
ï¿½ï¿½ fï¿½2
</p>
<p>2
</p>
<p>(10.13)
</p>
<p>where f D N ï¿½ 2 is the effective number of degrees of freedom of a dataset with
N measurements of the pairs of variables. The form of the distribution function is</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Linear Correlation Coefficient 189
</p>
<p>reminiscent of the t distribution, which in fact plays a role in the determination of
this distribution.
Proof The proof starts with the determination of the probability distribution
function of a suitable function of r, and then, by change of variables, the
distribution of r is obtained.
</p>
<p>The best-fit parameter b is given by
</p>
<p>b2 D .N
P
</p>
<p>xiyi ï¿½P xiP yi/2
.N
P
</p>
<p>x2i ï¿½ .
P
</p>
<p>xi/2/2
D .
</p>
<p>P
.xi ï¿½ x/.yi ï¿½ y//2
.
P
.xi ï¿½ x/2/2 I
</p>
<p>and accordingly we obtain
</p>
<p>r2 D .
P
.xi ï¿½ x/.yi ï¿½ y//2
</p>
<p>P
.xi ï¿½ x/2P.yi ï¿½ y/2 D b
</p>
<p>2
</p>
<p>P
.xi ï¿½ x/2
</p>
<p>P
.yi ï¿½ y/2 : (10.14)
</p>
<p>Also, using (8.5), the best-fit parameter a can be shown to be equal to a D
y ï¿½ bx, and therefore we obtain
</p>
<p>S2 ï¿½
X
</p>
<p>.yi ï¿½ a ï¿½ bxi/2 D
X
</p>
<p>.yi ï¿½ y/2 ï¿½ b2
X
</p>
<p>.xi ï¿½ x/2: (10.15)
</p>
<p>Notice that S2=ï¿½2 D ï¿½2min, where ï¿½2 is the common variance of the Y
measurements, and therefore using (10.14) and (10.15) it follows that
</p>
<p>S2
P
.yi ï¿½ y/2 D 1 ï¿½ r
</p>
<p>2
</p>
<p>or, alternatively,
</p>
<p>rp
1ï¿½ r2 D
</p>
<p>b
pP
</p>
<p>.xi ï¿½ x/2
S
</p>
<p>: (10.16)
</p>
<p>Equation (10.16) provides the means to determine the distribution function of
r=
p
1ï¿½r2. First, notice that the variance of b is given by
</p>
<p>ï¿½2b D
Nï¿½2
</p>
<p>N
P
</p>
<p>x2i ï¿½ .
P
</p>
<p>xi/
2
D ï¿½
</p>
<p>2
</p>
<p>P
.xi ï¿½ x/2 :
</p>
<p>According to (8.35), s2 D S2=.Nï¿½2/ is the unbiased estimator of the variance
ï¿½2, since two parameters have been fit to the data. Assuming that the true
parameter for the slope of the distribution is Ë, then
</p>
<p>b ï¿½ Ë
ï¿½b
D b ï¿½ Ë
ï¿½=
pP
</p>
<p>.xi ï¿½ x/2
ï¿½ N.0; 1/</p>
<p/>
</div>
<div class="page"><p/>
<p>190 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>is therefore distributed like a standard Gaussian. In the earlier equation, if we
replace ï¿½2 with the sample variance s2 D S2=.N ï¿½ 2/, and enforce the null
hypothesis that the variables X and Y are uncorrelated (Ë D 0), we obtain
a new variable that is distributed like a t distribution with N ï¿½ 2 degrees of
freedom,
</p>
<p>b
</p>
<p>S
</p>
<p>p
N ï¿½ 2
</p>
<p>r
X
</p>
<p>.xi ï¿½ x/2 ï¿½ t.N ï¿½ 2/:
</p>
<p>Using (10.16), we find that the variable
</p>
<p>r
p
N ï¿½ 2p
1 ï¿½ r2 (10.17)
</p>
<p>is distributed like a t distribution with f D Nï¿½2 degrees of freedom and, since
it is a monotonic function of r, its distribution can be related to the distribution
fr.r/ via a simple change of variables, following the method described in
Sect. 4.4.1.
</p>
<p>Starting with v D rpN ï¿½ 2=p1 ï¿½ r2, and
</p>
<p>fT.v/ D 1p
	f
</p>
<p>ï¿½ ..f C 1/=2/
ï¿½ .f=2/
</p>
<p>ï¿½
</p>
<p>1 ï¿½ v
2
</p>
<p>f
</p>
<p>ï¿½ï¿½ fC1
2
</p>
<p>with
</p>
<p>dv
</p>
<p>dr
D
p
N ï¿½ 2
</p>
<p>.1 ï¿½ r2/3=2 ;
</p>
<p>the equation of change of variables fr.r/ D fT.v/dv=dr yields (10.13) after a
few steps of algebra. ut
</p>
<p>10.5.2 Hypothesis Testing
</p>
<p>A test for the presence of linear relationship between two variables makes use of the
distribution function of r derived in the previous section. In the absence of linear
relationship, we expect a value of r close to 0, while values close to the extremes of
Ë1 indicate a strong correlation between the two variables. Since the null hypothesis
is that there is no correlation, we use a two-tailed test to define the critical value of
r via
</p>
<p>P.jrj &gt; rcrit/ D 1 ï¿½
Z rcrit
</p>
<p>ï¿½rcrit
fr.r
</p>
<p>0/dr0 D 1 ï¿½ p; (10.18)</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Linear Correlation Coefficient 191
</p>
<p>where p is intended, as usual, as a number close to 1 (e.g., p=0.9 or 90 % confi-
dence). Critical values of r for various probability levels are listed in Table A.24.
</p>
<p>If the measured value of r exceeds the critical value, the null hypothesis must be
discarded. This is an indication that there is a linear relationship between the two
quantities and further modelling of Y vs. X or X vs. Y is warranted. In practice,
the linear correlation coefficient test should be performed prior to attempting any
regression between the two variables.
</p>
<p>Example 10.6 The two fits to the data from Pearson&rsquo;s experiment (page 30) are
illustrated in Fig. 10.4. A linear regression provides a best-fit slope of b D 0:25
(dashed line) and of b0 D 0:33 (dot-dash line), respectively, when using the father&rsquo;s
stature (x axis) or the mother&rsquo;s stature as the independent variable. For these fits
we use the data provided in Table 2.3. Each combination of father&ndash;mother heights
is counted a number of times equal to its frequency of occurrence, for a total of
N D 1; 079 datapoints.
</p>
<p>The linear correlation coefficient for these data is r D 0:29, which is also equal
to
p
bb0. For N D 1; 079 datapoints, Table A.24 indicates that the hypothesis of
</p>
<p>no correlation between the two quantities must be discarded at &gt; 99% confidence,
since the critical value at 99 % confidence is ï¿½0.081, and our measurement exceeds
it. As a result, we conclude that the two quantities are likely to be truly correlated.
The origin of the correlation is probably with the fact that people have a preference
to marry a person of similar height, or more precisely, a person of a height that is
linearly proportional to their own. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ The ï¿½2min statistic: It applies to Gaussian data and it is distributed like a ï¿½2
distribution with N ï¿½ m degrees of freedom.
</p>
<p>ï¿½ The Cash statistic: It applies to Poisson data and it is defined as
</p>
<p>C D ï¿½2
X
</p>
<p>yi ln.y.xi/=yi/C 2
X
.y.xi/ ï¿½ yi/:
</p>
<p>It is approximately distributed like ï¿½2min.
ï¿½ Confidence intervals for ï¿½2min statistic: They are obtained from the condi-
</p>
<p>tion that 
ï¿½2 ï¿½ ï¿½2.m/, where m is the number of parameters of interest.
ï¿½ Interesting parameters: A subset of all model parameters for which we are
</p>
<p>interested in calculating confidence intervals.
ï¿½ Linear correlation coefficient: The quantity ï¿½1 
 r 
 1 that determines
</p>
<p>whether there is a linear correlation between two variables.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 10 Goodness of Fit and Parameter Uncertainty
</p>
<p>Problems
</p>
<p>10.1 Use the same data as in Problem 8.2 to answer the following questions.
</p>
<p>(a) Plot the 2-dimensional confidence contours at 68 and 90 % significance, by
sampling the (a,b) parameter space in a suitable interval around the best-fit
values.
</p>
<p>(b) Using a suitable 2-dimensional confidence contour, determine the 68 % con-
fidence intervals on each parameter separately, and compare with the analytic
results obtained from the linear regression method.
</p>
<p>10.2 Find the minimum ï¿½2 of the linear fit to the radius vs. ratio data of Table 6.1
and the number of degrees of freedom of the fit. Determine if the null hypothesis
can be rejected at the 99 % confidence level.
</p>
<p>10.3 Consider a simple dataset with the following measurements, assumed to be
derived from a counting process. Show that the best-fit value of the parameter a for
</p>
<p>x y
</p>
<p>0 1
</p>
<p>1 1
</p>
<p>2 1
</p>
<p>the model y D eax is a D 0 and derive its 68 % confidence interval.
10.4 Consider the same dataset as in Problem 10.3 but assume that the y mea-
surements are Gaussian, with variances equal to the measurements. Show that the
confidence interval of the best-fit parameter a D 0 is given by ï¿½a D
</p>
<p>p
1=5.
</p>
<p>10.5 Consider the same dataset as in Problem 10.3 but assume a constant fit
function, y D a. Show that the best-fit is given by a D 1 and that the 68 %
confidence interval corresponds to a standard deviation of
</p>
<p>p
1=3.
</p>
<p>10.6 Consider the biometric data in Pearson&rsquo;s experiment (page 30). Calculate the
average father height (X variable) for each value of the mother&rsquo;s height (Y variable),
and the average mother height for each value of the father&rsquo;s height. Using these two
averaged datasets, perform a linear regression of Y on X, where Y is the average
value you have calculated, and, similarly, the linear regression of X on Y. Calculate
the best-fit parameters a, b (regression of Y on X) and a0, b0 (regression of X on
Y), assuming that each datapoint in your two sets has the same uncertainty. This
problem is an alternative method to perform the linear regressions of Fig. 10.4, and
it yields similar results to the case of a fit to the &ldquo;raw&rdquo; data, i.e., without averaging.
</p>
<p>10.7 Calculate the linear correlation coefficient for the data of Hubble&rsquo;s experiment
(logarithm of velocity, and magnitude m), page 157. Determine whether the</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 The Linear Correlation Coefficient 193
</p>
<p>hypothesis of uncorrelation between the two quantities can be rejected at the 99 %
confidence level.
</p>
<p>10.8 Use the data from Table 6.1 for the radius vs. ratio, assuming that the radius is
the independent variable with no error. Draw the 68 and 90 % confidence contours
on the two fit parameters a and b, and calculate the 68 % confidence interval on the
b parameter.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 11
Systematic Errors and Intrinsic Scatter
</p>
<p>Abstract Certain types of uncertainty are difficult to estimate and may not be
accounted in the initial error budget. This sometimes leads to a poor goodness-of-fit
statistic and the rejection of the model used to fit the data. These missing sources
of uncertainty may either be associated with the data themselves or with the model
used to describe the data. In both cases, we describe methods to account for these
errors and ensure that hypothesis testing is not biased by them.
</p>
<p>11.1 What to Do When the Goodness-of-Fit Test Fails
</p>
<p>The first step to ensure that a dataset is accurately described by a model is to test that
the goodness-of-fit statistic is acceptable. For example, when the data have Gaussian
errors, ï¿½2min can be used as the goodness-of-fit statistic. If the value of ï¿½
</p>
<p>2
min exceeds
</p>
<p>a critical value, it is recommended that one rejects the model. At that point, the
standard option is to use an alternative model, and repeat the testing procedure.
</p>
<p>There are cases when it is reasonable to try a bit harder and investigate further
whether the model and the dataset may still be compatible, despite the poor
goodness of fit. The general situation when additional effort is warranted is in the
case of a model that generally follows the data without severe outliers, yet the best-
fit statistic (such as ï¿½2min) indicates that the model is not acceptable. An example of
this situation is that of Fig. 10.1: the best-fit linear model follows the distribution
of the data without systematic deviations, yet its high value of ï¿½2min D 60:5 for 23
degrees of freedom cannot be formally accepted at any level of confidence.
</p>
<p>In this chapter we describe two types of analysis that can be performed when
the fit of a dataset to a model is poor. The first method assumes that the model
itself has a degree of uncertainty that results in an intrinsic scatter above and
beyond the variance of the data (Sect. 11.2). The second investigates whether
there are additional sources of error in the data that may not have been properly
accounted (Sect. 11.3). The two methods are conceptually different but result in
similar modifications to the analysis.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_11
</p>
<p>195</p>
<p/>
</div>
<div class="page"><p/>
<p>196 11 Systematic Errors and Intrinsic Scatter
</p>
<p>11.2 Intrinsic Scatter and Debiased Variance
</p>
<p>When fitting a dataset to a model we assume that the data are drawn from a parent
model that is described by a number of parameters. As such, we surmise that there
are exact model parameters that describe the parent distribution of the data, although
we don&rsquo;t know their precise values. We use the data to estimate them, typically
through a maximum likelihood method that consists of finding model parameters
that maximize the likelihood of the data being drawn from that model (Chap. 8). For
Gaussian data, the maximum likelihood method consists of finding the minimum of
the ï¿½2 statistic.
</p>
<p>A possible reason for a poor value of the minimum ï¿½2 statistic is that the model
itself, although generally accurate, may have an intrinsic scatter or variance that
needs to be accounted in the determination of the fit statistic. In other words, the
parent model may not be exact but it may feature an inherent degree of variability.
The goal of this section is to provide a method to describe and measure such scatter.
</p>
<p>11.2.1 Direct Calculation of the Intrinsic Scatter
</p>
<p>Each measurement in a dataset can be described as the sum of two variables,
</p>
<p>yi D ï¿½i C ï¿½i; (11.1)
</p>
<p>where ï¿½i represents the parent value from which the measurement yi is drawn and
ï¿½i is the variable representing the measurement error. Usually, we assume that ï¿½i D
y.xi/ is a fixed number, estimated by the least-squares (or other) method. Since ï¿½i
is a variable of zero mean, and its variance is simply the measurement variance ï¿½2i ,
(11.1) implies that the variance of the measurement yi is just ï¿½2i .
</p>
<p>The model ï¿½i may, however, be considered a variable with non-zero variance.
This is to describe the fact that the model is not known exactly, but has an intrinsic
degree of variability measured by its variance ï¿½2int D Var.ï¿½i/. For simplicity, we
assume that this model variance is constant for all points along the model. Under the
assumption that the measurement error and the model are independent, variances of
the variables on the right-hand side of (11.1) add and this yields to
</p>
<p>ï¿½2int D Var.yi/ ï¿½ ï¿½2i : (11.2)
</p>
<p>The equation means that the intrinsic variance is obtained as the difference of the
data variance minus the variance due to measurement errors. In keeping up with the
definitions of (11.1), Var.yi/ refers to the total variance of the i-th variable at location
xi. It is meaningful to calculate the average variance for all the yi&rsquo;s assuming that
each measurement is drawn from a parent mean of Oyi, the best-fit value of the model
y.xi/. In so doing, we make use of the fact that the model is not constant but it varies</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 Intrinsic Scatter and Debiased Variance 197
</p>
<p>at different positions. As a result, (11.2) can be used to calculate the intrinsic scatter
or variance of the model ï¿½2int as
</p>
<p>ï¿½2int D
1
</p>
<p>N ï¿½m
NX
</p>
<p>iD1
.yi ï¿½ Oyi/2 ï¿½ 1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
ï¿½2i : (11.3)
</p>
<p>where m is the number of model parameters. The intrinsic variance can also be
referred to as the debiased variance, because of the subtraction of the expected
scatter (due to measurement errors) from the total sample variance. Equation (11.3)
can be considered a generalization of (2.11) in two ways. First, the presence of errors
in the measurements of yi leads to the addition of the last term on the right-hand side.
Second, the total variance of the data are calculated not relative to the data mean y
but to the parent mean of each measurement. It is possible that the second term in
the right-hand side of (11.3) is larger than the first term, leading to a negative value
for the intrinsic variance. This is an indication that, within the statistical errors ï¿½i,
there is no evidence for an intrinsic scatter of the model. This method to estimate
the intrinsic scatter is derived from [2] and [24].
</p>
<p>It is important to remember that in calculating the intrinsic scatter we have made
the assumption that the model is an accurate representation of the data. This means
that we can no longer test for the null hypothesis that the model represents the parent
distribution&mdash;we have already assumed this to be the case.
</p>
<p>When the model is constant, with Oyi D y being the sample mean, the intrinsic
scatter is calculated as
</p>
<p>ï¿½2int D
1
</p>
<p>N ï¿½ 1
NX
</p>
<p>iD1
.yi ï¿½ y/2 ï¿½ 1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
ï¿½2i : (11.4)
</p>
<p>In this case, (11.4) is an unbiased estimate of the variance of Y.
</p>
<p>11.2.2 Alternative Method to Estimate the Intrinsic Scatter
</p>
<p>An alternative method to measure the amount of extra variance in a fit makes use of
the fact that, for a Gaussian dataset, the expected value of the reduced ï¿½2min is one.
A large value of the minimum ï¿½2 can be reduced by increasing the size of the errors
until ï¿½2red ' 1, or
</p>
<p>ï¿½2min D
NX
</p>
<p>iD1
</p>
<p>.yi ï¿½ Oyi/2
ï¿½2i C ï¿½2int
</p>
<p>' N ï¿½ m (11.5)
</p>
<p>where m is the number of free model parameters, and ï¿½int is the intrinsic scatter
that makes the reduced ï¿½2 unity. In (11.5) we have made the following substitution</p>
<p/>
</div>
<div class="page"><p/>
<p>198 11 Systematic Errors and Intrinsic Scatter
</p>
<p>relative to the standard use of the ï¿½2min method:
</p>
<p>ï¿½2i ! ï¿½2i C ï¿½2int: (11.6)
</p>
<p>This method is only approximate, in that an acceptable model need not yield exactly
a value of ï¿½2red D 1. This method to estimate the intrinsic scatter is nonetheless
useful as an estimate of the level of scatter present in the data. Like in the earlier
method, the analyst is making the assumption that the model fits the data and that
the extra variance is attributed to an intrinsic variability of the model (ï¿½2int).
</p>
<p>Example 11.1 The example shown in Fig. 10.1 illustrates a case in which the data
do not show systematic deviations from a best-fit model, and yet the ï¿½2 test would
require a rejection of the model. The quantities Energy 1 (independent variable) and
Energy 2 were fit to a linear model, the best-fit linear model yielded a fit statistic of
ï¿½2min D 60:5 for 23 degrees of freedom and the model was therefore not acceptable.
</p>
<p>Making use of the methods developed in this section, we can estimate the
intrinsic scatter that makes the model consistent with the data. Using (11.3), the
intrinsic scatter is estimated to be ï¿½int D 2:5. This means that the model has a
typical uniform variability of 2.5 units (the units are those of the y axis, in this case
used to measure energy). Using (11.5), a value of ï¿½int D 1:6 is needed to obtain
a reduced ï¿½2min of unity. The two methods were not expected to provide the same
answer since they are based on different assumptions. }
</p>
<p>11.3 Systematic Errors
</p>
<p>The errors described so far in this book are usually referred to as random errors,
since they describe the uncertainties in the random variables of interest. There are
many sources of random error. A common source of randome error is the Poisson or
counting error which derives from measuring N counts in an experiment and results
in an error of
</p>
<p>p
N. Another source of error is due to the presence of a background
</p>
<p>that needs to be subtracted from the measured signal. In general, any instrument used
to record data will have sources of error that causes the measurements to fluctuate
randomly around its mean value.
</p>
<p>One of the main tasks of a data analyst is to find all the important sources of error
that contribute to the variance of the random variable of interest. A typical case is
the measurement of a total signal T in the presence of a background B, where the
random variable of interest is the background-subtracted signal S,
</p>
<p>S D T ï¿½ B: (11.7)
</p>
<p>If the background is measured independently from the signal T, then the variance of
the source is
</p>
<p>ï¿½2S D ï¿½2T C ï¿½2B: (11.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Systematic Errors 199
</p>
<p>The lesson to learn is that the variance of the random variable of interest S increases
when the background is subtracted. If one assumes that there is no background, or
that the background is constant (ï¿½2B D 0), the random error associated with S may
be erroneously underestimated.
</p>
<p>The term statistical error is often used as a synonym of random error. Sometimes,
however, it is used to designate the leading source of random error, such as the
Poisson uncertainty in a counting experiment, not including other sources of random
error that are equally statistical or random in nature. Such use is not accurate, but the
reader should be aware that there is no universally accepted meaning for the term
&ldquo;statistical error.&rdquo;
</p>
<p>The term systematic error designates sources of error that systematically shift
the signal of interest either too high or too low. Sources of systematic errors need to
be identified to correct the erroneous offset. A typical example is an instrument that
is miscalibrated and systematically reports measurements that have an erroneous
offset. Even after the correction for the offset, it is however quite likely that there still
remains a source of error, for example associated with the fact that such correction
may not be uniform for all datapoints. If the systematic error is additive in nature,
i.e., it shifts the random variable X according to X0 D X Ë E, then the variance of
the data is to be modified according to
</p>
<p>ï¿½
0 2
i D ï¿½2i C ï¿½2E: (11.9)
</p>
<p>The term ï¿½2E denotes the variance of the systematic error E. If E is known exactly,
then it would ideally have zero variance. But in all practical cases, there will be an
additional source of variance from the correction of a systematic error that needs
to be accounted. The modification of the error ï¿½i due to the presence of a source
of systematic error is therefore identical in form to the presence of intrinsic error
[compare (11.6) and (11.9)].
</p>
<p>If the systematic error is multiplicative in nature, i.e., X0 D E ï¿½ X, it may be
convenient to use the logarithms, logX0 D logX C logE and then proceed as in the
case of a linear offset.
</p>
<p>Example 11.2 Continuing with the example shown in Fig. 10.1, we can use the
results provided in Example 11.1 to say that an additional error of ï¿½E D 1:6 would
yield a fit statistic of ï¿½2min;red D 1. This means that a possible interpretation for
the large value of ï¿½2min is that we had neglected an additional source of error ï¿½E.
This additional source of error would be in place of the intrinsic scatter, since either
correction to the calculation of ï¿½2min is sufficient to bring the data in agreement with
the model.
</p>
<p>The errors of the data in Fig. 10.1 accounted for several sources of random
error, including Poisson errors in the counting of photons from these sources, the
background subtraction and for errors associated with the model used to describe
the distribution of energy. The additional error of order ï¿½E D 1:6 for each datapoint
may therefore be (a) an intrinsic error of the model (as described in Example 11.1),
(b) an additional error from the correction of certain systematic errors that were</p>
<p/>
</div>
<div class="page"><p/>
<p>200 11 Systematic Errors and Intrinsic Scatter
</p>
<p>performed in the process of the analysis or (c) an additional random error that were
not already included in the original error budget. The magnitude of possible errors
in cases (b) and (c) can be estimated based on the knowledge of the collection of
the data and its analysis. If such errors cannot be as large as required to obtain
an acceptable fit, the only remaining option is to attribute this error to an intrinsic
variance of the model or to conclude that the model is not an accurate description of
the data. }
</p>
<p>11.4 Estimate of Model Parameters with Systematic Errors
or Intrinsic Scatter
</p>
<p>In Sects. 11.2 and 11.3 we have assumed that intrinsic scatter or additional sources
of systematic errors could be estimated using the best-fit values Oyi obtained from
the fit without these errors. Systematic errors or intrinsic scatter, however, do have
an effect on the estimate of model parameters. The presence of systematic errors or
intrinsic scatter, as discussed earlier in this chapter, is accounted with the addition
of another source of variance to the data according to
</p>
<p>ï¿½
0 2
i D ï¿½2i C ï¿½2: (11.10)
</p>
<p>The quantity ï¿½ is either the systematic error ï¿½E not accounted in the initial estimate
of ï¿½i, or the intrinsic scatter ï¿½int. Both cases lead to the same effect on the overall
error budget and the ï¿½2 fit statistic to minimize becomes
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>.yi ï¿½ y.xi//2
ï¿½2i C ï¿½2
</p>
<p>: (11.11)
</p>
<p>It is clear that repeating the fitting procedure with the larger ï¿½
0
</p>
<p>i errors instead
of the original error will lead to new best-fit values and new uncertainties for the
model parameters. The effect of the larger errors is to de-weight datapoints that
have small values of ï¿½i and in general to provide larger confidence intervals for the
model parameters. An acceptable procedure to obtain truly best-fit values of model
parameters and their confidence intervals is to first estimate the additional source of
error ï¿½ (either an intrinsic scatter or additional statistical or systematic errors) and
then repeat the fit.
</p>
<p>Example 11.3 The linear fit to the data of Table 6.1 for Energy 1 (independent
variable) and Energy 2 resulted in a ï¿½2min D 60:5 for 23 degrees of freedom. The
fit was not acceptable at any level of confidence. In Example 11.1 we calculated
that an additional variance of ï¿½2 D 1:6 yields a ï¿½2min D 23. We fit the data with
the addition of this error to the dependent variable and find the best-fit values of
a D ï¿½0:085Ë 0:48, b D 1:05Ë 0:05.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Estimate of Model Parameters with Systematic Errors or Intrinsic Scatter 201
</p>
<p>For comparison, the fit obtained with the original errors returned values of a D
ï¿½0:26Ë 0:088, b D 1:04Ë 0:27. These values could not be properly called &ldquo;best-
fit,&rdquo; since the fit was not acceptable. Yet, comparison between these values and those
for the ï¿½2red D 1:0 case shows that best-fit parameters are affected by the additional
source of error and that the confidence intervals become larger with the increased
errors, as expected. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Intrinsic scatter: An uncertainty of the model that increases the measure-
ment error according to yi D ï¿½i C ï¿½i.
</p>
<p>ï¿½ Debiased variance: A correction to the measured variance that accounts
for the presence of measurement errors,
</p>
<p>ï¿½2int D
1
</p>
<p>N ï¿½ m
X
</p>
<p>.yi ï¿½ Oyi/2 ï¿½ 1
N
</p>
<p>X
ï¿½2i :
</p>
<p>The square root provides a measure of the intrinsic scatter.
ï¿½ Systematic error: A type of measurement error ï¿½E that systematically shifts
</p>
<p>the measurements (as opposed to the statistical error ï¿½i). The two errors
typically are added in quadrature, ï¿½
</p>
<p>0 2
i D ï¿½2i C ï¿½2.
</p>
<p>Problems
</p>
<p>11.1 Fit the data from Table 6.1 for the radius vs. ratio using a linear model and
calculate the intrinsic scatter using the best-fit linear model.
</p>
<p>11.2 Using the same data as in Problem 11.1, provide an additional estimate of the
intrinsic scatter using the ï¿½2red ' 1 method.
11.3 Justify the 1=.N ï¿½ m/ and 1=.N ï¿½ 1/ coefficients in (11.3) and (11.4).
11.4 Using the data for the Hubble measurements of page 157, assume that each
measurement of log v has an uncertainty of ï¿½ D 0:01. Estimate the intrinsic scatter
in the linear regression of log v vs. m.
</p>
<p>11.5 Using the data of Problem 8.2, estimate the intrinsic scatter in the linear fit of
the X;Y data.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 12
Fitting Two-Variable Datasets with Bivariate
Errors
</p>
<p>Abstract The maximum likelihood method for the fit of a two-variable dataset
described in Chap. 8 assumes that one of the variables (the independent variable
X) has negligible errors. There are many applications where this assumption is
not applicable and uncertainties in both variables must be taken into account. This
chapter expands the treatment of Chap. 8 to the fit of a two-variable dataset with
errors in both variables.
</p>
<p>12.1 Two-Variable Datasets with Bivariate Errors
</p>
<p>Throughout Chaps. 8 and 10 we have assumed a simple error model where the
independent variable X is known without error, and all sources of uncertainty in
the fit are due to the dependent variable Y. The two-variable dataset (X;Y) was
effectively treated as a sequence of random variables of values yi Ë ï¿½i at a fixed
location xi with a parent model y.xi/.
</p>
<p>There are many applications, however, in which both variables have comparable
uncertainties (ï¿½x ' ï¿½y) and there is no reason to treat one variable as independent.
In general, a two-variable dataset is described by the datapoints
</p>
<p>.xi Ë ï¿½xi; yi Ë ï¿½yi/
</p>
<p>and the covariance ï¿½2xyi between the two measurements. One example is the two
measurements of energy in the data in Table 6.1, where it would be appropriate to
account for errors in both measurements. There is in fact no particular reason why
one measurement should be considered as the independent variable and the other
the dependent variable.
</p>
<p>There are several methods to deal with two-variable datasets with bivariate error.
Given the complexity of the statistical model, there is not a uniquely accepted
solution to the general problem of fitting data with bivariate errors. This chapter
presents two methods for the linear fit to data with two-variable errors. The first
method (Sect. 12.2) applies to a linear fit and it is an extension of the least-squares
method of Sect. 8.3. The second method (Sect. 12.3) is based on an alternative
definition of ï¿½2 and it applies to any type of fit function. Although this method
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_12
</p>
<p>203</p>
<p/>
</div>
<div class="page"><p/>
<p>204 12 Fitting Two-Variable Datasets with Bivariate Errors
</p>
<p>does not have an analytic solution, it can be easily implemented using numerical
methods such as Monte Carlo Markov chains described later in this book.
</p>
<p>12.2 Generalized Least-Squares Linear Fit to Bivariate Data
</p>
<p>In the case of identical measurement errors on the dependent variable Y and no
error on the independent variable X, the least-squares method described in Sect. 8.3
estimated the parameters of the linear model as
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>b D Cov.X;Y/
Var.X/
</p>
<p>D
PN
</p>
<p>iD1.xi ï¿½ x/.yi ï¿½ y/
PN
</p>
<p>iD1.xi ï¿½ x/2
</p>
<p>a D E.Y/ ï¿½ bE.X/ D 1
N
</p>
<p>NP
</p>
<p>iD1
yi ï¿½ b 1
</p>
<p>N
</p>
<p>NP
</p>
<p>iD1
xi:
</p>
<p>(12.1)
</p>
<p>A generalization of this least-squares method accounts for the presence of
measurement errors in the estimate of the variances and the covariance in (12.1).
The methods of analysis presented in this section were developed by Akritas and
Bershady [2] and others [22, 24]. Those references can be used as source of
additional information on these methods for bivariate data.
</p>
<p>Measurements of the X and Y variables can be described by
</p>
<p>(
xi D ï¿½xi C ï¿½xi
yi D ï¿½yi C ï¿½yi;
</p>
<p>(12.2)
</p>
<p>each the sum of a parent quantity and a measurement error, as in (11.1). Accord-
ingly, the variances of the parent variables are given by
</p>
<p>(
Var.ï¿½xi/ D Var.xi/ï¿½ ï¿½2xi
Var.ï¿½yi/ D Var.yi/ï¿½ ï¿½2yi:
</p>
<p>(12.3)
</p>
<p>This means that in (12.1) one must replace the sample covariance and variance by a
debiased or intrinsic covariance and variance, i.e., quantities that take into account
the presence of measurement errors.
</p>
<p>The method of analysis that led to (12.1) assumes that the variable Y depends on
X. In other words, we assumed that X is the independent variable. In this case, we
talk of a fit of Y-given-X, or Y=X, and we write the linear model as
</p>
<p>y D aY=X C bY=Xx: (12.4)
</p>
<p>Modification of (12.1) with (12.3) (and an equivalent formula for the covariance)
leads to the following estimator for the slope and intercept of the linear Y=X model:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Generalized Least-Squares Linear Fit to Bivariate Data 205
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>bY=X D
Cov.X;Y/ ï¿½ ï¿½2xy
Var.X/ï¿½ ï¿½2x
</p>
<p>D
PN
</p>
<p>iD1.xi ï¿½ x/.yi ï¿½ y/ ï¿½
PN
</p>
<p>iD1 ï¿½2xyi
PN
</p>
<p>iD1.xi ï¿½ x/2 ï¿½
PN
</p>
<p>iD1 ï¿½2xi
aY=X D y ï¿½ bY=Xx:
</p>
<p>(12.5)
</p>
<p>In this equation the sample variance and covariance of (12.1) were replaced with
the corresponding intrinsic quantities, and the subscript Y=X indicates that X was
considered as the independent variable.
</p>
<p>A different result is obtained if Y is considered as the independent variable. In
that case, the X-given-Y (or X=Y) model is described as
</p>
<p>x D a0 C b0y: (12.6)
</p>
<p>The same equations above apply by exchanging the two variables X and Y:
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>b0 D
PN
</p>
<p>iD1.xi ï¿½ x/.yi ï¿½ y/ ï¿½
PN
</p>
<p>iD1 ï¿½2xyi
PN
</p>
<p>iD1.yi ï¿½ y/2 ï¿½
PN
</p>
<p>iD1 ï¿½2yi
a0 D x ï¿½ b0y:
</p>
<p>It is convenient to compare the results of the Y/X and X/Y fits by rewriting the latter
in the usual form with x as the independent variable:
</p>
<p>y D aX=Y C bX=Yx D ï¿½a
0
</p>
<p>b0
C x
</p>
<p>b0
</p>
<p>for which we find that the slope and intercept are given by
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>bX=Y D
PN
</p>
<p>iD1.yi ï¿½ y/2 ï¿½
PN
</p>
<p>iD1 ï¿½2yi
PN
</p>
<p>iD1.xi ï¿½ x/.yi ï¿½ y/ï¿½
PN
</p>
<p>iD1 ï¿½2xyi
aX=Y D y ï¿½ bX=Yx:
</p>
<p>(12.7)
</p>
<p>In general the two estimators Y/X and X/Y will give different results for the
best-fit line. This difference highlights the importance of interpreting the data to
determine which variable should be considered the independent quantity.
</p>
<p>Uncertainties in the parameters a and b and the covariance between them have
been calculated by Akritas and Bershady [2]. For the Y/X estimator they can be</p>
<p/>
</div>
<div class="page"><p/>
<p>206 12 Fitting Two-Variable Datasets with Bivariate Errors
</p>
<p>obtained via the following variables:
</p>
<p>ï¿½i D
.xi ï¿½ Nx/.yi ï¿½ bY=Xxi ï¿½ aY=X/C bY=Xï¿½2xi ï¿½ ï¿½2xyi
</p>
<p>1
</p>
<p>N
</p>
<p>P
.xi ï¿½ Nx/2 ï¿½ 1
</p>
<p>N
</p>
<p>P
ï¿½2xi
</p>
<p>ï¿½i Dyi ï¿½ bY=Xxi ï¿½ Nxï¿½i:
(12.8)
</p>
<p>With these, the variances of a and b and the covariance is given by
</p>
<p>8
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
:
</p>
<p>ï¿½2bY=X D
1
</p>
<p>N
</p>
<p>P
.ï¿½i ï¿½ Nï¿½/2
</p>
<p>ï¿½2aY=X D
1
</p>
<p>N
</p>
<p>P
.ï¿½i ï¿½ Nï¿½/2
</p>
<p>ï¿½2ab D
1
</p>
<p>N
</p>
<p>P
.ï¿½i ï¿½ Nï¿½/.ï¿½i ï¿½ Nï¿½/:
</p>
<p>(12.9)
</p>
<p>For the X/Y estimator there are equivalent formulas for the ï¿½ and ï¿½ variables that
need to be used in place of (12.8):
</p>
<p>ï¿½i D
.yi ï¿½ Ny/.yi ï¿½ bX=Yxi ï¿½ aX=Y/C bX=Yï¿½2xyi ï¿½ ï¿½2yi
</p>
<p>1
</p>
<p>N
</p>
<p>P
.xi ï¿½ Nx/.yi ï¿½ Ny/ï¿½ 1
</p>
<p>N
</p>
<p>P
ï¿½2xyi
</p>
<p>ï¿½i Dyi ï¿½ bX=Yxi ï¿½ Nxï¿½i:
(12.10)
</p>
<p>These values can then be used to calculate variances and the covariance of the
parameters as in the Y/X fit.
</p>
<p>Example 12.1 In Fig. 12.1 we illustrate the difference in the best-fit models when X
is the independent variable (12.5) or Y is the independent variable (12.7), using the
data of Table 6.1. The Y/X parameters are aY=X D ï¿½0:367 and bY=X D 1:118 and
the X/Y parameters are aX=Y D ï¿½0:521 and bX=Y D 1:132. Unfortunately there is no
definitive prescription to decide which variable should be regarded as independent.
In this example each variable could be equally treated as the independent variable
and the difference between the two best-fit models is relatively small. The difference
between the two models for a value of the x axis of 1 is approximately 20 %. Note
that the linear model and the data were plotted in a logarithmic scale to provide a
more compact figure.
</p>
<p>Also, the data of Table 6.1 do not report any covariance measurement and
therefore the best-fit lines were calculated assuming independence between all
measurements (ï¿½2xyi D 0). }
</p>
<p>The example based on the data of Table 6.1 show that there is not just a single
slope for the best-fit linear model, but that the results depend on which variable
is assumed to be independent, as in the case of no measurement errors available</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Generalized Least-Squares Linear Fit to Bivariate Data 207
</p>
<p>Fig. 12.1 Linear model fits to the data of Table 6.1 using the debiased variance method. The solid
line is the model that uses Energy 1 as the independent variable X (12.4), the dashed line is the
model that uses Energy 2 as the independent variable Y (12.6). Note the logarithmic scale for both
axes
</p>
<p>(Sect. 8.5). In certain cases it may be appropriate to use a model that is intermediate
between the two Y/X and X/Y results. This is called the bisector model, which
consists of the linear model that bisects the two lines obtained from the Y/X and
X/Y fits described above. This method is also described by Akritas and Bershady
[2] and Isobe and Feigelson [22] and the best-fit bisector line can be obtained from
the following formulae:
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>bbis D
bY=XbX=Y ï¿½ 1C
</p>
<p>q
.1C b2Y=X/.1C b2X=Y/
</p>
<p>bY=X C bX=Y
abis D Ny ï¿½ bbis Nx:
</p>
<p>(12.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>208 12 Fitting Two-Variable Datasets with Bivariate Errors
</p>
<p>The uncertainties in the slope and intercept parameters can also be obtained using
this definition for the ï¿½ and ï¿½ variables:
</p>
<p>ï¿½i D
.1C b2X=Y/bbis
</p>
<p>.bY=X C bX=Y/
q
.1C b2Y=X/.1C b2X=Y/
</p>
<p>ï¿½Y=XC
</p>
<p>.1C b2Y=X/bbis
.bY=X C bX=Y/
</p>
<p>q
.1C b2Y=X/.1C b2X=Y/
</p>
<p>ï¿½X=Y
</p>
<p>ï¿½i Dyi ï¿½ bbisxi ï¿½ Nxï¿½i;
</p>
<p>(12.12)
</p>
<p>where ï¿½Y=X is the ï¿½ variable defined in (12.8) for the Y/X fit and ï¿½X=Y is the ï¿½ variable
defined in (12.10) for the X/Y fit.
</p>
<p>Example 12.2 Figure 12.2 shows the fit to the variables Radius (X variable) and
Ratio of thermal energies (Y variable) from Table 6.1. The solid line is the Y/X
best-fit line with parameters a D 1:1253 and b D ï¿½0:0005, the dashed line is
the X/Y best-fit line with parameters a D 1:4260 and b D ï¿½0:0018 and the dot-
dash line is the bisector line with parameters a D 1:2778 and b D ï¿½0:0011.
Notice how the Y/X and X/Y regressions give significantly different results. This
is in part due to the presence of substantial scatter in the data, which results in
several datapoints significantly distant from the best-fit regression lines. In the other
</p>
<p>Fig. 12.2 Fit to the data of Table 6.1 using errors in both variables (see Example 12.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Linear Fit Using Bivariate Errors in the ï¿½2 Statistic 209
</p>
<p>example of regression with errors in both variables (Fig. 12.1) the Y/X and X/Y
best-fit lines were in better agreement. }
</p>
<p>12.3 Linear Fit Using Bivariate Errors in the ï¿½2 Statistic
</p>
<p>An alternative method to fit a dataset with errors in both variables is to re-define the
ï¿½2 statistic to account for the presence of errors in the X variable. In the case of a
linear fit, the square of the deviation of each datapoint yi from the model is given by
</p>
<p>.yi ï¿½ a ï¿½ bxi/2: (12.13)
</p>
<p>When there is no error in the X variable, the variance of the variable in (12.13) is
simply the variance of Y, ï¿½2yi. In the presence of a variance ï¿½
</p>
<p>2
xi for X, the variance of
</p>
<p>the linear combination yi ï¿½ a ï¿½ bxi is given by
</p>
<p>Var.yi ï¿½ a ï¿½ bxi/ D ï¿½2yi C b2ï¿½2xi;
</p>
<p>where a and b are the parameters of the linear model and the variables X and Y are
assumed to be independent. This suggests a new definition of the ï¿½2 function for
this dataset [35, 40], namely
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>.yi ï¿½ a ï¿½ bxi/2
ï¿½2yi C b2ï¿½2xi
</p>
<p>: (12.14)
</p>
<p>Since each term at the denominator is the variance of the term at the numerator,
the new ï¿½2 variable defined in (12.14) is ï¿½2-distributed with f D N ï¿½ 2 degrees of
freedom.
</p>
<p>The complication with the minimization of this function is that the unknown
parameter b appears both at the numerator and the denominator of the function that
needs to be minimized. As a result, an analytic solution to the maximum likelihood
method cannot be given in general. Fortunately, the problem of finding the values of
a and b that minimize (12.14) can be solved numerically. This method for the linear
fit of two-variable data with errors in both coordinates is therefore of common use,
and it is further described in [35].</p>
<p/>
</div>
<div class="page"><p/>
<p>210 12 Fitting Two-Variable Datasets with Bivariate Errors
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Data with bivariate errors: A two-variable dataset that has errors in both
variables. For these data there is no commonly accepted fit method.
</p>
<p>ï¿½ Generalized least-squares fit to bivariate data: An extension of the
traditional ML fit to two-variable data. When x is the independent variable
the best-fit parameters of the linear model are
</p>
<p>8
</p>
<p>&lt;Ì
</p>
<p>:Ì
</p>
<p>bY=X D
Cov.X;Y/ ï¿½ ï¿½2xy
Var.X/ï¿½ ï¿½2x
</p>
<p>aY=X D y ï¿½ bY=Xx:
</p>
<p>ï¿½ Bisector model: A best-fit model for bivariate data that bisects the Y=X and
X=Y models, intended to provide and intermediate model.
</p>
<p>ï¿½ Use of bivariate errors in ï¿½2: The ï¿½2 statistic can also be redefined to
accommodate bivariate errors according to
</p>
<p>ï¿½2 D
NX
</p>
<p>iD1
</p>
<p>.yi ï¿½ a ï¿½ bxi/2
ï¿½2yi C b2ï¿½2xi
</p>
<p>:
</p>
<p>Problems
</p>
<p>12.1 Use the bivariate error data of Energy 1 and Energy 2 from Table 6.1. Calculate
the best-fit parameters and errors of the linear model Y=X, where X is Energy 1 and
Y is Energy 2.
</p>
<p>12.2 Use the bivariate error data of Energy 1 and Energy 2 from Table 6.1. Calculate
the best-fit parameters and errors of the linear model X=Y, where X is Energy 1 and
Y is Energy 2.
</p>
<p>12.3 For the Energy 1 and Energy 2 data of Table 6.1, use the results of
Problems 12.1 and 12.2 to calculate the bisector model to the Energy 1 vs. Energy
2 data.
</p>
<p>12.4 Repeat Problem 12.1 for the Ratio vs. Radius data of Table 6.1.
</p>
<p>12.5 Repeat Problem 12.2 for the Ratio vs. Radius data of Table 6.1.
</p>
<p>12.6 Repeat Problem 12.3 for the Ratio vs. Radius data of Table 6.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 13
Model Comparison
</p>
<p>Abstract The availability of alternative models to fit a dataset requires a quanti-
tative method for comparing the goodness of fit to different models. For Gaussian
data, a lower reduced ï¿½2 of one model with respect to another is already indicative of
a better fit, but the outstanding question is whether the value is significantly lower,
or whether a lower value can be just the result of statistical fluctuations. For this
purpose we develop the distribution function of the F statistic, useful to compare
the goodness of fit between two models and the need for an additional &ldquo;nested&rdquo;
model component, and the Kolmogorov&ndash;Smirnov statistics, useful in providing a
quantitative measure of the goodness of fit, and in comparing two datasets regardless
of their fit to a specific model.
</p>
<p>13.1 The F Test
</p>
<p>For Gaussian data, the ï¿½2 statistic is used for determining if the fit to a given parent
function y.x/ is acceptable. It is possible that several different parent functions yield
a goodness of fit that is acceptable. This may be the case when there are alternative
models to explain the experimental data, and the data analyst is faced with the
decision to determine what model best fits the experimental data. In this situation,
the procedure to follow is to decide first a confidence level that is considered
acceptable, say 90 or 99 %, and discard all models that do not satisfy this criterion.
The remaining models are all acceptable, although a lower ï¿½2min certainly indicates
a better fit.
</p>
<p>The first version of the F test applies to independent measurements of the ï¿½2
</p>
<p>fit statistic, and its application is therefore limited to cases that compare different
datasets. A more common application of the F test is to compare the fit of a given
dataset between two models that have a nested component, i.e., one model is a
simplified version of the other. For nested model components one can determine
whether the additional component is really needed to fit the data.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_13
</p>
<p>211</p>
<p/>
</div>
<div class="page"><p/>
<p>212 13 Model Comparison
</p>
<p>13.1.1 F-Test for Two Independent ï¿½2 Measurements
</p>
<p>Consider the case of two ï¿½2min values obtained by fitting data from a given
experiment to two different functions, y1.x/ and y2.x/. If both models equally well
approximate the parent model, then we would expect that the two values of ï¿½2
</p>
<p>would be similar, after taking into consideration that they may have a different
number of degrees of freedom. But if one is a better approximation to the parent
model, then the value of ï¿½2 for such model would be significantly lower than for
the other. We therefore want to proceed to determine whether both ï¿½2min statistics
are consistent with the null hypothesis that the data are drawn from the respective
model. The statistic to use to compare the two values of ï¿½2 must certainly also take
into account the numbers of degrees of freedom, which is related to the number
of model parameters used in each determination of ï¿½2. In fact, a larger number of
model parameters may result in fact result in a lower value of ï¿½2min, simply because
of the larger flexibility that the model has in following the data. For example, a
dataset of N points will always be fitted perfectly by a polynomial having N terms,
but this does not mean that a simpler model may not be just as good a model for the
data, and the underlying experiment.
</p>
<p>Following the theory described in Sect. 7.4, we define the F statistic as
</p>
<p>F D ï¿½
2
1;min=f1
</p>
<p>ï¿½22;min=f2
D ï¿½
</p>
<p>2
1;min;red
</p>
<p>ï¿½22;min;red
; (13.1)
</p>
<p>where f1 and f2 are the degrees of freedom of ï¿½21;min and ï¿½
2
2;min. Assuming that the
</p>
<p>two ï¿½2 statistics are independent, then F will be distributed like the F statistic with
f1; f2 degrees of freedom, having a mean of approximately 1 [see (7.22) and (7.24)].
</p>
<p>There is an ambiguity in the definition of which of the two models is labeled as 1
and which as 2, since two numbers can be constructed that are the reciprocal of each
other, F12 D 1=F21. The usual form of the F-test is that in which the value of the
statistic is F &gt; 1, and therefore we choose the largest of F12 and F21 to implement
a one-tailed test of the null hypothesis with significance p,
</p>
<p>1 ï¿½ p D
Z 1
</p>
<p>Fcrit
</p>
<p>fF.f ; x/dx D P.F ï¿½ Fcrit/: (13.2)
</p>
<p>Critical values Fcrit are reported in Tables A.8, A.9, A.10, A.11, A.12, A.13, A.14,
and A.15 for various confidence levels p.
</p>
<p>The null hypothesis is that the two values of ï¿½2min are distributed following
a ï¿½2 distributions; this, in turn, means that the respective fitting functions used
to determine each ï¿½2min are both good approximations of the parent distribution.
Therefore the test based on this distribution can reject the hypothesis that both fitting
functions are the parent distribution. If the test rejects the hypothesis at the desired
confidence level, then only one of the models will still stand after the test&mdash;the one</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 The F Test 213
</p>
<p>at the denominator with the lowest reduced ï¿½2&mdash;even if the value of ï¿½2min alone was
not able to discriminate between the two models.
</p>
<p>Example 13.1 Consider the radius vs. ratio data of Table 6.1 (see also Prob-
lem 11.1). The linear fit to the entire dataset is not acceptable, and therefore a linear
model for all measurements must be discarded. If we consider measurements 1
through 5, 6 through 10, and 11 through 15, a linear fit to these two subsets results in
the values of best-fit parameters and ï¿½2 shown in the table, along with the probability
to exceed the value of the fit statistic.
</p>
<p>Measurements a b ï¿½2min Probability
</p>
<p>1&ndash;5 0:97Ë 0:09 ï¿½0:0002Ë 0:0002 5.05 0.17
6&ndash;10 1:27Ë 0:22 ï¿½0:0007Ë 0:0011 6.19 0.10
</p>
<p>10&ndash;15 0:75Ë 0:09 ï¿½0:0002Ë 0:0003 18.59 0.0
</p>
<p>The third sample provides an unacceptable fit to the linear model, and therefore
this subset cannot be further considered. For the first two samples, the fits are
acceptable at the 90 % confidence level, and we can construct the F statistic as
</p>
<p>F D ï¿½
2
min.6 ï¿½ 10/
ï¿½2min.1 ï¿½ 5/
</p>
<p>D 1:23:
</p>
<p>Both ï¿½2 have the same number of degrees of freedom (3), and Table A.13 shows
that the value of 1.23 is certainly well within the 90 % confidence limit for the F
statistics (Fcrit ' 5:4). This test shows that both subsets are equally well described
by a linear fit, and therefore the F-test cannot discriminate between them.
</p>
<p>To illustrate the power of the F-test, assume that there is another set of five
measurements that yield a ï¿½2min D 1:0 when fit to a linear model. This fit is clearly
acceptable in terms of its ï¿½2 probability. Constructing an F statistic between this
new set and set 6&ndash;10, we would obtain
</p>
<p>F D ï¿½
2
min.6 ï¿½ 10/
ï¿½2min.new/
</p>
<p>D 6:19:
</p>
<p>In this case, the value of F is not consistent at the 90 % level with the F distribution
with f1 D f2 D 3 degrees of freedom (the measured value exceeds the critical value).
The F-test therefore results in the conclusion that, at the 90 % confidence level, the
two sets are not equally likely to be drawn from a linear model, with the new set
providing a better match. }
</p>
<p>It is important to note that the hypothesis of independence of the two ï¿½2 is not
justified if the same data are used for both statistics. In practice, this means that the F
statistic cannot be used to compare the fit of a given dataset to two different models.
The test can still be used to test whether two different datasets, derived from the
same experiment but with independent measurements, are equally well described
by the same parametric model, as shown in the example above. In this case, the</p>
<p/>
</div>
<div class="page"><p/>
<p>214 13 Model Comparison
</p>
<p>null hypothesis is that both datasets are drawn from the same parent model, and a
rejection of the hypothesis means that both datasets cannot derive from the same
distribution.
</p>
<p>13.1.2 F-Test for an Additional Model Component
</p>
<p>Consider a model y.x/ with m adjustable parameters, and another model y.x/
obtained by fixing p of the m parameters to a reference (fixed) value. In this case,
the y.x/ model is said to be nested into the more general model, and the task is to
determine whether the additional p parameters of the general model are required to
fit the data.
</p>
<p>Example 13.2 An example of nested models are polynomial models. The general
model can be taken as a polynomial of second order,
</p>
<p>y.x/ D aC bxC cx2
</p>
<p>and the nested model as a linear model,
</p>
<p>y.x/ D aC bx:
</p>
<p>The nested model is obtained from the general model with c D 0 and has one fewer
degree of freedom than the general model. }
</p>
<p>Following the same discussion as in Chap. 10, we can say that
</p>
<p>(
ï¿½2min ï¿½ ï¿½2.N ï¿½m/ (full model)
ï¿½2min ï¿½ ï¿½2.N ï¿½mC p/ (&ldquo;nested&rdquo; model):
</p>
<p>(13.3)
</p>
<p>Clearly ï¿½2min &lt; ï¿½
2
min because of the additional free parameters used in the
</p>
<p>determination of ï¿½2min. A lower value of ï¿½
2
min does not necessarily mean that the
</p>
<p>additional parameters of the general model are required. The nested model can in
fact achieve an equal or even better fit relative to the parent distribution of the fit
statistic, i.e., a lower ï¿½2red, because of the larger number of degrees of freedom. In
general, a model with fewer parameters is to be preferred to a model with larger
number of parameters because of its more economical description of the data,
provided that it gives an acceptable fit.
</p>
<p>In Sect.10.3 we discussed that, when comparing the true value of the fit statistic
ï¿½2true for the parent model to the minimum ï¿½
</p>
<p>2
min obtained by minimizing a set of p
</p>
<p>free parameters, 
ï¿½2 D ï¿½2true ï¿½ ï¿½2min and ï¿½2min are independent of one another, and
that 
ï¿½2 is distributed like ï¿½2 with p degrees of freedom. There are situations in
which the same properties apply to the two ï¿½2 statistics described in (13.3), such</p>
<p/>
</div>
<div class="page"><p/>
<p>13.1 The F Test 215
</p>
<p>that the statistic 
ï¿½2 is distributed like
</p>
<p>
ï¿½2 D ï¿½2min ï¿½ ï¿½2min ï¿½ ï¿½2.p/; (13.4)
</p>
<p>and it is independent of ï¿½2min. One such case of practical importance is precisely the
one under consideration, i.e., when there is a nested model component described by
parameters that are independent of the other model parameters. A typical example
is an additional polynomial term in the fit function, as illustrated in the example
above.
</p>
<p>In this case, the null hypothesis we test is that y.x/ and y.x/ are equivalent models,
i.e., adding the p parameters does not constitute a significant change or improvement
to the model. Under this hypothesis we can use the two independent statistics 
ï¿½2
</p>
<p>and ï¿½2min, and construct a bona fide F statistic as
</p>
<p>F D 
ï¿½
2=p
</p>
<p>ï¿½2min=.N ï¿½m/
: (13.5)
</p>
<p>This statistic tests the null hypothesis using an F distribution with f1 D p, f2 D Nï¿½m
degrees of freedom. A rejection of the hypothesis indicates that the two models
y.x/ and y.x/ are not equivalent. In practice, a rejection constitutes a positive result,
indicating that the additional model parameters in the nested component are actually
needed to fit the data. A common situation is when there is a single additional
model parameter, p D 1, and the corresponding critical values of F are reported
in Table A.8. A discussion of certain practical cases in which additional model
components may obey (13.4) is provided in a research article by Protassov [36].
</p>
<p>Example 13.3 The data of Table 10.1 and Fig. 10.2 are well fit by a linear model,
while a constant model appears not to be a good fit to all measurements. Using only
the middle three measurements, we want to compare the goodness of fit to a linear
model, and that to a constant model, and determine whether the addition of the b
parameter provides a significant improvement to the fit.
</p>
<p>The best-fit linear model has a ï¿½2min D 0:13 which, for f2 D Nï¿½m D 1 degree of
freedom, with a probability to exceed this value of 72 %, i.e., it is an excellent fit. A
constant model has a ï¿½2min D 7:9, which, for 2 degrees of freedom, has a probability
to exceed this value of ï¿½0.01, i.e., it is acceptable at the 99 % confidence level,
but not at the 90 % level. If the analyst requires a level of confidence 
90 %, then
the constant model should be discarded, and no further analysis of the experiment is
needed. If the analyst can accept a 99 % confidence level, we can determine whether
the improvement in ï¿½2 between the constant and the linear model is significant. We
construct the statistic
</p>
<p>F D ï¿½
2
min ï¿½ ï¿½2min
ï¿½2min
</p>
<p>1
</p>
<p>1
D 59:4</p>
<p/>
</div>
<div class="page"><p/>
<p>216 13 Model Comparison
</p>
<p>which, according to Table A.8 for f1 D 1 and f2 D 1, is significant at the 99 %
(and therefore 95 %) confidence level, but not at 90 % or lower. In fact, the critical
value of the F distribution with f1 D 1, f2 D 1 at the 99 % confidence level is
Fcrit D 4; 052. Therefore a data analyst willing to accept a 99 % confidence level
should conclude that the additional model component b is not required, since there
is ï¿½1 % (actually, ï¿½5 %) probability that such an improvement in the ï¿½2 statistic is
due by chance, and not by the fact that the general model is truly a more accurate
description of the data. }
</p>
<p>The example above illustrates the principle of simplicity or parsimony in the
analysis of data. When choosing between two models, both with an acceptable fit
statistic at the same confidence level (in the previous example at the 99 % level),
one should prefer the model with fewer parameters, even if its fit statistic (e.g.,
the reduced ï¿½2min) is inferior to that of the more complex model. This general
guiding principle is sometimes referred to as Occam&rsquo;s razor, after the Middle Ages
philosopher and Franciscan friar William of Occam.
</p>
<p>13.2 Kolmogorov&ndash;Smirnov Tests
</p>
<p>Kolmogorov&ndash;Smirnov tests are a different method for the comparison of a one-
dimensional dataset to a model, or for the comparison of two datasets to one another.
The tests make use of the cumulative distribution function, and are applicable to
measurements of a single variable X, for example to determine if it is distributed
like a Gaussian. For two-variable dataset, the ï¿½2 and F tests remain the most viable
option.
</p>
<p>The greatest advantage the Kolmogorov&ndash;Smirnov test is that it does not require
the data to be binned, and, for the case of the comparison between two dataset, it
does not require any parameterization of the data. These advantages come at the
expense of a more complicated mathematical treatment to find the distribution func-
tion of the test statistic. Fortunately, numerical tables and analytical approximations
make these tests manageable.
</p>
<p>13.2.1 Comparison of Data to a Model
</p>
<p>Consider a random variable X with cumulative distribution function F.x/. The data
consist of N measurements, and for simplicity we assume that they are in increasing
order, x1 
 x2 
 : : : 
 xN . This condition can be achieved by re-labelling the
measurements, which preserves the statistical properties of the data. The goal is to
construct a statistic that describes the difference between the sample distribution of
the data and a specified distribution, to test whether the data are compatible with
this distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Kolmogorov&ndash;Smirnov Tests 217
</p>
<p>Start with the sample cumulative distribution
</p>
<p>FN.x/ D 1
N
</p>
<p>[# of measurements 
 x]: (13.6)
</p>
<p>By definition, 0 
 FN.x/ 
 1. The test statistic we want to use is defined as
DN D max
</p>
<p>x
jFN.x/ï¿½ F.x/j; (13.7)
</p>
<p>where F.x/ is the parent distribution, and the maximum value of the difference
between the parent distribution and the sample distribution is calculated for all
values in the support of X.
</p>
<p>One of the remarkable properties of the statistic DN is that it has the same
distribution for any underlying distribution of X, providedX is a continuous variable.
The proof that DN has the same distribution regardless of the distribution of X
illustrates the properties of the cumulative distribution and of the quantile function
presented in Sect. 4.8.
</p>
<p>Proof We assume that F.x/ is continuous and strictly increasing. This is
certainly the case for a Gaussian distribution, or any other distribution that
does not have intervals where the distribution functions is f .x/ D 0. We make
the change of variables y D F.x/, so that the measurement xk corresponds to
yk D F.xk/. This change of variables is such that
</p>
<p>FN.x/ D .# of xi &lt; x/
N
</p>
<p>D .# of yk &lt; y/
N
</p>
<p>D UN.y/
</p>
<p>where UN.y/ is the sample cumulative distribution of Y and 0 
 y 
 1. The
cumulative distribution of Y is
</p>
<p>U.y/ D P.Y &lt; y/ D P.X &lt; x/ D F.x/ D y:
The fact that the cumulative distribution is U.y/ D y shows that Y is a uniform
distribution between 0 and 1. As a result, the statistic DN is equivalent to
</p>
<p>DN D max
0ï¿½yï¿½1 jUN.y/ï¿½ U.y/j
</p>
<p>where Y is a uniform distribution. Since this is true no matter the original
distribution X, DN has the same distribution for any X. Note that this derivation
relies on the continuity of X, and this assumption must be verified to apply the
resulting Kolmogorov&ndash;Smirnov test. ut
</p>
<p>The distribution function of the statistic DN was determined by Kol-
mogorov in 1933 [25], and it is not easy to evaluate analytically. In the limit
of large N, the cumulative distribution of DN is given by
</p>
<p>lim
N!1P.DN &lt; z=
</p>
<p>p
N/ D
</p>
<p>1X
</p>
<p>rDï¿½1
.ï¿½1/reï¿½2r2z2 ï¿½ Ë.z/: (13.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>218 13 Model Comparison
</p>
<p>Table 13.1 Critical points of
the Kolmogorov distribution
DN for large values of N
</p>
<p>Confidence level
</p>
<p>p
p
NDN
</p>
<p>0:50 0:828
</p>
<p>0:60 0:895
</p>
<p>0:70 0:973
</p>
<p>0:80 1:073
</p>
<p>0:90 1:224
</p>
<p>0:95 1:358
</p>
<p>0:99 1:628
</p>
<p>The functionË.z/ can also be used to approximate the probability distribution
of DN for small values of N, using
</p>
<p>P.DN &lt; z=.
p
N C 0:12C 0:11=pN// ' Ë.z/: (13.9)
</p>
<p>A useful numerical approximation for P.DN &lt; z/ is also provided in [30].
</p>
<p>The probability distribution of DN can be used to test whether a sample
distribution is consistent with a model distribution. Critical values of the DN
distribution with probability p,
</p>
<p>P.DN 
 Tcrit/ D p (13.10)
</p>
<p>are shown in Table 13.1 in the limit of large N. For small N, critical values of the
DN statistic are provided in Table A.25. If the measured value for DN is greater than
the critical value, then the null hypothesis must be rejected, and the data are not
consistent with the model. The test allows no free parameters, i.e., the distribution
that represents the null hypothesis must be fully specified.
</p>
<p>Example 13.4 Consider the data from Thomson&rsquo;s experiment to measure the ratio
m=e of an electron (page 23). We can use the DN statistic to test whether either of
the two measurement of the variable m=e is consistent with a given hypothesis. It is
necessary to realize that the Kolmogorov&ndash;Smirnov test applies to a fully specified
hypothesis H0, i.e., the parent distribution F.x/ cannot have free parameter that are
to be determined by a fit to the data. We use a fiducial hypothesis that the ratio
is described by a Gaussian distribution of ï¿½ D 5:7 (the true value in units of
107 g Coulombï¿½1, though the units are unnecessary for this test), and a variance
of ï¿½2 D 1. Both measurements are inconsistent with this model, as can be seen
from Fig. 13.1. See Problem 13.1 for a quantitative analysis of the results. }</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Kolmogorov&ndash;Smirnov Tests 219
</p>
<p>Fig. 13.1 Kolmogorov&ndash;Smirnov test applied to the measurements of the ratio m=e from Thom-
son&rsquo;s experiments described on page 23. The black line corresponds to the measurements for
Tube 1, and the red line to those of Tube 2 (measurements have been multiplied by 107). The
dot-dashed line is the cumulative distribution of a Gaussian with ï¿½ D 5:7 (the correct value) and
a fiducial variance of ï¿½2 D 1
</p>
<p>13.2.2 Two-Sample Kolmogorov&ndash;Smirnov Test
</p>
<p>A similar statistic can be defined to compare two datasets:
</p>
<p>DNM D max
x
jFM.x/ï¿½ GN.x/j (13.11)
</p>
<p>where FM.x/ is the sample cumulative distribution of a set of M observations, and
GN.x/ that of another independent set of N observations; in this case, there is no
parent model used in the testing. The statistic DNM measures the maximum deviation
between the two cumulative distributions, and by nature it is a discrete distribution.
In this case, we can show that the distribution of the statistic is the same as in (13.9),
provided that the change
</p>
<p>N ! MN
M C N
</p>
<p>is made. This number can be considered as the effective number of datapoints
of the two distributions. For the two-sample Kolmogorov&ndash;Smirnov DNM test we
can therefore use the same table as in the Kolmogorov&ndash;Smirnov one-sample test,
provided N is substituted with MN=.M C N/ and that N and M are both large.</p>
<p/>
</div>
<div class="page"><p/>
<p>220 13 Model Comparison
</p>
<p>As N and M become large, the statistic approaches the following distribution:
</p>
<p>lim
N;M!1P
</p>
<p> 
</p>
<p>DNM &lt; z=
</p>
<p>r
MN
</p>
<p>M C N
</p>
<p>!
</p>
<p>D Ë.z/: (13.12)
</p>
<p>Proof We have already shown that for a sample distribution with M points,
</p>
<p>FM.x/ï¿½ F.x/ D UM.y/ï¿½ U.y/;
</p>
<p>where U is a uniform distribution in (0,1). Since
</p>
<p>FM.x/ï¿½ GN.x/ D FM.x/ ï¿½ F ï¿½ .GN.x/ ï¿½G/;
</p>
<p>where F D G is the parent distribution, it follows that FM.x/ï¿½GN.x/ D UN ï¿½
VN , where UM and VN are the sample distribution of two uniform variables.
Therefore the statistic
</p>
<p>DNM D max
x
jFM.x/ ï¿½GN.x/j
</p>
<p>is independent of the parent distribution, same as for the statistic DN .
Next we show how the factor
</p>
<p>p
1=N C 1=M originates. It is clear that the
</p>
<p>expectation of FM.x/ ï¿½ GN.x/ is zero, at least in the limit of large N and M;
the second moment can be calculated as
</p>
<p>EÅ.FM.x/ï¿½ GN.x//2ï¿½ D EÅ.FM.x/ ï¿½ F.x//2ï¿½
CEÅ.GN.x/ï¿½ G.x//2ï¿½C 2EÅ.FM.x/ ï¿½ F.x//.GN.x/ ï¿½G.x//ï¿½
</p>
<p>D EÅ.FM.x/ï¿½ F.x//2ï¿½C EÅ.GN.x/ï¿½ G.x//2ï¿½
</p>
<p>In fact, since FM.x/ ï¿½ F.x/ is independent of GN.x/ ï¿½ G.x/, their covariance
is zero. Each of the two remaining terms can be evaluated using the following
calculation:
</p>
<p>E
ï¿½
.FM.x/ï¿½ F.x//2
</p>
<p>ï¿½ D E
ï¿½
1
</p>
<p>M
.f# of xi&rsquo;s &lt; xg ï¿½MF.x//2
</p>
<p>ï¿½
</p>
<p>D
</p>
<p>1
</p>
<p>M2
E
ï¿½
.f# of xi&rsquo;s &lt; xg ï¿½ EÅf# of xi&rsquo;s &lt; xgï¿½/2
</p>
<p>ï¿½
:
</p>
<p>For a fixed value of x, the variable {# of xi&rsquo;s &lt; x} is a binomial distribution
in which &ldquo;success&rdquo; is represented by one measurement being &lt; x, and the
probability of success is p D F.x/. The expectation in the equation above is</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Kolmogorov&ndash;Smirnov Tests 221
</p>
<p>therefore equivalent to the variance of a binomial distribution with M tries, for
which ï¿½2 D Mp.1ï¿½ p/, leading to
</p>
<p>E
ï¿½
.FM.x/ ï¿½ F.x//2
</p>
<p>ï¿½ D 1
M
F.x/.1ï¿½ F.x//:
</p>
<p>It follows that
</p>
<p>EÅ.FM.x/ ï¿½GN.x//2ï¿½ D
ï¿½
1
</p>
<p>M
C 1
</p>
<p>N
</p>
<p>ï¿½
</p>
<p>F.x/.1ï¿½ F.x//
</p>
<p>A simple way to make the mean square of FM.x/ï¿½GN.x/ independent of N and
M is to divide it by
</p>
<p>p
1=M C 1=N. This requirement is therefore a necessary
</p>
<p>condition for the variable
p
NM=.N CM/DNM to be independent of N and M.
</p>
<p>Finally, we show that
p
NM=.N CM/DNM is distributed in the same way
</p>
<p>as
p
NDN , at least in the asymptotic limit of large N and M. Using the results
</p>
<p>from the DN distribution derived in the previous section, we start with
</p>
<p>max
x
</p>
<p>Ë
Ë
Ë
Ë
Ë
</p>
<p>r
MN
</p>
<p>M C N .FM.x/ ï¿½GN.x//
Ë
Ë
Ë
Ë
Ë
D max
</p>
<p>0ï¿½yï¿½1
</p>
<p>Ë
Ë
Ë
Ë
Ë
</p>
<p>r
MN
</p>
<p>M C N .UM ï¿½ VN//
Ë
Ë
Ë
Ë
Ë
:
</p>
<p>The variable can be rewritten as
</p>
<p>r
MN
</p>
<p>M C N .UM ï¿½ U C .V ï¿½ VN// D
r
</p>
<p>N
</p>
<p>M C N .
p
M.UM ï¿½U//
</p>
<p>C
r
</p>
<p>M
</p>
<p>M C N .
p
N.VN ï¿½ V//:
</p>
<p>Using the central limit theorem, it can be shown that the two variables Ë Dp
M.UM ï¿½U/ and Ë D
</p>
<p>p
N.VN ï¿½V/ have the same distribution, which tends
</p>
<p>to a Gaussian in the limit of large M. We then write
</p>
<p>r
MN
</p>
<p>M C N .FM.x/ ï¿½GN.x// D
r
</p>
<p>N
</p>
<p>M C N Ë C
r
</p>
<p>M
</p>
<p>M C NË
</p>
<p>and use the property that, for two independent and identically distributed
Gaussian variables Ë and Ë the variable a ï¿½ Ë C b ï¿½ Ë is distributed like Ë,
provided that a2 C b2 D 1. We therefore conclude that, in the asymptotic
limit,
</p>
<p>DNM D max
x
</p>
<p>Ë
Ë
Ë
Ë
Ë
</p>
<p>r
MN
</p>
<p>M C N .FM.x/ ï¿½GN.x//
Ë
Ë
Ë
Ë
Ë
ï¿½ max
</p>
<p>x
</p>
<p>Ë
Ë
Ë
p
N.VN ï¿½ V/
</p>
<p>Ë
Ë
Ë D DN :
</p>
<p>ut</p>
<p/>
</div>
<div class="page"><p/>
<p>222 13 Model Comparison
</p>
<p>Example 13.5 We can use the two-sample Kolmogorov&ndash;Smirnov statistic to com-
pare the data from Tube #1 and Tube #2 of Thomson&rsquo;s experiment to measure the
ratio m=e of an electron (page 23). The result, shown in Fig. 13.1, indicates that the
two measurements are not in agreement with one another. See Problem 13.2 for a
quantitative analysis of this test. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ F Test: A test to compare two independent ï¿½2 measurements,
</p>
<p>F D ï¿½21;red=ï¿½22;red:
</p>
<p>ï¿½ F Test for additional component: The significance of an additional model
component with p parameters can be tested using
</p>
<p>F D 
ï¿½
2=p
</p>
<p>ï¿½2min=.N ï¿½m/
when the additional component is nested within the general model.
</p>
<p>ï¿½ Kolmogorov&ndash;Smirnov test: A non-parametric test to compare a one-
variable dataset to a model or two datasets with one another.
</p>
<p>Problems
</p>
<p>13.1 Using the data from Thomson&rsquo;s experiment at page 23, determine the values
of the Kolmogorov&ndash;Smirnov statistic DN for the measurement of Tube #1 and Tube
#2, when compared with a Gaussian model for the measurement with ï¿½ D 5:7 and
ï¿½2 D1. Determine at what confidence level you can reject the hypothesis that the
two measurements are consistent with the model.
</p>
<p>13.2 Using the data from Thomson&rsquo;s experiment at page 23, determine the values
of the two-sample Kolmogorov&ndash;Smirnov statistic DNM for comparison between the
two measurements. Determine at what confidence level you can reject the hypothesis
that the two measurements are consistent with one another.
</p>
<p>13.3 Using the data of Table 10.1, determine whether the hypothesis that the last
three measurements are described by a simple constant model can be rejected at the
99 % confidence level.
</p>
<p>13.4 A given dataset with N D 5 points is fit to a linear model, for a fit statistic of
ï¿½2min. When adding an additional nested parameter to the fit, p D 1, determine by
how much should the ï¿½2min be reduced for the additional parameter to be significant
at the 90 % confidence level.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Kolmogorov&ndash;Smirnov Tests 223
</p>
<p>13.5 A dataset is fit to model 1, with minimum ï¿½2 fit statistic of ï¿½21 D 10 for 5
degrees of freedom; the same dataset is also fit to another model, with ï¿½22 D 5 for 4
degrees of freedom. Determine which model is acceptable at the 90 % confidence,
and whether the F test can be used to choose one of the two models.
</p>
<p>13.6 A dataset of size N is successfully fit with a model, to give a fit statistic ï¿½2min. A
model with a nested component with 1 additional independent parameter for a total
of m parameters is then fit to ï¿½2min, providing a reduction in the fit statistic of 
ï¿½
</p>
<p>2.
Determine what is the minimum
ï¿½2 that, in the limit of a large number of degrees
of freedom, provides 90 % confidence that the additional parameter is significant.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 14
Monte Carlo Methods
</p>
<p>Abstract The term Monte Carlo refers to the use of random variables to evaluate
quantities such as integrals or parameters of fit functions that are typically too
complex to evaluate via other analytic methods. This chapter presents elementary
Monte Carlo methods that are of common use in data analysis and statistics,
in particular the bootstrap and jackknife methods to estimate parameters of fit
functions.
</p>
<p>14.1 What is a Monte Carlo Analysis?
</p>
<p>The term Monte Carlo derives from the name of a locality in the Principality of
Monaco known for its resorts and casinos. In statistics and data analysis Monte
Carlo is an umbrella word that means the use of computer-aided numerical methods
to solve a specific problem, typically with the aid of random numbers.
</p>
<p>Traditional Monte Carlo methods include numerical integration of functions that
can be graphed but that don&rsquo;t have a simple analytic solution and simulation of ran-
dom variables using random samples from a uniform distribution. Another problem
that benefits by the use of random numbers is the estimation of uncertainties in the
best-fit parameters of analytical models used to fit data. There are cases when an
analytical solution for the error in the parameters is not available. In many of those
cases, the bootstrap or the jackknife methods can be used to obtain reliable estimates
for those uncertainties.
</p>
<p>Among many other applications, Monte Carlo Markov chains stand out as a class
of Monte Carlo methods that is now commonplace across many fields of research.
The theory of Markov chains (Chap. 15) dates to the early twentieth century, yet
only over the past 20 years or so it has found widespread use as Monte Carlo Markov
chains (Chap. 16) because of the computational power necessary to implement the
method.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_14
</p>
<p>225</p>
<p/>
</div>
<div class="page"><p/>
<p>226 14 Monte Carlo Methods
</p>
<p>14.2 Traditional Monte Carlo Integration
</p>
<p>A common numerical task is the evaluation of the integral of a function f .x/ for
which analytic solution is either unavailable or too complicated to calculate exactly,
</p>
<p>I D
Z
</p>
<p>A
f .x/dx: (14.1)
</p>
<p>We want to derive a method to approximate this integral by randomly drawing
N samples from the support A. For simplicity, we assume that the domain of the
variable f .x/ is a subset of real numbers between a and b. We start by drawing
samples from a uniform distribution between these two values,
</p>
<p>g.x/ D
8
&lt;
</p>
<p>:
</p>
<p>1
</p>
<p>b ï¿½ a if a 
 x 
 b
0 otherwise:
</p>
<p>(14.2)
</p>
<p>Recall that for a random variable X with continuous distribution f .x/, the expecta-
tion (or mean value) is defined as
</p>
<p>EÅXï¿½ D
Z 1
</p>
<p>ï¿½1
xg.x/dx (14.3)
</p>
<p>(2.6); we have also shown that the mean can be approximated as
</p>
<p>EÅXï¿½ ' 1
N
</p>
<p>NX
</p>
<p>iD1
xi
</p>
<p>where xi are independent measurements of that variable. The expectation of the
function f .x/ of a random variable is
</p>
<p>EÅ f .x/ï¿½ D
Z 1
</p>
<p>ï¿½1
f .x/g.x/dx;
</p>
<p>and it can be estimated using the Law of Large Numbers (Sect. 4.5):
</p>
<p>EÅ f .x/ï¿½ ' 1
N
</p>
<p>NX
</p>
<p>iD1
f .xi/: (14.4)
</p>
<p>These equations can be used to approximate the integral in (14.1) as a simple sum:
</p>
<p>I D .b ï¿½ a/
Z b
</p>
<p>a
f .x/g.x/dx D .b ï¿½ a/EÅ f .x/ï¿½ ' .b ï¿½ a/ 1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
f .xi/: (14.5)
</p>
<p>Equation (14.5) can be implemented by drawing N random uniform samples xi from
the support, then calculating f .xi/, and evaluating the sum. This is the basic Monte
Carlo integration method, and it can be easily implemented by using a random
number generator available in most programming languages.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Dart Monte Carlo Integration and Function Evaluation 227
</p>
<p>The method can be generalized to more than one dimension; if the support A ï¿½
R
</p>
<p>n has volume V , then the integration of an n-dimensional function f .x/ is given by
the following sum:
</p>
<p>I D V
N
</p>
<p>NX
</p>
<p>iD1
f .xi/ (14.6)
</p>
<p>It is clear that the precision in the evaluation of the integral depends on the
number of samples drawn. The error made by this method of integration can be
estimated using the following interpretation of (14.6): the quantity Vf .x/ is the
random variable of interest, and I is the expected value. Therefore, the variance
of the random variable is given by the usual expression,
</p>
<p>ï¿½2I D
V2
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
. f .xi/ ï¿½ Nf /2: (14.7)
</p>
<p>This means that the relative error in the calculation of the integral is
</p>
<p>ï¿½I
</p>
<p>I
D 1p
</p>
<p>N
</p>
<p>q
PN
</p>
<p>iD1. f .xi/ ï¿½ Nf /2
PN
</p>
<p>iD1 f .xi/
/ 1p
</p>
<p>N
I (14.8)
</p>
<p>as expected, the relative error decreases like the square root of N, same as for a
Poisson variable. Equation (14.8) can be used to determine how many samples are
needed to estimate an integral with a given precision.
</p>
<p>14.3 Dart Monte Carlo Integration and Function Evaluation
</p>
<p>Another method to integrate a function, or to perform related mathematical opera-
tions, can be shown by way of an example. Assume that we want to measure the area
of a circle of radiusR. One can draw a random sample of N values in the .x; y/ plane,
as shown in Fig. 14.1, and count all the points that fall within the circle, N.R/. The
area of the circle, or any other figure with known analytic function, is accordingly
estimated as
</p>
<p>A D N.R/
N
	 V (14.9)
</p>
<p>in which V is the volume sampled by the two random variables. In the case of a
circle of radius R D 1 we have V D 4, and since the known area is A D 	R2, this
method provides an approximation to the number 	 .</p>
<p/>
</div>
<div class="page"><p/>
<p>228 14 Monte Carlo Methods
</p>
<p>Fig. 14.1 Monte Carlo method to perform a calculation of the area of a circle (also a simulation
of the number 	), with N D 1000 iterations
</p>
<p>Notice that (14.9) is equivalent to (14.6), in which the sum
P
</p>
<p>f .xi/ becomes
N.R/, where f .xi/ D 1 indicates that a given random data point xi falls within the
boundaries of the figure of interest.
</p>
<p>Example 14.1 (Simulation of the Number 	) Figure 14.1 shows a Monte Carlo
simulation of the number 	 , using 1000 random numbers drawn in a box of linear
size 2, encompassing a circle of radius R D 1. The simulation has a number N.R/ D
772 of points within the unit circle, resulting in an estimate of the area of the circle
as 	R2 D 0:772 	 4 D 3:088. Compared with the exact result of 	 D 3:14159, the
simulation has an error of 1.7 %. According to (14.8), a 1000 iteration simulation has
an expected relative error of order 3.1 %, therefore the specific simulation reported
in Fig. 14.1 is consisted with the expected error, and more numbers must be drawn
to improve the precision. }
</p>
<p>14.4 Simulation of Random Variables
</p>
<p>A method for the simulation of a random variable was discussed in Sect. 4.8. Since
the generation of random samples from a uniform random variable was involved,
this method also falls under the category of Monte Carlo simulations.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Simulation of Random Variables 229
</p>
<p>The method is based on (4.42):
</p>
<p>X D Fï¿½1.U/;
</p>
<p>in which Fï¿½1 represents the inverse of the cumulative distribution of the target
variable X, and U represents a uniform random variable between 0 and 1. In
Sect. 4.8 we provided the examples on how to use (4.42) to simulate an exponential
distribution, which has a simple analytic function for its cumulative distribution.
</p>
<p>The Gaussian distribution is perhaps the most common variable in many
statistical applications, and its generation cannot be accomplished by (4.42), since
the cumulative distribution is a special function and F.x/ does not have a close form.
A method to overcome this limitation was discussed in Sect. 4.8.2, and it consists of
using two uniform random variables U and V to simulate two standard Gaussians X
and Y of zero mean and unit variance via (4.45),
</p>
<p>(
X D pï¿½2 ln.1 ï¿½U/ ï¿½ cos.2	V/
Y D pï¿½2 ln.1 ï¿½U/ ï¿½ sin.2	V/: (14.10)
</p>
<p>A Gaussian X0 of mean ï¿½ and variance ï¿½2 is related to the standard Gaussian X
by the transformation
</p>
<p>X D X
0 ï¿½ ï¿½
ï¿½
</p>
<p>;
</p>
<p>and therefore it can be simulated via
</p>
<p>X0 D
ï¿½p
ï¿½2 ln.1 ï¿½U/ ï¿½ cos.2	V/
</p>
<p>	
ï¿½ C ï¿½: (14.11)
</p>
<p>Figure 14.2 shows a simulation of a Gaussian distribution function using (14.11).
Precision can be improved with increasing number of samples.
</p>
<p>Fig. 14.2 Monte Carlo
simulation of the probability
distribution function of a
Gaussian of ï¿½ D 1 and
ï¿½2 D 2 using 1000 samples
according to (14.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>230 14 Monte Carlo Methods
</p>
<p>14.5 Monte Carlo Estimates of Errors for Two-Variable
Datasets
</p>
<p>The two methods presented in this section, the bootstrap and the jackknife, are
among the most common techniques to estimate best-fit parameters and their uncer-
tainties in the fit to two-variable datasets. We have seen in previous chapters that the
best-fit parameters and their uncertainties can be estimated analytically, for example,
in the case of a linear regression with known errors in the dependent variable. In
those cases, the exact analytical solution is typically the most straightforward to
implement. When the analytic solution to a maximum likelihood fit is unavailable,
then ï¿½2 minimization followed by the ï¿½2min C 
ï¿½2 criterion can also be used to
measure best-fit values and uncertainties in the parameters. Finally, Markov chain
Monte Carlo methods to be presented in Chap. 16 can also be used in virtually any
case for which the likelihood can be calculated.
</p>
<p>The two methods presented in this section have a long history of use in statistical
data analysis, and had been in use since well before the Markov chain Monte Carlo
methods became of wide use. The bootstrap and jackknife methods are typically
easier to implement than a Monte Carlo Markov chain. In particular, the bootstrap
uses a large number of repetitions of the dataset, and therefore is computer intensive;
the older jackknife method instead uses just a small number of additional random
datasets, and requires less computing resources.
</p>
<p>14.5.1 The Bootstrap Method
</p>
<p>Consider a dataset Z composed of N measurements of either a random variable or,
more generally, a pair of variables. The bootstrap method consists of generating
as large a number of random, &ldquo;synthetic&rdquo; datasets based on the original set. Each
set is then used to determine the distribution of the random variable (e.g., for the
one-dimensional case) or of the best-fit parameters for the y.x/ model (for the two&ndash;
dimensional case). The method has the following steps:
</p>
<p>1. Draw at random N datapoints from the original set Z, with replacement, to form
a synthetic dataset Zi. The new dataset has therefore the same dimension as the
original set, but a few of the original points may be repeated, and a few missing.
</p>
<p>2. For each dataset Zi, calculate the parameter(s) of interest ai. For example, the
parameters can be calculated using a ï¿½2 minimization technique.
</p>
<p>3. Repeat this process as many times as possible, say Nboot times.
4. At the end of the process, the parameters an, n D 1; : : : ;Nboot, approximate the
</p>
<p>posterior distribution of the parameter of interest. These values can therefore
be used to construct the sample distribution function for the parameters, and
therefore obtain the best-fit value and confidence intervals.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Monte Carlo Estimates of Errors for Two-Variable Datasets 231
</p>
<p>Notice that one advantage of the bootstrap method is that it can be used even in cases
in which the errors on the datapoints are not available, which is a very common
occurrence. In this situation, the direct maximum likelihood method applied to the
original set Z alone would not provide uncertainties in the best-fit parameters, as
explained in Sect. 8.5. Since at each iteration the best-fit parameters alone must
be evaluated, a dataset without errors in the dependent variable can still be fit to
find the best-fit parameters, and the bootstrap method will provide an estimate of
the uncertainties. This is one of the main reasons why the bootstrap method is so
common.
</p>
<p>Example 14.2 (Bootstrap Analysis of Hubble&rsquo;s Data) We perform a bootstrap anal-
ysis on the data from Hubble&rsquo;s experiment of page 157. The dataset Z consists of the
ten measurements of the magnitude m and logarithm of the velocity log v, as shown
in Fig. 8.2. We generate 10,000 random synthetic datasets of ten measurements each,
for which typically a few of the original datapoints are repeated. Given that error
bars on the dependent variable log v were not given, we assume that the uncertainties
have a common value for all measurement (and therefore the value of the error is
irrelevant for the determination of the best-fit parameters). For each dataset Zi we
perform a linear regression to obtain the best-fit values of the parameters ai and bi.
</p>
<p>The sample distributions of the parameters are shown in Fig. 14.3; from them,
we can take the median of the distribution as the &ldquo;best-fit&rdquo; value for the parameter,
and the 68 % confidence interval as the central range of each parameter that
contains 68 % of the parameter occurrences. It is clear that both distributions are
somewhat asymmetric; the situation does not improve with a larger number of
bootstrap samples, since there is only a finite number of synthetic datasets that
</p>
<p>Fig. 14.3 Monte Carlo bootstrap method applied to the data from Hubble&rsquo;s experiment. (Left)
Sample distribution of parameter a, with a median of a D 0:54 and a 68 % central range of 0.45&ndash;
0.70. (Right) Distribution of b, with median b D 0:197 and a central range of 0.188&ndash;0.202. The
best-fit values of the original dataset Z were found to be a D 0:55 and b D 0:197 (see page 159)</p>
<p/>
</div>
<div class="page"><p/>
<p>232 14 Monte Carlo Methods
</p>
<p>can be generated at random, with replacement, from the original dataset (see
Problem 14.1). }
</p>
<p>A key feature of the bootstrap method is that it is an unbiased estimator for the
model parameters. We can easily prove this general property in the special case of a
one-dimensional dataset, with the goal of estimating the sample mean and variance
of the random variable X from N independent measurements. It is clear that we
would normally not use the bootstrap method in this situation, since (2.8) and (5.4)
provide the exact solution to the problem. The following proof is used to show that
the bootstrap method provides unbiased estimates for the mean and variance of a
random variable.
Proof The sample average calculated for a given bootstrap dataset Zi is given
by
</p>
<p>Nxi D 1
N
</p>
<p>NX
</p>
<p>jD1
xjnji (14.12)
</p>
<p>where nji is the number of occurrence of datapoint xj in the synthetic set Zi. If
nji D 0 it means that xj was not selected for the set, nji D 1 it means that there
is just one occurrence of xj (as in the original set), and so on. The number
nji 
 N, and it is a random variable that is distributed like a binomial with
p D 1=N, since the drawing for each bootstrap set is done at random, and with
replacement. Therefore, we find that
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>EÅnjiï¿½ D Np D 1
Var.nij/ ï¿½ ï¿½2i D Np.1 ï¿½ p/ D
</p>
<p>N ï¿½ 1
N
</p>
<p>(14.13)
</p>
<p>where the expectation is calculated for a given dataset Z, drawing a large
number of bootstrap sample based on that specific set. It follows that Nxi is
an unbiased estimator of the sample mean,
</p>
<p>EÅ Nxiï¿½ D 1
N
</p>
<p>NX
</p>
<p>jD1
xjEÅnjiï¿½ D Nx: (14.14)
</p>
<p>The expectation operator used in the equation above relates to the way in
which a specific synthetic dataset can be drawn, i.e., indicates an &ldquo;average&rdquo;
over a specific dataset. The operation of expectation should also be repeated
to average over all possible datasets Z consisting of N measurements of the
random variable X, and that operation will also result in an expectation that is
equal to the parent mean of X,
</p>
<p>EÅNxï¿½ D ï¿½: (14.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Monte Carlo Estimates of Errors for Two-Variable Datasets 233
</p>
<p>Although we used the same symbol for the expectation of (14.14) and (14.15),
the two operations are therefore different in nature.
</p>
<p>The proof that the variance of the sample mean of dataset Zi is an unbiased
estimator of the parent variance ï¿½2=N is complicated by the fact that the
random variables nij are not independent. In fact, they are related by
</p>
<p>NX
</p>
<p>iD1
nij D N; (14.16)
</p>
<p>and this enforces a negative correlation between the variables that vanishes
only in the limit of very large N. It can be shown that the covariance of the
nij&rsquo;s (say, the covariance between nij and nki, were i &curren; k, and i labels the
dataset) is given by
</p>
<p>ï¿½2jk D ï¿½
1
</p>
<p>N
: (14.17)
</p>
<p>The proof of (14.17) is left as an exercise, and it is based on the use of (14.16),
and (4.3) (see Problem 14.2).
</p>
<p>The variance of Nxi can be calculated using (4.3), since Nxi is a linear
combination of N random variables nij:
</p>
<p>Var. Nxi/ D Var
0
</p>
<p>@
1
</p>
<p>N
</p>
<p>NX
</p>
<p>jD1
xjnji
</p>
<p>1
</p>
<p>A D
</p>
<p>1
</p>
<p>N2
</p>
<p>0
</p>
<p>@
NX
</p>
<p>jD1
x2j ï¿½
</p>
<p>2
i C 2
</p>
<p>NX
</p>
<p>jD1
</p>
<p>NX
</p>
<p>kDjC1
xjxkï¿½
</p>
<p>2
jk
</p>
<p>1
</p>
<p>A D
</p>
<p>1
</p>
<p>N2
</p>
<p>0
</p>
<p>@
N ï¿½ 1
N
</p>
<p>NX
</p>
<p>jD1
x2j ï¿½
</p>
<p>2
</p>
<p>N
</p>
<p>NX
</p>
<p>jD1
</p>
<p>NX
</p>
<p>kDjC1
xjxk
</p>
<p>1
</p>
<p>A
</p>
<p>in which we have used the results of (14.13) and (14.17). Next, we need to
calculate the expectation of this variance, in the sense of varying the dataset Z
itself:
</p>
<p>EÅVar. Nxi/ï¿½ D N ï¿½ 1
N3
</p>
<p>EÅ
NX
</p>
<p>jD1
x2j ï¿½ï¿½
</p>
<p>2
</p>
<p>N3
</p>
<p>0
</p>
<p>@
1
</p>
<p>2
</p>
<p>X
</p>
<p>j&curren;k
EÅxjxkï¿½
</p>
<p>1
</p>
<p>A (14.18)
</p>
<p>The last sum in the equation above is over all pairs .j; k/; the factor 1/2 takes
into account the double-counting of terms such as xjxk and xkxj, and the sum</p>
<p/>
</div>
<div class="page"><p/>
<p>234 14 Monte Carlo Methods
</p>
<p>contains a total of N.N ï¿½ 1/ identical terms. Since the measurements xi, xj are
independent and identically distributed, EÅxixkï¿½ D EÅxjï¿½2, it follows that
</p>
<p>EÅVar. Nxi/ï¿½ D N ï¿½ 1
N2
</p>
<p>

EÅx2i ï¿½ ï¿½ EÅxiï¿½2
</p>
<p>ï¿½ D N ï¿½ 1
N2
</p>
<p>ï¿½2 D N ï¿½ 1
N
</p>
<p>ï¿½2ï¿½
</p>
<p>where ï¿½2 is the variance of the random variable X, and ï¿½2ï¿½ D ï¿½2=N the
variance of the sample mean. The equation states that EÅVar. Nxi/ï¿½ D EÅs2ï¿½,
where s2 is the sample variance of X. We showed in Sect. 5.1.2 that the sample
variance is an unbiased estimator of the variance of the mean, provided it is
multiplied by the known factor N=.N ï¿½ 1/. In practice, when calculating the
variance from the N bootstrap samples, we should use the factor 1=.N ï¿½ 1/
instead of 1=N, as is normally done according to (5.6). ut
</p>
<p>14.5.2 The Jackknife Method
</p>
<p>The jackknife method is an older Monte Carlo method that makes use of just N
resampled datasets to estimate best-fit parameters and their uncertainties. As in the
bootstrap method, we consider a dataset Z of N independent measurements either
of a random variable X or of a pair of random variables. The method consists of the
following steps:
</p>
<p>1. Generate a resampled dataset Zj by deleting the jth element from the dataset. This
resampled dataset has therefore dimension N ï¿½ 1.
</p>
<p>2. Each dataset Zj is used to estimate the parameters of interest. For example, apply
the linear regression method to dataset Zj and find the best-fit values of the linear
model, aj and bj.
</p>
<p>3. The parameters of interest are also calculated from the full-dimensional dataset
Z, as one normally would. The best-fit parameters are called Oa.
</p>
<p>4. For each dataset Zj, define the pseudo-values a?j as
</p>
<p>a?j D N Oa ï¿½ .N ï¿½ 1/aj (14.19)
</p>
<p>5. The jackknife estimate of each parameter of interest and its uncertainty are given
by the following equations:
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>a? D 1
N
</p>
<p>nP
</p>
<p>jD1
a?j
</p>
<p>ï¿½2a? D
1
</p>
<p>N.N ï¿½ 1/
NP
</p>
<p>jD1
.a?j ï¿½ a?/2:
</p>
<p>(14.20)
</p>
<p>To prove that (14.20) provide an accurate estimate for the parameters and their
errors, we apply them to the simple case of the estimate of the mean from a sample of</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Monte Carlo Estimates of Errors for Two-Variable Datasets 235
</p>
<p>N measurements. In this case we want to show that the expectation of the jackknife
estimate of the mean a? is equal to the parent mean ï¿½, and that the expectation of
its variance ï¿½2a? is equal to ï¿½
</p>
<p>2=N.
</p>
<p>Proof For a sample of N measurements of a random variable x, the sample
mean and its variance are given by
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>Nx D 1
N
</p>
<p>NP
</p>
<p>jD1
xi
</p>
<p>s2
</p>
<p>N
D 1
</p>
<p>N.N ï¿½ 1/
NP
</p>
<p>jD1
.xi ï¿½ Nx/2:
</p>
<p>(14.21)
</p>
<p>The proof consists of showing that a?j D xj, so that a? is the sample mean
and ï¿½2a? is the sample variance. The result follows from:
</p>
<p>aj D 1
N ï¿½ 1
</p>
<p>X
</p>
<p>i&curren;j
xi; Oa D 1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
xi
</p>
<p>) a?j DN
1
</p>
<p>N
</p>
<p>NX
</p>
<p>iD1
xi ï¿½ N ï¿½ 1
</p>
<p>N ï¿½ 1
X
</p>
<p>i&curren;j
xi D xj:
</p>
<p>Notice that the factor of 1=.N ï¿½ 1/ was used in the calculation of the sample
variance, according to (5.6). ut
</p>
<p>Example 14.3 In the case of the Hubble experiment of page 157, we can use the
jackknife method to estimate the best-fit parameters of the fit to a linear model of
m versus log v. According to (14.20), we find that a? D 0:52, ï¿½a? D 0:13, and
b? D 0:199, ï¿½b? D 0:008. These estimates are in very good agreement with the
results of the bootstrap method, and those of the direct fit to the original dataset for
which, however, we could not provide uncertainties in the fit parameters. }
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Monte Carlo method: Any numerical method that makes use of random
variables to perform calculations that are too complex to be performed
analytically, such as Monte Carlo integration and &ldquo;dart&rdquo; methods.
</p>
<p>ï¿½ Bootstrap method: A common method to estimate model parameters that
uses a large number of synthetic datasets obtained by re-sampling of the
original data.
</p>
<p>ï¿½ Jackknife method: A simple method to estimate model parameters that uses
just N re-sampled datasets.</p>
<p/>
</div>
<div class="page"><p/>
<p>236 14 Monte Carlo Methods
</p>
<p>Problems
</p>
<p>14.1 Calculate how many synthetic bootstrap datasets can be generated at random
from a dataset Z with N unique datapoints. Notice that the order in which the
datapoints appear in the dataset is irrelevant.
</p>
<p>14.2 For a bootstrap dataset Zj constructed from a set Z of N independent
measurements of a variable X, show that the covariance between the number of
occurrence nji and njk is given by (14.17),
</p>
<p>ï¿½2ik D ï¿½
1
</p>
<p>N
:
</p>
<p>14.3 Perform a numerical simulation of the number 	 , and determine how many
samples are sufficient to achieve a precision of 0.1 %. The first six significant digits
of the number are 	 D 3:14159.
14.4 Perform a bootstrap simulation on the Hubble data presented in Fig. 14.3, and
find the 68 % central confidence ranges on the parameters a and b.
</p>
<p>14.5 Using the data of Problem 8.2, run a bootstrap simulation with N D 1000
iterations for the fit to a linear model. After completion of the simulation, plot the
sample probability distribution function of the parameters a and b, and find the
median and 68 % confidence intervals on the fit parameters. Describe the possible
reason why the distribution of the fit parameters are not symmetric.
</p>
<p>14.6 Use the data of Problem 8.2, but assuming that the errors in the dependent
variable y are unknown. Run a bootstrap simulation with N D 1000 iterations, and
determine the median and 68 % confidence intervals on the parameters a and b to
the fit to a linear model.
</p>
<p>14.7 Using the data of Problem 8.2, assuming that the errors in the dependent
variable y are unknown, estimate the values of a and b to the fit to a linear model
using a jackknife method.
</p>
<p>14.8 Given two uniform random variables U1 and U2 between ï¿½R and CR, as
often available in common programming software, provide an analytic expression
to simulate a Gaussian variable of mean ï¿½ and variance ï¿½2.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 15
Introduction to Markov Chains
</p>
<p>Abstract The theory of Markov chains is rooted in the work of Russian mathe-
matician Andrey Markov and has an extensive body of literature to establish its
mathematical foundations. The availability of computing resources has recently
made it possible to use Markov chains to analyze a variety of scientific data. Monte
Carlo Markov chains are now one of the most popular methods of data analysis.
This chapter presents the key mathematical properties of Markov chains, necessary
to understand its implementation as Monte Carlo Markov chains.
</p>
<p>15.1 Stochastic Processes and Markov Chains
</p>
<p>This section presents key mathematical properties of Markov chains. The treatment
is somewhat theoretical, but necessary to ensure that the applications we make to the
analysis of data are consistent with the mathematics of Markov chains, which can
be very complex. The goal is therefore that of defining and understanding a basic
set of definitions and properties necessary to use Markov chains for the analysis of
data, especially via the Monte Carlo simulations.
</p>
<p>Markov chains are a specific type of stochastic processes, or sequence of random
variables. A typical example of Markov chain is the so-called random walk, in which
at each time step a person randomly takes a step either to the left, or to the right. As
time progresses, the location of the person is the random variable of interest, and the
collection of such random variables forms a Markov chain. The ultimate goal of a
Markov chain is to determine the stationary distribution of the random variable. For
the random walk, where we are interested in knowing the probability that at a given
time the person is located n steps to the right or to the left of the starting point.
</p>
<p>In the typical case of interest for the analysis of data, a dataset Z is fit to a
parametric model. The goal is to create a Markov chain for each parameter of
the model, in such a way that the stationary distribution for each parameter is
the distribution function of the parameter. The chain will therefore result in the
knowledge of the best-fit value of the parameter, and of confidence intervals, making
use of the information provided by the dataset.
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_15
</p>
<p>237</p>
<p/>
</div>
<div class="page"><p/>
<p>238 15 Introduction to Markov Chains
</p>
<p>15.2 Mathematical Properties of Markov Chains
</p>
<p>A stochastic process is defined as a sequence of variables Xt,
</p>
<p>fXt; for tï¿½Tg (15.1)
</p>
<p>where t labels the sequence. The domain for the index t is indicated as T to signify
&ldquo;time.&rdquo; The domain is usually a subset of the real numbers (T ï¿½ R) or of the natural
numbers (T ï¿½ N). As time progresses, the random variables Xt change value, and
the stochastic process describes this evolution.
</p>
<p>A Markov chain is a particular stochastic process that satisfies the following
properties:
</p>
<p>1. The time domain is the natural numbers (T ï¿½ N), and each random variable
Xt can have values in a countable set, e.g., the natural numbers or even an
n-dimensional space (Nn), but not real numbers (Rn). A typical example of a
Markov chain is one in which Xi D n, where both i (the time index) and n (the
value of the random variable) are natural numbers. Therefore a Markov chain
takes the form of
</p>
<p>X1 ! X2 ! X3 ! : : :! Xn ! : : :
</p>
<p>The random variable Xi describes the state of the system at time t D i. The
fact that Markov chains must be defined by way of countable sets may appear
an insurmountable restriction, since it would appear that the natural domain for
an n-parameter space is Rn. While a formal extension of Markov chains to Rn
</p>
<p>is also possible, this is not a complication for any practical application, since
any parameter space can be somehow &ldquo;binned&rdquo; into a finite number of states.
For example, the position of the person in a random walk was &ldquo;binned&rdquo; into a
number of finite (or infinite but countable) positions, and a similar process can
be applied to virtually any parameter of interest for a given model. This means
that the variable under consideration can occupy one of a countable multitude
of states "1, "2,. . . , "n, . . . , and the random variable Xi identifies the state of the
system at time step i, Xi D "n.
</p>
<p>2. A far more important property that makes a stochastic process a Markov chain is
the fact that subsequent steps in the chain are only dependent on the current state
of the chain, and not on any of its previous history. This &ldquo;short memory&rdquo; property
is known as the Markovian property, and it is the key into the construction of
Markov chains for the purpose of data analysis. In mathematical terms, given
the present time t D n, the future state of the chain at t D n C 1 (XnC1)
depends only on the present time (Xn), but not on past history. Much of the efforts
in the construction of a Monte Carlo Markov chain lies in the identification</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Mathematical Properties of Markov Chains 239
</p>
<p>of a transition probability from state "i to state "j between consecutive time
steps,
</p>
<p>pij D P.XnC1 D "j=Xn D "i/: (15.2)
</p>
<p>A Markov chain requires that this probability be time-independent, and therefore
a Markov chain has the property of time homogeneity. In Chap. 16 we will see
how the transition probability takes into account the likelihood of the data Z with
the model.
</p>
<p>The two properties described above result in the fact that Markov chain is a
sequence of states determined by transition probabilities pij (also referred to as
transition kernel) that are fixed in time. The ultimate goal is to determine the
probability to find the system in each of the allowed states. With an eye towards
future applications for the analysis of data, each state may represent values of one
or many parameters, and therefore a Markov chain makes it possible to reconstruct
the probability distribution of the parameters.
</p>
<p>Example 15.1 (Random Walk) The random walk is a Markov chain that represents
the location of a person who randomly takes a step of unit length forward with
probability p, or a step backward with probability q D 1 ï¿½ p (typically p D q D
1=2). The state of the system is defined by the location i at which the person find
itself at time t D n,
</p>
<p>Xn D {Location i along the NC axis}
</p>
<p>where NC indicates all positive and negative integers. For this chain, the time
domain is the set of positive numbers (T D N), and the position can be any negative
or positive integer (NC). The transition probability describes the fact that the person
can only take either a step forward or backward:
</p>
<p>pij D
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>p if j D iC 1, or move forward
q if j D i ï¿½ 1, or move backward
0 otherwise:
</p>
<p>(15.3)
</p>
<p>}
The chain satisfies the Markovian property, since the transition probability depends
only on its present position, and not on previous history.
</p>
<p>Example 15.2 Another case of a Markov chain is a simple model of diffusion,
known as the Ehrenfest chain. Consider two boxes with a total of m balls. At each
time step, one selects a ball at random from either box, and replaces it in the other
box. The state of the system can be defined via the random variable
</p>
<p>Xn D {Number of balls in the first box}:</p>
<p/>
</div>
<div class="page"><p/>
<p>240 15 Introduction to Markov Chains
</p>
<p>The random variable can have only a finite number of values .0; 1; : : : ;m/. At each
time step, the transition probability is
</p>
<p>pij D
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>m ï¿½ i
m
</p>
<p>if j D iC 1 (box had i balls, now has iC 1)
i
</p>
<p>m
if j D iï¿½ 1 (box had i balls, now has iï¿½ 1):
</p>
<p>(15.4)
</p>
<p>For example, in the first case it means that we chose one of m ï¿½ i balls from the
second box. The transition probabilities depend only on the number of balls in the
first box at any given time, and are completely independent of how the box came to
have that many balls. This chain therefore satisfies the Markovian property. }
</p>
<p>15.3 Recurrent and Transient States
</p>
<p>We are interested in knowing how often a state is visited by the chain and, in
particular, whether a given state can be visited infinitely often. Assume that the
system is initially in state "i. We define uk the probability that the system returns to
the initial state in exactly k time steps, and vn the probability that the system returns
to the initial state at time n, with the possibility that it may have returned there other
times prior to n. Clearly, it is true that vn ï¿½ un.
</p>
<p>To determine whether a state is recurrent or transient, we define
</p>
<p>u ï¿½
1X
</p>
<p>nD1
un (15.5)
</p>
<p>as the probability of the system returning the initial state "i for the first time at
some time n. The state can be classified as recurrent or transient according to the
probability of returning to that state:
</p>
<p>(
u D 1 state is recurrentI
u &lt; 1 state is transient:
</p>
<p>(15.6)
</p>
<p>Therefore a recurrent state is one that will certainly be visited again by the chain.
Notice that no indication is given as to the time at which the system will return to
the initial state.
</p>
<p>We also state a few theorems that are relevant to the understanding of recurrent
states. Proofs of these theorems can be found, for example, in the textbook by Ross
[38] or other books on stochastic processes, and are not reported here.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Recurrent and Transient States 241
</p>
<p>Theorem 15.1 With vn the probability that the system returns to a state "i at time n,
</p>
<p>state "i is recurrent &rdquo;
1X
</p>
<p>nD1
vn D 1: (15.7)
</p>
<p>This theorem states that, if the system does return to a given state, then it will
do so infinitely often. Also, since this is a necessary and sufficient condition, any
transient state will not be visited by the chain an infinite number of times. This
means that transient states will not be visited any more after a given time, i.e., they
are only visited during an initial period. The fact that recurrent states are visited
infinitely often means that it is possible to construct a sample distribution function
for recurrent states with a precision that is function of the length of the chain. No
information is, however, provided on the timing of the return to a recurrent state.
</p>
<p>We also introduce the definition of accessible states: a state "j is said to be
accessible from state "i if pij.m/ &gt; 0 for some natural number m, meaning that
there is a non-zero probability of reaching this state from another state in m time
steps. The following theorems establish properties of accessible states, and how the
property of accessibility relates to that of recurrence.
</p>
<p>Theorem 15.2 If a state "j is accessible from a recurrent state "i, then "j is also
recurrent, and "i is accessible from "j.
</p>
<p>This theorem states that once the system reaches a recurrent state, the states
visited previously by the chain must also be recurrent, and therefore will be visited
again infinitely often. This means that recurrent states form a network, or class, of
states that share the property of recurrence, and these are the states that the chain
will sample over and over again as function of time.
</p>
<p>Theorem 15.3 If a Markov chain has a finite number of states, then each state is
accessible from any other state, and all states are recurrent.
</p>
<p>This theorem ensures that all states in a finite chain will be visited infinitely often,
and therefore the chain will sample all states as function of time. This property is
of special relevance for Monte Carlo Markov chain methods in which the states of
the chain are possible values of the parameters. As the chain progresses, all values
of the parameters are accessible, and will be visited in proportion of the posterior
distribution of the parameters.
</p>
<p>Example 15.3 (Recurrence of States of the Random Walk) Consider the random
walk with transition probabilities given by (15.3). We want to determine whether
the initial state of the chain is a recurrent or a transient state for the chain.
The probability of returning to the initial state in k steps is clearly given by the
binomial distribution,
</p>
<p>pii.k/ D
(
0 if k is odd
</p>
<p>C.n; k/pnqn if k D 2n is even (15.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>242 15 Introduction to Markov Chains
</p>
<p>where
</p>
<p>C.n; k/ D
 
k
</p>
<p>n
</p>
<p>!
</p>
<p>D kÅ 
.k ï¿½ n/Å nÅ  (15.9)
</p>
<p>is the number of combinations [of n successes out of k D 2n tries, see (3.3)]. Using
Stirling&rsquo;s approximation for the factorial function in the binomial coefficient,
</p>
<p>nÅ  ' p2	nnneï¿½n;
</p>
<p>the probability to return at time k D 2n to the initial state becomes
</p>
<p>vk D pii.k/ D
 
2n
</p>
<p>n
</p>
<p>!
</p>
<p>pnqn D .2n/Å 
.nÅ /2
</p>
<p>pnqn '
p
4	n
</p>
<p>2	n
</p>
<p>.2n/2neï¿½2n
</p>
<p>n2neï¿½2n
pnqn D .4pq/
</p>
<p>n
</p>
<p>p
	n
</p>
<p>which holds only for k even.
This equation can be used in conjunction with Theorem 15.1 to see if the initial
</p>
<p>state is transient or recurrent. Consider the series
</p>
<p>1X
</p>
<p>nD1
vn D
</p>
<p>1X
</p>
<p>nD1
</p>
<p>1p
	n
.4pq/n:
</p>
<p>According to Theorem 15.1, the divergence of this series is a necessary and
sufficient condition to prove that the initial state is recurrent.
</p>
<p>(a) p &curren; q. In this case, x D 4pq &lt; 1 and
1X
</p>
<p>nD1
</p>
<p>1p
	n
.4pq/n &lt;
</p>
<p>1X
</p>
<p>nD1
xn D x
</p>
<p>1 ï¿½ x I
</p>
<p>since x &lt; 1, the series converges and therefore the state is transient. This means
that the system may return to the initial state, but only for a finite number of
times, even after an infinite time. Notice that as time progresses the state of the
system will drift in the direction that has a probability&gt; 1=2.
</p>
<p>(b) p D q D 1=2, thus 4pq D 1. The series becomes
</p>
<p>1p
	
</p>
<p>1X
</p>
<p>nD1
</p>
<p>1
</p>
<p>n1=2
: (15.10)
</p>
<p>It can be shown (see Problem 15.2) that this series diverges, and therefore a random
walk with the same probability of taking a step to the left or to the right will return
to the origin infinitely often. }</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Limiting Probabilities and Stationary Distribution 243
</p>
<p>15.4 Limiting Probabilities and Stationary Distribution
</p>
<p>The ultimate goal of a Markov chain is to calculate the probability that a system
occupies a given state "i after a large number n of steps. This probability is called
the limiting probability. According to the frequentist approach defined in (1.2), it is
given by
</p>
<p>p?j D limn!1 pj.n/; (15.11)
</p>
<p>where pj.n/ is the probability of the system to be found in state "j at time t D n.
With the aid of the total probability theorem, the probability of the system to be in
state ï¿½j at time t D n is
</p>
<p>pj.n/ D
X
</p>
<p>k
</p>
<p>pk.n ï¿½ 1/pkj: (15.12)
</p>
<p>In fact pk.n ï¿½ 1/ represents the probability of being in state "k at time n ï¿½ 1, and
the set of probabilities pk.n ï¿½ 1/ forms a set of mutually exclusive events which
encompasses all possible outcomes, with the index k running over all possible states.
This formula can be used to calculate recursively the probability pj.n/ using the
probability at the previous step and the transition probabilities pkj, which do not
vary with time.
</p>
<p>Equation (15.12) can be written in a different form if the system is known to
be in state "i at an initial time t D 0:
</p>
<p>pij.n/ D P.Xn D "j/ D
X
</p>
<p>k
</p>
<p>pik.n ï¿½ 1/pkj (15.13)
</p>
<p>where pij.n/ is the probability of the system going from state "i to "j in n time
steps.
</p>
<p>The probabilities pj.n/ and pij.n/ change as the chain progresses. The
limiting probabilities p?j , on the other hand, are independent of time, and
they form the stationary distribution of the chain. General properties for
the stationary distribution can be given for Markov chains that have certain
specific properties. In the following we introduce additional definitions that
are useful to characterize Markov chains, and to determine the stationary
distribution of the chain.
</p>
<p>A number of states that are accessible from each other, meaning there is
a non-zero probability to reach one state from the other (pij &gt; 0), are said
to communicate, and all states that communicate are part of the same class.
The property of communication ($) is an equivalence relation, meaning that
it obeys the following three properties:</p>
<p/>
</div>
<div class="page"><p/>
<p>244 15 Introduction to Markov Chains
</p>
<p>(a) The reflexive property: i$ i;
(b) The symmetric property: if i$ j, then j$ i; and
(c) The transitive property: if i $ j and j $ k, then i $ k. Therefore, each
</p>
<p>class is separate from any other class of the same chain. A chain is said
to be irreducible if it has only one class, and thus all states communicate
with each other.
</p>
<p>Another property of Markov chains is periodicity. A state is said to be
periodic with period T if pii.n/ D 0 when n is not divisible by T, and T is the
largest such integer with this property. This means that the return to a given
state must occur in multiples of T time steps. A chain is said to be aperiodic if
T D 1, and return to a given state can occur at any time. It can be shown that
all states in a class share the same period.
</p>
<p>The uniqueness of the stationary distribution and an equation that can be
used to determine it are established by the following theorems.
</p>
<p>Theorem 15.4 An irreducible aperiodic Markov chain belongs to either of
the following two classes:
</p>
<p>1. All states are positive recurrent. In this case, p?i D 	i is the stationary
distribution, and this distribution is unique.
</p>
<p>2. All states are transient or null recurrent; in this case, there is no stationary
distribution.
</p>
<p>This theorem establishes that a &ldquo;well behaved&rdquo; Markov chain, i.e., one with
positive recurrent states, does have a stationary distribution, and that this
distribution is unique. Positive recurrent states, defined in Sect. 15.3, are those
for which the expected time to return to the same state is finite, while the time
to return to a transient or null recurrent state is infinite. This theorem also
ensures that, regardless of the starting point of the chain, the same stationary
distribution will eventually be reached.
</p>
<p>Theorem 15.5 The limiting probabilities are the solution of the system of linear
equations
</p>
<p>p?j D
NX
</p>
<p>iD1
p?i pij: (15.14)
</p>
<p>Proof According to the recursion formula (15.12),
</p>
<p>pj.n/ D
NX
</p>
<p>iD1
pi.n ï¿½ 1/pij: (15.15)
</p>
<p>Therefore the result follows by taking the limit n!1 of the above equation.
ut</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Limiting Probabilities and Stationary Distribution 245
</p>
<p>If we consider a chain with a probability distribution at a time t0 that
satisfies
</p>
<p>pj.t0/ D
NX
</p>
<p>iD1
pi.t0/pij; (15.16)
</p>
<p>then the probability distribution of the states satisfies (15.14), and the chain
has reached the stationary distribution pj.n/ D p?j . Theorem 15.5 guarantees
that, from that point on, the chain will maintain its stationary distribution.
The importance of a stationary distribution is that, as time elapses, the
chain samples this distribution. The sample distribution of the chain, e.g., a
hystogram plot of the occurrence of each state, can therefore be used as an
approximation of the posterior distribution.
</p>
<p>Example 15.4 (Stationary Distribution of the Ehrenfest Chain) We want to find a
distribution function p?j that is the stationary distribution of the Ehrenfest chain. This
case is of interest because the finite number of states makes the calculation of the
stationary distribution easier to achieve analytically. The condition for a stationary
distribution is
</p>
<p>p?j D
NX
</p>
<p>iD1
p?i pij
</p>
<p>where N is the number of states of the chain. The condition can also be written in
matrix notation. Recall that the transition probabilities for the Ehrenfest chain are
</p>
<p>pij D
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>m ï¿½ i
m
</p>
<p>if j D iC 1
i
</p>
<p>m
if j D iï¿½ 1;
</p>
<p>and they can be written as a transition matrix P
</p>
<p>P D Åpijï¿½ D
</p>
<p>2
</p>
<p>6
6
6
6
6
6
6
4
</p>
<p>0 1 0 0 : : : 0 0
1
</p>
<p>m
0
</p>
<p>m ï¿½ 1
m
</p>
<p>0 : : : 0 0
</p>
<p>0
2
</p>
<p>m
0
</p>
<p>m ï¿½ 2
m
</p>
<p>: : : 0 0
</p>
<p>. . . . . . . . . . . .
0 0 0 0 : : : 1 0
</p>
<p>3
</p>
<p>7
7
7
7
7
7
7
5
</p>
<p>: (15.17)
</p>
<p>Notice that the sum of each line is one, since
</p>
<p>X
</p>
<p>j
</p>
<p>pij D 1</p>
<p/>
</div>
<div class="page"><p/>
<p>246 15 Introduction to Markov Chains
</p>
<p>is the probability of going from state "i to any state "j. In (15.17) you can regard the
vertical index to be i D 0; : : : ;m, and the horizontal index j D 0; : : : ;m.
</p>
<p>The way in which we typically use (15.14) is simply to verify whether a
distribution is the stationary distribution of the chain. In the case of the Ehrenfest
chain, we try the binomial distribution as the stationary distribution,
</p>
<p>pi D
 
m
</p>
<p>i
</p>
<p>!
</p>
<p>piqmï¿½i i D 0; : : : ;m;
</p>
<p>in which p and q represent the probability of finding a ball in either box. At
equilibrium one expects p D q D 1=2, since even an initially uneven distribution of
balls between the two boxes should result in an even distribution at later times. It is
therefore reasonable to expect that the probability of having i balls in the first box,
out of a total of m, is equivalent to that of i positive outcomes in a binary experiment.
</p>
<p>To prove this hypothesis, consider p D Åp0; p1; : : : ; pmï¿½ as a row vector of
dimension mC 1, and verify the equation
</p>
<p>p D pP; (15.18)
</p>
<p>which is the matrix notation for the condition of a stationary distribution. For the
Erhenfest chain, this condition is
</p>
<p>Åp0; p1; : : : ; pmï¿½ D Åp0; p1; : : : ; pmï¿½
</p>
<p>2
</p>
<p>6
6
6
6
6
6
6
4
</p>
<p>0 1 0 0 : : : 0 0
1
</p>
<p>m
0
</p>
<p>m ï¿½ 1
m
</p>
<p>0 : : : 0 0
</p>
<p>0
2
</p>
<p>m
0
</p>
<p>m ï¿½ 2
m
</p>
<p>: : : 0 0
</p>
<p>. . . . . . . . . . . .
0 0 0 0 : : : 1 0
</p>
<p>3
</p>
<p>7
7
7
7
7
7
7
5
</p>
<p>:
</p>
<p>For a given state i, only two terms (at most) contribute to the sum,
</p>
<p>pi D piï¿½1piï¿½1;i C piC1piC1;i: (15.19)
</p>
<p>From this we can prove that the p D q D 1=2 binomial is the stationary distribution
of the Ehrenfest chain (see Problem 15.1). }</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Limiting Probabilities and Stationary Distribution 247
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Markov chain: A stochastic process or sequence of random variables as
function of an integer time variable.
</p>
<p>ï¿½ Markovian property: It is the key property of Markov chains, stating that
the state of the system at a given time depends only on the state at the
previous time step.
</p>
<p>ï¿½ Recurrent and transient state: A recurrent state occurs infinitely often
while a transient state only occurs a finite number of times in the Markov
chain.
</p>
<p>ï¿½ Stationary distribution: It is the asymptotic distribution of each variable,
obtained after a large number of time steps of the Markov chain. When
the variable represents a model parameter, the stationary distribution is the
posterior distribution of the parameter.
</p>
<p>Problems
</p>
<p>15.1 Consider the Markov chain for the Ehrenfest chain described in Example 15.4.
Show that the stationary distribution is the binomial with p D q D 1=2.
15.2 Show that the random walk with p D q D 1=2 (15.10) returns to the origin
infinitely often, and therefore the origin is a recurrent state of the chain.
</p>
<p>15.3 For the random walk with p &curren; p, show that the origin is a transient state.
15.4 Assume that the diffusion model of Example 15.2 is modified in such a way
that at each time step one has the option to choose one box at random from which
to replace a ball to the other box.
</p>
<p>(a) Determine the transition probabilities pij for this process.
(b) Determine whether this process is a Markov chain.
</p>
<p>15.5 Using the model of diffusion of Problem 15.4, determine if the binomial
distribution with p D q D 1=2 is the stationary distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>Chapter 16
Monte Carlo Markov Chains
</p>
<p>Abstract Monte Carlo Markov Chains (MCMC) are a powerful method to ana-
lyze scientific data that has become popular with the availability of modern-day
computing resources. The basic idea behind an MCMC is to determine the
probability distribution function of quantities of interest, such as model parameters,
by repeatedly querying datasets used for their measurement. The resulting sequence
of values form a Markov chain that can be analyzed to find best-fit values and
confidence intervals. The modern-day data analyst will find that MCMCs are an
essential tool that permits tasks that are simply not possible with other methods,
such as the simultaneous estimate of parameters for multi-parametric models of
virtually any level of complexity, even in the presence of correlation among the
parameters.
</p>
<p>16.1 Introduction to Monte Carlo Markov chains
</p>
<p>A typical data analysis problem is the fit of data to a model with adjustable
parameters. Chapter 8 presented the maximum likelihood method to determine the
best-fit values and confidence intervals for the parameters. For the linear regression
to a two-variable dataset, in which the independent variable is assumed to be known
and the dependent variable has errors associated with its measurements, we found
an analytic solution for the best-fit parameters and its uncertainties (Sect. 8.3). Even
the case of a multiple linear regressions is considerably more complex to solve
analytically (Chap. 9) and most fits to non-linear functions do not have analytic
solutions at all.
</p>
<p>When an analytic solution is not available, the ï¿½2min method to search for
best-fit parameters and their confidence intervals is still applicable, as described
in Sect. 10.3. The main complication is the computational cost of sampling the
parameter space in search of ï¿½2min and surfaces of constant ï¿½
</p>
<p>2
min C 
ï¿½2. Consider,
</p>
<p>for example, a model with 10 free parameters: even a very coarse sampling of 10
values for each parameter will result in 1010 evaluations of the likelihood, or ï¿½2, to
cover the entire parameter space. Moreover, it is not always possible to improve the
situation by searching for just a few interesting parameters at a time, e.g., fixing the
value of the background while searching for the flux of the source. In fact, there may
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4_16
</p>
<p>249</p>
<p/>
</div>
<div class="page"><p/>
<p>250 16 Monte Carlo Markov Chains
</p>
<p>be correlation among parameters and this requires that the parameters be estimated
simultaneously.
</p>
<p>The Monte Carlo Markov chain (MCMC) methods presented in this chapter
provide a way to bypass altogether the need for a uniform sampling of parameter
space. This is achieved by constructing a Markov chain that only samples the
interesting region of parameters space, i.e., the region near the maximum of the
likelihood. The method is so versatile and computationally efficient that MCMC
techniques have become the leading analysis method in many fields of data analysis.
</p>
<p>16.2 Requirements and Goals of a Monte Carlo
Markov Chain
</p>
<p>A Monte Carlo Markov chain makes use of a dataset Z and a model with m
adjustable parameters, ï¿½ D .ï¿½1; : : : ; ï¿½m/, for which it is possible to calculate the
likelihood
</p>
<p>L D P.Z=ï¿½/: (16.1)
</p>
<p>Usually, the calculation of the likelihood is the most intensive task for an MCMC.
It necessary to be able to evaluate the likelihood for all possible parameter values.
</p>
<p>According to Bayesian statistic, one is allowed to have a prior knowledge on the
parameters, even before they are measured (see Sect. 1.7). The prior knowledge may
come from experiments that were conducted beforehand, or from any other type a
priori belief on the parameters. The prior probability distribution will be referred to
as p.ï¿½/.
</p>
<p>The information we seek is the probability distribution of the model parameters
after the measurements are made, i.e., the posterior distribution P.ï¿½=Z/. According
to Bayes&rsquo; theorem, the posterior distribution is given by
</p>
<p>P.ï¿½=Z/ D P.ï¿½/P.Z=ï¿½/
P.Z/
</p>
<p>D P.ï¿½/ ï¿½ L
P.Z/
</p>
<p>; (16.2)
</p>
<p>where the quantity P.Z/ D R P.Z=ï¿½/P.ï¿½/dï¿½ is a normalization constant.
Taken at face value, (16.2) appears to be very complicated, as it requires a multi-
</p>
<p>dimensional integration of the term P.Z/. The alternative provided by a Monte
Carlo Markov chain is the construction of a sequence of dependent samples for the
parameters ï¿½ in the form of a Markov chain. Such Markov chain is constructed
in such a way that each parameter value appears in the chain in proportion to
this posterior distribution. With this method, it will be shown that the value of the
normalization constant P.Z/ becomes unimportant, thus alleviating significantly the
computational burden. The goal of a Monte Carlo Markov chain is therefore that
of creating a sequence of parameter values that has as its stationary distribution the</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Metropolis&ndash;Hastings Algorithm 251
</p>
<p>posterior distribution of the parameters. After the chain is run for a large number
of iterations, the posterior distribution is obtained via the sample distribution of the
parameters in the chain.
</p>
<p>There are several algorithms to sample the parameter space that satisfy the
requirement of having the posterior distribution of the parameters P.ï¿½=Z/ as the
stationary distribution of the chain. A very common algorithm that can be used in
most applications is that of Metropolis and Hastings [19, 32]. It is surprisingly easy
to implement, and therefore constitutes a reference for any MCMC implementation.
Another algorithm is that of Gibbs, but its use is limited by certain specific require-
ments on the distribution function of the parameters.Both algorithms presented in
this chapter provide a way to sample values of the parameters and describe a way to
accept them into the Markov chain.
</p>
<p>16.3 The Metropolis&ndash;Hastings Algorithm
</p>
<p>The Metroplis&ndash;Hastings algorithm [19, 32] was devised well before personal com-
puters became of widespread use. In this section we first describe the algorithm and
then prove that the resulting Markov chain has the desired stationary distribution.
The method has the following steps.
</p>
<p>1. The Metropolis&ndash;Hastings algorithm starts with an arbitrary choice of the initial
values of the model parameters, ï¿½0 D .ï¿½01 ; : : : ; ï¿½0m/. This initial set of parameters
is automatically accepted into the chain. As will be explained later, some of the
initial links in the MCMC will later be discarded to offset the arbitrary choice of
the starting point.
</p>
<p>2. A candidate for the next link of the chain, ï¿½ 0, is then drawn from a proposal
(or auxiliary) distribution q.ï¿½ 0=ï¿½n/, where ï¿½n is the current link in the chain.
The distribution q.ï¿½ 0=ï¿½n/ is the probability of drawing a given candidate ï¿½ 0,
given that the chain is in state ï¿½n. There is a large amount of freedom in the
choice of the auxiliary distribution, which can depend on the current state of
the chain ï¿½n, according to the Markovian property, but not on its prior history.
One of the simplest choices for a proposal distribution is an m-dimensional
uniform distribution of fixed width in the neighborhood of the current parameter.
A uniform prior is very simple to implement, and it is the default choice in many
applications. More complex candidate distributions can be implemented using,
e.g., the method of simulation of variables described in Sect. 4.8.
</p>
<p>3. A prior distribution p.ï¿½/ has to be assumed before a decision can be made
whether the candidate is accepted into the chain or rejected. The Metropolis&ndash;
Hastings algorithm gives freedom on the choice of the prior distribution as well.
A typical choice of prior is another uniform distribution between two hard limits,
enforcing a prior knowledge that a given parameter may not exceed certain
boundaries. Sometimes the boundaries are set by nature of the parameter itself,
e.g., certain parameter may only be positive numbers, or in a fixed interval range.</p>
<p/>
</div>
<div class="page"><p/>
<p>252 16 Monte Carlo Markov Chains
</p>
<p>Other priors may be more restrictive. Consider the case of the measurement of
the slope of the curve in the Hubble experiment presented on page 157. It is clear
that, after a preliminary examination of the data, the slope parameter b will not
be a negative number, and will not be larger than, say, b D 2. Therefore one can
safely assume a prior on this parameter equal to p.b/ D 1=2, for 0 
 b 
 2.
Much work on priors has been done by Jeffreys [23], in search of mathematical
functions that express the lack of prior knowledge, known as Jeffreys priors.
For many applications, though, simple uniform prior distributions are typically
sufficient.
</p>
<p>4. After drawing a random candidate ï¿½ 0, we must decide whether to accept it into
the chain or reject it. This choice is made according to the following acceptance
probability, which is the heart of the Metropolis&ndash;Hastings algorithm:
</p>
<p>Ë.ï¿½ 0=ï¿½n/ D min
ï¿½
	.ï¿½ 0/q.ï¿½n=ï¿½ 0/
	.ï¿½n/q.ï¿½ 0=ï¿½n/
</p>
<p>; 1
</p>
<p>&#13;
</p>
<p>; (16.3)
</p>
<p>The acceptance probability Ë.ï¿½ 0=ï¿½n/ determines the probability of going from
ï¿½n to the new candidate state ï¿½ 0, where q.ï¿½ 0=ï¿½n/ is the proposal distribution,
and 	.ï¿½ 0/ D P.ï¿½=Z/ is the intended stationary distribution of the chain.
Equation (16.3) means that the probability of going to a new value in the chain,
ï¿½ 0, is proportional to the ratio of the posterior distribution of the candidate to that
of the previous link. The acceptance probability can also be re-written by making
use of Bayes&rsquo; theorem (16.2), as
</p>
<p>Ë.ï¿½ 0=ï¿½n/ D min
ï¿½
p.ï¿½ 0/P.Z=ï¿½ 0/q.ï¿½n=ï¿½ 0/
p.ï¿½n/P.Z=ï¿½n/q.ï¿½ 0=ï¿½n/
</p>
<p>; 1
</p>
<p>&#13;
</p>
<p>(16.4)
</p>
<p>In this form, the acceptance probability can be calculated based on known
quantites. The term p.ï¿½n/q.ï¿½ 0=ï¿½n/ at the denominator represents the probability
of occurrence of a given candidate ï¿½ 0; in fact, the first term is the prior probability
of the n-th link in the chain, and the second term is the probability of generating
the candidate, once the chain is at that state. The other term, L D P.Z=ï¿½n/,
is the likelihood of the current link in the chain. At the numerator, all terms
have reverse order of conditioning between the current link and the candidate.
Therefore all quantities in (16.4) are known, since p.ï¿½n/ and q.ï¿½ 0=ï¿½n/ (and their
conjugates) are chosen by the analyst and the likelihood can be calculated for all
model parameters.
</p>
<p>Acceptance probability means that the candidate is accepted in the chain in
proportion to the value of Ë.ï¿½ 0=ï¿½n/. Two cases are possible:
</p>
<p>&bull; Ë D 1: This means that the candidate will be accepted in the chain, since the
probability of acceptance is 100 %. The candidate becomes the next link in the
chain, ï¿½nC1 D ï¿½ 0. The min operator guarantees that the probability is never
greater than 1, which would not be meaningful.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Metropolis&ndash;Hastings Algorithm 253
</p>
<p>&bull; Ë &lt; 1: This means that the candidate can only be accepted in the chain with a
probability Ë. To enforce this probability of acceptance, it is sufficient to draw
a random number 0 
 u 
 1 and then accept or reject the candidate according
to the following criterion:
</p>
<p>(
if Ë ï¿½ u) candidate is accepted, ï¿½nC1 D ï¿½ 0
if Ë &lt; u) candidate is rejected, ï¿½nC1 D ï¿½n :
</p>
<p>(16.5)
</p>
<p>It is important to notice that if the candidate is rejected, then the chain doesn&rsquo;t
move from its current location and a new link equal to the previous one is added
to the chain. This means that at each time step in the chain a new link is added,
either by repeating the last link (if the candidate is rejected) or by adding a
different link (if the candidate is accepted).
</p>
<p>The logic of the Metropolis&ndash;Hastings algorithm can be easily understood in the
case of uniform priors and auxiliary distributions. In that case, the candidate is
accepted in proportion to just the ratio of the likelihoods, since all other terms in
(16.3) cancel out:
</p>
<p>Ë.ï¿½ 0=ï¿½n/ D min
ï¿½
L .ï¿½ 0/
L .ï¿½n/
</p>
<p>; 1
</p>
<p>&#13;
</p>
<p>: (16.6)
</p>
<p>If the candidate has a higher likelihood than the current link, it is automatically
accepted. If the likelihood of the candidate is lower than the likelihood of the current
link, then it is accepted in proportion to the ratio of the likelihoods of the candidate
and of the current link. The possibility of accepting a parameter of lower likelihood
permits a sampling of the parameter space, instead of a simple search for the point
of maximum likelihood which would only result in a point estimate.
</p>
<p>We now show that use of the Metropolis&ndash;Hastings algorithm creates a Markov
chain that has 	.ï¿½n/ D P.ï¿½n=Z/ as its stationary distribution. For this purpose, we
will show that the posterior distribution of the parameters satisfies the relationship
</p>
<p>	.ï¿½n/ D
X
</p>
<p>j
</p>
<p>	.ï¿½j/pjn; (16.7)
</p>
<p>where pjn are the transition probabilities of the Markov chain and the index j runs
over all possible states.
</p>
<p>Proof (Justification of the Metropolis&ndash;Hastings Algorithm) To prove that the
Metropolis&ndash;Hastings algorithm leads to a Markov chain with the desired
stationary distribution, consider the time-reversed chain:
</p>
<p>original chain: X0 ! X1 ! X2 ! : : :! Xn ! : : :
time-reversed chain: X0  X1  : : :Xn  XnC1  : : : :</p>
<p/>
</div>
<div class="page"><p/>
<p>254 16 Monte Carlo Markov Chains
</p>
<p>The time-reversed chain is defined by the transition probability p?ij:
</p>
<p>p?ij D P.Xn D "j=XnC1 D "i/ D
P.Xn D "j;XnC1 D "i/
</p>
<p>P.XnC1 D "i/ D
</p>
<p>P.XnC1 D "i=Xn D "j/P.Xn D "j/
P.XnC1 D "i/ ;
</p>
<p>leading to the following relationship with the transition probability of the
original chain:
</p>
<p>) p?ij D
pji	.ï¿½j/
</p>
<p>	.ï¿½i/
(16.8)
</p>
<p>If the original chain is time-reversible, then p?ij D pij, and the time-reversed
process is also a Markov chain. In this case, the stationary distribution will
follow the relationship
</p>
<p>	.ï¿½i/ ï¿½ pij D pji ï¿½ 	.ï¿½j/ (16.9)
</p>
<p>known as the equation of detailed balance. The detailed balance is the
hallmark of a time-reversible Markov chain, stating that the probability to
move forward and backwards is the same, once the stationary distribution is
reached. Therefore, if the transition probability of the Metropolis&ndash;Hastings
algorithm satisfies this equation, with 	.ï¿½/ D P.ï¿½=Z/, then the chain
is time reversible, and with the desired stationary distribution. Moreover,
Theorem 15.4 can be used to prove that this distribution is unique.
</p>
<p>The Metropolis&ndash;Hastings algorithm enforces a specific transition probabil-
ity between states ï¿½i and ï¿½j,
</p>
<p>pij D q.ï¿½j=ï¿½i/Ë.ï¿½j=ï¿½i/ if ï¿½i &curren; ï¿½j (16.10)
</p>
<p>where q is the probability of generating the candidate (or proposal distri-
bution), and Ë the probability of accepting it. One can also show that the
probability of remaining at the same state ï¿½i is
</p>
<p>pii D 1 ï¿½
X
</p>
<p>j&curren;i
q.ï¿½j=ï¿½i/Ë.ï¿½j=ï¿½i/:
</p>
<p>where the sum is over all possible states.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Metropolis&ndash;Hastings Algorithm 255
</p>
<p>According to the transition probability described by (16.3),
</p>
<p>Ë.ï¿½j=ï¿½i/ D min
ï¿½
p.ï¿½j/P.Z=ï¿½j/q.ï¿½i=ï¿½j/
</p>
<p>p.ï¿½i/P.Z=ï¿½i/q.ï¿½j=ï¿½i/
; 1
</p>
<p>&#13;
</p>
<p>D min
ï¿½
	.ï¿½j/q.ï¿½i=ï¿½j/
</p>
<p>	.ï¿½i/q.ï¿½j=ï¿½i/
; 1
</p>
<p>&#13;
</p>
<p>in which we have substituted 	.ï¿½i/ ï¿½ p.ï¿½i=Z/ D P.Z=ï¿½i/p.ï¿½i/=p.Z/ as the
posterior distribution. Notice that the probability p.Z/ cancels out, therefore
its value does not play a role in the construction of the chain.
</p>
<p>It is clear that, if Ë.ï¿½j=ï¿½i/ &lt; 1, then Ë.ï¿½i=ï¿½j/ D 1, thanks to the min
operation. Assume, without loss of generality, that Ë.ï¿½i; ï¿½j/ &lt; 1:
</p>
<p>Ë.ï¿½j=ï¿½i/ D 	.ï¿½j/q.ï¿½i=ï¿½j/
	.ï¿½i/q.ï¿½j=ï¿½i/
</p>
<p>) Ë.ï¿½j=ï¿½i/	.ï¿½i/q.ï¿½j=ï¿½i/ D 	.ï¿½j/q.ï¿½i=ï¿½j/ ï¿½ Ë.ï¿½i=ï¿½j/
</p>
<p>Now, since we assumed Ë.ï¿½j=ï¿½i/ &lt; 1, the operation of min becomes
redundant. Using (16.10) the previous equation simplifies to
</p>
<p>pij ï¿½ 	.ï¿½i/ D pji ï¿½ 	.ï¿½j/
</p>
<p>which shows that the Metropolis&ndash;Hastings algorithm satisfies the detailed
balance equation; it thus generates a time-reversible Markov chain, with
stationary distribution equal to the posterior distribution. ut
</p>
<p>Example 16.1 The data from Hubble&rsquo;s experiment (page 157) can be used to run a
Monte Carlo Markov chain to obtain the posterior distribution of the parameters
a and b. This fit was also performed using a maximum likelihood method (see
page 159) in which the common uncertainty in the dependent variable, log v, was
estimated according to the method described in Sect. 8.5.
</p>
<p>Using these data, a chain is constructed using uniform priors on the two fit
parameters a and b:
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>p.a/ D 10
7
</p>
<p>for 0:2 
 b 
 0.9
p.b/ D 10 for 0:15 
 a 
 0.25:
</p>
<p>The proposal distributions are also uniform distributions, respectively, of fixed
width 0.1 and 0.02 for a and b, and centered at the current value of the parameters:
</p>
<p>(
p.ï¿½nC1=an/ D 5 for an ï¿½ 0:1 
 ï¿½nC1 
 an C 0:1
p.ï¿½nC1=bn/ D 25 for bn ï¿½ 0:02 
 ï¿½nC1 
 bn C 0:02</p>
<p/>
</div>
<div class="page"><p/>
<p>256 16 Monte Carlo Markov Chains
</p>
<p>in which an and bn are, respectively, the n-th links of the chain, and ï¿½nC1 represent
the candidate for the .nC 1/-th link of the chain, for each parameter.
</p>
<p>In practice, once the choice of a uniform distribution with fixed width is made, the
actual value of the prior and proposals distributions are not used explicitly. In fact,
the acceptance probability becomes simply a function of the ratio of the likelihoods,
or of the ï¿½2&rsquo;s:
</p>
<p>Ë.ï¿½ 0=ï¿½n/ D min
ï¿½
L .ï¿½ 0/
L .ï¿½n/
</p>
<p>; 1
</p>
<p>&#13;
</p>
<p>D min
ï¿½
</p>
<p>e
ï¿½2.ï¿½n/ï¿½ï¿½2.ï¿½ 0/
</p>
<p>2 ; 1
</p>
<p>&#13;
</p>
<p>}
where ï¿½2.ï¿½n/ and ï¿½2.ï¿½ 0/ are the minimum ï¿½2&rsquo;s calculated, respectively, using the
n-th link of the chain and the candidate parameters (Fig. 16.1).
</p>
<p>A few steps of the chain are reported in Table 16.1. Where two consecutive links
in the chain are identical, it is an indication that the candidate parameter drawn at
that iteration was rejected, and the previous link was therefore repeated. Figure 16.2
shows the sample distributions of the two fit parameters from a chain with 100,000
links. A wider prior on parameter a would make it possible to explore further the
tails of the distribution.
</p>
<p>Fig. 16.1 MCMC for parameters a, b of linear model fit to the data in Table 8.1. The chain was
run for 10,000 iterations, using uniform priors on both parameters (between 0.15 and 0.25 for a,
and 0.2 and 0.9 for b). The chain started at a D 0:90 and b D 0:25. The proposal distributions
were also uniform, with width of, respectively, 0.2 for a and 0.04 for b, centered at the current
value of the chain</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 The Metropolis&ndash;Hastings Algorithm 257
</p>
<p>Table 16.1 Sample of MCMC chain for the Hubble data
</p>
<p>n a b ï¿½2.ï¿½n/
</p>
<p>1 0:90000 0:25000 3909:55420 136 0:80627 0:18064 11:47313
</p>
<p>2 0:94116 0:24395 3563:63110 137 0:77326 0:18284 10:63887
</p>
<p>3 0:96799 0:23951 3299:28149 138 0:77326 0:18284 10:63887
</p>
<p>4 0:96799 0:23951 3299:28149 139 0:77326 0:18284 10:63887
</p>
<p>5 0:96799 0:23951 3299:28149 140 0:77326 0:18284 10:63887
</p>
<p>6 0:96799 0:23951 3299:28149 . . . . . . . . . . . . . . . . . . .
</p>
<p>7 0:97868 0:22983 2503:21655 1141 0:42730 0:20502 8:90305
</p>
<p>8 0:97868 0:22983 2503:21655 1142 0:42730 0:20502 8:90305
</p>
<p>9 0:96878 0:22243 1885:28088 1143 0:42174 0:20494 8:68957
</p>
<p>10 1:01867 0:21679 1714:54456 1144 0:42174 0:20494 8:68957
</p>
<p>. . . . . . . . . . . . . . . . . . . 1145 0:42174 0:20494 8:68957
</p>
<p>21 1:08576 0:19086 563:56506 1146 0:42174 0:20494 8:68957
</p>
<p>22 1:06243 0:19165 536:47919 1147 0:42174 0:20494 8:68957
</p>
<p>23 1:06243 0:19165 536:47919 1148 0:42174 0:20494 8:68957
</p>
<p>24 1:06559 0:18244 254:36528 1149 0:42174 0:20494 8:68957
</p>
<p>25 1:06559 0:18244 254:36528 1150 0:43579 0:20323 8:65683
</p>
<p>26 1:06559 0:18244 254:36528 . . . . . . . . . . . . . . . . . . .
</p>
<p>27 1:06559 0:18244 254:36528 9991 0:66217 0:19189 12:43171
</p>
<p>28 1:06559 0:18244 254:36528 9992 0:62210 0:19118 8:52254
</p>
<p>29 1:04862 0:17702 118:84048 9993 0:62210 0:19118 8:52254
</p>
<p>30 1:04862 0:17702 118:84048 9994 0:62210 0:19118 8:52254
</p>
<p>. . . . . . . . . . . . . . . . . . . 9995 0:62210 0:19118 8:52254
</p>
<p>131 0:84436 0:17885 13:11242 9996 0:62210 0:19118 8:52254
</p>
<p>132 0:84436 0:17885 13:11242 9997 0:62210 0:19118 8:52254
</p>
<p>133 0:84436 0:17885 13:11242 9998 0:62210 0:19118 8:52254
</p>
<p>134 0:80627 0:18064 11:47313 9999 0:64059 0:18879 11:11325
</p>
<p>135 0:80627 0:18064 11:47313 10;000 0:64059 0:18879 11:11325
</p>
<p>Fig. 16.2 Sample distribution function for parameters a and b, constructed using a histogram plot
of 100,000 samples of a MCMC ran with the same parameters as Fig. 16.1</p>
<p/>
</div>
<div class="page"><p/>
<p>258 16 Monte Carlo Markov Chains
</p>
<p>16.4 The Gibbs Sampler
</p>
<p>The Gibbs sampler is another algorithm that creates a Markov chain having as
stationary distribution the posterior distribution of the parameters. This algorithm
is based on the availability of the full conditional distribution, defined as
</p>
<p>	i.ï¿½i/ D 	.ï¿½ijï¿½j; j &curren; i/ (16.11)
</p>
<p>The full conditional distribution is the (posterior) distribution of a given parameter,
given that the values of all other parameters are known. If the full conditional
distributions are known and can be sampled from, then a simple algorithm can be
implemented:
</p>
<p>1. Start the chain at a given value of the parameters, ï¿½0 D .ï¿½10 ; : : : ; ï¿½m0 /.
2. Obtain a new value in the chain through successive generations:
</p>
<p>ï¿½11 drawn from 	.ï¿½1jï¿½20 ; ï¿½30 ; : : :/
ï¿½21 drawn from 	.ï¿½2jï¿½11 ; ï¿½30 ; : : :/
: : :
</p>
<p>ï¿½m1 drawn from 	.ï¿½mjï¿½11 ; ï¿½21 ; : : : ; ï¿½mï¿½11 /
</p>
<p>3. Iterate until convergence to stationary distribution is reached.
</p>
<p>The justification of this method can be found in [15]. In the case of data fitting
with a dataset Z and a model with m adjustable parameters, usually it is not possible
to know the full conditional distributions, thus this method is not as common as
the Metropolis&ndash;Hastings algorithm. The great advantage of the Gibbs sampler is the
fact that the acceptance is 100 %, since there is no rejection of candidates for the
Markov chain, unlike the case of the Metropolis&ndash;Hastings algorithm.
</p>
<p>Example 16.2 This example reproduces an application presented by Carlin et al.
[8], and illustrates a possible application in which the knowledge of the full
conditional distribution results in the possibility of implementing a Gibbs sampler.
</p>
<p>Consider the case in which a Poisson dataset of n numbers, yi, i D 1; : : : ; n, is fit
to a step-function model:
</p>
<p>(
y D ï¿½ if i 
m
y D ï¿½ if i &gt;m (16.12)
</p>
<p>The model therefore has three parameters, the values ï¿½, ï¿½, and the point of
discontinuity, m. This situation could be a set of measurements of a quantity that
may suddenly change its value at an unknown time, say the voltage in a given portion
of an electric circuit after a switch has been opened or closed.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 259
</p>
<p>Assume that the priors on the parameters are, respectively, a gamma distributions
for ï¿½ and ï¿½, p.ï¿½/ D G.Ë; Ë/ and p.ï¿½/ D G.ï¿½; Ä±/, and a uniform distribution for
m, p.m/ D 1=n (see Sect. 7.2 for definition of the gamma distribution). According
to Bayes&rsquo; theorem, the posterior distribution is proportional to the product of the
likelihood and the priors:
</p>
<p>	.ï¿½; ï¿½;m/ / P.y1; : : : ; yn=ï¿½; ï¿½;m/ ï¿½ p.ï¿½/p.ï¿½/p.m/: (16.13)
</p>
<p>The posterior is therefore given by
</p>
<p>	.ï¿½; ï¿½;m/ /
mY
</p>
<p>iD1
eï¿½ï¿½ï¿½yi
</p>
<p>nY
</p>
<p>iDmC1
eï¿½ï¿½ï¿½yi ï¿½ ï¿½Ëï¿½1eï¿½Ëï¿½ ï¿½ ï¿½ï¿½ï¿½1eï¿½Ä±ï¿½ ï¿½ 1
</p>
<p>n
</p>
<p>) 	.ï¿½; ï¿½;m/ / ï¿½.ËC
Pm
</p>
<p>iD1 yiï¿½1/eï¿½.ËCm/ï¿½ ï¿½ ï¿½.ï¿½C
Pn
</p>
<p>iDmC1 yiï¿½1/eï¿½.Ä±Cnï¿½m/ï¿½:
</p>
<p>The equation above indicates that the conditional posteriors, obtained by fixing all
parameters except one, are given by
</p>
<p>8
ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
&lt;
</p>
<p>ËÌ
ËÌ
ËÌ
ËÌ
ËÌ
:
</p>
<p>	ï¿½.ï¿½/ D G
ï¿½
</p>
<p>Ë C
mP
</p>
<p>iD1
yi; Ë C m
</p>
<p>ï¿½
</p>
<p>	ï¿½.ï¿½/ D G
ï¿½
</p>
<p>ï¿½ C
nP
</p>
<p>iDmC1
yi; Ä± C n ï¿½ m
</p>
<p>ï¿½
</p>
<p>	m.m/ D ï¿½
.ËCPmiD1 yiï¿½1/eï¿½.ËCm/ï¿½ ï¿½ ï¿½.ï¿½C
</p>
<p>Pn
iDmC1 yiï¿½1/eï¿½.Ä±Cnï¿½m/ï¿½
</p>
<p>Pn
iD1
</p>
<p>ï¿½
ï¿½.ËC
</p>
<p>Pm
iD1 yiï¿½1/eï¿½.ËCm/ï¿½ ï¿½ ï¿½.ï¿½C
</p>
<p>Pn
iDmC1 yiï¿½1/eï¿½.Ä±Cnï¿½m/ï¿½
</p>
<p>	 :
</p>
<p>(16.14)
</p>
<p>This is therefore an example of a case where the conditional posterior distributions
are known, and therefore the Gibbs algorithm is applicable. The only complication
is the simulation of the three conditional distributions, which can be achieved using
the methods described in Sect. 4.8. }
</p>
<p>16.5 Tests of Convergence
</p>
<p>It is necessary to test that the MCMC has reached convergence to the stationary
distribution before inference on the posterior distribution can be made. Convergence
indicates that the chain has started to sample the posterior distribution, so that the
MCMC samples are representative of the distribution of interest, and are not biased
by such choices as the starting point of the chain. The period of time required for
the chain to reach convergence goes under the name of burn-in period, and varies
from chain to chain according to a variety of factors, such as the choice of prior and
proposal distributions. We therefore must identify and remove such initial period</p>
<p/>
</div>
<div class="page"><p/>
<p>260 16 Monte Carlo Markov Chains
</p>
<p>from the chain prior to further analysis. The Geweke z-score test and the Gelman-
Rubin test are two of the most common tests used to identify the burn-in period.
</p>
<p>Another important consideration is that the chain must be run for a sufficient
number of iterations, so that the sample distribution becomes a good approximation
of the true posterior distribution. It is clear that the larger the number of iterations
after the burn-in period, the more accurate will be the estimates of the parameters of
the posterior distribution. In practice it is convenient to know the minimum stopping
time that enables to estimate the posterior distribution with the required precision.
The Raftery-Lewis test is designed to give an approximate estimate of both the burn-
in time and the minimum required stopping time.
</p>
<p>Typical considerations concerning the burn-in period and the stopping time of
a chain can be illustrated with the example of three chains based on the data from
Table 10.1. The chains were run, respectively, with a uniform proposal distribution
of 1, 10, and 100 for both parameters of the linear model, starting at the same point
(Figs. 16.3, 16.4 and 16.5). The chain with a narrower proposal distribution requires
a longer time to reach the stationary value of the parameters, in part because at
each time interval the candidate can be chosen in just a limited neighborhood of the
</p>
<p>Fig. 16.3 MCMC for parameters a, b of linear model fit to the data in Table 10.1, using a uniform
proposal distribution with width of 1 for both parameters. The chain started at a D 12 and b D 6.
In grey is the sample distribution obtained by removing the initial 2000 iterations, the ones that are
most affected by the arbitrary choice of starting point</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 261
</p>
<p>Fig. 16.4 MCMC for parameters a, b of linear model fit to the data in Table 10.1, using a uniform
proposal distribution with width of 10 for both parameters. The chain started at same values as in
Fig. 16.3
</p>
<p>previous link. Moreover, the sampling of parameter space is less uniform because
the chain requires longer time to span the entire parameter range. The intermediate
value for the proposal distribution results in an almost immediate convergence, and
the sampling of parameter space is clearly more uniform. An increase in the size
of the proposal distribution, however, may eventually lead to slow convergence and
poor sampling, as indicated by the chain with the largest value of the proposal width.
In this case, candidates are drawn from regions of parameter space that have very
low likelihood, or large ï¿½2, and therefore the chain has a tendency to remain at the
same location for extended periods of time, with low acceptance rate. The result
is a chain with poor coverage of parameter space and poorly determined sample
distribution for their parameters. A smoother distribution is preferable, because it
leads to a more accurate determination of the median, and of confidence ranges on
the parameters.
</p>
<p>Another consideration is that elements in the chain are more or less correlated
to one another, according to the choice of the proposal distribution, and other
choices in the construction of the chain. Links in the chains are always correlated
by construction, since the next link in the chain typically depends on the current
state of the chain. In principle a Markov chain can be constructed that does not</p>
<p/>
</div>
<div class="page"><p/>
<p>262 16 Monte Carlo Markov Chains
</p>
<p>Fig. 16.5 MCMC for parameters a, b of linear model fit to the data in Table 10.1, using a uniform
proposal distribution with width of 50 for both parameters. The chain started at same values as in
Fig. 16.3
</p>
<p>depend on the current state of the chain, but in most cases it is convenient to make
full use of the Markovian property that allows to make use of the current state of
the chain. The chains in Figs. 16.3, 16.4 and 16.5 illustrate the fact that the degree
of correlation varies with the proposal distribution choice. For example, the chain
with the narrowest proposal distribution appears more correlated than that with the
intermediate choice for the width; also, the chain with the largest width appears to
have periods with the highest degree of correlation, when the chain does not move
for hundreds of iterations. This shows that the degree of correlation is a nonlinear
function of the proposal distribution width, and that fine-tuning is always required
to obtain a chain with good mixing properties. The degree of correlation among
elements of the chain will become important when we desire to estimate the variance
of the mean from a specific segment of the chain, since the formulas derived earlier
in Chap. 4 apply only to independent samples.
</p>
<p>Testing for convergence and stopping time of the chain are critical tasks for
a Monte Carlo Markov chain. The tests discussed below are some of the more
common analytic tools and can be implemented with relative ease.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 263
</p>
<p>16.5.1 The Geweke Z Score Test
</p>
<p>A simple test of convergence is provided by the difference of the mean of two
segments of the chain. Under the null hypothesis that the chain is sampling the
same distribution during both segments, the sample means are expected to be drawn
from the same parent mean. Consider segment A at the beginning of the chain, and
segmentB at the end of the chain; for simplicity, consider one parameter at a time.
If the chain is of length N, the prescription is to use an initial segment of NA D 0:1N
elements, and a final segment with NB D 0:5N links, although those choices are
somewhat arbitrary, and segments of different length can also be used.
</p>
<p>The mean of each parameter in the two segments A and B is calculated as
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p> A D 1
NA
</p>
<p>NAP
</p>
<p>jD1
 j
</p>
<p> B D 1
NB
</p>
<p>NP
</p>
<p>jDNï¿½NBC1
 j:
</p>
<p>(16.15)
</p>
<p>To compare the two sample means, it is also necessary to estimate their sample
variances ï¿½2
</p>
<p> A
and ï¿½2
</p>
<p> B
. This task is complicated significantly by the fact that one
</p>
<p>cannot just use (2.11), because of the correlation between links of the chain. One
possibility to overcome this difficulty is to thin the chain by using only every n-th
iteration, so that the thinned chain better approximates independent samples.
</p>
<p>The test statistic is the Z score of the difference between the means of the two
segments:
</p>
<p>ZG D  B ï¿½  Aq
ï¿½2
 A
C ï¿½2
</p>
<p> B
</p>
<p>: (16.16)
</p>
<p>Under the assumption that the two means follow the same distribution and that they
are uncorrelated, the Z-score is distributed as a standard Gaussian, ZG ï¿½ N.0; 1/.
For this reason the two segments of the chain are typically separated by a large
number of iterations. An application of the Geweke Z score is to step the start of
segment A forward in time, until the ZG scores don&rsquo;t exceed approximately Ë3,
which correspond to a Ë3ï¿½ deviation in the means of the two segments. The burn-
in period that needs to be excised is that before the Z scores stabilize around the
expected values. An example of the use of this test statistic is provided in Fig. 16.6,
in which ZG was calculated from the chain with proposal width 10. An initial
segment of length 20 % of the total chain length is compared to the final 40 % of
the chain, by stepping the beginning of the initial segment until it overlaps with
the final segment. By using all links in the chain to estimate the variance of the</p>
<p/>
</div>
<div class="page"><p/>
<p>264 16 Monte Carlo Markov Chains
</p>
<p>Fig. 16.6 Geweke Z scores with segment A and segment B, respectively, 20 and 40 % of the total
chain length. The results correspond to the chain run with a proposal width of 10. The Z scores are
calculated by using only every other 10-th iteration
</p>
<p>mean, the variance would be underestimated because of the correlation among links,
leading to erroneously large values of ZG. If the chain is thinned by a factor of 10,
then the estimate of the variance using (2.11) is more accurate, and the resulting Z
scores show that the chains converge nearly immediately, as is also clear by a visual
inspection of the chain from Fig. 16.4.
</p>
<p>The effect of the starting point in the evaluation of the burn-in period is shown in
Fig. 16.3, in which it is apparent that it takes about 2000 iterations for the chain to
forget the initial position, and to start sampling the posterior distribution, centered
at the dashed lines. A larger proposal distribution, as in Fig. 16.4, makes it easier
to reach the posterior distribution more rapidly, to the point that no burn-in period
is visible in this case. In the presence of a burn-in period, the sample distribution
must be constructed by excising the initial portion of the chain, as shown in the grey
histogram plot of Fig. 16.3.
</p>
<p>16.5.2 The Gelman&ndash;Rubin Test
</p>
<p>The Gelman&ndash;Rubin test investigates the effect of initial conditions on the conver-
gence properties of the MCMC and makes use of m parallel chains starting from
different initial points. Initially, the m chain will be far apart because of the different
starting points. As the chains start sampling the stationary distribution, they will
have the same statistical properties.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 265
</p>
<p>The test is based on two estimates of the variance, or variability, of the chains: the
within-chain variance for each of the m chains W, and the between-chain varianceB.
At the beginning of the chain, W will underestimate the true variance of the model
parameters, because the chains have not had time to sample all possible values. On
the other hand, B will initially overestimate the variance, because of the different
starting points. The test devised by Gelman and Rubin [17] defines the ratio of
the within-to-between variance as a test to measure convergence of the chains, to
identify an initial burn-in period that should be removed because of the lingering
effect of initial conditions.
</p>
<p>For each parameter, consider m chains of N iterations each, where N i is the mean
of each chain i D 1; : : : ;m and N the mean of the means:
</p>
<p>8
ËÌ
&lt;Ì
</p>
<p>ËÌ
:Ì
</p>
<p>N i D 1
N
</p>
<p>NP
</p>
<p>jD1
 j
</p>
<p>N D 1
m
</p>
<p>mP
</p>
<p>iD1
N i:
</p>
<p>(16.17)
</p>
<p>The between-chain variance B is defined as the average of the variances of the m
chains,
</p>
<p>B D N
m ï¿½ 1
</p>
<p>mX
</p>
<p>iD1
. N i ï¿½ N /2 (16.18)
</p>
<p>Notice that, in (16.18), B=N is the variance of the means N i. The within-chain
variance W is defined by
</p>
<p>s2i D
1
</p>
<p>N ï¿½ 1
NX
</p>
<p>jD1
. j ï¿½ N i/2
</p>
<p>W D 1
m
</p>
<p>mX
</p>
<p>iD1
s2i :
</p>
<p>(16.19)
</p>
<p>The quantity Oï¿½2 , defined as
</p>
<p>Oï¿½2 D
ï¿½
N ï¿½ 1
N
</p>
<p>ï¿½
</p>
<p>W C 1
N
B (16.20)
</p>
<p>is intended to be an unbiased estimator of the variance of the parameter under the
hypothesis that the stationary distribution is being sampled. At the beginning of a
</p>
<p>chain&mdash;before the stationary distribution is reached&mdash; Oï¿½2 overestimates the variance,
because of the different initial starting points. It was suggested by Brooks and
Gelman [6] to add an additional term to this estimate of the variance, to account</p>
<p/>
</div>
<div class="page"><p/>
<p>266 16 Monte Carlo Markov Chains
</p>
<p>for the variability in the estimate of the means, so that the estimate of the within-
chain variance to use becomes
</p>
<p>OV D Oï¿½2 C
B
</p>
<p>mN
: (16.21)
</p>
<p>Convergence can be monitored by use of the following statistic:
</p>
<p>p
OR ï¿½
</p>
<p>s
OV
W
; (16.22)
</p>
<p>which should converge to 1 when the stationary distribution in all chains has been
reached. A common use of this statistic is to repeat the calculation of the Gelman&ndash;
Rubin statistic after excising an increasingly longer initial portion of the chain, until
approximately
</p>
<p>p
OR 
 1:2: (16.23)
</p>
<p>A procedure to test for convergence of the chain using the Gelman&ndash;Rubin
statistic is to divide the chain into segments of length b, such that the N iterations
are divided into N=b batches. For each segment starting at iteration i D k 	 b; k D
0; : : : ;N=b ï¿½ 1, we can calculate the value OR and claim convergence of the chains
when (16.23) is satisfied. Figure 16.7 shows results of this test run on m D 2 chains
</p>
<p>Fig. 16.7 Gelman&ndash;Rubin statistic OR calculated from m D 2 chains with the same distributions as
in Fig. 16.3, one starting at a D 12, b D 6, and the other at a D 300 and b D 300. The chain
rapidly converges to its stationary distribution, and appears to forget about the starting point after
approximately 500 iterations. The values of OR were calculated in segments of length b D 200,
starting at iteration i D 0</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 267
</p>
<p>Fig. 16.8 Plot of the average of the parameters a and b for the two chains used in Fig. 16.7 (top
lines are for parameter a, bottom lines for b). For both parameters, the two chains sample the same
posterior distribution after approximately 500 iterations
</p>
<p>based on the data of Table 10.1, starting at different values: one chain starting at
a value that is close to the posterior mean of the parameters, and one starting at
values that were intentionally chosen to be much larger than the parent values. After
approximately 500 or so iterations, the within-chain and between-chain estimates of
the variance become comparable, and the value of OR approaches the value of one.
</p>
<p>Another related tool that aids in assessing convergence is the plot of the mean
of the parameters, shown in Fig. 16.8: it takes approximately 500 iterations for the
chain starting with high initial values, to begin sampling the stationary distribution.
It is clear that, from that point on, both chains hover around similar values of the
parameters. One should also check that, individually, both OV and W also stabilize to
a common value as function of starting point of the batch, and not just their ratio OR.
In fact, under the hypothesis of convergence, both within-chain and between-chain
variances should converge to a common value.
</p>
<p>Similar procedures to monitor convergence using the Gelman&ndash;Rubin may instead
use batches of increasing length, starting from one of length b, and increasing to 2b,
etc., optionally discarding the first half of each batch. Moreover, thinning can be
implemented when calculating means, to reduce the effect of correlation among the
samples. In all cases, the goal is to show that eventually the value of OR stabilizes
around unity.
</p>
<p>16.5.3 The Raftery&ndash;Lewis Test
</p>
<p>An ideal test for the convergence of MCMCs is one that determines the length of the
burn-in period, and how long should the chain be run to achieve a given precision in</p>
<p/>
</div>
<div class="page"><p/>
<p>268 16 Monte Carlo Markov Chains
</p>
<p>the estimate of the model parameters. The Raftery&ndash;Lewis test provides estimates of
both quantities, based on just a short test run of the chain. The test was developed by
Raftery and Lewis [37], and it uses the comparison of the short sample chain with an
uncorrelated chain to make inferences on the convergence properties of the chain.
In this section we describe the application of the test and refer the reader interested
in its justification to [37].
</p>
<p>The starting point to use the Raftery&ndash;Lewis test is to determine what inferences
we want to make from the Markov chain. Typically we want to estimate confidence
intervals at a given significance for each parameter, which means we need to
estimate two values ï¿½min and ï¿½max for each parameter ï¿½ such that their interval
contains a probability 1ï¿½q (e.g., respectively, q D 0:32, 0.10 or 0.01 for confidence
level 68, 90 or 99 %),
</p>
<p>1 ï¿½ q D
Z ï¿½max
</p>
<p>ï¿½min
</p>
<p>	.ï¿½/dï¿½:
</p>
<p>Consider, for example, the case of a 90 % confidence interval: the two parameter
values ï¿½min and ï¿½max are respectively the q D 0:95 and the q D 0:05 quantiles, so
that the interval .ï¿½min; ï¿½max/ will contain 90 % of the posterior probability for that
parameter.
</p>
<p>One can think of each quantile as a statistic, meaning that we can only
approximately estimate their values Oï¿½min and Oï¿½max. The Raftery&ndash;Lewis test lets us
estimate any quantile Oï¿½q such that it satisfies P.ï¿½ 
 Oï¿½q/ D 1 ï¿½ q to within Ër, with
probability s (say 95 % probability, s D 0:95). We have therefore introduced two
additional probabilities, r and s, which should not be confused with the quantile q.
Consider, for example, that the requirement is to estimate the q D 0:05 quantile,
with a precision of r D 0:01 and a probability of achieving this precision of
s D 0:95. This corresponds to accepting that the 90 % confidence interval resulting
from such estimate of the q D 0:05 quantile (and of the q D 0:95 as well) may in
reality be a 88 % or a 92 % confidence interval, 95 % of the time.
</p>
<p>The Raftery&ndash;Lewis test uses the information provided by the sample chain,
together with the desired quantile q and the tolerances r and s, and returns the
number of burn-in iterations, and the required number of iterationsN. A justification
for this test can be found in [37], and the test can be simply run using widely
available software such as the gibbsit code or the the CODA software [28, 34]. Note
that the required number of iterations are a function of the quantile to be estimated,
with estimation of smaller quantiles typically requiring longer iterations.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 269
</p>
<p>Summary of Key Concepts for this Chapter
</p>
<p>ï¿½ Monte Carlo Markov chain (MCMC): A numerical method to implement
a Markov chain, with the goal of estimating the posterior distribution of
model parameters via
</p>
<p>P.ï¿½=Z/ / P.ï¿½/L:
</p>
<p>ï¿½ Metropolis&ndash;Hastings algorithm: A commonly used method to draw and
accept or reject candidates for the MCMC. It is based on an acceptance
probability that simplifies to a ratio of likelihoods,
</p>
<p>Ë.ï¿½
0
</p>
<p>=ï¿½n/ D min
(
L.ï¿½ 0/
L.ï¿½n/ ; 1
</p>
<p>)
</p>
<p>when the priors and proposal distributions are uniform.
ï¿½ The Gibbs Sampler: An alternative algorithm to create an MCMC that
</p>
<p>makes use of the full conditional distribution of each parameter.
ï¿½ Convergence tests: Tests to ensure that the MCMC is sampling the
</p>
<p>intended posterior distribution. They typically require to excise a burn-in
time when the MCMC has not yet reached the stationary distribution.
</p>
<p>ï¿½ Geweke z-score test: A simple test of convergence that makes use of z-
scores of two segments of the chain.
</p>
<p>ï¿½ Gelman&ndash;Rubin test: A convergence test that requires multiple parallel
chains and makes use of between-chain and within-chain variances.
</p>
<p>ï¿½ Raftery&ndash;Lewis test: A convergence test that compares a sample of the
MCMC to an uncorrelated chain to determine burn-in time and required
length of the chain.
</p>
<p>Problems
</p>
<p>16.1 Prove that, in the presence of positive correlation among MCMC samples, the
variance of the sample mean is larger than that of an independent chain.
</p>
<p>16.2 Using the data of logm and velocity from Table 8.1 of Hubble&rsquo;s experiment,
construct a Monte Carlo Markov chain for the fit to a linear model with 10,000
iterations. Use uniform distributions for the prior and proposal distributions of the
two model parameters a and b, the latter with widths of 0.1 and 0.02, respectively,
for a and b in the neighborhood of the current value. You can start your chain at
values of a D 0:2 and b D 0:9. After completion of the chain, plot the sample
distribution of the two model parameters.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 16 Monte Carlo Markov Chains
</p>
<p>16.3 A one-parameter chain is constructed such that in two intervals A and B the
following values are accepted into the chain:
</p>
<p>A W 10; 11; 13; 11; 10
B W 7; 8; 1; 11; 10; 8I
</p>
<p>where A is an initial interval, and B an interval at the end of the chain. Not knowing
how the chain was constructed, use the Geweke z score to determine whether the
chain might have converged.
</p>
<p>16.4 Using the data of Table 10.1, construct a Monte Carlo Markov chain for the
parameters of the linear model, with 10,000 iterations. Use uniform distributions for
the prior and proposal distributions, the latter with a width of 10 for both parameters.
Start the chain at a D 12 and b D 6. After completion of the chain, plot the sample
distribution of the two model parameters.
</p>
<p>16.5 Consider the following portions of two one-parameter chains, run in parallel
and starting from different initial positions:
</p>
<p>7; 8; 1; 11; 10; 8
</p>
<p>11; 11; 8; 10; 9; 12:
</p>
<p>Using two segments of length b D 3, calculate the Gelman&ndash;Rubin statistic
p OR for
</p>
<p>both segments under the hypothesis of uncorrelated samples.
</p>
<p>16.6 Consider the step-function model described in Example 16.2, and a dataset
consisting of n measurements. Assuming that the priors on the parameters ï¿½, ï¿½ and
m are uniform, show that the full conditional distributions are given by
</p>
<p>8
ËÌ
ËÌ
ËÌ
&lt;Ì
</p>
<p>ËÌ
ËÌ
ËÌ
:Ì
</p>
<p>	ï¿½.ï¿½/ D G
ï¿½
</p>
<p>mP
</p>
<p>iD1
yi C 1;m
</p>
<p>ï¿½
</p>
<p>	ï¿½.ï¿½/ D G
ï¿½
</p>
<p>nP
</p>
<p>iDmC1
yi C 1; nï¿½ m
</p>
<p>ï¿½
</p>
<p>	m.m/ D e
ï¿½mï¿½ï¿½
</p>
<p>Pm
iD1 yieï¿½.nï¿½m/ï¿½ï¿½
</p>
<p>Pn
iDmC1 yi
</p>
<p>Pn
lD1 eï¿½lï¿½ï¿½
</p>
<p>Pl
iD1 yieï¿½.nï¿½l/ï¿½ï¿½
</p>
<p>Pn
iDlC1 yi
</p>
<p>;
</p>
<p>(16.24)
</p>
<p>where G represents the gamma distribution.
</p>
<p>16.7 Consider the step-function model described in Example 16.2, and a dataset
consisting of the following five measurements:
</p>
<p>0; 1; 3; 4; 2:
</p>
<p>Start a Metropolis&ndash;Hastings MCMC at ï¿½ D 0, ï¿½ D 2 and m D 1, and use uniform
priors on all three parameters. Assume for simplicity that all parameters can only</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Tests of Convergence 271
</p>
<p>be integer, and use uniform proposal distributions that span the ranges 
ï¿½ D Ë2,

ï¿½ D Ë2 and 
m D Ë2, and that the following numbers are drawn in the first
three iterations:
</p>
<p>Iteration 
ï¿½ 
ï¿½ 
m Ë
</p>
<p>1 C1 ï¿½1 C1 0:5
2 C1 C2 C1 0:7
3 ï¿½1 ï¿½2 C1 0:1
</p>
<p>With this information, calculate the first four links of the Metropolis&ndash;Hastings
MCMC.
</p>
<p>16.8 Consider a Monte Carlo Markov chain constructed with a Metropolis&ndash;
Hastings algorithm, using uniform prior and proposal distribution. At a given
iteration, the chain is at the point of maximum likelihood or, equivalently, minimum
ï¿½2. Calculate the probability of acceptance of a candidate that has, respectively,

ï¿½2 D 1, 2, and 10.</p>
<p/>
</div>
<div class="page"><p/>
<p>Appendix: Numerical Tables
</p>
<p>A.1 The Gaussian Distribution and the Error Function
</p>
<p>The Gaussian distribution (3.11) is defined as
</p>
<p>f .x/ D 1p
2	ï¿½2
</p>
<p>eï¿½
.xï¿½ï¿½/2
2ï¿½2 : (A.1)
</p>
<p>The maximum value is obtained at x D ï¿½, and the value of x where the Gaussian is
a times the peak value is given by
</p>
<p>z ï¿½ x ï¿½ ï¿½
ï¿½
D pï¿½2 ln a: (A.2)
</p>
<p>Figure A.1 shows a standard Gaussian normalized to its peak value, and values of a
times the peak value are tabulated in Table A.1. The Half Width at Half Maximum
(HWHM) has a value of approximately 1.18ï¿½ .
</p>
<p>The error function is defined in (3.13) as
</p>
<p>erf z D 1p
	
</p>
<p>Z z
</p>
<p>ï¿½z
eï¿½x2dx (A.3)
</p>
<p>and it is related to the integral of the Gaussian distribution defined in (3.12),
</p>
<p>A.z/ D
Z ï¿½Czï¿½
</p>
<p>ï¿½ï¿½zï¿½
f .x/dx D 1p
</p>
<p>2	
</p>
<p>Z z
</p>
<p>ï¿½z
eï¿½
</p>
<p>x2
</p>
<p>2 dx: (A.4)
</p>
<p>The relationship between the two integrals is given by
</p>
<p>erf
ï¿½
z=
p
2
	
D A.z/: (A.5)
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4
</p>
<p>273</p>
<p/>
</div>
<div class="page"><p/>
<p>274 Appendix: Numerical Tables
</p>
<p>Fig. A.1 Normalized values of the probability distribution function of a standard Gaussian (ï¿½ D 0
and ï¿½ D 1)
Table A.1 Values of a times the peak value for a Gaussian distribution
</p>
<p>a z a z a z a z a z
</p>
<p>0:980 0:201 0:960 0:286 0:940 0:352 0:920 0:408 0:900 0:459
</p>
<p>0:880 0:506 0:860 0:549 0:840 0:591 0:820 0:630 0:800 0:668
</p>
<p>0:780 0:705 0:760 0:741 0:740 0:776 0:720 0:811 0:700 0:845
</p>
<p>0:680 0:878 0:660 0:912 0:640 0:945 0:620 0:978 0:600 1:011
</p>
<p>0:580 1:044 0:560 1:077 0:540 1:110 0:520 1:144 0:500 1:177
</p>
<p>0:480 1:212 0:460 1:246 0:440 1:281 0:420 1:317 0:400 1:354
</p>
<p>0:380 1:391 0:360 1:429 0:340 1:469 0:320 1:510 0:300 1:552
</p>
<p>0:280 1:596 0:260 1:641 0:240 1:689 0:220 1:740 0:200 1:794
</p>
<p>0:180 1:852 0:160 1:914 0:140 1:983 0:120 2:059 0:100 2:146
</p>
<p>0:080 2:248 0:060 2:372 0:040 2:537 0:020 2:797 0:010 3:035
</p>
<p>The function A.z/ describes the integrated probability of a Gaussian distribution
to have values between ï¿½ ï¿½ zï¿½ and ï¿½C zï¿½ . The number z therefore represents the
number of ï¿½ by which the interval extends in each direction. The function A.z/ is
tabulated in Table A.2, where each number in the table corresponds to a number z
given by the number in the left column (e.g., 0.0, 0.1, etc.), and for which the second
decimal digit is given by the number in the top column (e.g., the value of 0.007979
corresponds to z D 0:01).
</p>
<p>The cumulative distribution of a standard Gaussian function was defined in (3.14)
as
</p>
<p>B.z/ D
Z z
</p>
<p>ï¿½1
1p
2	
</p>
<p>eï¿½
t2
</p>
<p>2 dtI (A.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>A.1 The Gaussian Distribution and the Error Function 275
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
V
</p>
<p>al
ue
</p>
<p>s
of
</p>
<p>th
e
</p>
<p>in
te
</p>
<p>gr
al
A
.z
/
</p>
<p>as
a
</p>
<p>fu
nc
</p>
<p>ti
on
</p>
<p>of
z,
</p>
<p>th
e
</p>
<p>nu
m
</p>
<p>be
r
</p>
<p>of
st
</p>
<p>an
da
</p>
<p>rd
er
</p>
<p>ro
rs
ï¿½
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
9
7
9
</p>
<p>0
:0
1
5
9
5
7
</p>
<p>0
:0
2
3
9
3
3
</p>
<p>0
:0
3
1
9
0
7
</p>
<p>0
:0
3
9
8
7
8
</p>
<p>0
:0
4
7
8
4
4
</p>
<p>0
:0
5
5
8
0
6
</p>
<p>0
:0
6
3
7
6
3
</p>
<p>0
:0
7
1
7
1
3
</p>
<p>0
:1
</p>
<p>0
:0
7
9
6
5
6
</p>
<p>0
:0
8
7
5
9
1
</p>
<p>0
:0
9
5
5
1
7
</p>
<p>0
:1
0
3
4
3
4
</p>
<p>0
:1
1
1
3
4
0
</p>
<p>0
:1
1
9
2
3
5
</p>
<p>0
:1
2
7
1
1
9
</p>
<p>0
:1
3
4
9
9
0
</p>
<p>0
:1
4
2
8
4
7
</p>
<p>0
:1
5
0
6
9
1
</p>
<p>0
:2
</p>
<p>0
:1
5
8
5
1
9
</p>
<p>0
:1
6
6
3
3
2
</p>
<p>0
:1
7
4
1
2
9
</p>
<p>0
:1
8
1
9
0
8
</p>
<p>0
:1
8
9
6
7
0
</p>
<p>0
:1
9
7
4
1
3
</p>
<p>0
:2
0
5
1
3
6
</p>
<p>0
:2
1
2
8
4
0
</p>
<p>0
:2
2
0
5
2
3
</p>
<p>0
:2
2
8
1
8
4
</p>
<p>0
:3
</p>
<p>0
:2
3
5
8
2
3
</p>
<p>0
:2
4
3
4
3
9
</p>
<p>0
:2
5
1
0
3
2
</p>
<p>0
:2
5
8
6
0
0
</p>
<p>0
:2
6
6
1
4
4
</p>
<p>0
:2
7
3
6
6
1
</p>
<p>0
:2
8
1
1
5
3
</p>
<p>0
:2
8
8
6
1
8
</p>
<p>0
:2
9
6
0
5
5
</p>
<p>0
:3
0
3
4
6
4
</p>
<p>0
:4
</p>
<p>0
:3
1
0
8
4
4
</p>
<p>0
:3
1
8
1
9
4
</p>
<p>0
:3
2
5
5
1
5
</p>
<p>0
:3
3
2
8
0
4
</p>
<p>0
:3
4
0
0
6
3
</p>
<p>0
:3
4
7
2
9
0
</p>
<p>0
:3
5
4
4
8
4
</p>
<p>0
:3
6
1
6
4
5
</p>
<p>0
:3
6
8
7
7
3
</p>
<p>0
:3
7
5
8
6
6
</p>
<p>0
:5
</p>
<p>0
:3
8
2
9
2
5
</p>
<p>0
:3
8
9
9
4
9
</p>
<p>0
:3
9
6
9
3
7
</p>
<p>0
:4
0
3
8
8
8
</p>
<p>0
:4
1
0
8
0
3
</p>
<p>0
:4
1
7
6
8
1
</p>
<p>0
:4
2
4
5
2
1
</p>
<p>0
:4
3
1
3
2
2
</p>
<p>0
:4
3
8
0
8
6
</p>
<p>0
:4
4
4
8
1
0
</p>
<p>0
:6
</p>
<p>0
:4
5
1
4
9
4
</p>
<p>0
:4
5
8
1
3
8
</p>
<p>0
:4
6
4
7
4
2
</p>
<p>0
:4
7
1
3
0
6
</p>
<p>0
:4
7
7
8
2
8
</p>
<p>0
:4
8
4
3
0
8
</p>
<p>0
:4
9
0
7
4
6
</p>
<p>0
:4
9
7
1
4
2
</p>
<p>0
:5
0
3
4
9
6
</p>
<p>0
:5
0
9
8
0
6
</p>
<p>0
:7
</p>
<p>0
:5
1
6
0
7
3
</p>
<p>0
:5
2
2
2
9
6
</p>
<p>0
:5
2
8
4
7
5
</p>
<p>0
:5
3
4
6
1
0
</p>
<p>0
:5
4
0
7
0
0
</p>
<p>0
:5
4
6
7
4
6
</p>
<p>0
:5
5
2
7
4
6
</p>
<p>0
:5
5
8
7
0
0
</p>
<p>0
:5
6
4
6
0
9
</p>
<p>0
:5
7
0
4
7
2
</p>
<p>0
:8
</p>
<p>0
:5
7
6
2
8
9
</p>
<p>0
:5
8
2
0
6
0
</p>
<p>0
:5
8
7
7
8
4
</p>
<p>0
:5
9
3
4
6
1
</p>
<p>0
:5
9
9
0
9
2
</p>
<p>0
:6
0
4
6
7
5
</p>
<p>0
:6
1
0
2
1
1
</p>
<p>0
:6
1
5
7
0
0
</p>
<p>0
:6
2
1
1
4
1
</p>
<p>0
:6
2
6
5
3
4
</p>
<p>0
:9
</p>
<p>0
:6
3
1
8
8
0
</p>
<p>0
:6
3
7
1
7
8
</p>
<p>0
:6
4
2
4
2
8
</p>
<p>0
:6
4
7
6
2
9
</p>
<p>0
:6
5
2
7
8
3
</p>
<p>0
:6
5
7
8
8
8
</p>
<p>0
:6
6
2
9
4
5
</p>
<p>0
:6
6
7
9
5
4
</p>
<p>0
:6
7
2
9
1
4
</p>
<p>0
:6
7
7
8
2
6
</p>
<p>1
:0
</p>
<p>0
:6
8
2
6
9
0
</p>
<p>0
:6
8
7
5
0
5
</p>
<p>0
:6
9
2
2
7
2
</p>
<p>0
:6
9
6
9
9
0
</p>
<p>0
:7
0
1
6
6
0
</p>
<p>0
:7
0
6
2
8
2
</p>
<p>0
:7
1
0
8
5
6
</p>
<p>0
:7
1
5
3
8
1
</p>
<p>0
:7
1
9
8
5
8
</p>
<p>0
:7
2
4
2
8
7
</p>
<p>1
:1
</p>
<p>0
:7
2
8
6
6
8
</p>
<p>0
:7
3
3
0
0
1
</p>
<p>0
:7
3
7
2
8
7
</p>
<p>0
:7
4
1
5
2
4
</p>
<p>0
:7
4
5
7
1
4
</p>
<p>0
:7
4
9
8
5
6
</p>
<p>0
:7
5
3
9
5
2
</p>
<p>0
:7
5
7
9
9
9
</p>
<p>0
:7
6
2
0
0
0
</p>
<p>0
:7
6
5
9
5
4
</p>
<p>1
:2
</p>
<p>0
:7
6
9
8
6
1
</p>
<p>0
:7
7
3
7
2
1
</p>
<p>0
:7
7
7
5
3
5
</p>
<p>0
:7
8
1
3
0
3
</p>
<p>0
:7
8
5
0
2
5
</p>
<p>0
:7
8
8
7
0
1
</p>
<p>0
:7
9
2
3
3
1
</p>
<p>0
:7
9
5
9
1
6
</p>
<p>0
:7
9
9
4
5
5
</p>
<p>0
:8
0
2
9
5
0
</p>
<p>1
:3
</p>
<p>0
:8
0
6
3
9
9
</p>
<p>0
:8
0
9
8
0
5
</p>
<p>0
:8
1
3
1
6
5
</p>
<p>0
:8
1
6
4
8
2
</p>
<p>0
:8
1
9
7
5
5
</p>
<p>0
:8
2
2
9
8
4
</p>
<p>0
:8
2
6
1
7
0
</p>
<p>0
:8
2
9
3
1
3
</p>
<p>0
:8
3
2
4
1
4
</p>
<p>0
:8
3
5
4
7
1
</p>
<p>1
:4
</p>
<p>0
:8
3
8
4
8
7
</p>
<p>0
:8
4
1
4
6
1
</p>
<p>0
:8
4
4
3
9
3
</p>
<p>0
:8
4
7
2
8
3
</p>
<p>0
:8
5
0
1
3
3
</p>
<p>0
:8
5
2
9
4
2
</p>
<p>0
:8
5
5
7
1
0
</p>
<p>0
:8
5
8
4
3
9
</p>
<p>0
:8
6
1
1
2
7
</p>
<p>0
:8
6
3
7
7
6
</p>
<p>1
:5
</p>
<p>0
:8
6
6
3
8
6
</p>
<p>0
:8
6
8
9
5
7
</p>
<p>0
:8
7
1
4
8
9
</p>
<p>0
:8
7
3
9
8
4
</p>
<p>0
:8
7
6
4
4
0
</p>
<p>0
:8
7
8
8
5
9
</p>
<p>0
:8
8
1
2
4
0
</p>
<p>0
:8
8
3
5
8
5
</p>
<p>0
:8
8
5
8
9
4
</p>
<p>0
:8
8
8
1
6
6
</p>
<p>1
:6
</p>
<p>0
:8
9
0
4
0
2
</p>
<p>0
:8
9
2
6
0
3
</p>
<p>0
:8
9
4
7
6
8
</p>
<p>0
:8
9
6
8
9
9
</p>
<p>0
:8
9
8
9
9
5
</p>
<p>0
:9
0
1
0
5
7
</p>
<p>0
:9
0
3
0
8
6
</p>
<p>0
:9
0
5
0
8
1
</p>
<p>0
:9
0
7
0
4
3
</p>
<p>0
:9
0
8
9
7
2
</p>
<p>1
:7
</p>
<p>0
:9
1
0
8
6
9
</p>
<p>0
:9
1
2
7
3
5
</p>
<p>0
:9
1
4
5
6
8
</p>
<p>0
:9
1
6
3
7
0
</p>
<p>0
:9
1
8
1
4
1
</p>
<p>0
:9
1
9
8
8
2
</p>
<p>0
:9
2
1
5
9
3
</p>
<p>0
:9
2
3
2
7
3
</p>
<p>0
:9
2
4
9
2
4
</p>
<p>0
:9
2
6
5
4
6
</p>
<p>1
:8
</p>
<p>0
:9
2
8
1
4
0
</p>
<p>0
:9
2
9
7
0
5
</p>
<p>0
:9
3
1
2
4
1
</p>
<p>0
:9
3
2
7
5
0
</p>
<p>0
:9
3
4
2
3
2
</p>
<p>0
:9
3
5
6
8
7
</p>
<p>0
:9
3
7
1
1
5
</p>
<p>0
:9
3
8
5
1
7
</p>
<p>0
:9
3
9
8
9
2
</p>
<p>0
:9
4
1
2
4
2
</p>
<p>1
:9
</p>
<p>0
:9
4
2
5
6
7
</p>
<p>0
:9
4
3
8
6
7
</p>
<p>0
:9
4
5
1
4
2
</p>
<p>0
:9
4
6
3
9
4
</p>
<p>0
:9
4
7
6
2
1
</p>
<p>0
:9
4
8
8
2
4
</p>
<p>0
:9
5
0
0
0
5
</p>
<p>0
:9
5
1
1
6
2
</p>
<p>0
:9
5
2
2
9
7
</p>
<p>0
:9
5
3
4
0
9
</p>
<p>2
:0
</p>
<p>0
:9
5
4
5
0
0
</p>
<p>0
:9
5
5
5
6
9
</p>
<p>0
:9
5
6
6
1
7
</p>
<p>0
:9
5
7
6
4
4
</p>
<p>0
:9
5
8
6
5
0
</p>
<p>0
:9
5
9
6
3
6
</p>
<p>0
:9
6
0
6
0
2
</p>
<p>0
:9
6
1
5
4
8
</p>
<p>0
:9
6
2
4
7
5
</p>
<p>0
:9
6
3
3
8
3
</p>
<p>2
:1
</p>
<p>0
:9
6
4
2
7
2
</p>
<p>0
:9
6
5
1
4
2
</p>
<p>0
:9
6
5
9
9
4
</p>
<p>0
:9
6
6
8
2
9
</p>
<p>0
:9
6
7
6
4
6
</p>
<p>0
:9
6
8
4
4
5
</p>
<p>0
:9
6
9
2
2
8
</p>
<p>0
:9
6
9
9
9
4
</p>
<p>0
:9
7
0
7
4
3
</p>
<p>0
:9
7
1
4
7
6
</p>
<p>2
:2
</p>
<p>0
:9
7
2
1
9
4
</p>
<p>0
:9
7
2
8
9
5
</p>
<p>0
:9
7
3
5
8
2
</p>
<p>0
:9
7
4
2
5
3
</p>
<p>0
:9
7
4
9
0
9
</p>
<p>0
:9
7
5
5
5
1
</p>
<p>0
:9
7
6
1
7
9
</p>
<p>0
:9
7
6
7
9
3
</p>
<p>0
:9
7
7
3
9
3
</p>
<p>0
:9
7
7
9
7
9
</p>
<p>2
:3
</p>
<p>0
:9
7
8
5
5
2
</p>
<p>0
:9
7
9
1
1
2
</p>
<p>0
:9
7
9
6
6
0
</p>
<p>0
:9
8
0
1
9
4
</p>
<p>0
:9
8
0
7
1
7
</p>
<p>0
:9
8
1
2
2
7
</p>
<p>0
:9
8
1
7
2
5
</p>
<p>0
:9
8
2
2
1
2
</p>
<p>0
:9
8
2
6
8
8
</p>
<p>0
:9
8
3
1
5
2
</p>
<p>2
:4
</p>
<p>0
:9
8
3
6
0
5
</p>
<p>0
:9
8
4
0
4
8
</p>
<p>0
:9
8
4
4
8
0
</p>
<p>0
:9
8
4
9
0
2
</p>
<p>0
:9
8
5
3
1
3
</p>
<p>0
:9
8
5
7
1
5
</p>
<p>0
:9
8
6
1
0
7
</p>
<p>0
:9
8
6
4
8
9
</p>
<p>0
:9
8
6
8
6
2
</p>
<p>0
:9
8
7
2
2
6
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>276 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
(c
</p>
<p>on
ti
</p>
<p>nu
ed
</p>
<p>)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
8
7
5
8
1
</p>
<p>0
:9
8
7
9
2
7
</p>
<p>0
:9
8
8
2
6
5
</p>
<p>0
:9
8
8
5
9
4
</p>
<p>0
:9
8
8
9
1
5
</p>
<p>0
:9
8
9
2
2
8
</p>
<p>0
:9
8
9
5
3
3
</p>
<p>0
:9
8
9
8
3
1
</p>
<p>0
:9
9
0
1
2
0
</p>
<p>0
:9
9
0
4
0
3
</p>
<p>2
:6
</p>
<p>0
:9
9
0
6
7
8
</p>
<p>0
:9
9
0
9
4
6
</p>
<p>0
:9
9
1
2
0
7
</p>
<p>0
:9
9
1
4
6
2
</p>
<p>0
:9
9
1
7
1
0
</p>
<p>0
:9
9
1
9
5
1
</p>
<p>0
:9
9
2
1
8
6
</p>
<p>0
:9
9
2
4
1
5
</p>
<p>0
:9
9
2
6
3
8
</p>
<p>0
:9
9
2
8
5
5
</p>
<p>2
:7
</p>
<p>0
:9
9
3
0
6
6
</p>
<p>0
:9
9
3
2
7
2
</p>
<p>0
:9
9
3
4
7
2
</p>
<p>0
:9
9
3
6
6
7
</p>
<p>0
:9
9
3
8
5
7
</p>
<p>0
:9
9
4
0
4
1
</p>
<p>0
:9
9
4
2
2
0
</p>
<p>0
:9
9
4
3
9
5
</p>
<p>0
:9
9
4
5
6
5
</p>
<p>0
:9
9
4
7
3
0
</p>
<p>2
:8
</p>
<p>0
:9
9
4
8
9
0
</p>
<p>0
:9
9
5
0
4
6
</p>
<p>0
:9
9
5
1
9
8
</p>
<p>0
:9
9
5
3
4
6
</p>
<p>0
:9
9
5
4
8
9
</p>
<p>0
:9
9
5
6
2
8
</p>
<p>0
:9
9
5
7
6
4
</p>
<p>0
:9
9
5
8
9
6
</p>
<p>0
:9
9
6
0
2
4
</p>
<p>0
:9
9
6
1
4
8
</p>
<p>2
:9
</p>
<p>0
:9
9
6
2
6
9
</p>
<p>0
:9
9
6
3
8
6
</p>
<p>0
:9
9
6
5
0
0
</p>
<p>0
:9
9
6
6
1
1
</p>
<p>0
:9
9
6
7
1
8
</p>
<p>0
:9
9
6
8
2
3
</p>
<p>0
:9
9
6
9
2
4
</p>
<p>0
:9
9
7
0
2
2
</p>
<p>0
:9
9
7
1
1
8
</p>
<p>0
:9
9
7
2
1
1
</p>
<p>3
:0
</p>
<p>0
:9
9
7
3
0
1
</p>
<p>0
:9
9
7
3
8
8
</p>
<p>0
:9
9
7
4
7
3
</p>
<p>0
:9
9
7
5
5
5
</p>
<p>0
:9
9
7
6
3
5
</p>
<p>0
:9
9
7
7
1
2
</p>
<p>0
:9
9
7
7
8
7
</p>
<p>0
:9
9
7
8
6
0
</p>
<p>0
:9
9
7
9
3
0
</p>
<p>0
:9
9
7
9
9
9
</p>
<p>3
:1
</p>
<p>0
:9
9
8
0
6
5
</p>
<p>0
:9
9
8
1
3
0
</p>
<p>0
:9
9
8
1
9
2
</p>
<p>0
:9
9
8
2
5
2
</p>
<p>0
:9
9
8
3
1
1
</p>
<p>0
:9
9
8
3
6
8
</p>
<p>0
:9
9
8
4
2
3
</p>
<p>0
:9
9
8
4
7
6
</p>
<p>0
:9
9
8
5
2
8
</p>
<p>0
:9
9
8
5
7
8
</p>
<p>3
:2
</p>
<p>0
:9
9
8
6
2
6
</p>
<p>0
:9
9
8
6
7
3
</p>
<p>0
:9
9
8
7
1
9
</p>
<p>0
:9
9
8
7
6
3
</p>
<p>0
:9
9
8
8
0
5
</p>
<p>0
:9
9
8
8
4
6
</p>
<p>0
:9
9
8
8
8
6
</p>
<p>0
:9
9
8
9
2
5
</p>
<p>0
:9
9
8
9
6
2
</p>
<p>0
:9
9
8
9
9
9
</p>
<p>3
:3
</p>
<p>0
:9
9
9
0
3
4
</p>
<p>0
:9
9
9
0
6
7
</p>
<p>0
:9
9
9
1
0
0
</p>
<p>0
:9
9
9
1
3
2
</p>
<p>0
:9
9
9
1
6
3
</p>
<p>0
:9
9
9
1
9
2
</p>
<p>0
:9
9
9
2
2
1
</p>
<p>0
:9
9
9
2
4
9
</p>
<p>0
:9
9
9
2
7
6
</p>
<p>0
:9
9
9
3
0
1
</p>
<p>3
:4
</p>
<p>0
:9
9
9
3
2
7
</p>
<p>0
:9
9
9
3
5
1
</p>
<p>0
:9
9
9
3
7
4
</p>
<p>0
:9
9
9
3
9
7
</p>
<p>0
:9
9
9
4
1
9
</p>
<p>0
:9
9
9
4
4
0
</p>
<p>0
:9
9
9
4
6
0
</p>
<p>0
:9
9
9
4
8
0
</p>
<p>0
:9
9
9
4
9
9
</p>
<p>0
:9
9
9
5
1
7
</p>
<p>3
:5
</p>
<p>0
:9
9
9
5
3
5
</p>
<p>0
:9
9
9
5
5
2
</p>
<p>0
:9
9
9
5
6
9
</p>
<p>0
:9
9
9
5
8
5
</p>
<p>0
:9
9
9
6
0
0
</p>
<p>0
:9
9
9
6
1
5
</p>
<p>0
:9
9
9
6
3
0
</p>
<p>0
:9
9
9
6
4
3
</p>
<p>0
:9
9
9
6
5
7
</p>
<p>0
:9
9
9
6
7
0
</p>
<p>3
:6
</p>
<p>0
:9
9
9
6
8
2
</p>
<p>0
:9
9
9
6
9
4
</p>
<p>0
:9
9
9
7
0
6
</p>
<p>0
:9
9
9
7
1
7
</p>
<p>0
:9
9
9
7
2
8
</p>
<p>0
:9
9
9
7
3
8
</p>
<p>0
:9
9
9
7
4
8
</p>
<p>0
:9
9
9
7
5
8
</p>
<p>0
:9
9
9
7
6
7
</p>
<p>0
:9
9
9
7
7
6
</p>
<p>3
:7
</p>
<p>0
:9
9
9
7
8
5
</p>
<p>0
:9
9
9
7
9
3
</p>
<p>0
:9
9
9
8
0
1
</p>
<p>0
:9
9
9
8
0
9
</p>
<p>0
:9
9
9
8
1
6
</p>
<p>0
:9
9
9
8
2
4
</p>
<p>0
:9
9
9
8
3
1
</p>
<p>0
:9
9
9
8
3
7
</p>
<p>0
:9
9
9
8
4
4
</p>
<p>0
:9
9
9
8
5
0
</p>
<p>3
:8
</p>
<p>0
:9
9
9
8
5
6
</p>
<p>0
:9
9
9
8
6
1
</p>
<p>0
:9
9
9
8
6
7
</p>
<p>0
:9
9
9
8
7
2
</p>
<p>0
:9
9
9
8
7
7
</p>
<p>0
:9
9
9
8
8
2
</p>
<p>0
:9
9
9
8
8
7
</p>
<p>0
:9
9
9
8
9
2
</p>
<p>0
:9
9
9
8
9
6
</p>
<p>0
:9
9
9
9
0
0
</p>
<p>3
:9
</p>
<p>0
:9
9
9
9
0
4
</p>
<p>0
:9
9
9
9
0
8
</p>
<p>0
:9
9
9
9
1
2
</p>
<p>0
:9
9
9
9
1
5
</p>
<p>0
:9
9
9
9
1
9
</p>
<p>0
:9
9
9
9
2
2
</p>
<p>0
:9
9
9
9
2
5
</p>
<p>0
:9
9
9
9
2
9
</p>
<p>0
:9
9
9
9
3
2
</p>
<p>0
:9
9
9
9
3
4
</p>
<p>4
:0
</p>
<p>0
:9
9
9
9
3
7
</p>
<p>0
:9
9
9
9
4
0
</p>
<p>0
:9
9
9
9
4
2
</p>
<p>0
:9
9
9
9
4
5
</p>
<p>0
:9
9
9
9
4
7
</p>
<p>0
:9
9
9
9
4
9
</p>
<p>0
:9
9
9
9
5
1
</p>
<p>0
:9
9
9
9
5
3
</p>
<p>0
:9
9
9
9
5
5
</p>
<p>0
:9
9
9
9
5
7
</p>
<p>4
:1
</p>
<p>0
:9
9
9
9
5
9
</p>
<p>0
:9
9
9
9
6
1
</p>
<p>0
:9
9
9
9
6
3
</p>
<p>0
:9
9
9
9
6
4
</p>
<p>0
:9
9
9
9
6
6
</p>
<p>0
:9
9
9
9
6
7
</p>
<p>0
:9
9
9
9
6
9
</p>
<p>0
:9
9
9
9
7
0
</p>
<p>0
:9
9
9
9
7
1
</p>
<p>0
:9
9
9
9
7
3
</p>
<p>4
:2
</p>
<p>0
:9
9
9
9
7
4
</p>
<p>0
:9
9
9
9
7
5
</p>
<p>0
:9
9
9
9
7
6
</p>
<p>0
:9
9
9
9
7
7
</p>
<p>0
:9
9
9
9
7
8
</p>
<p>0
:9
9
9
9
7
9
</p>
<p>0
:9
9
9
9
8
0
</p>
<p>0
:9
9
9
9
8
1
</p>
<p>0
:9
9
9
9
8
2
</p>
<p>0
:9
9
9
9
8
3
</p>
<p>4
:3
</p>
<p>0
:9
9
9
9
8
3
</p>
<p>0
:9
9
9
9
8
4
</p>
<p>0
:9
9
9
9
8
5
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>0
:9
9
9
9
8
7
</p>
<p>0
:9
9
9
9
8
7
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>4
:4
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>4
:5
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>4
:6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>4
:7
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>4
:8
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>4
:9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 The ï¿½2 Distribution 277
</p>
<p>and it is therefore related to the integral A.z/ by
</p>
<p>B.z/ D 1
2
C A.z/
</p>
<p>2
: (A.7)
</p>
<p>The values of B.z/ are tabulated in Table A.3. Each number in the table corresponds
to a number z given by the number in the left column (e.g., 0.0, 0.1, etc.), and for
which the second decimal digit is given by the number in the top column (e.g., the
value of 0.503990 corresponds to z D 0:01).
</p>
<p>Critical values of the standard Gaussian distribution functions corresponding to
selected values of the integrals A.z/ and B.z/ are shown in Table A.4. They indicate
the value of the variable z required to include a given probability, and are useful for
either two-sided or one-sided rejection regions in hypothesis testing.
</p>
<p>A.2 Upper and Lower Limits for a Poisson Distribution
</p>
<p>The Gehrels approximation described in [16] can be used to calculate upper
and lower limits for a Poisson distribution, when nobs counts are recorded. The
confidence level is described by the parameter S, corresponding to the number of
standard deviations ï¿½ for a Gaussian distribution; for example, S D 1 corresponds
to an 84.1 % confidence level, S D 2 to a 97.7 %, and S D 3 corresponds to 99.9 %;
see Table 5.2 for correspondence between values of S and probability. The upper
and lower limits are described, in the simplest approximation, by
</p>
<p>8
ËÌ
&lt;
</p>
<p>ËÌ
:
</p>
<p>ï¿½up D nobs C S
2 C 3
4
C S
</p>
<p>r
</p>
<p>nobs C 3
4
</p>
<p>ï¿½lo D nobs
ï¿½
</p>
<p>1 ï¿½ 1
9nobs
</p>
<p>ï¿½ S
3
p
nobs
</p>
<p>ï¿½3 (A.8)
</p>
<p>and more accurate approximations are provided in [16] (Tables A.5 and A.6).
</p>
<p>A.3 The ï¿½2 Distribution
</p>
<p>The probability distribution function for a ï¿½2 variable is defined in (7.11) as
</p>
<p>fï¿½2 .z/ D
ï¿½
1
</p>
<p>2
</p>
<p>ï¿½f=2
1
</p>
<p>ï¿½ . f=2/
eï¿½
</p>
<p>z
2 z
</p>
<p>f
2
ï¿½1
;</p>
<p/>
</div>
<div class="page"><p/>
<p>278 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.3
V
</p>
<p>al
ue
</p>
<p>s
of
</p>
<p>th
e
</p>
<p>in
te
</p>
<p>gr
al
B
.z
/
</p>
<p>as
a
</p>
<p>fu
nc
</p>
<p>ti
on
</p>
<p>of
z
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:5
0
0
0
0
0
</p>
<p>0
:5
0
3
9
9
0
</p>
<p>0
:5
0
7
9
7
9
</p>
<p>0
:5
1
1
9
6
7
</p>
<p>0
:5
1
5
9
5
4
</p>
<p>0
:5
1
9
9
3
9
</p>
<p>0
:5
2
3
9
2
2
</p>
<p>0
:5
2
7
9
0
3
</p>
<p>0
:5
3
1
8
8
2
</p>
<p>0
:5
3
5
8
5
7
</p>
<p>0
:1
</p>
<p>0
:5
3
9
8
2
8
</p>
<p>0
:5
4
3
7
9
6
</p>
<p>0
:5
4
7
7
5
9
</p>
<p>0
:5
5
1
7
1
7
</p>
<p>0
:5
5
5
6
7
0
</p>
<p>0
:5
5
9
6
1
8
</p>
<p>0
:5
6
3
5
6
0
</p>
<p>0
:5
6
7
4
9
5
</p>
<p>0
:5
7
1
4
2
4
</p>
<p>0
:5
7
5
3
4
6
</p>
<p>0
:2
</p>
<p>0
:5
7
9
2
6
0
</p>
<p>0
:5
8
3
1
6
6
</p>
<p>0
:5
8
7
0
6
5
</p>
<p>0
:5
9
0
9
5
4
</p>
<p>0
:5
9
4
8
3
5
</p>
<p>0
:5
9
8
7
0
7
</p>
<p>0
:6
0
2
5
6
8
</p>
<p>0
:6
0
6
4
2
0
</p>
<p>0
:6
1
0
2
6
2
</p>
<p>0
:6
1
4
0
9
2
</p>
<p>0
:3
</p>
<p>0
:6
1
7
9
1
2
</p>
<p>0
:6
2
1
7
2
0
</p>
<p>0
:6
2
5
5
1
6
</p>
<p>0
:6
2
9
3
0
0
</p>
<p>0
:6
3
3
0
7
2
</p>
<p>0
:6
3
6
8
3
1
</p>
<p>0
:6
4
0
5
7
7
</p>
<p>0
:6
4
4
3
0
9
</p>
<p>0
:6
4
8
0
2
8
</p>
<p>0
:6
5
1
7
3
2
</p>
<p>0
:4
</p>
<p>0
:6
5
5
4
2
2
</p>
<p>0
:6
5
9
0
9
7
</p>
<p>0
:6
6
2
7
5
8
</p>
<p>0
:6
6
6
4
0
2
</p>
<p>0
:6
7
0
0
3
2
</p>
<p>0
:6
7
3
6
4
5
</p>
<p>0
:6
7
7
2
4
2
</p>
<p>0
:6
8
0
8
2
3
</p>
<p>0
:6
8
4
3
8
7
</p>
<p>0
:6
8
7
9
3
3
</p>
<p>0
:5
</p>
<p>0
:6
9
1
4
6
3
</p>
<p>0
:6
9
4
9
7
5
</p>
<p>0
:6
9
8
4
6
9
</p>
<p>0
:7
0
1
9
4
4
</p>
<p>0
:7
0
5
4
0
2
</p>
<p>0
:7
0
8
8
4
1
</p>
<p>0
:7
1
2
2
6
1
</p>
<p>0
:7
1
5
6
6
1
</p>
<p>0
:7
1
9
0
4
3
</p>
<p>0
:7
2
2
4
0
5
</p>
<p>0
:6
</p>
<p>0
:7
2
5
7
4
7
</p>
<p>0
:7
2
9
0
6
9
</p>
<p>0
:7
3
2
3
7
1
</p>
<p>0
:7
3
5
6
5
3
</p>
<p>0
:7
3
8
9
1
4
</p>
<p>0
:7
4
2
1
5
4
</p>
<p>0
:7
4
5
3
7
3
</p>
<p>0
:7
4
8
5
7
1
</p>
<p>0
:7
5
1
7
4
8
</p>
<p>0
:7
5
4
9
0
3
</p>
<p>0
:7
</p>
<p>0
:7
5
8
0
3
7
</p>
<p>0
:7
6
1
1
4
8
</p>
<p>0
:7
6
4
2
3
8
</p>
<p>0
:7
6
7
3
0
5
</p>
<p>0
:7
7
0
3
5
0
</p>
<p>0
:7
7
3
3
7
3
</p>
<p>0
:7
7
6
3
7
3
</p>
<p>0
:7
7
9
3
5
0
</p>
<p>0
:7
8
2
3
0
5
</p>
<p>0
:7
8
5
2
3
6
</p>
<p>0
:8
</p>
<p>0
:7
8
8
1
4
5
</p>
<p>0
:7
9
1
0
3
0
</p>
<p>0
:7
9
3
8
9
2
</p>
<p>0
:7
9
6
7
3
1
</p>
<p>0
:7
9
9
5
4
6
</p>
<p>0
:8
0
2
3
3
8
</p>
<p>0
:8
0
5
1
0
6
</p>
<p>0
:8
0
7
8
5
0
</p>
<p>0
:8
1
0
5
7
1
</p>
<p>0
:8
1
3
2
6
7
</p>
<p>0
:9
</p>
<p>0
:8
1
5
9
4
0
</p>
<p>0
:8
1
8
5
8
9
</p>
<p>0
:8
2
1
2
1
4
</p>
<p>0
:8
2
3
8
1
5
</p>
<p>0
:8
2
6
3
9
2
</p>
<p>0
:8
2
8
9
4
4
</p>
<p>0
:8
3
1
4
7
3
</p>
<p>0
:8
3
3
9
7
7
</p>
<p>0
:8
3
6
4
5
7
</p>
<p>0
:8
3
8
9
1
3
</p>
<p>1
:0
</p>
<p>0
:8
4
1
3
4
5
</p>
<p>0
:8
4
3
7
5
3
</p>
<p>0
:8
4
6
1
3
6
</p>
<p>0
:8
4
8
4
9
5
</p>
<p>0
:8
5
0
8
3
0
</p>
<p>0
:8
5
3
1
4
1
</p>
<p>0
:8
5
5
4
2
8
</p>
<p>0
:8
5
7
6
9
1
</p>
<p>0
:8
5
9
9
2
9
</p>
<p>0
:8
6
2
1
4
4
</p>
<p>1
:1
</p>
<p>0
:8
6
4
3
3
4
</p>
<p>0
:8
6
6
5
0
1
</p>
<p>0
:8
6
8
6
4
3
</p>
<p>0
:8
7
0
7
6
2
</p>
<p>0
:8
7
2
8
5
7
</p>
<p>0
:8
7
4
9
2
8
</p>
<p>0
:8
7
6
9
7
6
</p>
<p>0
:8
7
9
0
0
0
</p>
<p>0
:8
8
1
0
0
0
</p>
<p>0
:8
8
2
9
7
7
</p>
<p>1
:2
</p>
<p>0
:8
8
4
9
3
1
</p>
<p>0
:8
8
6
8
6
1
</p>
<p>0
:8
8
8
7
6
8
</p>
<p>0
:8
9
0
6
5
2
</p>
<p>0
:8
9
2
5
1
3
</p>
<p>0
:8
9
4
3
5
1
</p>
<p>0
:8
9
6
1
6
6
</p>
<p>0
:8
9
7
9
5
8
</p>
<p>0
:8
9
9
7
2
8
</p>
<p>0
:9
0
1
4
7
5
</p>
<p>1
:3
</p>
<p>0
:9
0
3
2
0
0
</p>
<p>0
:9
0
4
9
0
2
</p>
<p>0
:9
0
6
5
8
3
</p>
<p>0
:9
0
8
2
4
1
</p>
<p>0
:9
0
9
8
7
8
</p>
<p>0
:9
1
1
4
9
2
</p>
<p>0
:9
1
3
0
8
5
</p>
<p>0
:9
1
4
6
5
7
</p>
<p>0
:9
1
6
2
0
7
</p>
<p>0
:9
1
7
7
3
6
</p>
<p>1
:4
</p>
<p>0
:9
1
9
2
4
4
</p>
<p>0
:9
2
0
7
3
1
</p>
<p>0
:9
2
2
1
9
7
</p>
<p>0
:9
2
3
6
4
2
</p>
<p>0
:9
2
5
0
6
7
</p>
<p>0
:9
2
6
4
7
1
</p>
<p>0
:9
2
7
8
5
5
</p>
<p>0
:9
2
9
2
2
0
</p>
<p>0
:9
3
0
5
6
4
</p>
<p>0
:9
3
1
8
8
8
</p>
<p>1
:5
</p>
<p>0
:9
3
3
1
9
3
</p>
<p>0
:9
3
4
4
7
9
</p>
<p>0
:9
3
5
7
4
5
</p>
<p>0
:9
3
6
9
9
2
</p>
<p>0
:9
3
8
2
2
0
</p>
<p>0
:9
3
9
4
3
0
</p>
<p>0
:9
4
0
6
2
0
</p>
<p>0
:9
4
1
7
9
3
</p>
<p>0
:9
4
2
9
4
7
</p>
<p>0
:9
4
4
0
8
3
</p>
<p>1
:6
</p>
<p>0
:9
4
5
2
0
1
</p>
<p>0
:9
4
6
3
0
1
</p>
<p>0
:9
4
7
3
8
4
</p>
<p>0
:9
4
8
4
5
0
</p>
<p>0
:9
4
9
4
9
8
</p>
<p>0
:9
5
0
5
2
9
</p>
<p>0
:9
5
1
5
4
3
</p>
<p>0
:9
5
2
5
4
1
</p>
<p>0
:9
5
3
5
2
2
</p>
<p>0
:9
5
4
4
8
6
</p>
<p>1
:7
</p>
<p>0
:9
5
5
4
3
5
</p>
<p>0
:9
5
6
3
6
7
</p>
<p>0
:9
5
7
2
8
4
</p>
<p>0
:9
5
8
1
8
5
</p>
<p>0
:9
5
9
0
7
1
</p>
<p>0
:9
5
9
9
4
1
</p>
<p>0
:9
6
0
7
9
7
</p>
<p>0
:9
6
1
6
3
7
</p>
<p>0
:9
6
2
4
6
2
</p>
<p>0
:9
6
3
2
7
3
</p>
<p>1
:8
</p>
<p>0
:9
6
4
0
7
0
</p>
<p>0
:9
6
4
8
5
3
</p>
<p>0
:9
6
5
6
2
1
</p>
<p>0
:9
6
6
3
7
5
</p>
<p>0
:9
6
7
1
1
6
</p>
<p>0
:9
6
7
8
4
4
</p>
<p>0
:9
6
8
5
5
8
</p>
<p>0
:9
6
9
2
5
9
</p>
<p>0
:9
6
9
9
4
6
</p>
<p>0
:9
7
0
6
2
1
</p>
<p>1
:9
</p>
<p>0
:9
7
1
2
8
4
</p>
<p>0
:9
7
1
9
3
4
</p>
<p>0
:9
7
2
5
7
1
</p>
<p>0
:9
7
3
1
9
7
</p>
<p>0
:9
7
3
8
1
1
</p>
<p>0
:9
7
4
4
1
2
</p>
<p>0
:9
7
5
0
0
3
</p>
<p>0
:9
7
5
5
8
1
</p>
<p>0
:9
7
6
1
4
9
</p>
<p>0
:9
7
6
7
0
5
</p>
<p>2
:0
</p>
<p>0
:9
7
7
2
5
0
</p>
<p>0
:9
7
7
7
8
5
</p>
<p>0
:9
7
8
3
0
9
</p>
<p>0
:9
7
8
8
2
2
</p>
<p>0
:9
7
9
3
2
5
</p>
<p>0
:9
7
9
8
1
8
</p>
<p>0
:9
8
0
3
0
1
</p>
<p>0
:9
8
0
7
7
4
</p>
<p>0
:9
8
1
2
3
8
</p>
<p>0
:9
8
1
6
9
2
</p>
<p>2
:1
</p>
<p>0
:9
8
2
1
3
6
</p>
<p>0
:9
8
2
5
7
1
</p>
<p>0
:9
8
2
9
9
7
</p>
<p>0
:9
8
3
4
1
5
</p>
<p>0
:9
8
3
8
2
3
</p>
<p>0
:9
8
4
2
2
3
</p>
<p>0
:9
8
4
6
1
4
</p>
<p>0
:9
8
4
9
9
7
</p>
<p>0
:9
8
5
3
7
2
</p>
<p>0
:9
8
5
7
3
8
</p>
<p>2
:2
</p>
<p>0
:9
8
6
0
9
7
</p>
<p>0
:9
8
6
4
4
8
</p>
<p>0
:9
8
6
7
9
1
</p>
<p>0
:9
8
7
1
2
7
</p>
<p>0
:9
8
7
4
5
5
</p>
<p>0
:9
8
7
7
7
6
</p>
<p>0
:9
8
8
0
9
0
</p>
<p>0
:9
8
8
3
9
7
</p>
<p>0
:9
8
8
6
9
7
</p>
<p>0
:9
8
8
9
9
0
</p>
<p>2
:3
</p>
<p>0
:9
8
9
2
7
6
</p>
<p>0
:9
8
9
5
5
6
</p>
<p>0
:9
8
9
8
3
0
</p>
<p>0
:9
9
0
0
9
7
</p>
<p>0
:9
9
0
3
5
9
</p>
<p>0
:9
9
0
6
1
4
</p>
<p>0
:9
9
0
8
6
3
</p>
<p>0
:9
9
1
1
0
6
</p>
<p>0
:9
9
1
3
4
4
</p>
<p>0
:9
9
1
5
7
6
</p>
<p>2
:4
</p>
<p>0
:9
9
1
8
0
3
</p>
<p>0
:9
9
2
0
2
4
</p>
<p>0
:9
9
2
2
4
0
</p>
<p>0
:9
9
2
4
5
1
</p>
<p>0
:9
9
2
6
5
7
</p>
<p>0
:9
9
2
8
5
8
</p>
<p>0
:9
9
3
0
5
4
</p>
<p>0
:9
9
3
2
4
5
</p>
<p>0
:9
9
3
4
3
1
</p>
<p>0
:9
9
3
6
1
3</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 The ï¿½2 Distribution 279
</p>
<p>2
:5
</p>
<p>0
:9
9
3
7
9
1
</p>
<p>0
:9
9
3
9
6
4
</p>
<p>0
:9
9
4
1
3
3
</p>
<p>0
:9
9
4
2
9
7
</p>
<p>0
:9
9
4
4
5
8
</p>
<p>0
:9
9
4
6
1
4
</p>
<p>0
:9
9
4
7
6
7
</p>
<p>0
:9
9
4
9
1
5
</p>
<p>0
:9
9
5
0
6
0
</p>
<p>0
:9
9
5
2
0
2
</p>
<p>2
:6
</p>
<p>0
:9
9
5
3
3
9
</p>
<p>0
:9
9
5
4
7
3
</p>
<p>0
:9
9
5
6
0
4
</p>
<p>0
:9
9
5
7
3
1
</p>
<p>0
:9
9
5
8
5
5
</p>
<p>0
:9
9
5
9
7
6
</p>
<p>0
:9
9
6
0
9
3
</p>
<p>0
:9
9
6
2
0
8
</p>
<p>0
:9
9
6
3
1
9
</p>
<p>0
:9
9
6
4
2
8
</p>
<p>2
:7
</p>
<p>0
:9
9
6
5
3
3
</p>
<p>0
:9
9
6
6
3
6
</p>
<p>0
:9
9
6
7
3
6
</p>
<p>0
:9
9
6
8
3
4
</p>
<p>0
:9
9
6
9
2
8
</p>
<p>0
:9
9
7
0
2
1
</p>
<p>0
:9
9
7
1
1
0
</p>
<p>0
:9
9
7
1
9
8
</p>
<p>0
:9
9
7
2
8
2
</p>
<p>0
:9
9
7
3
6
5
</p>
<p>2
:8
</p>
<p>0
:9
9
7
4
4
5
</p>
<p>0
:9
9
7
5
2
3
</p>
<p>0
:9
9
7
5
9
9
</p>
<p>0
:9
9
7
6
7
3
</p>
<p>0
:9
9
7
7
4
5
</p>
<p>0
:9
9
7
8
1
4
</p>
<p>0
:9
9
7
8
8
2
</p>
<p>0
:9
9
7
9
4
8
</p>
<p>0
:9
9
8
0
1
2
</p>
<p>0
:9
9
8
0
7
4
</p>
<p>2
:9
</p>
<p>0
:9
9
8
1
3
5
</p>
<p>0
:9
9
8
1
9
3
</p>
<p>0
:9
9
8
2
5
0
</p>
<p>0
:9
9
8
3
0
6
</p>
<p>0
:9
9
8
3
5
9
</p>
<p>0
:9
9
8
4
1
2
</p>
<p>0
:9
9
8
4
6
2
</p>
<p>0
:9
9
8
5
1
1
</p>
<p>0
:9
9
8
5
5
9
</p>
<p>0
:9
9
8
6
0
6
</p>
<p>3
:0
</p>
<p>0
:9
9
8
6
5
1
</p>
<p>0
:9
9
8
6
9
4
</p>
<p>0
:9
9
8
7
3
7
</p>
<p>0
:9
9
8
7
7
8
</p>
<p>0
:9
9
8
8
1
8
</p>
<p>0
:9
9
8
8
5
6
</p>
<p>0
:9
9
8
8
9
4
</p>
<p>0
:9
9
8
9
3
0
</p>
<p>0
:9
9
8
9
6
5
</p>
<p>0
:9
9
9
0
0
0
</p>
<p>3
:1
</p>
<p>0
:9
9
9
0
3
3
</p>
<p>0
:9
9
9
0
6
5
</p>
<p>0
:9
9
9
0
9
6
</p>
<p>0
:9
9
9
1
2
6
</p>
<p>0
:9
9
9
1
5
6
</p>
<p>0
:9
9
9
1
8
4
</p>
<p>0
:9
9
9
2
1
2
</p>
<p>0
:9
9
9
2
3
8
</p>
<p>0
:9
9
9
2
6
4
</p>
<p>0
:9
9
9
2
8
9
</p>
<p>3
:2
</p>
<p>0
:9
9
9
3
1
3
</p>
<p>0
:9
9
9
3
3
7
</p>
<p>0
:9
9
9
3
5
9
</p>
<p>0
:9
9
9
3
8
1
</p>
<p>0
:9
9
9
4
0
3
</p>
<p>0
:9
9
9
4
2
3
</p>
<p>0
:9
9
9
4
4
3
</p>
<p>0
:9
9
9
4
6
3
</p>
<p>0
:9
9
9
4
8
1
</p>
<p>0
:9
9
9
4
9
9
</p>
<p>3
:3
</p>
<p>0
:9
9
9
5
1
7
</p>
<p>0
:9
9
9
5
3
4
</p>
<p>0
:9
9
9
5
5
0
</p>
<p>0
:9
9
9
5
6
6
</p>
<p>0
:9
9
9
5
8
2
</p>
<p>0
:9
9
9
5
9
6
</p>
<p>0
:9
9
9
6
1
1
</p>
<p>0
:9
9
9
6
2
5
</p>
<p>0
:9
9
9
6
3
8
</p>
<p>0
:9
9
9
6
5
1
</p>
<p>3
:4
</p>
<p>0
:9
9
9
6
6
3
</p>
<p>0
:9
9
9
6
7
6
</p>
<p>0
:9
9
9
6
8
7
</p>
<p>0
:9
9
9
6
9
9
</p>
<p>0
:9
9
9
7
1
0
</p>
<p>0
:9
9
9
7
2
0
</p>
<p>0
:9
9
9
7
3
0
</p>
<p>0
:9
9
9
7
4
0
</p>
<p>0
:9
9
9
7
5
0
</p>
<p>0
:9
9
9
7
5
9
</p>
<p>3
:5
</p>
<p>0
:9
9
9
7
6
8
</p>
<p>0
:9
9
9
7
7
6
</p>
<p>0
:9
9
9
7
8
5
</p>
<p>0
:9
9
9
7
9
3
</p>
<p>0
:9
9
9
8
0
0
</p>
<p>0
:9
9
9
8
0
8
</p>
<p>0
:9
9
9
8
1
5
</p>
<p>0
:9
9
9
8
2
2
</p>
<p>0
:9
9
9
8
2
9
</p>
<p>0
:9
9
9
8
3
5
</p>
<p>3
:6
</p>
<p>0
:9
9
9
8
4
1
</p>
<p>0
:9
9
9
8
4
7
</p>
<p>0
:9
9
9
8
5
3
</p>
<p>0
:9
9
9
8
5
9
</p>
<p>0
:9
9
9
8
6
4
</p>
<p>0
:9
9
9
8
6
9
</p>
<p>0
:9
9
9
8
7
4
</p>
<p>0
:9
9
9
8
7
9
</p>
<p>0
:9
9
9
8
8
4
</p>
<p>0
:9
9
9
8
8
8
</p>
<p>3
:7
</p>
<p>0
:9
9
9
8
9
3
</p>
<p>0
:9
9
9
8
9
7
</p>
<p>0
:9
9
9
9
0
1
</p>
<p>0
:9
9
9
9
0
5
</p>
<p>0
:9
9
9
9
0
8
</p>
<p>0
:9
9
9
9
1
2
</p>
<p>0
:9
9
9
9
1
5
</p>
<p>0
:9
9
9
9
1
9
</p>
<p>0
:9
9
9
9
2
2
</p>
<p>0
:9
9
9
9
2
5
</p>
<p>3
:8
</p>
<p>0
:9
9
9
9
2
8
</p>
<p>0
:9
9
9
9
3
1
</p>
<p>0
:9
9
9
9
3
4
</p>
<p>0
:9
9
9
9
3
6
</p>
<p>0
:9
9
9
9
3
9
</p>
<p>0
:9
9
9
9
4
1
</p>
<p>0
:9
9
9
9
4
4
</p>
<p>0
:9
9
9
9
4
6
</p>
<p>0
:9
9
9
9
4
8
</p>
<p>0
:9
9
9
9
5
0
</p>
<p>3
:9
</p>
<p>0
:9
9
9
9
5
2
</p>
<p>0
:9
9
9
9
5
4
</p>
<p>0
:9
9
9
9
5
6
</p>
<p>0
:9
9
9
9
5
8
</p>
<p>0
:9
9
9
9
6
0
</p>
<p>0
:9
9
9
9
6
1
</p>
<p>0
:9
9
9
9
6
3
</p>
<p>0
:9
9
9
9
6
4
</p>
<p>0
:9
9
9
9
6
6
</p>
<p>0
:9
9
9
9
6
7
</p>
<p>4
:0
</p>
<p>0
:9
9
9
9
6
9
</p>
<p>0
:9
9
9
9
7
0
</p>
<p>0
:9
9
9
9
7
1
</p>
<p>0
:9
9
9
9
7
3
</p>
<p>0
:9
9
9
9
7
4
</p>
<p>0
:9
9
9
9
7
5
</p>
<p>0
:9
9
9
9
7
6
</p>
<p>0
:9
9
9
9
7
7
</p>
<p>0
:9
9
9
9
7
8
</p>
<p>0
:9
9
9
9
7
9
</p>
<p>4
:1
</p>
<p>0
:9
9
9
9
8
0
</p>
<p>0
:9
9
9
9
8
1
</p>
<p>0
:9
9
9
9
8
1
</p>
<p>0
:9
9
9
9
8
2
</p>
<p>0
:9
9
9
9
8
3
</p>
<p>0
:9
9
9
9
8
4
</p>
<p>0
:9
9
9
9
8
5
</p>
<p>0
:9
9
9
9
8
5
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>4
:2
</p>
<p>0
:9
9
9
9
8
7
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>4
:3
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>0
:9
9
9
9
9
3
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
4
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>4
:4
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
5
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
6
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>4
:5
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
7
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>4
:6
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
8
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>4
:7
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>0
:9
9
9
9
9
9
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>4
:8
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>4
:9
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0
</p>
<p>1
:0
0
0
0
0
0</p>
<p/>
</div>
<div class="page"><p/>
<p>280 Appendix: Numerical Tables
</p>
<p>Table A.4 Table of critical
values of the standard
Gaussian distribution to
include a given probability,
for two-sided confidence
intervals (-z, z/ of the integral
A.z/, and for one-sided
intervals (-1, z/ of the
integral B.z/
</p>
<p>Probability Two-sided z One-sided z
</p>
<p>0:01 0:013 ï¿½2:326
0:05 0:063 ï¿½1:645
0:10 0:126 ï¿½1:282
0:20 0:253 ï¿½0:842
0:30 0:385 ï¿½0:524
0:40 0:524 ï¿½0:253
0:50 0:674 ï¿½0:000
0:60 0:842 0:253
</p>
<p>0:70 1:036 0:524
</p>
<p>0:80 1:282 0:842
</p>
<p>0:90 1:645 1:282
</p>
<p>0:95 1:960 1:645
</p>
<p>0:99 2:576 2:326
</p>
<p>0:999 3:290 3:090
</p>
<p>0:9999 3:890 3:718
</p>
<p>Table A.5 Selected upper
limits for a Poisson variable
using the Gehrels
approximation
</p>
<p>Upper limits
</p>
<p>Poisson parameter S or confidence level
</p>
<p>S = 1 S = 2 S = 3
</p>
<p>nobs (1-ï¿½ , or 84.1 %) (2-ï¿½ , or 97.7 %) (3-ï¿½ , or 99.9 %)
</p>
<p>0 1:87 3:48 5:60
</p>
<p>1 3:32 5:40 7:97
</p>
<p>2 4:66 7:07 9:97
</p>
<p>3 5:94 8:62 11:81
</p>
<p>4 7:18 10:11 13:54
</p>
<p>5 8:40 11:55 15:19
</p>
<p>6 9:60 12:95 16:79
</p>
<p>7 10:78 14:32 18:35
</p>
<p>8 11:96 15:67 19:87
</p>
<p>9 13:12 16:99 21:37
</p>
<p>10 14:28 18:31 22:84
</p>
<p>20 25:56 30:86 36:67
</p>
<p>30 36:55 42:84 49:64
</p>
<p>40 47:38 54:52 62:15
</p>
<p>50 58:12 66:00 74:37
</p>
<p>60 68:79 77:34 86:38
</p>
<p>70 79:41 88:57 98:23
</p>
<p>80 89:99 99:72 109:96
</p>
<p>90 100:53 110:80 121:58
</p>
<p>100 111:04 121:82 133:11</p>
<p/>
</div>
<div class="page"><p/>
<p>A.3 The ï¿½2 Distribution 281
</p>
<p>Table A.6 Selected lower
limits for a Poisson variable
using the Gehrels
approximation
</p>
<p>Lower limits
</p>
<p>Poisson parameter S or confidence level
</p>
<p>S = 1 S = 2 S = 3
</p>
<p>nobs (1-ï¿½ , or 84.1 %) (2-ï¿½ , or 97.7 %) (3-ï¿½ , or 99.9 %)
</p>
<p>1 0:17 0:01 0:00
</p>
<p>2 0:71 0:21 0:03
</p>
<p>3 1:37 0:58 0:17
</p>
<p>4 2:09 1:04 0:42
</p>
<p>5 2:85 1:57 0:75
</p>
<p>6 3:63 2:14 1:13
</p>
<p>7 4:42 2:75 1:56
</p>
<p>8 5:24 3:38 2:02
</p>
<p>9 6:06 4:04 2:52
</p>
<p>10 6:90 4:71 3:04
</p>
<p>20 15:57 12:08 9:16
</p>
<p>30 24:56 20:07 16:16
</p>
<p>40 33:70 28:37 23:63
</p>
<p>50 42:96 36:88 31:40
</p>
<p>60 52:28 45:53 39:38
</p>
<p>70 61:66 54:28 47:52
</p>
<p>80 71:08 63:13 55:79
</p>
<p>90 80:53 72:04 64:17
</p>
<p>100 90:02 81:01 72:63
</p>
<p>where f is the number of degrees of freedom. The critical value or p-quantile of the
distribution is given by
</p>
<p>Pï¿½2.z 
 ï¿½2crit/ D
Z ï¿½2crit
</p>
<p>0
</p>
<p>fï¿½2 .z/dz D p (A.9)
</p>
<p>or, equivalently,
</p>
<p>Pï¿½2.z ï¿½ ï¿½2crit/ D
Z 1
</p>
<p>ï¿½2crit
</p>
<p>fï¿½2.z/dz D 1 ï¿½ p: (A.10)
</p>
<p>The critical value is a function of the number of degrees of freedom f and the level
of probability p. Normally p is intended as a large number, such as 0.68, 0.90, or
0.99, meaning that there is just a 32, 10, or 1 % probability to have values higher
than the critical value ï¿½2crit.
</p>
<p>As described in Sect. 7.2, the ï¿½2 distribution has the following mean and
variance:
</p>
<p>ï¿½
ï¿½ D f
ï¿½2 D 2f :</p>
<p/>
</div>
<div class="page"><p/>
<p>282 Appendix: Numerical Tables
</p>
<p>It is convenient to tabulate the value of reduced ï¿½2, or ï¿½2crit=f , that corresponds to
a given probability level, as function of the number of degrees of freedom. Selected
critical values of the ï¿½2 distribution are reported in Table A.7. When using this table,
remember to multiply the tabulated reduced ï¿½2 by the number of defrees of freedom
f to obtain the value of ï¿½2.
</p>
<p>If Z is a ï¿½2-distributed variable with f degrees of freedoms,
</p>
<p>lim
f!1
</p>
<p>Z ï¿½ fp
2f
D N.0; 1/: (A.11)
</p>
<p>In fact, a ï¿½2 variable is obtained as the sum of independent distributions (Sect. 7.2),
to which the central theorem limit applies (Sect. 4.3). For a large number of degrees
of freedom, the standard Gaussian distribution can be used to supplement Table A.7
according to (A.11). For example, for p D 0:99, the one-sided critical value of the
standard Gaussian is approximately 2.326, according to Table A.4. Using this value
into (A.11) for f = 200 would give a critical value for the ï¿½2 distribution of 1.2326
(compare to 1.247 from Table A.7). The values of f D 1 in Table A.7 is obtained
using the Gaussian approximation, according to (A.11).
</p>
<p>A.4 The F Distribution
</p>
<p>The F distribution with f1, f2 degrees of freedom is defined in (7.22) as
</p>
<p>fF.z/ D
ï¿½
</p>
<p>ï¿½
f1 C f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f1
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f2
2
</p>
<p>ï¿½
</p>
<p>ï¿½
f1
f2
</p>
<p>ï¿½ f1
2 z
</p>
<p>f1
2
ï¿½1
</p>
<p>ï¿½
</p>
<p>1C z f1
f2
</p>
<p>ï¿½ f1Cf2
2
</p>
<p>:
</p>
<p>The critical value Fcrit that includes a probability p is given by
</p>
<p>P.z ï¿½ Fcrit/ D
Z 1
</p>
<p>Fcrit
</p>
<p>fF.z/dz D 1 ï¿½ p; (A.12)
</p>
<p>and it is a function of the degrees of freedom f1 and f2. In Table A.8 are reported
the critical values for various probability levels p, for a fixed value f1 = 1, and as
function of f2. Tables A.9, A.10, A.11, A.12, A.13, A.14, and A.15 have the critical
values as function of both f1 and f2.
</p>
<p>Asymptotic values when f1 and f2 approach infinity can be found using (7.25),
reported here for convenience:
</p>
<p>8
&lt;
</p>
<p>:
</p>
<p>lim
f2!1
</p>
<p>fF.z; f1; f2/ D fï¿½2 .x; f1/ where x D f1z
lim
</p>
<p>f1!1
fF.z; f1; f2/ D fï¿½2 .x; f2/ where x D f2=z:</p>
<p/>
</div>
<div class="page"><p/>
<p>A.4 The F Distribution 283
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.7
C
</p>
<p>ri
ti
</p>
<p>ca
lv
</p>
<p>al
ue
</p>
<p>s
of
</p>
<p>th
e
ï¿½
2
</p>
<p>di
st
</p>
<p>ri
bu
</p>
<p>ti
on
</p>
<p>Pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p
</p>
<p>to
ha
</p>
<p>ve
a
</p>
<p>va
lu
</p>
<p>e
of
</p>
<p>re
du
</p>
<p>ce
d
ï¿½
2
</p>
<p>be
lo
</p>
<p>w
th
</p>
<p>e
cr
</p>
<p>it
ic
</p>
<p>al
va
</p>
<p>lu
e
</p>
<p>f
0
:0
1
</p>
<p>0
:0
5
</p>
<p>0
:1
0
</p>
<p>0
:2
0
</p>
<p>0
:3
0
</p>
<p>0
:4
0
</p>
<p>0
:5
0
</p>
<p>0
:6
0
</p>
<p>0
:7
0
</p>
<p>0
:8
0
</p>
<p>0
:9
0
</p>
<p>0
:9
5
</p>
<p>0
:9
9
</p>
<p>1
0
:0
0
0
1
6
</p>
<p>0
:0
0
3
9
0
</p>
<p>0
:0
1
5
8
0
</p>
<p>0
:0
6
4
2
</p>
<p>0
:1
4
8
5
</p>
<p>0
:2
7
5
0
</p>
<p>0
:4
5
4
9
</p>
<p>0
:7
0
8
3
</p>
<p>1
:0
7
4
2
</p>
<p>1
:6
4
2
4
</p>
<p>2
:7
0
5
5
</p>
<p>3
:8
4
1
5
</p>
<p>6
:6
3
4
9
</p>
<p>2
0
:0
1
0
1
</p>
<p>0
:0
5
1
3
</p>
<p>0
:1
0
5
4
</p>
<p>0
:2
2
3
1
</p>
<p>0
:3
5
6
7
</p>
<p>0
:5
1
0
8
</p>
<p>0
:6
9
3
1
</p>
<p>0
:9
1
6
3
</p>
<p>1
:2
0
4
0
</p>
<p>1
:6
0
9
4
</p>
<p>2
:3
0
2
6
</p>
<p>2
:9
9
5
7
</p>
<p>4
:6
0
5
2
</p>
<p>3
0
:0
3
8
3
</p>
<p>0
:1
1
7
3
</p>
<p>0
:1
9
4
8
</p>
<p>0
:3
3
5
1
</p>
<p>0
:4
7
4
6
</p>
<p>0
:6
2
3
1
</p>
<p>0
:7
8
8
7
</p>
<p>0
:9
8
2
1
</p>
<p>1
:2
2
1
6
</p>
<p>1
:5
4
7
2
</p>
<p>2
:0
8
3
8
</p>
<p>2
:6
0
4
9
</p>
<p>3
:7
8
1
6
</p>
<p>4
0
:0
7
4
3
</p>
<p>0
:1
7
7
7
</p>
<p>0
:2
6
5
9
</p>
<p>0
:4
1
2
2
</p>
<p>0
:5
4
8
7
</p>
<p>0
:6
8
8
2
</p>
<p>0
:8
3
9
2
</p>
<p>1
:0
1
1
2
</p>
<p>1
:2
1
9
6
</p>
<p>1
:4
9
7
2
</p>
<p>1
:9
4
4
9
</p>
<p>2
:3
7
1
9
</p>
<p>3
:3
1
9
2
</p>
<p>5
0
:1
1
0
9
</p>
<p>0
:2
2
9
1
</p>
<p>0
:3
2
2
1
</p>
<p>0
:4
6
8
5
</p>
<p>0
:6
0
0
0
</p>
<p>0
:7
3
1
1
</p>
<p>0
:8
7
0
3
</p>
<p>1
:0
2
6
4
</p>
<p>1
:2
1
2
9
</p>
<p>1
:4
5
7
8
</p>
<p>1
:8
4
7
3
</p>
<p>2
:2
1
4
1
</p>
<p>3
:0
1
7
2
</p>
<p>6
0
:1
4
5
4
</p>
<p>0
:2
7
2
6
</p>
<p>0
:3
6
7
4
</p>
<p>0
:5
1
1
7
</p>
<p>0
:6
3
7
9
</p>
<p>0
:7
6
1
7
</p>
<p>0
:8
9
1
4
</p>
<p>1
:0
3
5
1
</p>
<p>1
:2
0
5
2
</p>
<p>1
:4
2
6
3
</p>
<p>1
:7
7
4
1
</p>
<p>2
:0
9
8
6
</p>
<p>2
:8
0
2
0
</p>
<p>7
0
:1
7
7
0
</p>
<p>0
:3
0
9
6
</p>
<p>0
:4
0
4
7
</p>
<p>0
:5
4
6
0
</p>
<p>0
:6
6
7
3
</p>
<p>0
:7
8
4
7
</p>
<p>0
:9
0
6
5
</p>
<p>1
:0
4
0
5
</p>
<p>1
:1
9
7
6
</p>
<p>1
:4
0
0
5
</p>
<p>1
:7
1
6
7
</p>
<p>2
:0
0
9
6
</p>
<p>2
:6
3
9
3
</p>
<p>8
0
:2
0
5
8
</p>
<p>0
:3
4
1
6
</p>
<p>0
:4
3
6
2
</p>
<p>0
:5
7
4
2
</p>
<p>0
:6
9
0
9
</p>
<p>0
:8
0
2
8
</p>
<p>0
:9
1
8
0
</p>
<p>1
:0
4
3
8
</p>
<p>1
:1
9
0
6
</p>
<p>1
:3
7
8
8
</p>
<p>1
:6
7
0
2
</p>
<p>1
:9
3
8
4
</p>
<p>2
:5
1
1
3
</p>
<p>9
0
:2
3
2
0
</p>
<p>0
:3
6
9
5
</p>
<p>0
:4
6
3
1
</p>
<p>0
:5
9
7
8
</p>
<p>0
:7
1
0
4
</p>
<p>0
:8
1
7
4
</p>
<p>0
:9
2
7
0
</p>
<p>1
:0
4
6
0
</p>
<p>1
:1
8
4
1
</p>
<p>1
:3
6
0
2
</p>
<p>1
:6
3
1
5
</p>
<p>1
:8
7
9
9
</p>
<p>2
:4
0
7
3
</p>
<p>10
0
:2
5
6
</p>
<p>0
:3
9
4
</p>
<p>0
:4
8
7
</p>
<p>0
:6
1
8
</p>
<p>0
:7
2
7
</p>
<p>0
:8
3
0
</p>
<p>0
:9
3
4
</p>
<p>1
:0
4
7
</p>
<p>1
:1
7
8
</p>
<p>1
:3
4
4
</p>
<p>1
:5
9
9
</p>
<p>1
:8
3
1
</p>
<p>2
:3
2
1
</p>
<p>11
0
:2
7
8
</p>
<p>0
:4
1
6
</p>
<p>0
:5
0
7
</p>
<p>0
:6
3
5
</p>
<p>0
:7
4
1
</p>
<p>0
:8
4
0
</p>
<p>0
:9
4
0
</p>
<p>1
:0
4
8
</p>
<p>1
:1
7
3
</p>
<p>1
:3
3
0
</p>
<p>1
:5
7
0
</p>
<p>1
:7
8
9
</p>
<p>2
:2
4
8
</p>
<p>12
0
:2
9
8
</p>
<p>0
:4
3
6
</p>
<p>0
:5
2
5
</p>
<p>0
:6
5
1
</p>
<p>0
:7
5
3
</p>
<p>0
:8
4
8
</p>
<p>0
:9
4
5
</p>
<p>1
:0
4
9
</p>
<p>1
:1
6
8
</p>
<p>1
:3
1
8
</p>
<p>1
:5
4
6
</p>
<p>1
:7
5
2
</p>
<p>2
:1
8
5
</p>
<p>13
0
:3
1
6
</p>
<p>0
:4
5
3
</p>
<p>0
:5
4
2
</p>
<p>0
:6
6
4
</p>
<p>0
:7
6
4
</p>
<p>0
:8
5
6
</p>
<p>0
:9
4
9
</p>
<p>1
:0
4
9
</p>
<p>1
:1
6
3
</p>
<p>1
:3
0
7
</p>
<p>1
:5
2
4
</p>
<p>1
:7
2
0
</p>
<p>2
:1
3
0
</p>
<p>14
0
:3
3
3
</p>
<p>0
:4
6
9
</p>
<p>0
:5
5
6
</p>
<p>0
:6
7
6
</p>
<p>0
:7
7
3
</p>
<p>0
:8
6
3
</p>
<p>0
:9
5
3
</p>
<p>1
:0
4
9
</p>
<p>1
:1
5
9
</p>
<p>1
:2
9
6
</p>
<p>1
:5
0
5
</p>
<p>1
:6
9
2
</p>
<p>2
:0
8
2
</p>
<p>15
0
:3
4
9
</p>
<p>0
:4
8
4
</p>
<p>0
:5
7
0
</p>
<p>0
:6
8
7
</p>
<p>0
:7
8
1
</p>
<p>0
:8
6
9
</p>
<p>0
:9
5
6
</p>
<p>1
:0
4
9
</p>
<p>1
:1
5
5
</p>
<p>1
:2
8
7
</p>
<p>1
:4
8
7
</p>
<p>1
:6
6
6
</p>
<p>2
:0
3
9
</p>
<p>16
0
:3
6
3
</p>
<p>0
:4
9
8
</p>
<p>0
:5
8
2
</p>
<p>0
:6
9
7
</p>
<p>0
:7
8
9
</p>
<p>0
:8
7
4
</p>
<p>0
:9
5
9
</p>
<p>1
:0
4
9
</p>
<p>1
:1
5
1
</p>
<p>1
:2
7
9
</p>
<p>1
:4
7
1
</p>
<p>1
:6
4
3
</p>
<p>2
:0
0
0
</p>
<p>17
0
:3
7
7
</p>
<p>0
:5
1
0
</p>
<p>0
:5
9
3
</p>
<p>0
:7
0
6
</p>
<p>0
:7
9
6
</p>
<p>0
:8
7
9
</p>
<p>0
:9
6
1
</p>
<p>1
:0
4
8
</p>
<p>1
:1
4
8
</p>
<p>1
:2
7
1
</p>
<p>1
:4
5
7
</p>
<p>1
:6
2
3
</p>
<p>1
:9
6
5
</p>
<p>18
0
:3
9
0
</p>
<p>0
:5
2
2
</p>
<p>0
:6
0
4
</p>
<p>0
:7
1
4
</p>
<p>0
:8
0
2
</p>
<p>0
:8
8
3
</p>
<p>0
:9
6
3
</p>
<p>1
:0
4
8
</p>
<p>1
:1
4
5
</p>
<p>1
:2
6
4
</p>
<p>1
:4
4
4
</p>
<p>1
:6
0
4
</p>
<p>1
:9
3
4
</p>
<p>19
0
:4
0
2
</p>
<p>0
:5
3
2
</p>
<p>0
:6
1
3
</p>
<p>0
:7
2
2
</p>
<p>0
:8
0
8
</p>
<p>0
:8
8
7
</p>
<p>0
:9
6
5
</p>
<p>1
:0
4
8
</p>
<p>1
:1
4
2
</p>
<p>1
:2
5
8
</p>
<p>1
:4
3
2
</p>
<p>1
:5
8
7
</p>
<p>1
:9
0
5
</p>
<p>20
0
:4
1
3
</p>
<p>0
:5
4
3
</p>
<p>0
:6
2
2
</p>
<p>0
:7
2
9
</p>
<p>0
:8
1
3
</p>
<p>0
:8
9
0
</p>
<p>0
:9
6
7
</p>
<p>1
:0
4
8
</p>
<p>1
:1
3
9
</p>
<p>1
:2
5
2
</p>
<p>1
:4
2
1
</p>
<p>1
:5
7
1
</p>
<p>1
:8
7
8
</p>
<p>30
0
:4
9
8
</p>
<p>0
:6
1
6
</p>
<p>0
:6
8
7
</p>
<p>0
:7
7
9
</p>
<p>0
:8
5
0
</p>
<p>0
:9
1
5
</p>
<p>0
:9
7
8
</p>
<p>1
:0
4
4
</p>
<p>1
:1
1
8
</p>
<p>1
:2
0
8
</p>
<p>1
:3
4
2
</p>
<p>1
:4
5
9
</p>
<p>1
:6
9
6
</p>
<p>40
0
:5
5
4
</p>
<p>0
:6
6
3
</p>
<p>0
:7
2
6
</p>
<p>0
:8
0
9
</p>
<p>0
:8
7
2
</p>
<p>0
:9
2
8
</p>
<p>0
:9
8
3
</p>
<p>1
:0
4
1
</p>
<p>1
:1
0
4
</p>
<p>1
:1
8
2
</p>
<p>1
:2
9
5
</p>
<p>1
:3
9
4
</p>
<p>1
:5
9
2
</p>
<p>50
0
:5
9
4
</p>
<p>0
:6
9
5
</p>
<p>0
:7
5
4
</p>
<p>0
:8
2
9
</p>
<p>0
:8
8
6
</p>
<p>0
:9
3
7
</p>
<p>0
:9
8
7
</p>
<p>1
:0
3
8
</p>
<p>1
:0
9
4
</p>
<p>1
:1
6
3
</p>
<p>1
:2
6
3
</p>
<p>1
:3
5
0
</p>
<p>1
:5
2
3
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)</p>
<p/>
</div>
<div class="page"><p/>
<p>284 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.7
(c
</p>
<p>on
ti
</p>
<p>nu
ed
</p>
<p>)
</p>
<p>Pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p
</p>
<p>to
ha
</p>
<p>ve
a
</p>
<p>va
lu
</p>
<p>e
of
</p>
<p>re
du
</p>
<p>ce
d
ï¿½
2
</p>
<p>be
lo
</p>
<p>w
th
</p>
<p>e
cr
</p>
<p>it
ic
</p>
<p>al
va
</p>
<p>lu
e
</p>
<p>f
0
:0
1
</p>
<p>0
:0
5
</p>
<p>0
:1
0
</p>
<p>0
:2
0
</p>
<p>0
:3
0
</p>
<p>0
:4
0
</p>
<p>0
:5
0
</p>
<p>0
:6
0
</p>
<p>0
:7
0
</p>
<p>0
:8
0
</p>
<p>0
:9
0
</p>
<p>0
:9
5
</p>
<p>0
:9
9
</p>
<p>60
0
:6
2
5
</p>
<p>0
:7
2
0
</p>
<p>0
:7
7
4
</p>
<p>0
:8
4
4
</p>
<p>0
:8
9
7
</p>
<p>0
:9
4
4
</p>
<p>0
:9
8
9
</p>
<p>1
:0
3
6
</p>
<p>1
:0
8
7
</p>
<p>1
:1
5
0
</p>
<p>1
:2
4
0
</p>
<p>1
:3
1
8
</p>
<p>1
:4
7
3
</p>
<p>70
0
:6
4
9
</p>
<p>0
:7
3
9
</p>
<p>0
:7
9
0
</p>
<p>0
:8
5
6
</p>
<p>0
:9
0
5
</p>
<p>0
:9
4
9
</p>
<p>0
:9
9
0
</p>
<p>1
:0
3
4
</p>
<p>1
:0
8
1
</p>
<p>1
:1
3
9
</p>
<p>1
:2
2
2
</p>
<p>1
:2
9
3
</p>
<p>1
:4
3
5
</p>
<p>80
0
:6
6
9
</p>
<p>0
:7
5
5
</p>
<p>0
:8
0
3
</p>
<p>0
:8
6
5
</p>
<p>0
:9
1
1
</p>
<p>0
:9
5
2
</p>
<p>0
:9
9
2
</p>
<p>1
:0
3
2
</p>
<p>1
:0
7
6
</p>
<p>1
:1
3
0
</p>
<p>1
:2
0
7
</p>
<p>1
:2
7
4
</p>
<p>1
:4
0
4
</p>
<p>90
0
:6
8
6
</p>
<p>0
:7
6
8
</p>
<p>0
:8
1
4
</p>
<p>0
:8
7
3
</p>
<p>0
:9
1
7
</p>
<p>0
:9
5
5
</p>
<p>0
:9
9
3
</p>
<p>1
:0
3
1
</p>
<p>1
:0
7
3
</p>
<p>1
:1
2
3
</p>
<p>1
:1
9
5
</p>
<p>1
:2
5
7
</p>
<p>1
:3
7
9
</p>
<p>10
0
</p>
<p>0
:7
0
0
</p>
<p>0
:7
8
0
</p>
<p>0
:8
2
3
</p>
<p>0
:8
8
0
</p>
<p>0
:9
2
1
</p>
<p>0
:9
5
8
</p>
<p>0
:9
9
3
</p>
<p>1
:0
3
0
</p>
<p>1
:0
6
9
</p>
<p>1
:1
1
7
</p>
<p>1
:1
8
5
</p>
<p>1
:2
4
3
</p>
<p>1
:3
5
8
</p>
<p>20
0
</p>
<p>0
:7
8
2
</p>
<p>0
:8
4
1
</p>
<p>0
:8
7
4
</p>
<p>0
:9
1
5
</p>
<p>0
:9
4
5
</p>
<p>0
:9
7
1
</p>
<p>0
:9
9
7
</p>
<p>1
:0
2
2
</p>
<p>1
:0
5
0
</p>
<p>1
:0
8
3
</p>
<p>1
:1
3
0
</p>
<p>1
:1
7
0
</p>
<p>1
:2
4
7
</p>
<p>30
0
</p>
<p>0
:8
2
0
</p>
<p>0
:8
7
0
</p>
<p>0
:8
9
7
</p>
<p>0
:9
3
1
</p>
<p>0
:9
5
6
</p>
<p>0
:9
7
7
</p>
<p>0
:9
9
8
</p>
<p>1
:0
1
9
</p>
<p>1
:0
4
1
</p>
<p>1
:0
6
8
</p>
<p>1
:1
0
6
</p>
<p>1
:1
3
8
</p>
<p>1
:2
0
0
</p>
<p>40
0
</p>
<p>0
:8
4
3
</p>
<p>0
:8
8
7
</p>
<p>0
:9
1
0
</p>
<p>0
:9
4
0
</p>
<p>0
:9
6
2
</p>
<p>0
:9
8
0
</p>
<p>0
:9
9
8
</p>
<p>1
:0
1
6
</p>
<p>1
:0
3
6
</p>
<p>1
:0
5
9
</p>
<p>1
:0
9
2
</p>
<p>1
:1
1
9
</p>
<p>1
:1
7
2
</p>
<p>50
0
</p>
<p>0
:8
6
</p>
<p>0
:9
0
</p>
<p>0
:9
2
</p>
<p>0
:9
5
</p>
<p>0
:9
7
</p>
<p>0
:9
8
</p>
<p>1
:0
0
</p>
<p>1
:0
1
</p>
<p>1
:0
3
</p>
<p>1
:0
5
</p>
<p>1
:0
8
</p>
<p>1
:1
0
</p>
<p>1
:1
5
</p>
<p>10
00
</p>
<p>0
:9
0
</p>
<p>0
:9
3
</p>
<p>0
:9
4
</p>
<p>0
:9
6
</p>
<p>0
:9
8
</p>
<p>0
:9
9
</p>
<p>1
:0
0
</p>
<p>1
:0
1
</p>
<p>1
:0
2
</p>
<p>1
:0
4
</p>
<p>1
:0
6
</p>
<p>1
:0
7
</p>
<p>1
:1
1
</p>
<p>1
0
:9
0
</p>
<p>0
:9
3
</p>
<p>0
:9
4
</p>
<p>0
:9
6
</p>
<p>0
:9
8
</p>
<p>0
:9
9
</p>
<p>1
:0
0
</p>
<p>1
:0
1
</p>
<p>1
:0
2
</p>
<p>1
:0
4
</p>
<p>1
:0
6
</p>
<p>1
:0
7
</p>
<p>1
:1
0</p>
<p/>
</div>
<div class="page"><p/>
<p>A.5 The Student&rsquo;s t Distribution 285
</p>
<p>Table A.8 Critical values of F statistics for f1 D 1 degrees of freedom
Probability p to have a value of F below the critical value
</p>
<p>f2 0:50 0:60 0:70 0:80 0:90 0:95 0:99
</p>
<p>1 1:000 1:894 3:852 9:472 39:863 161:448 4052:182
</p>
<p>2 0:667 1:125 1:922 3:556 8:526 18:513 98:503
</p>
<p>3 0:585 0:957 1:562 2:682 5:538 10:128 34:116
</p>
<p>4 0:549 0:885 1:415 2:351 4:545 7:709 21:198
</p>
<p>5 0:528 0:846 1:336 2:178 4:060 6:608 16:258
</p>
<p>6 0:515 0:820 1:286 2:073 3:776 5:987 13:745
</p>
<p>7 0:506 0:803 1:253 2:002 3:589 5:591 12:246
</p>
<p>8 0:499 0:790 1:228 1:951 3:458 5:318 11:259
</p>
<p>9 0:494 0:780 1:209 1:913 3:360 5:117 10:561
</p>
<p>10 0:490 0:773 1:195 1:883 3:285 4:965 10:044
</p>
<p>20 0:472 0:740 1:132 1:757 2:975 4:351 8:096
</p>
<p>30 0:466 0:729 1:112 1:717 2:881 4:171 7:562
</p>
<p>40 0:463 0:724 1:103 1:698 2:835 4:085 7:314
</p>
<p>50 0:462 0:721 1:097 1:687 2:809 4:034 7:171
</p>
<p>60 0:461 0:719 1:093 1:679 2:791 4:001 7:077
</p>
<p>70 0:460 0:717 1:090 1:674 2:779 3:978 7:011
</p>
<p>80 0:459 0:716 1:088 1:670 2:769 3:960 6:963
</p>
<p>90 0:459 0:715 1:087 1:667 2:762 3:947 6:925
</p>
<p>100 0:458 0:714 1:085 1:664 2:756 3:936 6:895
</p>
<p>200 0:457 0:711 1:080 1:653 2:731 3:888 6:763
</p>
<p>1 0:455 0:708 1:074 1:642 2:706 3:842 6:635
</p>
<p>For example, the critical values of the F distribution for f1 = 1 and in the limit of
large f2 are obtained from the first row of Table A.7.
</p>
<p>A.5 The Student&rsquo;s t Distribution
</p>
<p>The Student t distribution is given by (7.34),
</p>
<p>fT.t/ D 1p
f	
</p>
<p>ï¿½ .. f C 1/=2/
ï¿½ . f=2/
</p>
<p>	
ï¿½
</p>
<p>1C t
2
</p>
<p>f
</p>
<p>ï¿½ï¿½ 1
2
. fC1/
</p>
<p>;
</p>
<p>where f is the number of degrees of freedom. The probability p that the absolute
value of a t variable exceeds a critical value Tcrit is given by
</p>
<p>P.jtj 
 Tcrit/ D P.jNx ï¿½ ï¿½j 
 Tcrit ï¿½ s=
p
n/ D
</p>
<p>Z Tcrit
</p>
<p>ï¿½Tcrit
fT.t/dt D 1 ï¿½ p: (A.13)</p>
<p/>
</div>
<div class="page"><p/>
<p>286 Appendix: Numerical Tables
</p>
<p>Table A.9 Critical values of F statistic that include p D 0:50 probability
f1
</p>
<p>f2 2 4 6 8 10 20 40 60 80 100
</p>
<p>1 1:500 1:823 1:942 2:004 2:042 2:119 2:158 2:172 2:178 2:182
</p>
<p>2 1:000 1:207 1:282 1:321 1:345 1:393 1:418 1:426 1:430 1:433
</p>
<p>3 0:881 1:063 1:129 1:163 1:183 1:225 1:246 1:254 1:257 1:259
</p>
<p>4 0:828 1:000 1:062 1:093 1:113 1:152 1:172 1:178 1:182 1:184
</p>
<p>5 0:799 0:965 1:024 1:055 1:073 1:111 1:130 1:136 1:139 1:141
</p>
<p>6 0:780 0:942 1:000 1:030 1:048 1:084 1:103 1:109 1:113 1:114
</p>
<p>7 0:767 0:926 0:983 1:013 1:030 1:066 1:085 1:091 1:094 1:096
</p>
<p>8 0:757 0:915 0:971 1:000 1:017 1:053 1:071 1:077 1:080 1:082
</p>
<p>9 0:749 0:906 0:962 0:990 1:008 1:043 1:061 1:067 1:070 1:072
</p>
<p>10 0:743 0:899 0:954 0:983 1:000 1:035 1:053 1:059 1:062 1:063
</p>
<p>20 0:718 0:868 0:922 0:950 0:966 1:000 1:017 1:023 1:026 1:027
</p>
<p>30 0:709 0:858 0:912 0:939 0:955 0:989 1:006 1:011 1:014 1:016
</p>
<p>40 0:705 0:854 0:907 0:934 0:950 0:983 1:000 1:006 1:008 1:010
</p>
<p>50 0:703 0:851 0:903 0:930 0:947 0:980 0:997 1:002 1:005 1:007
</p>
<p>60 0:701 0:849 0:901 0:928 0:945 0:978 0:994 1:000 1:003 1:004
</p>
<p>70 0:700 0:847 0:900 0:927 0:943 0:976 0:993 0:998 1:001 1:003
</p>
<p>80 0:699 0:846 0:899 0:926 0:942 0:975 0:992 0:997 1:000 1:002
</p>
<p>90 0:699 0:845 0:898 0:925 0:941 0:974 0:991 0:996 0:999 1:001
</p>
<p>100 0:698 0:845 0:897 0:924 0:940 0:973 0:990 0:996 0:998 1:000
</p>
<p>200 0:696 0:842 0:894 0:921 0:937 0:970 0:987 0:992 0:995 0:997
</p>
<p>1 0:693 0:839 0:891 0:918 0:934 0:967 0:983 0:989 0:992 0:993
</p>
<p>These two-sided critical values are tabulated in Tables A.16, A.17, A.18, A.19, A.20,
A.21, and A.22 for selected values of f , as function of the critical value Tcrit. In these
tables, the left column indicates the value of Tcrit to the first decimal digit, and the
values on the top column are the second decimal digit.
</p>
<p>Table A.23 provides a comparison of the probability p for five critical values,
Tcrit D 1 through 5, as function of f : The case of f D 1 corresponds to a standard
Gaussian.
</p>
<p>A.6 The Linear Correlation Coefficient r
</p>
<p>The linear correlation coefficient is defined as
</p>
<p>r2 D .N
P
</p>
<p>xiyi ï¿½P xiP yi/2
ï¿½
N
P
</p>
<p>x2i ï¿½ .
P
</p>
<p>xi/
2
	 ï¿½
</p>
<p>N
P
</p>
<p>y2i ï¿½ .
P
</p>
<p>yi/
2
	 (A.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>A.6 The Linear Correlation Coefficient r 287
</p>
<p>Table A.10 Critical values of F statistic that include p D 0:60 probability
f1
</p>
<p>f2 2 4 6 8 10 20 40 60 80 100
</p>
<p>1 2:625 3:093 3:266 3:355 3:410 3:522 3:579 3:598 3:608 3:613
</p>
<p>2 1:500 1:718 1:796 1:835 1:859 1:908 1:933 1:941 1:945 1:948
</p>
<p>3 1:263 1:432 1:489 1:518 1:535 1:570 1:588 1:593 1:596 1:598
</p>
<p>4 1:162 1:310 1:359 1:383 1:397 1:425 1:439 1:444 1:446 1:448
</p>
<p>5 1:107 1:243 1:287 1:308 1:320 1:345 1:356 1:360 1:362 1:363
</p>
<p>6 1:072 1:200 1:241 1:260 1:272 1:293 1:303 1:307 1:308 1:309
</p>
<p>7 1:047 1:171 1:209 1:227 1:238 1:257 1:266 1:269 1:270 1:271
</p>
<p>8 1:030 1:150 1:186 1:203 1:213 1:231 1:239 1:241 1:242 1:243
</p>
<p>9 1:016 1:133 1:168 1:185 1:194 1:210 1:217 1:219 1:220 1:221
</p>
<p>10 1:006 1:120 1:154 1:170 1:179 1:194 1:200 1:202 1:203 1:204
</p>
<p>20 0:960 1:064 1:093 1:106 1:112 1:122 1:124 1:124 1:124 1:124
</p>
<p>30 0:945 1:046 1:074 1:085 1:090 1:097 1:097 1:097 1:096 1:096
</p>
<p>40 0:938 1:037 1:064 1:075 1:080 1:085 1:084 1:083 1:082 1:081
</p>
<p>50 0:933 1:032 1:058 1:068 1:073 1:078 1:076 1:074 1:073 1:072
</p>
<p>60 0:930 1:029 1:054 1:064 1:069 1:073 1:070 1:068 1:066 1:065
</p>
<p>70 0:928 1:026 1:052 1:061 1:066 1:069 1:066 1:064 1:062 1:061
</p>
<p>80 0:927 1:024 1:049 1:059 1:064 1:067 1:063 1:060 1:059 1:057
</p>
<p>90 0:926 1:023 1:048 1:057 1:062 1:065 1:061 1:058 1:056 1:054
</p>
<p>100 0:925 1:021 1:047 1:056 1:060 1:063 1:059 1:056 1:054 1:052
</p>
<p>200 0:921 1:016 1:041 1:050 1:054 1:055 1:050 1:046 1:043 1:041
</p>
<p>1 0:916 1:011 1:035 1:044 1:047 1:048 1:041 1:036 1:032 1:029
</p>
<p>and it is equal to the product bb0, where b is the best-fit slope of the linear regression
of Y on X, and b0 is the slope of the linear regression of X on Y. The probability
distribution function of r, under the hypothesis that the variables X and Y are not
correlated, is given by
</p>
<p>fr.r/ D 1p
	
</p>
<p>ï¿½ .
f C 1
2
</p>
<p>/
</p>
<p>ï¿½ .
f
</p>
<p>2
/
</p>
<p>ï¿½
1
</p>
<p>1 ï¿½ r2
ï¿½ï¿½
</p>
<p>f ï¿½ 2
2 (A.15)
</p>
<p>where N is the size of the sample, and f D Nï¿½ 2 is the effective number of degrees
of freedom of the dataset.
</p>
<p>In Table A.24 we report the critical values of r calculated from the following
equation,
</p>
<p>1 ï¿½ p D
Z rcrit
</p>
<p>ï¿½rcrit
fr.r/dr (A.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>288 Appendix: Numerical Tables
</p>
<p>Table A.11 Critical values of F statistic that include p D 0:70 probability
f1
</p>
<p>f2 2 4 6 8 10 20 40 60 80 100
</p>
<p>1 5:056 5:830 6:117 6:267 6:358 6:544 6:639 6:671 6:687 6:697
</p>
<p>2 2:333 2:561 2:640 2:681 2:705 2:754 2:779 2:787 2:791 2:794
</p>
<p>3 1:847 1:985 2:028 2:048 2:061 2:084 2:096 2:100 2:102 2:103
</p>
<p>4 1:651 1:753 1:781 1:793 1:800 1:812 1:818 1:819 1:820 1:821
</p>
<p>5 1:547 1:629 1:648 1:656 1:659 1:665 1:666 1:666 1:667 1:667
</p>
<p>6 1:481 1:551 1:565 1:570 1:571 1:572 1:570 1:570 1:569 1:569
</p>
<p>7 1:437 1:499 1:509 1:511 1:511 1:507 1:504 1:502 1:501 1:501
</p>
<p>8 1:405 1:460 1:467 1:468 1:466 1:460 1:455 1:452 1:451 1:450
</p>
<p>9 1:380 1:431 1:436 1:435 1:433 1:424 1:417 1:414 1:413 1:412
</p>
<p>10 1:361 1:408 1:412 1:409 1:406 1:395 1:387 1:384 1:382 1:381
</p>
<p>20 1:279 1:311 1:305 1:297 1:290 1:268 1:252 1:245 1:242 1:240
</p>
<p>30 1:254 1:280 1:271 1:261 1:253 1:226 1:206 1:197 1:192 1:189
</p>
<p>40 1:241 1:264 1:255 1:243 1:234 1:205 1:182 1:172 1:167 1:163
</p>
<p>50 1:233 1:255 1:245 1:233 1:223 1:192 1:167 1:156 1:150 1:146
</p>
<p>60 1:228 1:249 1:238 1:226 1:215 1:183 1:157 1:146 1:139 1:135
</p>
<p>70 1:225 1:245 1:233 1:221 1:210 1:177 1:150 1:138 1:131 1:127
</p>
<p>80 1:222 1:242 1:230 1:217 1:206 1:172 1:144 1:132 1:125 1:120
</p>
<p>90 1:220 1:239 1:227 1:214 1:203 1:168 1:140 1:127 1:120 1:115
</p>
<p>100 1:219 1:237 1:225 1:212 1:200 1:165 1:137 1:123 1:116 1:111
</p>
<p>200 1:211 1:228 1:215 1:201 1:189 1:152 1:121 1:106 1:097 1:091
</p>
<p>1 1:204 1:220 1:205 1:191 1:178 1:139 1:104 1:087 1:076 1:069
</p>
<p>where p is the probability for a given value of the correlation coefficient to exceed, in
absolute value, the critical value rcrit. The critical values are function of the number
of degrees of freedom, and of the probability p.
</p>
<p>To evaluate the probability distribution function in the case of large f , a
convenient approximation can be given using the asymptotic expansion for the
Gamma function (see [1]):
</p>
<p>ï¿½ .azC b/ ' p2	eï¿½az.az/azCbï¿½1=2: (A.17)
</p>
<p>For large values of f , the ratio of the Gamma functions can therefore be approxi-
mated as
</p>
<p>ï¿½
</p>
<p>ï¿½
f C 1
2
</p>
<p>ï¿½
</p>
<p>ï¿½
</p>
<p>ï¿½
f
</p>
<p>2
</p>
<p>ï¿½ '
r
</p>
<p>f
</p>
<p>2
:</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 289
</p>
<p>Table A.12 Critical values of F statistic that include p D 0:80 probability
f1
</p>
<p>f2 2 4 6 8 10 20 40 60 80 100
</p>
<p>1 12:000 13:644 14:258 14:577 14:772 15:171 15:374 15:442 15:477 15:497
</p>
<p>2 4:000 4:236 4:317 4:358 4:382 4:432 4:456 4:465 4:469 4:471
</p>
<p>3 2:886 2:956 2:971 2:976 2:979 2:983 2:984 2:984 2:984 2:984
</p>
<p>4 2:472 2:483 2:473 2:465 2:460 2:445 2:436 2:433 2:431 2:430
</p>
<p>5 2:259 2:240 2:217 2:202 2:191 2:166 2:151 2:146 2:143 2:141
</p>
<p>6 2:130 2:092 2:062 2:042 2:028 1:995 1:976 1:969 1:965 1:963
</p>
<p>7 2:043 1:994 1:957 1:934 1:918 1:879 1:857 1:849 1:844 1:842
</p>
<p>8 1:981 1:923 1:883 1:856 1:838 1:796 1:770 1:761 1:756 1:753
</p>
<p>9 1:935 1:870 1:826 1:798 1:778 1:732 1:704 1:694 1:689 1:686
</p>
<p>10 1:899 1:829 1:782 1:752 1:732 1:682 1:653 1:642 1:636 1:633
</p>
<p>20 1:746 1:654 1:596 1:558 1:531 1:466 1:424 1:408 1:399 1:394
</p>
<p>30 1:699 1:600 1:538 1:497 1:468 1:395 1:347 1:328 1:318 1:312
</p>
<p>40 1:676 1:574 1:509 1:467 1:437 1:360 1:308 1:287 1:276 1:269
</p>
<p>50 1:662 1:558 1:492 1:449 1:418 1:338 1:284 1:262 1:249 1:241
</p>
<p>60 1:653 1:548 1:481 1:437 1:406 1:324 1:268 1:244 1:231 1:223
</p>
<p>70 1:647 1:540 1:473 1:429 1:397 1:314 1:256 1:231 1:218 1:209
</p>
<p>80 1:642 1:535 1:467 1:422 1:390 1:306 1:247 1:222 1:208 1:199
</p>
<p>90 1:639 1:531 1:463 1:418 1:385 1:300 1:240 1:214 1:200 1:191
</p>
<p>100 1:636 1:527 1:459 1:414 1:381 1:296 1:234 1:208 1:193 1:184
</p>
<p>200 1:622 1:512 1:443 1:396 1:363 1:274 1:209 1:180 1:163 1:152
</p>
<p>1 1:609 1:497 1:426 1:379 1:344 1:252 1:182 1:150 1:130 1:117
</p>
<p>A.7 The Kolmogorov&ndash;Smirnov Test
</p>
<p>The one-sample Kolmogorov&ndash;Smirnov statistic DN is defined in (13.7) as
</p>
<p>DN D max
x
jFN.x/ï¿½ F.x/j;
</p>
<p>where F.x/ is the parent distribution, and FN.x/ the sample distribution.
The cumulative distribution of the test statistic can be approximated by
</p>
<p>P.DN &lt; z=.
p
N C 0:12C 0:11=pN// ' Ë.z/:
</p>
<p>where
</p>
<p>Ë.z/ D
1X
</p>
<p>rDï¿½1
.ï¿½1/reï¿½2r2z2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>290 Appendix: Numerical Tables
</p>
<p>Table A.13 Critical values of F statistic that include p D 0:90 probability
f1
</p>
<p>f2 2 4 6 8 10 20 40 60 80 100
</p>
<p>1 49:500 55:833 58:204 59:439 60:195 61:740 62:529 62:794 62:927 63:007
</p>
<p>2 9:000 9:243 9:326 9:367 9:392 9:441 9:466 9:475 9:479 9:481
</p>
<p>3 5:462 5:343 5:285 5:252 5:230 5:184 5:160 5:151 5:147 5:144
</p>
<p>4 4:325 4:107 4:010 3:955 3:920 3:844 3:804 3:790 3:782 3:778
</p>
<p>5 3:780 3:520 3:404 3:339 3:297 3:207 3:157 3:140 3:132 3:126
</p>
<p>6 3:463 3:181 3:055 2:983 2:937 2:836 2:781 2:762 2:752 2:746
</p>
<p>7 3:257 2:960 2:827 2:752 2:703 2:595 2:535 2:514 2:504 2:497
</p>
<p>8 3:113 2:806 2:668 2:589 2:538 2:425 2:361 2:339 2:328 2:321
</p>
<p>9 3:006 2:693 2:551 2:469 2:416 2:298 2:232 2:208 2:196 2:189
</p>
<p>10 2:924 2:605 2:461 2:377 2:323 2:201 2:132 2:107 2:095 2:087
</p>
<p>20 2:589 2:249 2:091 1:999 1:937 1:794 1:708 1:677 1:660 1:650
</p>
<p>30 2:489 2:142 1:980 1:884 1:820 1:667 1:573 1:538 1:519 1:507
</p>
<p>40 2:440 2:091 1:927 1:829 1:763 1:605 1:506 1:467 1:447 1:434
</p>
<p>50 2:412 2:061 1:895 1:796 1:729 1:568 1:465 1:424 1:402 1:389
</p>
<p>60 2:393 2:041 1:875 1:775 1:707 1:544 1:437 1:395 1:372 1:358
</p>
<p>70 2:380 2:027 1:860 1:760 1:691 1:526 1:418 1:374 1:350 1:335
</p>
<p>80 2:370 2:016 1:849 1:748 1:680 1:513 1:403 1:358 1:334 1:318
</p>
<p>90 2:362 2:008 1:841 1:739 1:671 1:503 1:391 1:346 1:320 1:304
</p>
<p>100 2:356 2:002 1:834 1:732 1:663 1:494 1:382 1:336 1:310 1:293
</p>
<p>200 2:329 1:973 1:804 1:701 1:631 1:458 1:339 1:289 1:261 1:242
</p>
<p>1 2:303 1:945 1:774 1:670 1:599 1:421 1:295 1:240 1:207 1:185
</p>
<p>and it is independent of the form of the parent distribution F.x/: For large values of
N, we can use the asymptotic equation
</p>
<p>P.DN &lt; z=
p
N/ D Ë.z/:
</p>
<p>In Table A.25 are listed the critical values of
p
NDN for various levels of probability.
</p>
<p>Values of the Kolmogorov&ndash;Smirnov statistic above the critical value indicate a
rejection of the null hypothesis that the data are drawn from the parent model.
</p>
<p>The two-sample Kolmogorov&ndash;Smirnov statistic is
</p>
<p>DNM D max
x
jFM.x/ï¿½ GN.x/j
</p>
<p>where FM.x/ and GN.x/ are the sample cumulative distribution of two independent
sets of observations of size M and N. This statistic has the same distribution as the
one-sample Kolmogorov-Smirnov statistic, with the substitution of MN=.MCN/ in
place of N, and in the limit of large M and N, (13.12).</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 291
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
4
</p>
<p>C
ri
</p>
<p>ti
ca
</p>
<p>lv
al
</p>
<p>ue
s
</p>
<p>of
F
</p>
<p>st
at
</p>
<p>is
ti
</p>
<p>c
th
</p>
<p>at
in
</p>
<p>cl
ud
</p>
<p>e
p
D
0
:9
5
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
</p>
<p>f 1
f 2
</p>
<p>2
4
</p>
<p>6
8
</p>
<p>10
20
</p>
<p>40
60
</p>
<p>80
10
</p>
<p>0
</p>
<p>1
1
9
9
:5
0
0
</p>
<p>2
2
4
:5
8
3
</p>
<p>2
3
3
:9
8
6
</p>
<p>2
3
8
:8
8
3
</p>
<p>2
4
1
:8
8
2
</p>
<p>2
4
8
:0
1
3
</p>
<p>2
5
1
:1
4
3
</p>
<p>2
5
2
:1
9
6
</p>
<p>2
5
2
:7
2
4
</p>
<p>2
5
3
:0
4
1
</p>
<p>2
1
9
:0
0
0
</p>
<p>1
9
:2
4
7
</p>
<p>1
9
:3
3
0
</p>
<p>1
9
:3
7
1
</p>
<p>1
9
:3
9
6
</p>
<p>1
9
:4
4
6
</p>
<p>1
9
:4
7
1
</p>
<p>1
9
:4
7
9
</p>
<p>1
9
:4
8
3
</p>
<p>1
9
:4
8
6
</p>
<p>3
9
:5
5
2
</p>
<p>9
:1
1
7
</p>
<p>8
:9
4
1
</p>
<p>8
:8
4
5
</p>
<p>8
:7
8
6
</p>
<p>8
:6
6
0
</p>
<p>8
:5
9
4
</p>
<p>8
:5
7
2
</p>
<p>8
:5
6
1
</p>
<p>8
:5
5
4
</p>
<p>4
6
:9
4
4
</p>
<p>6
:3
8
8
</p>
<p>6
:1
6
3
</p>
<p>6
:0
4
1
</p>
<p>5
:9
6
4
</p>
<p>5
:8
0
3
</p>
<p>5
:7
1
7
</p>
<p>5
:6
8
8
</p>
<p>5
:6
7
3
</p>
<p>5
:6
6
4
</p>
<p>5
5
:7
8
6
</p>
<p>5
:1
9
2
</p>
<p>4
:9
5
0
</p>
<p>4
:8
1
8
</p>
<p>4
:7
3
5
</p>
<p>4
:5
5
8
</p>
<p>4
:4
6
4
</p>
<p>4
:4
3
1
</p>
<p>4
:4
1
5
</p>
<p>4
:4
0
5
</p>
<p>6
5
:1
4
3
</p>
<p>4
:5
3
4
</p>
<p>4
:2
8
4
</p>
<p>4
:1
4
7
</p>
<p>4
:0
6
0
</p>
<p>3
:8
7
4
</p>
<p>3
:7
7
4
</p>
<p>3
:7
4
0
</p>
<p>3
:7
2
2
</p>
<p>3
:7
1
2
</p>
<p>7
4
:7
3
7
</p>
<p>4
:1
2
0
</p>
<p>3
:8
6
6
</p>
<p>3
:7
2
6
</p>
<p>3
:6
3
6
</p>
<p>3
:4
4
4
</p>
<p>3
:3
4
0
</p>
<p>3
:3
0
4
</p>
<p>3
:2
8
6
</p>
<p>3
:2
7
5
</p>
<p>8
4
:4
5
9
</p>
<p>3
:8
3
8
</p>
<p>3
:5
8
1
</p>
<p>3
:4
3
8
</p>
<p>3
:3
4
7
</p>
<p>3
:1
5
0
</p>
<p>3
:0
4
3
</p>
<p>3
:0
0
5
</p>
<p>2
:9
8
6
</p>
<p>2
:9
7
5
</p>
<p>9
4
:2
5
6
</p>
<p>3
:6
3
3
</p>
<p>3
:3
7
4
</p>
<p>3
:2
3
0
</p>
<p>3
:1
3
7
</p>
<p>2
:9
3
6
</p>
<p>2
:8
2
6
</p>
<p>2
:7
8
7
</p>
<p>2
:7
6
8
</p>
<p>2
:7
5
6
</p>
<p>10
4
:1
0
3
</p>
<p>3
:4
7
8
</p>
<p>3
:2
1
7
</p>
<p>3
:0
7
2
</p>
<p>2
:9
7
8
</p>
<p>2
:7
7
4
</p>
<p>2
:6
6
1
</p>
<p>2
:6
2
1
</p>
<p>2
:6
0
1
</p>
<p>2
:5
8
8
</p>
<p>20
3
:4
9
3
</p>
<p>2
:8
6
6
</p>
<p>2
:5
9
9
</p>
<p>2
:4
4
7
</p>
<p>2
:3
4
8
</p>
<p>2
:1
2
4
</p>
<p>1
:9
9
4
</p>
<p>1
:9
4
6
</p>
<p>1
:9
2
2
</p>
<p>1
:9
0
7
</p>
<p>30
3
:3
1
6
</p>
<p>2
:6
9
0
</p>
<p>2
:4
2
1
</p>
<p>2
:2
6
6
</p>
<p>2
:1
6
5
</p>
<p>1
:9
3
2
</p>
<p>1
:7
9
2
</p>
<p>1
:7
4
0
</p>
<p>1
:7
1
2
</p>
<p>1
:6
9
5
</p>
<p>40
3
:2
3
2
</p>
<p>2
:6
0
6
</p>
<p>2
:3
3
6
</p>
<p>2
:1
8
0
</p>
<p>2
:0
7
7
</p>
<p>1
:8
3
9
</p>
<p>1
:6
9
3
</p>
<p>1
:6
3
7
</p>
<p>1
:6
0
8
</p>
<p>1
:5
8
9
</p>
<p>50
3
:1
8
3
</p>
<p>2
:5
5
7
</p>
<p>2
:2
8
6
</p>
<p>2
:1
3
0
</p>
<p>2
:0
2
6
</p>
<p>1
:7
8
4
</p>
<p>1
:6
3
4
</p>
<p>1
:5
7
6
</p>
<p>1
:5
4
4
</p>
<p>1
:5
2
5
</p>
<p>60
3
:1
5
0
</p>
<p>2
:5
2
5
</p>
<p>2
:2
5
4
</p>
<p>2
:0
9
7
</p>
<p>1
:9
9
2
</p>
<p>1
:7
4
8
</p>
<p>1
:5
9
4
</p>
<p>1
:5
3
4
</p>
<p>1
:5
0
2
</p>
<p>1
:4
8
1
</p>
<p>70
3
:1
2
8
</p>
<p>2
:5
0
3
</p>
<p>2
:2
3
1
</p>
<p>2
:0
7
4
</p>
<p>1
:9
6
9
</p>
<p>1
:7
2
2
</p>
<p>1
:5
6
6
</p>
<p>1
:5
0
4
</p>
<p>1
:4
7
1
</p>
<p>1
:4
5
0
</p>
<p>80
3
:1
1
1
</p>
<p>2
:4
8
6
</p>
<p>2
:2
1
4
</p>
<p>2
:0
5
6
</p>
<p>1
:9
5
1
</p>
<p>1
:7
0
3
</p>
<p>1
:5
4
5
</p>
<p>1
:4
8
2
</p>
<p>1
:4
4
8
</p>
<p>1
:4
2
6
</p>
<p>90
3
:0
9
8
</p>
<p>2
:4
7
3
</p>
<p>2
:2
0
1
</p>
<p>2
:0
4
3
</p>
<p>1
:9
3
8
</p>
<p>1
:6
8
8
</p>
<p>1
:5
2
8
</p>
<p>1
:4
6
4
</p>
<p>1
:4
2
9
</p>
<p>1
:4
0
7
</p>
<p>10
0
</p>
<p>3
:0
8
7
</p>
<p>2
:4
6
3
</p>
<p>2
:1
9
1
</p>
<p>2
:0
3
2
</p>
<p>1
:9
2
7
</p>
<p>1
:6
7
7
</p>
<p>1
:5
1
5
</p>
<p>1
:4
5
0
</p>
<p>1
:4
1
4
</p>
<p>1
:3
9
2
</p>
<p>20
0
</p>
<p>3
:0
4
1
</p>
<p>2
:4
1
7
</p>
<p>2
:1
4
4
</p>
<p>1
:9
8
5
</p>
<p>1
:8
7
8
</p>
<p>1
:6
2
3
</p>
<p>1
:4
5
5
</p>
<p>1
:3
8
5
</p>
<p>1
:3
4
6
</p>
<p>1
:3
2
1
</p>
<p>1
2
:9
9
6
</p>
<p>2
:3
7
2
</p>
<p>2
:0
9
9
</p>
<p>1
:9
3
8
</p>
<p>1
:8
3
1
</p>
<p>1
:5
7
1
</p>
<p>1
:3
9
4
</p>
<p>1
:3
1
8
</p>
<p>1
:2
7
3
</p>
<p>1
:2
4
3</p>
<p/>
</div>
<div class="page"><p/>
<p>292 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
5
</p>
<p>C
ri
</p>
<p>ti
ca
</p>
<p>lv
al
</p>
<p>ue
s
</p>
<p>of
F
</p>
<p>st
at
</p>
<p>is
ti
</p>
<p>c
th
</p>
<p>at
in
</p>
<p>cl
ud
</p>
<p>e
p
D
0
:9
9
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
</p>
<p>f 1
f 2
</p>
<p>2
4
</p>
<p>6
8
</p>
<p>10
20
</p>
<p>40
60
</p>
<p>80
10
</p>
<p>0
</p>
<p>1
4
9
9
9
:5
0
0
</p>
<p>5
6
2
4
:5
8
3
</p>
<p>5
8
5
8
:9
8
6
</p>
<p>5
9
8
1
:0
7
0
</p>
<p>6
0
5
5
:8
4
7
</p>
<p>6
2
0
8
:7
3
0
</p>
<p>6
2
8
6
:7
8
2
</p>
<p>6
3
1
3
:0
3
0
</p>
<p>6
3
2
6
:1
9
7
</p>
<p>6
3
3
4
:1
1
0
</p>
<p>2
9
9
:0
0
0
</p>
<p>9
9
:2
4
9
</p>
<p>9
9
:3
3
3
</p>
<p>9
9
:3
7
4
</p>
<p>9
9
:3
9
9
</p>
<p>9
9
:4
4
9
</p>
<p>9
9
:4
7
4
</p>
<p>9
9
:4
8
2
</p>
<p>9
9
:4
8
7
</p>
<p>9
9
:4
8
9
</p>
<p>3
3
0
:8
1
7
</p>
<p>2
8
:7
1
0
</p>
<p>2
7
:9
1
1
</p>
<p>2
7
:4
8
9
</p>
<p>2
7
:2
2
9
</p>
<p>2
6
:6
9
0
</p>
<p>2
6
:4
1
1
</p>
<p>2
6
:3
1
6
</p>
<p>2
6
:2
6
9
</p>
<p>2
6
:2
4
0
</p>
<p>4
1
8
:0
0
0
</p>
<p>1
5
:9
7
7
</p>
<p>1
5
:2
0
7
</p>
<p>1
4
:7
9
9
</p>
<p>1
4
:5
4
6
</p>
<p>1
4
:0
2
0
</p>
<p>1
3
:7
4
5
</p>
<p>1
3
:6
5
2
</p>
<p>1
3
:6
0
5
</p>
<p>1
3
:5
7
7
</p>
<p>5
1
3
:2
7
4
</p>
<p>1
1
:3
9
2
</p>
<p>1
0
:6
7
2
</p>
<p>1
0
:2
8
9
</p>
<p>1
0
:0
5
1
</p>
<p>9
:5
5
3
</p>
<p>9
:2
9
1
</p>
<p>9
:2
0
2
</p>
<p>9
:1
5
7
</p>
<p>9
:1
3
0
</p>
<p>6
1
0
:9
2
5
</p>
<p>9
:1
4
8
</p>
<p>8
:4
6
6
</p>
<p>8
:1
0
2
</p>
<p>7
:8
7
4
</p>
<p>7
:3
9
6
</p>
<p>7
:1
4
3
</p>
<p>7
:0
5
7
</p>
<p>7
:0
1
3
</p>
<p>6
:9
8
7
</p>
<p>7
9
:5
4
7
</p>
<p>7
:8
4
7
</p>
<p>7
:1
9
1
</p>
<p>6
:8
4
0
</p>
<p>6
:6
2
0
</p>
<p>6
:1
5
5
</p>
<p>5
:9
0
8
</p>
<p>5
:8
2
4
</p>
<p>5
:7
8
1
</p>
<p>5
:7
5
5
</p>
<p>8
8
:6
4
9
</p>
<p>7
:0
0
6
</p>
<p>6
:3
7
1
</p>
<p>6
:0
2
9
</p>
<p>5
:8
1
4
</p>
<p>5
:3
5
9
</p>
<p>5
:1
1
6
</p>
<p>5
:0
3
2
</p>
<p>4
:9
8
9
</p>
<p>4
:9
6
3
</p>
<p>9
8
:0
2
2
</p>
<p>6
:4
2
2
</p>
<p>5
:8
0
2
</p>
<p>5
:4
6
7
</p>
<p>5
:2
5
7
</p>
<p>4
:8
0
8
</p>
<p>4
:5
6
7
</p>
<p>4
:4
8
3
</p>
<p>4
:4
4
1
</p>
<p>4
:4
1
5
</p>
<p>10
7
:5
5
9
</p>
<p>5
:9
9
4
</p>
<p>5
:3
8
6
</p>
<p>5
:0
5
7
</p>
<p>4
:8
4
9
</p>
<p>4
:4
0
5
</p>
<p>4
:1
6
5
</p>
<p>4
:0
8
2
</p>
<p>4
:0
3
9
</p>
<p>4
:0
1
4
</p>
<p>20
5
:8
4
9
</p>
<p>4
:4
3
1
</p>
<p>3
:8
7
1
</p>
<p>3
:5
6
4
</p>
<p>3
:3
6
8
</p>
<p>2
:9
3
8
</p>
<p>2
:6
9
5
</p>
<p>2
:6
0
8
</p>
<p>2
:5
6
3
</p>
<p>2
:5
3
5
</p>
<p>30
5
:3
9
0
</p>
<p>4
:0
1
8
</p>
<p>3
:4
7
3
</p>
<p>3
:1
7
3
</p>
<p>2
:9
7
9
</p>
<p>2
:5
4
9
</p>
<p>2
:2
9
9
</p>
<p>2
:2
0
8
</p>
<p>2
:1
6
0
</p>
<p>2
:1
3
1
</p>
<p>40
5
:1
7
8
</p>
<p>3
:8
2
8
</p>
<p>3
:2
9
1
</p>
<p>2
:9
9
3
</p>
<p>2
:8
0
1
</p>
<p>2
:3
6
9
</p>
<p>2
:1
1
4
</p>
<p>2
:0
1
9
</p>
<p>1
:9
6
9
</p>
<p>1
:9
3
8
</p>
<p>50
5
:0
5
7
</p>
<p>3
:7
2
0
</p>
<p>3
:1
8
6
</p>
<p>2
:8
9
0
</p>
<p>2
:6
9
8
</p>
<p>2
:2
6
5
</p>
<p>2
:0
0
6
</p>
<p>1
:9
0
9
</p>
<p>1
:8
5
7
</p>
<p>1
:8
2
5
</p>
<p>60
4
:9
7
7
</p>
<p>3
:6
4
9
</p>
<p>3
:1
1
9
</p>
<p>2
:8
2
3
</p>
<p>2
:6
3
2
</p>
<p>2
:1
9
8
</p>
<p>1
:9
3
6
</p>
<p>1
:8
3
6
</p>
<p>1
:7
8
3
</p>
<p>1
:7
4
9
</p>
<p>70
4
:9
2
2
</p>
<p>3
:6
0
0
</p>
<p>3
:0
7
1
</p>
<p>2
:7
7
7
</p>
<p>2
:5
8
5
</p>
<p>2
:1
5
0
</p>
<p>1
:8
8
6
</p>
<p>1
:7
8
4
</p>
<p>1
:7
3
0
</p>
<p>1
:6
9
6
</p>
<p>80
4
:8
8
1
</p>
<p>3
:5
6
3
</p>
<p>3
:0
3
6
</p>
<p>2
:7
4
2
</p>
<p>2
:5
5
1
</p>
<p>2
:1
1
5
</p>
<p>1
:8
4
9
</p>
<p>1
:7
4
6
</p>
<p>1
:6
9
0
</p>
<p>1
:6
5
5
</p>
<p>90
4
:8
4
9
</p>
<p>3
:5
3
5
</p>
<p>3
:0
0
9
</p>
<p>2
:7
1
5
</p>
<p>2
:5
2
4
</p>
<p>2
:0
8
8
</p>
<p>1
:8
2
0
</p>
<p>1
:7
1
6
</p>
<p>1
:6
5
9
</p>
<p>1
:6
2
3
</p>
<p>10
0
</p>
<p>4
:8
2
4
</p>
<p>3
:5
1
3
</p>
<p>2
:9
8
8
</p>
<p>2
:6
9
4
</p>
<p>2
:5
0
3
</p>
<p>2
:0
6
7
</p>
<p>1
:7
9
7
</p>
<p>1
:6
9
2
</p>
<p>1
:6
3
4
</p>
<p>1
:5
9
8
</p>
<p>20
0
</p>
<p>4
:7
1
3
</p>
<p>3
:4
1
4
</p>
<p>2
:8
9
3
</p>
<p>2
:6
0
1
</p>
<p>2
:4
1
1
</p>
<p>1
:9
7
1
</p>
<p>1
:6
9
5
</p>
<p>1
:5
8
4
</p>
<p>1
:5
2
1
</p>
<p>1
:4
8
1
</p>
<p>1
4
:6
0
5
</p>
<p>3
:3
1
9
</p>
<p>2
:8
0
2
</p>
<p>2
:5
1
1
</p>
<p>2
:3
2
1
</p>
<p>1
:8
7
8
</p>
<p>1
:5
9
2
</p>
<p>1
:4
7
3
</p>
<p>1
:4
0
4
</p>
<p>1
:3
5
8</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 293
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
6
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>D
1
),
</p>
<p>or
pr
</p>
<p>ob
ab
</p>
<p>il
it
</p>
<p>y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
6
3
6
6
</p>
<p>0
:0
1
2
7
3
1
</p>
<p>0
:0
1
9
0
9
3
</p>
<p>0
:0
2
5
4
5
1
</p>
<p>0
:0
3
1
8
0
5
</p>
<p>0
:0
3
8
1
5
1
</p>
<p>0
:0
4
4
4
9
1
</p>
<p>0
:0
5
0
8
2
1
</p>
<p>0
:0
5
7
1
4
2
</p>
<p>0
:1
</p>
<p>0
:0
6
3
4
5
1
</p>
<p>0
:0
6
9
7
4
8
</p>
<p>0
:0
7
6
0
3
1
</p>
<p>0
:0
8
2
2
9
9
</p>
<p>0
:0
8
8
5
5
1
</p>
<p>0
:0
9
4
7
8
6
</p>
<p>0
:1
0
1
0
0
3
</p>
<p>0
:1
0
7
2
0
1
</p>
<p>0
:1
1
3
3
7
8
</p>
<p>0
:1
1
9
5
3
3
</p>
<p>0
:2
</p>
<p>0
:1
2
5
6
6
6
</p>
<p>0
:1
3
1
7
7
5
</p>
<p>0
:1
3
7
8
6
0
</p>
<p>0
:1
4
3
9
2
0
</p>
<p>0
:1
4
9
9
5
3
</p>
<p>0
:1
5
5
9
5
8
</p>
<p>0
:1
6
1
9
3
6
</p>
<p>0
:1
6
7
8
8
4
</p>
<p>0
:1
7
3
8
0
3
</p>
<p>0
:1
7
9
6
9
1
</p>
<p>0
:3
</p>
<p>0
:1
8
5
5
4
7
</p>
<p>0
:1
9
1
3
7
2
</p>
<p>0
:1
9
7
1
6
3
</p>
<p>0
:2
0
2
9
2
1
</p>
<p>0
:2
0
8
6
4
5
</p>
<p>0
:2
1
4
3
3
4
</p>
<p>0
:2
1
9
9
8
8
</p>
<p>0
:2
2
5
6
0
5
</p>
<p>0
:2
3
1
1
8
7
</p>
<p>0
:2
3
6
7
3
1
</p>
<p>0
:4
</p>
<p>0
:2
4
2
2
3
8
</p>
<p>0
:2
4
7
7
0
7
</p>
<p>0
:2
5
3
1
3
8
</p>
<p>0
:2
5
8
5
3
0
</p>
<p>0
:2
6
3
8
8
3
</p>
<p>0
:2
6
9
1
9
7
</p>
<p>0
:2
7
4
4
7
2
</p>
<p>0
:2
7
9
7
0
6
</p>
<p>0
:2
8
4
9
0
0
</p>
<p>0
:2
9
0
0
5
4
</p>
<p>0
:5
</p>
<p>0
:2
9
5
1
6
7
</p>
<p>0
:3
0
0
2
4
0
</p>
<p>0
:3
0
5
2
7
2
</p>
<p>0
:3
1
0
2
6
2
</p>
<p>0
:3
1
5
2
1
2
</p>
<p>0
:3
2
0
1
2
0
</p>
<p>0
:3
2
4
9
8
7
</p>
<p>0
:3
2
9
8
1
3
</p>
<p>0
:3
3
4
5
9
7
</p>
<p>0
:3
3
9
3
4
0
</p>
<p>0
:6
</p>
<p>0
:3
4
4
0
4
2
</p>
<p>0
:3
4
8
7
0
2
</p>
<p>0
:3
5
3
3
2
1
</p>
<p>0
:3
5
7
8
9
9
</p>
<p>0
:3
6
2
4
3
6
</p>
<p>0
:3
6
6
9
3
2
</p>
<p>0
:3
7
1
3
8
7
</p>
<p>0
:3
7
5
8
0
1
</p>
<p>0
:3
8
0
1
7
5
</p>
<p>0
:3
8
4
5
0
8
</p>
<p>0
:7
</p>
<p>0
:3
8
8
8
0
0
</p>
<p>0
:3
9
3
0
5
3
</p>
<p>0
:3
9
7
2
6
6
</p>
<p>0
:4
0
1
4
3
8
</p>
<p>0
:4
0
5
5
7
2
</p>
<p>0
:4
0
9
6
6
6
</p>
<p>0
:4
1
3
7
2
1
</p>
<p>0
:4
1
7
7
3
7
</p>
<p>0
:4
2
1
7
1
4
</p>
<p>0
:4
2
5
6
5
3
</p>
<p>0
:8
</p>
<p>0
:4
2
9
5
5
4
</p>
<p>0
:4
3
3
4
1
7
</p>
<p>0
:4
3
7
2
4
2
</p>
<p>0
:4
4
1
0
3
0
</p>
<p>0
:4
4
4
7
8
1
</p>
<p>0
:4
4
8
4
9
5
</p>
<p>0
:4
5
2
1
7
3
</p>
<p>0
:4
5
5
8
1
4
</p>
<p>0
:4
5
9
4
2
0
</p>
<p>0
:4
6
2
9
9
0
</p>
<p>0
:9
</p>
<p>0
:4
6
6
5
2
5
</p>
<p>0
:4
7
0
0
2
5
</p>
<p>0
:4
7
3
4
9
0
</p>
<p>0
:4
7
6
9
2
0
</p>
<p>0
:4
8
0
3
1
7
</p>
<p>0
:4
8
3
6
8
0
</p>
<p>0
:4
8
7
0
1
0
</p>
<p>0
:4
9
0
3
0
6
</p>
<p>0
:4
9
3
5
7
0
</p>
<p>0
:4
9
6
8
0
1
</p>
<p>1
:0
</p>
<p>0
:5
0
0
0
0
0
</p>
<p>0
:5
0
3
1
6
7
</p>
<p>0
:5
0
6
3
0
3
</p>
<p>0
:5
0
9
4
0
8
</p>
<p>0
:5
1
2
4
8
1
</p>
<p>0
:5
1
5
5
2
4
</p>
<p>0
:5
1
8
5
3
7
</p>
<p>0
:5
2
1
5
2
0
</p>
<p>0
:5
2
4
4
7
4
</p>
<p>0
:5
2
7
3
9
8
</p>
<p>1
:1
</p>
<p>0
:5
3
0
2
9
3
</p>
<p>0
:5
3
3
1
5
9
</p>
<p>0
:5
3
5
9
9
7
</p>
<p>0
:5
3
8
8
0
7
</p>
<p>0
:5
4
1
5
8
9
</p>
<p>0
:5
4
4
3
4
4
</p>
<p>0
:5
4
7
0
7
1
</p>
<p>0
:5
4
9
7
7
2
</p>
<p>0
:5
5
2
4
4
6
</p>
<p>0
:5
5
5
0
9
4
</p>
<p>1
:2
</p>
<p>0
:5
5
7
7
1
6
</p>
<p>0
:5
6
0
3
1
2
</p>
<p>0
:5
6
2
8
8
3
</p>
<p>0
:5
6
5
4
2
9
</p>
<p>0
:5
6
7
9
5
0
</p>
<p>0
:5
7
0
4
4
7
</p>
<p>0
:5
7
2
9
1
9
</p>
<p>0
:5
7
5
3
6
7
</p>
<p>0
:5
7
7
7
9
2
</p>
<p>0
:5
8
0
1
9
3
</p>
<p>1
:3
</p>
<p>0
:5
8
2
5
7
1
</p>
<p>0
:5
8
4
9
2
7
</p>
<p>0
:5
8
7
2
5
9
</p>
<p>0
:5
8
9
5
7
0
</p>
<p>0
:5
9
1
8
5
8
</p>
<p>0
:5
9
4
1
2
4
</p>
<p>0
:5
9
6
3
6
9
</p>
<p>0
:5
9
8
5
9
2
</p>
<p>0
:6
0
0
7
9
5
</p>
<p>0
:6
0
2
9
7
6
</p>
<p>1
:4
</p>
<p>0
:6
0
5
1
3
7
</p>
<p>0
:6
0
7
2
7
8
</p>
<p>0
:6
0
9
3
9
8
</p>
<p>0
:6
1
1
4
9
9
</p>
<p>0
:6
1
3
5
8
0
</p>
<p>0
:6
1
5
6
4
1
</p>
<p>0
:6
1
7
6
8
4
</p>
<p>0
:6
1
9
7
0
7
</p>
<p>0
:6
2
1
7
1
2
</p>
<p>0
:6
2
3
6
9
8
</p>
<p>1
:5
</p>
<p>0
:6
2
5
6
6
6
</p>
<p>0
:6
2
7
6
1
6
</p>
<p>0
:6
2
9
5
4
8
</p>
<p>0
:6
3
1
4
6
2
</p>
<p>0
:6
3
3
3
5
9
</p>
<p>0
:6
3
5
2
3
9
</p>
<p>0
:6
3
7
1
0
1
</p>
<p>0
:6
3
8
9
4
7
</p>
<p>0
:6
4
0
7
7
6
</p>
<p>0
:6
4
2
5
8
9
</p>
<p>1
:6
</p>
<p>0
:6
4
4
3
8
5
</p>
<p>0
:6
4
6
1
6
5
</p>
<p>0
:6
4
7
9
3
0
</p>
<p>0
:6
4
9
6
7
8
</p>
<p>0
:6
5
1
4
1
1
</p>
<p>0
:6
5
3
1
2
9
</p>
<p>0
:6
5
4
8
3
2
</p>
<p>0
:6
5
6
5
1
9
</p>
<p>0
:6
5
8
1
9
2
</p>
<p>0
:6
5
9
8
5
0
</p>
<p>1
:7
</p>
<p>0
:6
6
1
4
9
4
</p>
<p>0
:6
6
3
1
2
4
</p>
<p>0
:6
6
4
7
3
9
</p>
<p>0
:6
6
6
3
4
0
</p>
<p>0
:6
6
7
9
2
8
</p>
<p>0
:6
6
9
5
0
2
</p>
<p>0
:6
7
1
0
6
2
</p>
<p>0
:6
7
2
6
0
9
</p>
<p>0
:6
7
4
1
4
3
</p>
<p>0
:6
7
5
6
6
3
</p>
<p>1
:8
</p>
<p>0
:6
7
7
1
7
1
</p>
<p>0
:6
7
8
6
6
6
</p>
<p>0
:6
8
0
1
4
9
</p>
<p>0
:6
8
1
6
1
9
</p>
<p>0
:6
8
3
0
7
7
</p>
<p>0
:6
8
4
5
2
2
</p>
<p>0
:6
8
5
9
5
6
</p>
<p>0
:6
8
7
3
7
7
</p>
<p>0
:6
8
8
7
8
7
</p>
<p>0
:6
9
0
1
8
5
</p>
<p>1
:9
</p>
<p>0
:6
9
1
5
7
2
</p>
<p>0
:6
9
2
9
4
7
</p>
<p>0
:6
9
4
3
1
1
</p>
<p>0
:6
9
5
6
6
4
</p>
<p>0
:6
9
7
0
0
6
</p>
<p>0
:6
9
8
3
3
7
</p>
<p>0
:6
9
9
6
5
7
</p>
<p>0
:7
0
0
9
6
7
</p>
<p>0
:7
0
2
2
6
6
</p>
<p>0
:7
0
3
5
5
5
</p>
<p>2
:0
</p>
<p>0
:7
0
4
8
3
3
</p>
<p>0
:7
0
6
1
0
1
</p>
<p>0
:7
0
7
3
5
9
</p>
<p>0
:7
0
8
6
0
7
</p>
<p>0
:7
0
9
8
4
6
</p>
<p>0
:7
1
1
0
7
4
</p>
<p>0
:7
1
2
2
9
3
</p>
<p>0
:7
1
3
5
0
2
</p>
<p>0
:7
1
4
7
0
2
</p>
<p>0
:7
1
5
8
9
3
</p>
<p>2
:1
</p>
<p>0
:7
1
7
0
7
4
</p>
<p>0
:7
1
8
2
4
6
</p>
<p>0
:7
1
9
4
1
0
</p>
<p>0
:7
2
0
5
6
4
</p>
<p>0
:7
2
1
7
0
9
</p>
<p>0
:7
2
2
8
4
6
</p>
<p>0
:7
2
3
9
7
4
</p>
<p>0
:7
2
5
0
9
3
</p>
<p>0
:7
2
6
2
0
4
</p>
<p>0
:7
2
7
3
0
7
</p>
<p>2
:2
</p>
<p>0
:7
2
8
4
0
1
</p>
<p>0
:7
2
9
4
8
7
</p>
<p>0
:7
3
0
5
6
5
</p>
<p>0
:7
3
1
6
3
5
</p>
<p>0
:7
3
2
6
9
6
</p>
<p>0
:7
3
3
7
5
0
</p>
<p>0
:7
3
4
7
9
7
</p>
<p>0
:7
3
5
8
3
5
</p>
<p>0
:7
3
6
8
6
6
</p>
<p>0
:7
3
7
8
8
9
</p>
<p>2
:3
</p>
<p>0
:7
3
8
9
0
5
</p>
<p>0
:7
3
9
9
1
4
</p>
<p>0
:7
4
0
9
1
5
</p>
<p>0
:7
4
1
9
0
9
</p>
<p>0
:7
4
2
8
9
5
</p>
<p>0
:7
4
3
8
7
5
</p>
<p>0
:7
4
4
8
4
7
</p>
<p>0
:7
4
5
8
1
3
</p>
<p>0
:7
4
6
7
7
2
</p>
<p>0
:7
4
7
7
2
3
</p>
<p>2
:4
</p>
<p>0
:7
4
8
6
6
8
</p>
<p>0
:7
4
9
6
0
7
</p>
<p>0
:7
5
0
5
3
9
</p>
<p>0
:7
5
1
4
6
4
</p>
<p>0
:7
5
2
3
8
3
</p>
<p>0
:7
5
3
2
9
5
</p>
<p>0
:7
5
4
2
0
1
</p>
<p>0
:7
5
5
1
0
1
</p>
<p>0
:7
5
5
9
9
4
</p>
<p>0
:7
5
6
8
8
1
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>294 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
6
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:7
5
7
7
6
2
</p>
<p>0
:7
5
8
6
3
8
</p>
<p>0
:7
5
9
5
0
7
</p>
<p>0
:7
6
0
3
7
0
</p>
<p>0
:7
6
1
2
2
7
</p>
<p>0
:7
6
2
0
7
8
</p>
<p>0
:7
6
2
9
2
4
</p>
<p>0
:7
6
3
7
6
4
</p>
<p>0
:7
6
4
5
9
8
</p>
<p>0
:7
6
5
4
2
7
</p>
<p>2
:6
</p>
<p>0
:7
6
6
2
5
0
</p>
<p>0
:7
6
7
0
6
8
</p>
<p>0
:7
6
7
8
8
0
</p>
<p>0
:7
6
8
6
8
7
</p>
<p>0
:7
6
9
4
8
8
</p>
<p>0
:7
7
0
2
8
4
</p>
<p>0
:7
7
1
0
7
5
</p>
<p>0
:7
7
1
8
6
1
</p>
<p>0
:7
7
2
6
4
2
</p>
<p>0
:7
7
3
4
1
7
</p>
<p>2
:7
</p>
<p>0
:7
7
4
1
8
8
</p>
<p>0
:7
7
4
9
5
3
</p>
<p>0
:7
7
5
7
1
4
</p>
<p>0
:7
7
6
4
6
9
</p>
<p>0
:7
7
7
2
2
0
</p>
<p>0
:7
7
7
9
6
6
</p>
<p>0
:7
7
8
7
0
7
</p>
<p>0
:7
7
9
4
4
3
</p>
<p>0
:7
8
0
1
7
5
</p>
<p>0
:7
8
0
9
0
2
</p>
<p>2
:8
</p>
<p>0
:7
8
1
6
2
5
</p>
<p>0
:7
8
2
3
4
2
</p>
<p>0
:7
8
3
0
5
6
</p>
<p>0
:7
8
3
7
6
5
</p>
<p>0
:7
8
4
4
6
9
</p>
<p>0
:7
8
5
1
6
9
</p>
<p>0
:7
8
5
8
6
5
</p>
<p>0
:7
8
6
5
5
6
</p>
<p>0
:7
8
7
2
4
3
</p>
<p>0
:7
8
7
9
2
6
</p>
<p>2
:9
</p>
<p>0
:7
8
8
6
0
5
</p>
<p>0
:7
8
9
2
7
9
</p>
<p>0
:7
8
9
9
4
9
</p>
<p>0
:7
9
0
6
1
6
</p>
<p>0
:7
9
1
2
7
8
</p>
<p>0
:7
9
1
9
3
6
</p>
<p>0
:7
9
2
5
9
0
</p>
<p>0
:7
9
3
2
4
0
</p>
<p>0
:7
9
3
8
8
7
</p>
<p>0
:7
9
4
5
2
9
</p>
<p>3
:0
</p>
<p>0
:7
9
5
1
6
8
</p>
<p>0
:7
9
5
8
0
2
</p>
<p>0
:7
9
6
4
3
3
</p>
<p>0
:7
9
7
0
6
0
</p>
<p>0
:7
9
7
6
8
4
</p>
<p>0
:7
9
8
3
0
4
</p>
<p>0
:7
9
8
9
2
0
</p>
<p>0
:7
9
9
5
3
2
</p>
<p>0
:8
0
0
1
4
1
</p>
<p>0
:8
0
0
7
4
6
</p>
<p>3
:1
</p>
<p>0
:8
0
1
3
4
8
</p>
<p>0
:8
0
1
9
4
6
</p>
<p>0
:8
0
2
5
4
1
</p>
<p>0
:8
0
3
1
3
3
</p>
<p>0
:8
0
3
7
2
0
</p>
<p>0
:8
0
4
3
0
5
</p>
<p>0
:8
0
4
8
8
6
</p>
<p>0
:8
0
5
4
6
4
</p>
<p>0
:8
0
6
0
3
9
</p>
<p>0
:8
0
6
6
1
0
</p>
<p>3
:2
</p>
<p>0
:8
0
7
1
7
8
</p>
<p>0
:8
0
7
7
4
3
</p>
<p>0
:8
0
8
3
0
4
</p>
<p>0
:8
0
8
8
6
3
</p>
<p>0
:8
0
9
4
1
8
</p>
<p>0
:8
0
9
9
7
0
</p>
<p>0
:8
1
0
5
1
9
</p>
<p>0
:8
1
1
0
6
5
</p>
<p>0
:8
1
1
6
0
8
</p>
<p>0
:8
1
2
1
4
8
</p>
<p>3
:3
</p>
<p>0
:8
1
2
6
8
5
</p>
<p>0
:8
1
3
2
1
9
</p>
<p>0
:8
1
3
7
5
0
</p>
<p>0
:8
1
4
2
7
8
</p>
<p>0
:8
1
4
8
0
3
</p>
<p>0
:8
1
5
3
2
5
</p>
<p>0
:8
1
5
8
4
5
</p>
<p>0
:8
1
6
3
6
1
</p>
<p>0
:8
1
6
8
7
5
</p>
<p>0
:8
1
7
3
8
6
</p>
<p>3
:4
</p>
<p>0
:8
1
7
8
9
4
</p>
<p>0
:8
1
8
4
0
0
</p>
<p>0
:8
1
8
9
0
3
</p>
<p>0
:8
1
9
4
0
3
</p>
<p>0
:8
1
9
9
0
0
</p>
<p>0
:8
2
0
3
9
5
</p>
<p>0
:8
2
0
8
8
7
</p>
<p>0
:8
2
1
3
7
6
</p>
<p>0
:8
2
1
8
6
3
</p>
<p>0
:8
2
2
3
4
8
</p>
<p>3
:5
</p>
<p>0
:8
2
2
8
2
9
</p>
<p>0
:8
2
3
3
0
8
</p>
<p>0
:8
2
3
7
8
5
</p>
<p>0
:8
2
4
2
5
9
</p>
<p>0
:8
2
4
7
3
1
</p>
<p>0
:8
2
5
2
0
0
</p>
<p>0
:8
2
5
6
6
7
</p>
<p>0
:8
2
6
1
3
1
</p>
<p>0
:8
2
6
5
9
3
</p>
<p>0
:8
2
7
0
5
3
</p>
<p>3
:6
</p>
<p>0
:8
2
7
5
1
0
</p>
<p>0
:8
2
7
9
6
5
</p>
<p>0
:8
2
8
4
1
8
</p>
<p>0
:8
2
8
8
6
8
</p>
<p>0
:8
2
9
3
1
6
</p>
<p>0
:8
2
9
7
6
1
</p>
<p>0
:8
3
0
2
0
5
</p>
<p>0
:8
3
0
6
4
6
</p>
<p>0
:8
3
1
0
8
5
</p>
<p>0
:8
3
1
5
2
1
</p>
<p>3
:7
</p>
<p>0
:8
3
1
9
5
6
</p>
<p>0
:8
3
2
3
8
8
</p>
<p>0
:8
3
2
8
1
8
</p>
<p>0
:8
3
3
2
4
6
</p>
<p>0
:8
3
3
6
7
2
</p>
<p>0
:8
3
4
0
9
6
</p>
<p>0
:8
3
4
5
1
7
</p>
<p>0
:8
3
4
9
3
7
</p>
<p>0
:8
3
5
3
5
4
</p>
<p>0
:8
3
5
7
7
0
</p>
<p>3
:8
</p>
<p>0
:8
3
6
1
8
3
</p>
<p>0
:8
3
6
5
9
4
</p>
<p>0
:8
3
7
0
0
4
</p>
<p>0
:8
3
7
4
1
1
</p>
<p>0
:8
3
7
8
1
6
</p>
<p>0
:8
3
8
2
2
0
</p>
<p>0
:8
3
8
6
2
1
</p>
<p>0
:8
3
9
0
2
0
</p>
<p>0
:8
3
9
4
1
8
</p>
<p>0
:8
3
9
8
1
3
</p>
<p>3
:9
</p>
<p>0
:8
4
0
2
0
7
</p>
<p>0
:8
4
0
5
9
9
</p>
<p>0
:8
4
0
9
8
9
</p>
<p>0
:8
4
1
3
7
7
</p>
<p>0
:8
4
1
7
6
3
</p>
<p>0
:8
4
2
1
4
7
</p>
<p>0
:8
4
2
5
3
0
</p>
<p>0
:8
4
2
9
1
1
</p>
<p>0
:8
4
3
2
9
0
</p>
<p>0
:8
4
3
6
6
7
</p>
<p>4
:0
</p>
<p>0
:8
4
4
0
4
2
</p>
<p>0
:8
4
4
4
1
6
</p>
<p>0
:8
4
4
7
8
8
</p>
<p>0
:8
4
5
1
5
8
</p>
<p>0
:8
4
5
5
2
6
</p>
<p>0
:8
4
5
8
9
3
</p>
<p>0
:8
4
6
2
5
8
</p>
<p>0
:8
4
6
6
2
1
</p>
<p>0
:8
4
6
9
8
3
</p>
<p>0
:8
4
7
3
4
3
</p>
<p>4
:1
</p>
<p>0
:8
4
7
7
0
1
</p>
<p>0
:8
4
8
0
5
7
</p>
<p>0
:8
4
8
4
1
2
</p>
<p>0
:8
4
8
7
6
6
</p>
<p>0
:8
4
9
1
1
8
</p>
<p>0
:8
4
9
4
6
8
</p>
<p>0
:8
4
9
8
1
6
</p>
<p>0
:8
5
0
1
6
3
</p>
<p>0
:8
5
0
5
0
9
</p>
<p>0
:8
5
0
8
5
3
</p>
<p>4
:2
</p>
<p>0
:8
5
1
1
9
5
</p>
<p>0
:8
5
1
5
3
6
</p>
<p>0
:8
5
1
8
7
5
</p>
<p>0
:8
5
2
2
1
3
</p>
<p>0
:8
5
2
5
4
9
</p>
<p>0
:8
5
2
8
8
3
</p>
<p>0
:8
5
3
2
1
7
</p>
<p>0
:8
5
3
5
4
8
</p>
<p>0
:8
5
3
8
7
9
</p>
<p>0
:8
5
4
2
0
8
</p>
<p>4
:3
</p>
<p>0
:8
5
4
5
3
5
</p>
<p>0
:8
5
4
8
6
1
</p>
<p>0
:8
5
5
1
8
5
</p>
<p>0
:8
5
5
5
0
8
</p>
<p>0
:8
5
5
8
3
0
</p>
<p>0
:8
5
6
1
5
0
</p>
<p>0
:8
5
6
4
6
9
</p>
<p>0
:8
5
6
7
8
7
</p>
<p>0
:8
5
7
1
0
3
</p>
<p>0
:8
5
7
4
1
7
</p>
<p>4
:4
</p>
<p>0
:8
5
7
7
3
1
</p>
<p>0
:8
5
8
0
4
3
</p>
<p>0
:8
5
8
3
5
3
</p>
<p>0
:8
5
8
6
6
3
</p>
<p>0
:8
5
8
9
7
1
</p>
<p>0
:8
5
9
2
7
7
</p>
<p>0
:8
5
9
5
8
3
</p>
<p>0
:8
5
9
8
8
7
</p>
<p>0
:8
6
0
1
9
0
</p>
<p>0
:8
6
0
4
9
1
</p>
<p>4
:5
</p>
<p>0
:8
6
0
7
9
1
</p>
<p>0
:8
6
1
0
9
0
</p>
<p>0
:8
6
1
3
8
8
</p>
<p>0
:8
6
1
6
8
4
</p>
<p>0
:8
6
1
9
8
0
</p>
<p>0
:8
6
2
2
7
4
</p>
<p>0
:8
6
2
5
6
6
</p>
<p>0
:8
6
2
8
5
8
</p>
<p>0
:8
6
3
1
4
8
</p>
<p>0
:8
6
3
4
3
7
</p>
<p>4
:6
</p>
<p>0
:8
6
3
7
2
5
</p>
<p>0
:8
6
4
0
1
2
</p>
<p>0
:8
6
4
2
9
7
</p>
<p>0
:8
6
4
5
8
2
</p>
<p>0
:8
6
4
8
6
5
</p>
<p>0
:8
6
5
1
4
7
</p>
<p>0
:8
6
5
4
2
8
</p>
<p>0
:8
6
5
7
0
7
</p>
<p>0
:8
6
5
9
8
6
</p>
<p>0
:8
6
6
2
6
3
</p>
<p>4
:7
</p>
<p>0
:8
6
6
5
3
9
</p>
<p>0
:8
6
6
8
1
5
</p>
<p>0
:8
6
7
0
8
9
</p>
<p>0
:8
6
7
3
6
2
</p>
<p>0
:8
6
7
6
3
3
</p>
<p>0
:8
6
7
9
0
4
</p>
<p>0
:8
6
8
1
7
4
</p>
<p>0
:8
6
8
4
4
2
</p>
<p>0
:8
6
8
7
1
0
</p>
<p>0
:8
6
8
9
7
6
</p>
<p>4
:8
</p>
<p>0
:8
6
9
2
4
2
</p>
<p>0
:8
6
9
5
0
6
</p>
<p>0
:8
6
9
7
6
9
</p>
<p>0
:8
7
0
0
3
1
</p>
<p>0
:8
7
0
2
9
2
</p>
<p>0
:8
7
0
5
5
3
</p>
<p>0
:8
7
0
8
1
2
</p>
<p>0
:8
7
1
0
7
0
</p>
<p>0
:8
7
1
3
2
7
</p>
<p>0
:8
7
1
5
8
3
</p>
<p>4
:9
</p>
<p>0
:8
7
1
8
3
8
</p>
<p>0
:8
7
2
0
9
2
</p>
<p>0
:8
7
2
3
4
5
</p>
<p>0
:8
7
2
5
9
7
</p>
<p>0
:8
7
2
8
4
8
</p>
<p>0
:8
7
3
0
9
8
</p>
<p>0
:8
7
3
3
4
7
</p>
<p>0
:8
7
3
5
9
6
</p>
<p>0
:8
7
3
8
4
3
</p>
<p>0
:8
7
4
0
8
9</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 295
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
7
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>=
2)
</p>
<p>,o
r
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
0
7
1
</p>
<p>0
:0
1
4
1
4
1
</p>
<p>0
:0
2
1
2
0
8
</p>
<p>0
:0
2
8
2
7
3
</p>
<p>0
:0
3
5
3
3
3
</p>
<p>0
:0
4
2
3
8
8
</p>
<p>0
:0
4
9
4
3
7
</p>
<p>0
:0
5
6
4
7
8
</p>
<p>0
:0
6
3
5
1
1
</p>
<p>0
:1
</p>
<p>0
:0
7
0
5
3
5
</p>
<p>0
:0
7
7
5
4
8
</p>
<p>0
:0
8
4
5
4
9
</p>
<p>0
:0
9
1
5
3
8
</p>
<p>0
:0
9
8
5
1
3
</p>
<p>0
:1
0
5
4
7
4
</p>
<p>0
:1
1
2
4
2
0
</p>
<p>0
:1
1
9
3
4
9
</p>
<p>0
:1
2
6
2
6
1
</p>
<p>0
:1
3
3
1
5
4
</p>
<p>0
:2
</p>
<p>0
:1
4
0
0
2
8
</p>
<p>0
:1
4
6
8
8
2
</p>
<p>0
:1
5
3
7
1
5
</p>
<p>0
:1
6
0
5
2
6
</p>
<p>0
:1
6
7
3
1
3
</p>
<p>0
:1
7
4
0
7
8
</p>
<p>0
:1
8
0
8
1
7
</p>
<p>0
:1
8
7
5
3
2
</p>
<p>0
:1
9
4
2
2
0
</p>
<p>0
:2
0
0
8
8
1
</p>
<p>0
:3
</p>
<p>0
:2
0
7
5
1
4
</p>
<p>0
:2
1
4
1
1
9
</p>
<p>0
:2
2
0
6
9
5
</p>
<p>0
:2
2
7
2
4
1
</p>
<p>0
:2
3
3
7
5
6
</p>
<p>0
:2
4
0
2
3
9
</p>
<p>0
:2
4
6
6
9
1
</p>
<p>0
:2
5
3
1
1
0
</p>
<p>0
:2
5
9
4
9
6
</p>
<p>0
:2
6
5
8
4
8
</p>
<p>0
:4
</p>
<p>0
:2
7
2
1
6
6
</p>
<p>0
:2
7
8
4
4
8
</p>
<p>0
:2
8
4
6
9
5
</p>
<p>0
:2
9
0
9
0
6
</p>
<p>0
:2
9
7
0
8
0
</p>
<p>0
:3
0
3
2
1
8
</p>
<p>0
:3
0
9
3
1
8
</p>
<p>0
:3
1
5
3
8
0
</p>
<p>0
:3
2
1
4
0
3
</p>
<p>0
:3
2
7
3
8
8
</p>
<p>0
:5
</p>
<p>0
:3
3
3
3
3
3
</p>
<p>0
:3
3
9
2
4
0
</p>
<p>0
:3
4
5
1
0
6
</p>
<p>0
:3
5
0
9
3
2
</p>
<p>0
:3
5
6
7
1
8
</p>
<p>0
:3
6
2
4
6
2
</p>
<p>0
:3
6
8
1
6
6
</p>
<p>0
:3
7
3
8
2
9
</p>
<p>0
:3
7
9
4
5
0
</p>
<p>0
:3
8
5
0
2
9
</p>
<p>0
:6
</p>
<p>0
:3
9
0
5
6
7
</p>
<p>0
:3
9
6
0
6
2
</p>
<p>0
:4
0
1
5
1
6
</p>
<p>0
:4
0
6
9
2
6
</p>
<p>0
:4
1
2
2
9
5
</p>
<p>0
:4
1
7
6
2
0
</p>
<p>0
:4
2
2
9
0
3
</p>
<p>0
:4
2
8
1
4
4
</p>
<p>0
:4
3
3
3
4
1
</p>
<p>0
:4
3
8
4
9
6
</p>
<p>0
:7
</p>
<p>0
:4
4
3
6
0
7
</p>
<p>0
:4
4
8
6
7
6
</p>
<p>0
:4
5
3
7
0
2
</p>
<p>0
:4
5
8
6
8
4
</p>
<p>0
:4
6
3
6
2
4
</p>
<p>0
:4
6
8
5
2
1
</p>
<p>0
:4
7
3
3
7
6
</p>
<p>0
:4
7
8
1
8
7
</p>
<p>0
:4
8
2
9
5
6
</p>
<p>0
:4
8
7
6
8
2
</p>
<p>0
:8
</p>
<p>0
:4
9
2
3
6
6
</p>
<p>0
:4
9
7
0
0
8
</p>
<p>0
:5
0
1
6
0
7
</p>
<p>0
:5
0
6
1
6
4
</p>
<p>0
:5
1
0
6
7
9
</p>
<p>0
:5
1
5
1
5
2
</p>
<p>0
:5
1
9
5
8
3
</p>
<p>0
:5
2
3
9
7
3
</p>
<p>0
:5
2
8
3
2
2
</p>
<p>0
:5
3
2
6
2
9
</p>
<p>0
:9
</p>
<p>0
:5
3
6
8
9
5
</p>
<p>0
:5
4
1
1
2
1
</p>
<p>0
:5
4
5
3
0
6
</p>
<p>0
:5
4
9
4
5
0
</p>
<p>0
:5
5
3
5
5
5
</p>
<p>0
:5
5
7
6
1
9
</p>
<p>0
:5
6
1
6
4
4
</p>
<p>0
:5
6
5
6
2
9
</p>
<p>0
:5
6
9
5
7
5
</p>
<p>0
:5
7
3
4
8
2
</p>
<p>1
:0
</p>
<p>0
:5
7
7
3
5
1
</p>
<p>0
:5
8
1
1
8
0
</p>
<p>0
:5
8
4
9
7
2
</p>
<p>0
:5
8
8
7
2
5
</p>
<p>0
:5
9
2
4
4
1
</p>
<p>0
:5
9
6
1
2
0
</p>
<p>0
:5
9
9
7
6
1
</p>
<p>0
:6
0
3
3
6
6
</p>
<p>0
:6
0
6
9
3
3
</p>
<p>0
:6
1
0
4
6
5
</p>
<p>1
:1
</p>
<p>0
:6
1
3
9
6
0
</p>
<p>0
:6
1
7
4
2
0
</p>
<p>0
:6
2
0
8
4
4
</p>
<p>0
:6
2
4
2
3
3
</p>
<p>0
:6
2
7
5
8
8
</p>
<p>0
:6
3
0
9
0
7
</p>
<p>0
:6
3
4
1
9
3
</p>
<p>0
:6
3
7
4
4
4
</p>
<p>0
:6
4
0
6
6
2
</p>
<p>0
:6
4
3
8
4
6
</p>
<p>1
:2
</p>
<p>0
:6
4
6
9
9
7
</p>
<p>0
:6
5
0
1
1
5
</p>
<p>0
:6
5
3
2
0
1
</p>
<p>0
:6
5
6
2
5
5
</p>
<p>0
:6
5
9
2
7
6
</p>
<p>0
:6
6
2
2
6
6
</p>
<p>0
:6
6
5
2
2
5
</p>
<p>0
:6
6
8
1
5
3
</p>
<p>0
:6
7
1
0
5
0
</p>
<p>0
:6
7
3
9
1
7
</p>
<p>1
:3
</p>
<p>0
:6
7
6
7
5
3
</p>
<p>0
:6
7
9
5
6
0
</p>
<p>0
:6
8
2
3
3
7
</p>
<p>0
:6
8
5
0
8
5
</p>
<p>0
:6
8
7
8
0
4
</p>
<p>0
:6
9
0
4
9
4
</p>
<p>0
:6
9
3
1
5
6
</p>
<p>0
:6
9
5
7
9
0
</p>
<p>0
:6
9
8
3
9
7
</p>
<p>0
:7
0
0
9
7
5
</p>
<p>1
:4
</p>
<p>0
:7
0
3
5
2
7
</p>
<p>0
:7
0
6
0
5
1
</p>
<p>0
:7
0
8
5
4
9
</p>
<p>0
:7
1
1
0
2
1
</p>
<p>0
:7
1
3
4
6
6
</p>
<p>0
:7
1
5
8
8
6
</p>
<p>0
:7
1
8
2
8
0
</p>
<p>0
:7
2
0
6
4
9
</p>
<p>0
:7
2
2
9
9
3
</p>
<p>0
:7
2
5
3
1
2
</p>
<p>1
:5
</p>
<p>0
:7
2
7
6
0
7
</p>
<p>0
:7
2
9
8
7
8
</p>
<p>0
:7
3
2
1
2
5
</p>
<p>0
:7
3
4
3
4
8
</p>
<p>0
:7
3
6
5
4
7
</p>
<p>0
:7
3
8
7
2
4
</p>
<p>0
:7
4
0
8
7
8
</p>
<p>0
:7
4
3
0
0
9
</p>
<p>0
:7
4
5
1
1
8
</p>
<p>0
:7
4
7
2
0
4
</p>
<p>1
:6
</p>
<p>0
:7
4
9
2
6
9
</p>
<p>0
:7
5
1
3
1
2
</p>
<p>0
:7
5
3
3
3
4
</p>
<p>0
:7
5
5
3
3
5
</p>
<p>0
:7
5
7
3
1
4
</p>
<p>0
:7
5
9
2
7
3
</p>
<p>0
:7
6
1
2
1
2
</p>
<p>0
:7
6
3
1
3
0
</p>
<p>0
:7
6
5
0
2
9
</p>
<p>0
:7
6
6
9
0
8
</p>
<p>1
:7
</p>
<p>0
:7
6
8
7
6
7
</p>
<p>0
:7
7
0
6
0
7
</p>
<p>0
:7
7
2
4
2
8
</p>
<p>0
:7
7
4
2
3
0
</p>
<p>0
:7
7
6
0
1
3
</p>
<p>0
:7
7
7
7
7
8
</p>
<p>0
:7
7
9
5
2
5
</p>
<p>0
:7
8
1
2
5
4
</p>
<p>0
:7
8
2
9
6
5
</p>
<p>0
:7
8
4
6
5
8
</p>
<p>1
:8
</p>
<p>0
:7
8
6
3
3
4
</p>
<p>0
:7
8
7
9
9
3
</p>
<p>0
:7
8
9
6
3
5
</p>
<p>0
:7
9
1
2
6
0
</p>
<p>0
:7
9
2
8
6
8
</p>
<p>0
:7
9
4
4
6
0
</p>
<p>0
:7
9
6
0
3
6
</p>
<p>0
:7
9
7
5
9
6
</p>
<p>0
:7
9
9
1
4
0
</p>
<p>0
:8
0
0
6
6
8
</p>
<p>1
:9
</p>
<p>0
:8
0
2
1
8
1
</p>
<p>0
:8
0
3
6
7
9
</p>
<p>0
:8
0
5
1
6
1
</p>
<p>0
:8
0
6
6
2
8
</p>
<p>0
:8
0
8
0
8
1
</p>
<p>0
:8
0
9
5
1
9
</p>
<p>0
:8
1
0
9
4
3
</p>
<p>0
:8
1
2
3
5
2
</p>
<p>0
:8
1
3
7
4
8
</p>
<p>0
:8
1
5
1
2
9
</p>
<p>2
:0
</p>
<p>0
:8
1
6
4
9
7
</p>
<p>0
:8
1
7
8
5
1
</p>
<p>0
:8
1
9
1
9
2
</p>
<p>0
:8
2
0
5
1
9
</p>
<p>0
:8
2
1
8
3
3
</p>
<p>0
:8
2
3
1
3
4
</p>
<p>0
:8
2
4
4
2
3
</p>
<p>0
:8
2
5
6
9
8
</p>
<p>0
:8
2
6
9
6
1
</p>
<p>0
:8
2
8
2
1
2
</p>
<p>2
:1
</p>
<p>0
:8
2
9
4
5
0
</p>
<p>0
:8
3
0
6
7
7
</p>
<p>0
:8
3
1
8
9
1
</p>
<p>0
:8
3
3
0
9
4
</p>
<p>0
:8
3
4
2
8
5
</p>
<p>0
:8
3
5
4
6
4
</p>
<p>0
:8
3
6
6
3
2
</p>
<p>0
:8
3
7
7
8
8
</p>
<p>0
:8
3
8
9
3
4
</p>
<p>0
:8
4
0
0
6
8
</p>
<p>2
:2
</p>
<p>0
:8
4
1
1
9
1
</p>
<p>0
:8
4
2
3
0
4
</p>
<p>0
:8
4
3
4
0
6
</p>
<p>0
:8
4
4
4
9
7
</p>
<p>0
:8
4
5
5
7
8
</p>
<p>0
:8
4
6
6
4
9
</p>
<p>0
:8
4
7
7
1
0
</p>
<p>0
:8
4
8
7
6
0
</p>
<p>0
:8
4
9
8
0
1
</p>
<p>0
:8
5
0
8
3
1
</p>
<p>2
:3
</p>
<p>0
:8
5
1
8
5
2
</p>
<p>0
:8
5
2
8
6
4
</p>
<p>0
:8
5
3
8
6
5
</p>
<p>0
:8
5
4
8
5
8
</p>
<p>0
:8
5
5
8
4
1
</p>
<p>0
:8
5
6
8
1
5
</p>
<p>0
:8
5
7
7
8
0
</p>
<p>0
:8
5
8
7
3
5
</p>
<p>0
:8
5
9
6
8
2
</p>
<p>0
:8
6
0
6
2
1
</p>
<p>2
:4
</p>
<p>0
:8
6
1
5
5
0
</p>
<p>0
:8
6
2
4
7
1
</p>
<p>0
:8
6
3
3
8
4
</p>
<p>0
:8
6
4
2
8
8
</p>
<p>0
:8
6
5
1
8
3
</p>
<p>0
:8
6
6
0
7
1
</p>
<p>0
:8
6
6
9
5
0
</p>
<p>0
:8
6
7
8
2
2
</p>
<p>0
:8
6
8
6
8
5
</p>
<p>0
:8
6
9
5
4
1
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
7
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:8
7
0
3
8
9
</p>
<p>0
:8
7
1
2
2
9
</p>
<p>0
:8
7
2
0
6
1
</p>
<p>0
:8
7
2
8
8
7
</p>
<p>0
:8
7
3
7
0
4
</p>
<p>0
:8
7
4
5
1
5
</p>
<p>0
:8
7
5
3
1
8
</p>
<p>0
:8
7
6
1
1
4
</p>
<p>0
:8
7
6
9
0
2
</p>
<p>0
:8
7
7
6
8
4
</p>
<p>2
:6
</p>
<p>0
:8
7
8
4
5
9
</p>
<p>0
:8
7
9
2
2
7
</p>
<p>0
:8
7
9
9
8
8
</p>
<p>0
:8
8
0
7
4
3
</p>
<p>0
:8
8
1
4
9
0
</p>
<p>0
:8
8
2
2
3
2
</p>
<p>0
:8
8
2
9
6
6
</p>
<p>0
:8
8
3
6
9
5
</p>
<p>0
:8
8
4
4
1
7
</p>
<p>0
:8
8
5
1
3
2
</p>
<p>2
:7
</p>
<p>0
:8
8
5
8
4
2
</p>
<p>0
:8
8
6
5
4
5
</p>
<p>0
:8
8
7
2
4
2
</p>
<p>0
:8
8
7
9
3
3
</p>
<p>0
:8
8
8
6
1
8
</p>
<p>0
:8
8
9
2
9
8
</p>
<p>0
:8
8
9
9
7
1
</p>
<p>0
:8
9
0
6
3
9
</p>
<p>0
:8
9
1
3
0
1
</p>
<p>0
:8
9
1
9
5
7
</p>
<p>2
:8
</p>
<p>0
:8
9
2
6
0
8
</p>
<p>0
:8
9
3
2
5
3
</p>
<p>0
:8
9
3
8
9
3
</p>
<p>0
:8
9
4
5
2
7
</p>
<p>0
:8
9
5
1
5
6
</p>
<p>0
:8
9
5
7
8
0
</p>
<p>0
:8
9
6
3
9
8
</p>
<p>0
:8
9
7
0
1
1
</p>
<p>0
:8
9
7
6
1
9
</p>
<p>0
:8
9
8
2
2
2
</p>
<p>2
:9
</p>
<p>0
:8
9
8
8
2
0
</p>
<p>0
:8
9
9
4
1
3
</p>
<p>0
:9
0
0
0
0
1
</p>
<p>0
:9
0
0
5
8
4
</p>
<p>0
:9
0
1
1
6
3
</p>
<p>0
:9
0
1
7
3
6
</p>
<p>0
:9
0
2
3
0
5
</p>
<p>0
:9
0
2
8
6
9
</p>
<p>0
:9
0
3
4
2
9
</p>
<p>0
:9
0
3
9
8
4
</p>
<p>3
:0
</p>
<p>0
:9
0
4
5
3
4
</p>
<p>0
:9
0
5
0
8
0
</p>
<p>0
:9
0
5
6
2
2
</p>
<p>0
:9
0
6
1
5
9
</p>
<p>0
:9
0
6
6
9
2
</p>
<p>0
:9
0
7
2
2
0
</p>
<p>0
:9
0
7
7
4
5
</p>
<p>0
:9
0
8
2
6
5
</p>
<p>0
:9
0
8
7
8
0
</p>
<p>0
:9
0
9
2
9
2
</p>
<p>3
:1
</p>
<p>0
:9
0
9
8
0
0
</p>
<p>0
:9
1
0
3
0
3
</p>
<p>0
:9
1
0
8
0
3
</p>
<p>0
:9
1
1
2
9
8
</p>
<p>0
:9
1
1
7
9
0
</p>
<p>0
:9
1
2
2
7
8
</p>
<p>0
:9
1
2
7
6
2
</p>
<p>0
:9
1
3
2
4
2
</p>
<p>0
:9
1
3
7
1
8
</p>
<p>0
:9
1
4
1
9
1
</p>
<p>3
:2
</p>
<p>0
:9
1
4
6
6
0
</p>
<p>0
:9
1
5
1
2
5
</p>
<p>0
:9
1
5
5
8
6
</p>
<p>0
:9
1
6
0
4
4
</p>
<p>0
:9
1
6
4
9
9
</p>
<p>0
:9
1
6
9
5
0
</p>
<p>0
:9
1
7
3
9
7
</p>
<p>0
:9
1
7
8
4
1
</p>
<p>0
:9
1
8
2
8
2
</p>
<p>0
:9
1
8
7
1
9
</p>
<p>3
:3
</p>
<p>0
:9
1
9
1
5
3
</p>
<p>0
:9
1
9
5
8
3
</p>
<p>0
:9
2
0
0
1
0
</p>
<p>0
:9
2
0
4
3
4
</p>
<p>0
:9
2
0
8
5
5
</p>
<p>0
:9
2
1
2
7
3
</p>
<p>0
:9
2
1
6
8
7
</p>
<p>0
:9
2
2
0
9
8
</p>
<p>0
:9
2
2
5
0
7
</p>
<p>0
:9
2
2
9
1
2
</p>
<p>3
:4
</p>
<p>0
:9
2
3
3
1
4
</p>
<p>0
:9
2
3
7
1
3
</p>
<p>0
:9
2
4
1
0
9
</p>
<p>0
:9
2
4
5
0
2
</p>
<p>0
:9
2
4
8
9
2
</p>
<p>0
:9
2
5
2
7
9
</p>
<p>0
:9
2
5
6
6
4
</p>
<p>0
:9
2
6
0
4
5
</p>
<p>0
:9
2
6
4
2
4
</p>
<p>0
:9
2
6
8
0
0
</p>
<p>3
:5
</p>
<p>0
:9
2
7
1
7
3
</p>
<p>0
:9
2
7
5
4
3
</p>
<p>0
:9
2
7
9
1
1
</p>
<p>0
:9
2
8
2
7
6
</p>
<p>0
:9
2
8
6
3
9
</p>
<p>0
:9
2
8
9
9
8
</p>
<p>0
:9
2
9
3
5
5
</p>
<p>0
:9
2
9
7
1
0
</p>
<p>0
:9
3
0
0
6
2
</p>
<p>0
:9
3
0
4
1
1
</p>
<p>3
:6
</p>
<p>0
:9
3
0
7
5
8
</p>
<p>0
:9
3
1
1
0
3
</p>
<p>0
:9
3
1
4
4
5
</p>
<p>0
:9
3
1
7
8
4
</p>
<p>0
:9
3
2
1
2
1
</p>
<p>0
:9
3
2
4
5
6
</p>
<p>0
:9
3
2
7
8
8
</p>
<p>0
:9
3
3
1
1
8
</p>
<p>0
:9
3
3
4
4
5
</p>
<p>0
:9
3
3
7
7
1
</p>
<p>3
:7
</p>
<p>0
:9
3
4
0
9
4
</p>
<p>0
:9
3
4
4
1
4
</p>
<p>0
:9
3
4
7
3
3
</p>
<p>0
:9
3
5
0
4
9
</p>
<p>0
:9
3
5
3
6
3
</p>
<p>0
:9
3
5
6
7
5
</p>
<p>0
:9
3
5
9
8
4
</p>
<p>0
:9
3
6
2
9
2
</p>
<p>0
:9
3
6
5
9
7
</p>
<p>0
:9
3
6
9
0
0
</p>
<p>3
:8
</p>
<p>0
:9
3
7
2
0
1
</p>
<p>0
:9
3
7
5
0
0
</p>
<p>0
:9
3
7
7
9
7
</p>
<p>0
:9
3
8
0
9
2
</p>
<p>0
:9
3
8
3
8
5
</p>
<p>0
:9
3
8
6
7
6
</p>
<p>0
:9
3
8
9
6
5
</p>
<p>0
:9
3
9
2
5
2
</p>
<p>0
:9
3
9
5
3
7
</p>
<p>0
:9
3
9
8
2
0
</p>
<p>3
:9
</p>
<p>0
:9
4
0
1
0
1
</p>
<p>0
:9
4
0
3
8
0
</p>
<p>0
:9
4
0
6
5
7
</p>
<p>0
:9
4
0
9
3
3
</p>
<p>0
:9
4
1
2
0
6
</p>
<p>0
:9
4
1
4
7
8
</p>
<p>0
:9
4
1
7
4
8
</p>
<p>0
:9
4
2
0
1
6
</p>
<p>0
:9
4
2
2
8
2
</p>
<p>0
:9
4
2
5
4
7
</p>
<p>4
:0
</p>
<p>0
:9
4
2
8
0
9
</p>
<p>0
:9
4
3
0
7
0
</p>
<p>0
:9
4
3
3
3
0
</p>
<p>0
:9
4
3
5
8
7
</p>
<p>0
:9
4
3
8
4
3
</p>
<p>0
:9
4
4
0
9
7
</p>
<p>0
:9
4
4
3
5
0
</p>
<p>0
:9
4
4
6
0
1
</p>
<p>0
:9
4
4
8
5
0
</p>
<p>0
:9
4
5
0
9
8
</p>
<p>4
:1
</p>
<p>0
:9
4
5
3
4
3
</p>
<p>0
:9
4
5
5
8
8
</p>
<p>0
:9
4
5
8
3
1
</p>
<p>0
:9
4
6
0
7
2
</p>
<p>0
:9
4
6
3
1
1
</p>
<p>0
:9
4
6
5
5
0
</p>
<p>0
:9
4
6
7
8
6
</p>
<p>0
:9
4
7
0
2
1
</p>
<p>0
:9
4
7
2
5
5
</p>
<p>0
:9
4
7
4
8
7
</p>
<p>4
:2
</p>
<p>0
:9
4
7
7
1
7
</p>
<p>0
:9
4
7
9
4
6
</p>
<p>0
:9
4
8
1
7
4
</p>
<p>0
:9
4
8
4
0
0
</p>
<p>0
:9
4
8
6
2
5
</p>
<p>0
:9
4
8
8
4
8
</p>
<p>0
:9
4
9
0
7
0
</p>
<p>0
:9
4
9
2
9
0
</p>
<p>0
:9
4
9
5
0
9
</p>
<p>0
:9
4
9
7
2
7
</p>
<p>4
:3
</p>
<p>0
:9
4
9
9
4
3
</p>
<p>0
:9
5
0
1
5
8
</p>
<p>0
:9
5
0
3
7
2
</p>
<p>0
:9
5
0
5
8
4
</p>
<p>0
:9
5
0
7
9
5
</p>
<p>0
:9
5
1
0
0
5
</p>
<p>0
:9
5
1
2
1
3
</p>
<p>0
:9
5
1
4
2
0
</p>
<p>0
:9
5
1
6
2
6
</p>
<p>0
:9
5
1
8
3
0
</p>
<p>4
:4
</p>
<p>0
:9
5
2
0
3
4
</p>
<p>0
:9
5
2
2
3
5
</p>
<p>0
:9
5
2
4
3
6
</p>
<p>0
:9
5
2
6
3
6
</p>
<p>0
:9
5
2
8
3
4
</p>
<p>0
:9
5
3
0
3
1
</p>
<p>0
:9
5
3
2
2
7
</p>
<p>0
:9
5
3
4
2
2
</p>
<p>0
:9
5
3
6
1
5
</p>
<p>0
:9
5
3
8
0
7
</p>
<p>4
:5
</p>
<p>0
:9
5
3
9
9
8
</p>
<p>0
:9
5
4
1
8
8
</p>
<p>0
:9
5
4
3
7
7
</p>
<p>0
:9
5
4
5
6
5
</p>
<p>0
:9
5
4
7
5
2
</p>
<p>0
:9
5
4
9
3
7
</p>
<p>0
:9
5
5
1
2
1
</p>
<p>0
:9
5
5
3
0
5
</p>
<p>0
:9
5
5
4
8
7
</p>
<p>0
:9
5
5
6
6
8
</p>
<p>4
:6
</p>
<p>0
:9
5
5
8
4
8
</p>
<p>0
:9
5
6
0
2
7
</p>
<p>0
:9
5
6
2
0
5
</p>
<p>0
:9
5
6
3
8
1
</p>
<p>0
:9
5
6
5
5
7
</p>
<p>0
:9
5
6
7
3
2
</p>
<p>0
:9
5
6
9
0
5
</p>
<p>0
:9
5
7
0
7
8
</p>
<p>0
:9
5
7
2
5
0
</p>
<p>0
:9
5
7
4
2
0
</p>
<p>4
:7
</p>
<p>0
:9
5
7
5
9
0
</p>
<p>0
:9
5
7
7
5
9
</p>
<p>0
:9
5
7
9
2
6
</p>
<p>0
:9
5
8
0
9
3
</p>
<p>0
:9
5
8
2
5
9
</p>
<p>0
:9
5
8
4
2
4
</p>
<p>0
:9
5
8
5
8
7
</p>
<p>0
:9
5
8
7
5
0
</p>
<p>0
:9
5
8
9
1
2
</p>
<p>0
:9
5
9
0
7
3
</p>
<p>4
:8
</p>
<p>0
:9
5
9
2
3
3
</p>
<p>0
:9
5
9
3
9
2
</p>
<p>0
:9
5
9
5
5
1
</p>
<p>0
:9
5
9
7
0
8
</p>
<p>0
:9
5
9
8
6
5
</p>
<p>0
:9
6
0
0
2
0
</p>
<p>0
:9
6
0
1
7
5
</p>
<p>0
:9
6
0
3
2
9
</p>
<p>0
:9
6
0
4
8
1
</p>
<p>0
:9
6
0
6
3
4
</p>
<p>4
:9
</p>
<p>0
:9
6
0
7
8
5
</p>
<p>0
:9
6
0
9
3
5
</p>
<p>0
:9
6
1
0
8
5
</p>
<p>0
:9
6
1
2
3
3
</p>
<p>0
:9
6
1
3
8
1
</p>
<p>0
:9
6
1
5
2
8
</p>
<p>0
:9
6
1
6
7
4
</p>
<p>0
:9
6
1
8
2
0
</p>
<p>0
:9
6
1
9
6
4
</p>
<p>0
:9
6
2
1
0
8</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 297
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
8
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>=
3)
</p>
<p>,o
r
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
3
5
1
</p>
<p>0
:0
1
4
7
0
1
</p>
<p>0
:0
2
2
0
4
9
</p>
<p>0
:0
2
9
3
9
4
</p>
<p>0
:0
3
6
7
3
5
</p>
<p>0
:0
4
4
0
7
1
</p>
<p>0
:0
5
1
4
0
1
</p>
<p>0
:0
5
8
7
2
5
</p>
<p>0
:0
6
6
0
4
1
</p>
<p>0
:1
</p>
<p>0
:0
7
3
3
4
8
</p>
<p>0
:0
8
0
6
4
5
</p>
<p>0
:0
8
7
9
3
2
</p>
<p>0
:0
9
5
2
0
7
</p>
<p>0
:1
0
2
4
6
9
</p>
<p>0
:1
0
9
7
1
8
</p>
<p>0
:1
1
6
9
5
3
</p>
<p>0
:1
2
4
1
7
2
</p>
<p>0
:1
3
1
3
7
5
</p>
<p>0
:1
3
8
5
6
2
</p>
<p>0
:2
</p>
<p>0
:1
4
5
7
3
0
</p>
<p>0
:1
5
2
8
7
9
</p>
<p>0
:1
6
0
0
0
9
</p>
<p>0
:1
6
7
1
1
8
</p>
<p>0
:1
7
4
2
0
5
</p>
<p>0
:1
8
1
2
7
1
</p>
<p>0
:1
8
8
3
1
3
</p>
<p>0
:1
9
5
3
3
2
</p>
<p>0
:2
0
2
3
2
6
</p>
<p>0
:2
0
9
2
9
4
</p>
<p>0
:3
</p>
<p>0
:2
1
6
2
3
7
</p>
<p>0
:2
2
3
1
5
2
</p>
<p>0
:2
3
0
0
4
0
</p>
<p>0
:2
3
6
9
0
0
</p>
<p>0
:2
4
3
7
3
0
</p>
<p>0
:2
5
0
5
3
1
</p>
<p>0
:2
5
7
3
0
1
</p>
<p>0
:2
6
4
0
4
0
</p>
<p>0
:2
7
0
7
4
8
</p>
<p>0
:2
7
7
4
2
3
</p>
<p>0
:4
</p>
<p>0
:2
8
4
0
6
5
</p>
<p>0
:2
9
0
6
7
4
</p>
<p>0
:2
9
7
2
4
8
</p>
<p>0
:3
0
3
7
8
8
</p>
<p>0
:3
1
0
2
9
3
</p>
<p>0
:3
1
6
7
6
1
</p>
<p>0
:3
2
3
1
9
4
</p>
<p>0
:3
2
9
5
9
0
</p>
<p>0
:3
3
5
9
4
8
</p>
<p>0
:3
4
2
2
6
9
</p>
<p>0
:5
</p>
<p>0
:3
4
8
5
5
2
</p>
<p>0
:3
5
4
7
9
6
</p>
<p>0
:3
6
1
0
0
2
</p>
<p>0
:3
6
7
1
6
8
</p>
<p>0
:3
7
3
2
9
4
</p>
<p>0
:3
7
9
3
8
0
</p>
<p>0
:3
8
5
4
2
6
</p>
<p>0
:3
9
1
4
3
1
</p>
<p>0
:3
9
7
3
9
5
</p>
<p>0
:4
0
3
3
1
8
</p>
<p>0
:6
</p>
<p>0
:4
0
9
1
9
9
</p>
<p>0
:4
1
5
0
3
8
</p>
<p>0
:4
2
0
8
3
5
</p>
<p>0
:4
2
6
5
9
0
</p>
<p>0
:4
3
2
3
0
2
</p>
<p>0
:4
3
7
9
7
2
</p>
<p>0
:4
4
3
5
9
9
</p>
<p>0
:4
4
9
1
8
2
</p>
<p>0
:4
5
4
7
2
3
</p>
<p>0
:4
6
0
2
2
0
</p>
<p>0
:7
</p>
<p>0
:4
6
5
6
7
3
</p>
<p>0
:4
7
1
0
8
3
</p>
<p>0
:4
7
6
4
4
9
</p>
<p>0
:4
8
1
7
7
2
</p>
<p>0
:4
8
7
0
5
1
</p>
<p>0
:4
9
2
2
8
6
</p>
<p>0
:4
9
7
4
7
7
</p>
<p>0
:5
0
2
6
2
4
</p>
<p>0
:5
0
7
7
2
7
</p>
<p>0
:5
1
2
7
8
6
</p>
<p>0
:8
</p>
<p>0
:5
1
7
8
0
1
</p>
<p>0
:5
2
2
7
7
3
</p>
<p>0
:5
2
7
7
0
0
</p>
<p>0
:5
3
2
5
8
4
</p>
<p>0
:5
3
7
4
2
4
</p>
<p>0
:5
4
2
2
2
0
</p>
<p>0
:5
4
6
9
7
3
</p>
<p>0
:5
5
1
6
8
2
</p>
<p>0
:5
5
6
3
4
8
</p>
<p>0
:5
6
0
9
7
0
</p>
<p>0
:9
</p>
<p>0
:5
6
5
5
4
9
</p>
<p>0
:5
7
0
0
8
5
</p>
<p>0
:5
7
4
5
7
9
</p>
<p>0
:5
7
9
0
2
9
</p>
<p>0
:5
8
3
4
3
7
</p>
<p>0
:5
8
7
8
0
2
</p>
<p>0
:5
9
2
1
2
5
</p>
<p>0
:5
9
6
4
0
6
</p>
<p>0
:6
0
0
6
4
5
</p>
<p>0
:6
0
4
8
4
2
</p>
<p>1
:0
</p>
<p>0
:6
0
8
9
9
8
</p>
<p>0
:6
1
3
1
1
2
</p>
<p>0
:6
1
7
1
8
6
</p>
<p>0
:6
2
1
2
1
8
</p>
<p>0
:6
2
5
2
0
9
</p>
<p>0
:6
2
9
1
6
0
</p>
<p>0
:6
3
3
0
7
1
</p>
<p>0
:6
3
6
9
4
2
</p>
<p>0
:6
4
0
7
7
3
</p>
<p>0
:6
4
4
5
6
5
</p>
<p>1
:1
</p>
<p>0
:6
4
8
3
1
7
</p>
<p>0
:6
5
2
0
3
0
</p>
<p>0
:6
5
5
7
0
5
</p>
<p>0
:6
5
9
3
4
1
</p>
<p>0
:6
6
2
9
3
9
</p>
<p>0
:6
6
6
4
9
9
</p>
<p>0
:6
7
0
0
2
1
</p>
<p>0
:6
7
3
5
0
6
</p>
<p>0
:6
7
6
9
5
3
</p>
<p>0
:6
8
0
3
6
4
</p>
<p>1
:2
</p>
<p>0
:6
8
3
7
3
8
</p>
<p>0
:6
8
7
0
7
6
</p>
<p>0
:6
9
0
3
7
8
</p>
<p>0
:6
9
3
6
4
4
</p>
<p>0
:6
9
6
8
7
5
</p>
<p>0
:7
0
0
0
7
1
</p>
<p>0
:7
0
3
2
3
2
</p>
<p>0
:7
0
6
3
5
8
</p>
<p>0
:7
0
9
4
5
0
</p>
<p>0
:7
1
2
5
0
8
</p>
<p>1
:3
</p>
<p>0
:7
1
5
5
3
3
</p>
<p>0
:7
1
8
5
2
4
</p>
<p>0
:7
2
1
4
8
2
</p>
<p>0
:7
2
4
4
0
7
</p>
<p>0
:7
2
7
3
0
0
</p>
<p>0
:7
3
0
1
6
1
</p>
<p>0
:7
3
2
9
9
0
</p>
<p>0
:7
3
5
7
8
7
</p>
<p>0
:7
3
8
5
5
3
</p>
<p>0
:7
4
1
2
8
9
</p>
<p>1
:4
</p>
<p>0
:7
4
3
9
9
3
</p>
<p>0
:7
4
6
6
6
7
</p>
<p>0
:7
4
9
3
1
1
</p>
<p>0
:7
5
1
9
2
5
</p>
<p>0
:7
5
4
5
1
0
</p>
<p>0
:7
5
7
0
6
6
</p>
<p>0
:7
5
9
5
9
3
</p>
<p>0
:7
6
2
0
9
1
</p>
<p>0
:7
6
4
5
6
1
</p>
<p>0
:7
6
7
0
0
2
</p>
<p>1
:5
</p>
<p>0
:7
6
9
4
1
6
</p>
<p>0
:7
7
1
8
0
3
</p>
<p>0
:7
7
4
1
6
3
</p>
<p>0
:7
7
6
4
9
5
</p>
<p>0
:7
7
8
8
0
1
</p>
<p>0
:7
8
1
0
8
1
</p>
<p>0
:7
8
3
3
3
5
</p>
<p>0
:7
8
5
5
6
3
</p>
<p>0
:7
8
7
7
6
6
</p>
<p>0
:7
8
9
9
4
3
</p>
<p>1
:6
</p>
<p>0
:7
9
2
0
9
6
</p>
<p>0
:7
9
4
2
2
3
</p>
<p>0
:7
9
6
3
2
7
</p>
<p>0
:7
9
8
4
0
6
</p>
<p>0
:8
0
0
4
6
2
</p>
<p>0
:8
0
2
4
9
4
</p>
<p>0
:8
0
4
5
0
3
</p>
<p>0
:8
0
6
4
8
8
</p>
<p>0
:8
0
8
4
5
1
</p>
<p>0
:8
1
0
3
9
2
</p>
<p>1
:7
</p>
<p>0
:8
1
2
3
1
0
</p>
<p>0
:8
1
4
2
0
6
</p>
<p>0
:8
1
6
0
8
0
</p>
<p>0
:8
1
7
9
3
3
</p>
<p>0
:8
1
9
7
6
4
</p>
<p>0
:8
2
1
5
7
5
</p>
<p>0
:8
2
3
3
6
5
</p>
<p>0
:8
2
5
1
3
4
</p>
<p>0
:8
2
6
8
8
3
</p>
<p>0
:8
2
8
6
1
1
</p>
<p>1
:8
</p>
<p>0
:8
3
0
3
2
0
</p>
<p>0
:8
3
2
0
1
0
</p>
<p>0
:8
3
3
6
8
0
</p>
<p>0
:8
3
5
3
3
1
</p>
<p>0
:8
3
6
9
6
2
</p>
<p>0
:8
3
8
5
7
6
</p>
<p>0
:8
4
0
1
7
0
</p>
<p>0
:8
4
1
7
4
7
</p>
<p>0
:8
4
3
3
0
5
</p>
<p>0
:8
4
4
8
4
6
</p>
<p>1
:9
</p>
<p>0
:8
4
6
3
6
9
</p>
<p>0
:8
4
7
8
7
4
</p>
<p>0
:8
4
9
3
6
3
</p>
<p>0
:8
5
0
8
3
4
</p>
<p>0
:8
5
2
2
8
9
</p>
<p>0
:8
5
3
7
2
7
</p>
<p>0
:8
5
5
1
4
8
</p>
<p>0
:8
5
6
5
5
4
</p>
<p>0
:8
5
7
9
4
3
</p>
<p>0
:8
5
9
3
1
6
</p>
<p>2
:0
</p>
<p>0
:8
6
0
6
7
4
</p>
<p>0
:8
6
2
0
1
7
</p>
<p>0
:8
6
3
3
4
4
</p>
<p>0
:8
6
4
6
5
6
</p>
<p>0
:8
6
5
9
5
4
</p>
<p>0
:8
6
7
2
3
6
</p>
<p>0
:8
6
8
5
0
4
</p>
<p>0
:8
6
9
7
5
8
</p>
<p>0
:8
7
0
9
9
8
</p>
<p>0
:8
7
2
2
2
3
</p>
<p>2
:1
</p>
<p>0
:8
7
3
4
3
5
</p>
<p>0
:8
7
4
6
3
3
</p>
<p>0
:8
7
5
8
1
8
</p>
<p>0
:8
7
6
9
8
9
</p>
<p>0
:8
7
8
1
4
7
</p>
<p>0
:8
7
9
2
9
2
</p>
<p>0
:8
8
0
4
2
5
</p>
<p>0
:8
8
1
5
4
4
</p>
<p>0
:8
8
2
6
5
1
</p>
<p>0
:8
8
3
7
4
6
</p>
<p>2
:2
</p>
<p>0
:8
8
4
8
2
8
</p>
<p>0
:8
8
5
8
9
9
</p>
<p>0
:8
8
6
9
5
7
</p>
<p>0
:8
8
8
0
0
4
</p>
<p>0
:8
8
9
0
3
9
</p>
<p>0
:8
9
0
0
6
2
</p>
<p>0
:8
9
1
0
7
4
</p>
<p>0
:8
9
2
0
7
5
</p>
<p>0
:8
9
3
0
6
5
</p>
<p>0
:8
9
4
0
4
4
</p>
<p>2
:3
</p>
<p>0
:8
9
5
0
1
2
</p>
<p>0
:8
9
5
9
6
9
</p>
<p>0
:8
9
6
9
1
6
</p>
<p>0
:8
9
7
8
5
3
</p>
<p>0
:8
9
8
7
7
9
</p>
<p>0
:8
9
9
6
9
5
</p>
<p>0
:9
0
0
6
0
0
</p>
<p>0
:9
0
1
4
9
6
</p>
<p>0
:9
0
2
3
8
3
</p>
<p>0
:9
0
3
2
5
9
</p>
<p>2
:4
</p>
<p>0
:9
0
4
1
2
6
</p>
<p>0
:9
0
4
9
8
3
</p>
<p>0
:9
0
5
8
3
1
</p>
<p>0
:9
0
6
6
7
0
</p>
<p>0
:9
0
7
5
0
0
</p>
<p>0
:9
0
8
3
2
1
</p>
<p>0
:9
0
9
1
3
3
</p>
<p>0
:9
0
9
9
3
6
</p>
<p>0
:9
1
0
7
3
0
</p>
<p>0
:9
1
1
5
1
6
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>298 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
8
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
1
2
2
9
4
</p>
<p>0
:9
1
3
0
6
3
</p>
<p>0
:9
1
3
8
2
4
</p>
<p>0
:9
1
4
5
7
6
</p>
<p>0
:9
1
5
3
2
1
</p>
<p>0
:9
1
6
0
5
7
</p>
<p>0
:9
1
6
7
8
6
</p>
<p>0
:9
1
7
5
0
7
</p>
<p>0
:9
1
8
2
2
1
</p>
<p>0
:9
1
8
9
2
6
</p>
<p>2
:6
</p>
<p>0
:9
1
9
6
2
5
</p>
<p>0
:9
2
0
3
1
5
</p>
<p>0
:9
2
0
9
9
9
</p>
<p>0
:9
2
1
6
7
5
</p>
<p>0
:9
2
2
3
4
4
</p>
<p>0
:9
2
3
0
0
7
</p>
<p>0
:9
2
3
6
6
2
</p>
<p>0
:9
2
4
3
1
0
</p>
<p>0
:9
2
4
9
5
1
</p>
<p>0
:9
2
5
5
8
6
</p>
<p>2
:7
</p>
<p>0
:9
2
6
2
1
4
</p>
<p>0
:9
2
6
8
3
6
</p>
<p>0
:9
2
7
4
5
1
</p>
<p>0
:9
2
8
0
6
0
</p>
<p>0
:9
2
8
6
6
2
</p>
<p>0
:9
2
9
2
5
8
</p>
<p>0
:9
2
9
8
4
8
</p>
<p>0
:9
3
0
4
3
2
</p>
<p>0
:9
3
1
0
1
0
</p>
<p>0
:9
3
1
5
8
2
</p>
<p>2
:8
</p>
<p>0
:9
3
2
1
4
7
</p>
<p>0
:9
3
2
7
0
8
</p>
<p>0
:9
3
3
2
6
2
</p>
<p>0
:9
3
3
8
1
1
</p>
<p>0
:9
3
4
3
5
4
</p>
<p>0
:9
3
4
8
9
1
</p>
<p>0
:9
3
5
4
2
3
</p>
<p>0
:9
3
5
9
5
0
</p>
<p>0
:9
3
6
4
7
1
</p>
<p>0
:9
3
6
9
8
7
</p>
<p>2
:9
</p>
<p>0
:9
3
7
4
9
8
</p>
<p>0
:9
3
8
0
0
4
</p>
<p>0
:9
3
8
5
0
4
</p>
<p>0
:9
3
9
0
0
0
</p>
<p>0
:9
3
9
4
9
0
</p>
<p>0
:9
3
9
9
7
6
</p>
<p>0
:9
4
0
4
5
6
</p>
<p>0
:9
4
0
9
3
2
</p>
<p>0
:9
4
1
4
0
3
</p>
<p>0
:9
4
1
8
7
0
</p>
<p>3
:0
</p>
<p>0
:9
4
2
3
3
2
</p>
<p>0
:9
4
2
7
8
9
</p>
<p>0
:9
4
3
2
4
1
</p>
<p>0
:9
4
3
6
8
9
</p>
<p>0
:9
4
4
1
3
3
</p>
<p>0
:9
4
4
5
7
2
</p>
<p>0
:9
4
5
0
0
7
</p>
<p>0
:9
4
5
4
3
8
</p>
<p>0
:9
4
5
8
6
4
</p>
<p>0
:9
4
6
2
8
7
</p>
<p>3
:1
</p>
<p>0
:9
4
6
7
0
5
</p>
<p>0
:9
4
7
1
1
9
</p>
<p>0
:9
4
7
5
2
9
</p>
<p>0
:9
4
7
9
3
5
</p>
<p>0
:9
4
8
3
3
7
</p>
<p>0
:9
4
8
7
3
5
</p>
<p>0
:9
4
9
1
2
9
</p>
<p>0
:9
4
9
5
2
0
</p>
<p>0
:9
4
9
9
0
6
</p>
<p>0
:9
5
0
2
8
9
</p>
<p>3
:2
</p>
<p>0
:9
5
0
6
6
9
</p>
<p>0
:9
5
1
0
4
4
</p>
<p>0
:9
5
1
4
1
6
</p>
<p>0
:9
5
1
7
8
5
</p>
<p>0
:9
5
2
1
4
9
</p>
<p>0
:9
5
2
5
1
1
</p>
<p>0
:9
5
2
8
6
9
</p>
<p>0
:9
5
3
2
2
3
</p>
<p>0
:9
5
3
5
7
5
</p>
<p>0
:9
5
3
9
2
2
</p>
<p>3
:3
</p>
<p>0
:9
5
4
2
6
7
</p>
<p>0
:9
5
4
6
0
8
</p>
<p>0
:9
5
4
9
4
6
</p>
<p>0
:9
5
5
2
8
1
</p>
<p>0
:9
5
5
6
1
3
</p>
<p>0
:9
5
5
9
4
2
</p>
<p>0
:9
5
6
2
6
7
</p>
<p>0
:9
5
6
5
9
0
</p>
<p>0
:9
5
6
9
0
9
</p>
<p>0
:9
5
7
2
2
6
</p>
<p>3
:4
</p>
<p>0
:9
5
7
5
3
9
</p>
<p>0
:9
5
7
8
5
0
</p>
<p>0
:9
5
8
1
5
7
</p>
<p>0
:9
5
8
4
6
2
</p>
<p>0
:9
5
8
7
6
4
</p>
<p>0
:9
5
9
0
6
4
</p>
<p>0
:9
5
9
3
6
0
</p>
<p>0
:9
5
9
6
5
4
</p>
<p>0
:9
5
9
9
4
5
</p>
<p>0
:9
6
0
2
3
4
</p>
<p>3
:5
</p>
<p>0
:9
6
0
5
1
9
</p>
<p>0
:9
6
0
8
0
3
</p>
<p>0
:9
6
1
0
8
3
</p>
<p>0
:9
6
1
3
6
1
</p>
<p>0
:9
6
1
6
3
7
</p>
<p>0
:9
6
1
9
1
0
</p>
<p>0
:9
6
2
1
8
0
</p>
<p>0
:9
6
2
4
4
8
</p>
<p>0
:9
6
2
7
1
4
</p>
<p>0
:9
6
2
9
7
7
</p>
<p>3
:6
</p>
<p>0
:9
6
3
2
3
8
</p>
<p>0
:9
6
3
4
9
7
</p>
<p>0
:9
6
3
7
5
3
</p>
<p>0
:9
6
4
0
0
7
</p>
<p>0
:9
6
4
2
5
9
</p>
<p>0
:9
6
4
5
0
8
</p>
<p>0
:9
6
4
7
5
5
</p>
<p>0
:9
6
5
0
0
0
</p>
<p>0
:9
6
5
2
4
3
</p>
<p>0
:9
6
5
4
8
4
</p>
<p>3
:7
</p>
<p>0
:9
6
5
7
2
2
</p>
<p>0
:9
6
5
9
5
9
</p>
<p>0
:9
6
6
1
9
3
</p>
<p>0
:9
6
6
4
2
6
</p>
<p>0
:9
6
6
6
5
6
</p>
<p>0
:9
6
6
8
8
4
</p>
<p>0
:9
6
7
1
1
0
</p>
<p>0
:9
6
7
3
3
5
</p>
<p>0
:9
6
7
5
5
7
</p>
<p>0
:9
6
7
7
7
7
</p>
<p>3
:8
</p>
<p>0
:9
6
7
9
9
6
</p>
<p>0
:9
6
8
2
1
2
</p>
<p>0
:9
6
8
4
2
7
</p>
<p>0
:9
6
8
6
4
0
</p>
<p>0
:9
6
8
8
5
1
</p>
<p>0
:9
6
9
0
6
0
</p>
<p>0
:9
6
9
2
6
8
</p>
<p>0
:9
6
9
4
7
3
</p>
<p>0
:9
6
9
6
7
7
</p>
<p>0
:9
6
9
8
7
9
</p>
<p>3
:9
</p>
<p>0
:9
7
0
0
7
9
</p>
<p>0
:9
7
0
2
7
8
</p>
<p>0
:9
7
0
4
7
5
</p>
<p>0
:9
7
0
6
7
0
</p>
<p>0
:9
7
0
8
6
4
</p>
<p>0
:9
7
1
0
5
6
</p>
<p>0
:9
7
1
2
4
6
</p>
<p>0
:9
7
1
4
3
5
</p>
<p>0
:9
7
1
6
2
2
</p>
<p>0
:9
7
1
8
0
8
</p>
<p>4
:0
</p>
<p>0
:9
7
1
9
9
2
</p>
<p>0
:9
7
2
1
7
4
</p>
<p>0
:9
7
2
3
5
5
</p>
<p>0
:9
7
2
5
3
5
</p>
<p>0
:9
7
2
7
1
3
</p>
<p>0
:9
7
2
8
8
9
</p>
<p>0
:9
7
3
0
6
4
</p>
<p>0
:9
7
3
2
3
8
</p>
<p>0
:9
7
3
4
1
0
</p>
<p>0
:9
7
3
5
8
1
</p>
<p>4
:1
</p>
<p>0
:9
7
3
7
5
0
</p>
<p>0
:9
7
3
9
1
8
</p>
<p>0
:9
7
4
0
8
4
</p>
<p>0
:9
7
4
2
5
0
</p>
<p>0
:9
7
4
4
1
3
</p>
<p>0
:9
7
4
5
7
6
</p>
<p>0
:9
7
4
7
3
7
</p>
<p>0
:9
7
4
8
9
7
</p>
<p>0
:9
7
5
0
5
5
</p>
<p>0
:9
7
5
2
1
2
</p>
<p>4
:2
</p>
<p>0
:9
7
5
3
6
8
</p>
<p>0
:9
7
5
5
2
3
</p>
<p>0
:9
7
5
6
7
6
</p>
<p>0
:9
7
5
8
2
9
</p>
<p>0
:9
7
5
9
8
0
</p>
<p>0
:9
7
6
1
2
9
</p>
<p>0
:9
7
6
2
7
8
</p>
<p>0
:9
7
6
4
2
5
</p>
<p>0
:9
7
6
5
7
1
</p>
<p>0
:9
7
6
7
1
6
</p>
<p>4
:3
</p>
<p>0
:9
7
6
8
6
0
</p>
<p>0
:9
7
7
0
0
3
</p>
<p>0
:9
7
7
1
4
4
</p>
<p>0
:9
7
7
2
8
5
</p>
<p>0
:9
7
7
4
2
4
</p>
<p>0
:9
7
7
5
6
2
</p>
<p>0
:9
7
7
6
9
9
</p>
<p>0
:9
7
7
8
3
5
</p>
<p>0
:9
7
7
9
7
0
</p>
<p>0
:9
7
8
1
0
4
</p>
<p>4
:4
</p>
<p>0
:9
7
8
2
3
7
</p>
<p>0
:9
7
8
3
6
9
</p>
<p>0
:9
7
8
5
0
0
</p>
<p>0
:9
7
8
6
3
0
</p>
<p>0
:9
7
8
7
5
8
</p>
<p>0
:9
7
8
8
8
6
</p>
<p>0
:9
7
9
0
1
3
</p>
<p>0
:9
7
9
1
3
9
</p>
<p>0
:9
7
9
2
6
3
</p>
<p>0
:9
7
9
3
8
7
</p>
<p>4
:5
</p>
<p>0
:9
7
9
5
1
0
</p>
<p>0
:9
7
9
6
3
2
</p>
<p>0
:9
7
9
7
5
3
</p>
<p>0
:9
7
9
8
7
3
</p>
<p>0
:9
7
9
9
9
2
</p>
<p>0
:9
8
0
1
1
0
</p>
<p>0
:9
8
0
2
2
8
</p>
<p>0
:9
8
0
3
4
4
</p>
<p>0
:9
8
0
4
6
0
</p>
<p>0
:9
8
0
5
7
4
</p>
<p>4
:6
</p>
<p>0
:9
8
0
6
8
8
</p>
<p>0
:9
8
0
8
0
1
</p>
<p>0
:9
8
0
9
1
3
</p>
<p>0
:9
8
1
0
2
4
</p>
<p>0
:9
8
1
1
3
5
</p>
<p>0
:9
8
1
2
4
4
</p>
<p>0
:9
8
1
3
5
3
</p>
<p>0
:9
8
1
4
6
1
</p>
<p>0
:9
8
1
5
6
8
</p>
<p>0
:9
8
1
6
7
4
</p>
<p>4
:7
</p>
<p>0
:9
8
1
7
8
0
</p>
<p>0
:9
8
1
8
8
4
</p>
<p>0
:9
8
1
9
8
8
</p>
<p>0
:9
8
2
0
9
1
</p>
<p>0
:9
8
2
1
9
4
</p>
<p>0
:9
8
2
2
9
5
</p>
<p>0
:9
8
2
3
9
6
</p>
<p>0
:9
8
2
4
9
6
</p>
<p>0
:9
8
2
5
9
6
</p>
<p>0
:9
8
2
6
9
4
</p>
<p>4
:8
</p>
<p>0
:9
8
2
7
9
2
</p>
<p>0
:9
8
2
8
9
0
</p>
<p>0
:9
8
2
9
8
6
</p>
<p>0
:9
8
3
0
8
2
</p>
<p>0
:9
8
3
1
7
7
</p>
<p>0
:9
8
3
2
7
1
</p>
<p>0
:9
8
3
3
6
5
</p>
<p>0
:9
8
3
4
5
8
</p>
<p>0
:9
8
3
5
5
0
</p>
<p>0
:9
8
3
6
4
2
</p>
<p>4
:9
</p>
<p>0
:9
8
3
7
3
3
</p>
<p>0
:9
8
3
8
2
3
</p>
<p>0
:9
8
3
9
1
3
</p>
<p>0
:9
8
4
0
0
2
</p>
<p>0
:9
8
4
0
9
1
</p>
<p>0
:9
8
4
1
7
8
</p>
<p>0
:9
8
4
2
6
6
</p>
<p>0
:9
8
4
3
5
2
</p>
<p>0
:9
8
4
4
3
8
</p>
<p>0
:9
8
4
5
2
3</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 299
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
9
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>=
5)
</p>
<p>,o
r
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
5
9
2
</p>
<p>0
:0
1
5
1
8
3
</p>
<p>0
:0
2
2
7
7
2
</p>
<p>0
:0
3
0
3
5
9
</p>
<p>0
:0
3
7
9
4
2
</p>
<p>0
:0
4
5
5
2
0
</p>
<p>0
:0
5
3
0
9
3
</p>
<p>0
:0
6
0
6
5
9
</p>
<p>0
:0
6
8
2
1
9
</p>
<p>0
:1
</p>
<p>0
:0
7
5
7
7
0
</p>
<p>0
:0
8
3
3
1
2
</p>
<p>0
:0
9
0
8
4
4
</p>
<p>0
:0
9
8
3
6
6
</p>
<p>0
:1
0
5
8
7
5
</p>
<p>0
:1
1
3
3
7
2
</p>
<p>0
:1
2
0
8
5
6
</p>
<p>0
:1
2
8
3
2
5
</p>
<p>0
:1
3
5
7
8
0
</p>
<p>0
:1
4
3
2
1
8
</p>
<p>0
:2
</p>
<p>0
:1
5
0
6
4
0
</p>
<p>0
:1
5
8
0
4
3
</p>
<p>0
:1
6
5
4
2
9
</p>
<p>0
:1
7
2
7
9
5
</p>
<p>0
:1
8
0
1
4
1
</p>
<p>0
:1
8
7
4
6
6
</p>
<p>0
:1
9
4
7
6
9
</p>
<p>0
:2
0
2
0
5
0
</p>
<p>0
:2
0
9
3
0
8
</p>
<p>0
:2
1
6
5
4
2
</p>
<p>0
:3
</p>
<p>0
:2
2
3
7
5
1
</p>
<p>0
:2
3
0
9
3
5
</p>
<p>0
:2
3
8
0
9
2
</p>
<p>0
:2
4
5
2
2
3
</p>
<p>0
:2
5
2
3
2
6
</p>
<p>0
:2
5
9
4
0
1
</p>
<p>0
:2
6
6
4
4
6
</p>
<p>0
:2
7
3
4
6
3
</p>
<p>0
:2
8
0
4
4
8
</p>
<p>0
:2
8
7
4
0
3
</p>
<p>0
:4
</p>
<p>0
:2
9
4
3
2
7
</p>
<p>0
:3
0
1
2
1
8
</p>
<p>0
:3
0
8
0
7
7
</p>
<p>0
:3
1
4
9
0
2
</p>
<p>0
:3
2
1
6
9
4
</p>
<p>0
:3
2
8
4
5
1
</p>
<p>0
:3
3
5
1
7
3
</p>
<p>0
:3
4
1
8
6
0
</p>
<p>0
:3
4
8
5
1
0
</p>
<p>0
:3
5
5
1
2
4
</p>
<p>0
:5
</p>
<p>0
:3
6
1
7
0
1
</p>
<p>0
:3
6
8
2
4
1
</p>
<p>0
:3
7
4
7
4
2
</p>
<p>0
:3
8
1
2
0
6
</p>
<p>0
:3
8
7
6
3
0
</p>
<p>0
:3
9
4
0
1
5
</p>
<p>0
:4
0
0
3
6
1
</p>
<p>0
:4
0
6
6
6
7
</p>
<p>0
:4
1
2
9
3
2
</p>
<p>0
:4
1
9
1
5
6
</p>
<p>0
:6
</p>
<p>0
:4
2
5
3
4
0
</p>
<p>0
:4
3
1
4
8
2
</p>
<p>0
:4
3
7
5
8
2
</p>
<p>0
:4
4
3
6
4
1
</p>
<p>0
:4
4
9
6
5
7
</p>
<p>0
:4
5
5
6
3
0
</p>
<p>0
:4
6
1
5
6
1
</p>
<p>0
:4
6
7
4
4
9
</p>
<p>0
:4
7
3
2
9
3
</p>
<p>0
:4
7
9
0
9
4
</p>
<p>0
:7
</p>
<p>0
:4
8
4
8
5
1
</p>
<p>0
:4
9
0
5
6
5
</p>
<p>0
:4
9
6
2
3
4
</p>
<p>0
:5
0
1
8
5
9
</p>
<p>0
:5
0
7
4
4
0
</p>
<p>0
:5
1
2
9
7
6
</p>
<p>0
:5
1
8
4
6
8
</p>
<p>0
:5
2
3
9
1
5
</p>
<p>0
:5
2
9
3
1
7
</p>
<p>0
:5
3
4
6
7
4
</p>
<p>0
:8
</p>
<p>0
:5
3
9
9
8
6
</p>
<p>0
:5
4
5
2
5
3
</p>
<p>0
:5
5
0
4
7
6
</p>
<p>0
:5
5
5
6
5
3
</p>
<p>0
:5
6
0
7
8
5
</p>
<p>0
:5
6
5
8
7
1
</p>
<p>0
:5
7
0
9
1
3
</p>
<p>0
:5
7
5
9
1
0
</p>
<p>0
:5
8
0
8
6
1
</p>
<p>0
:5
8
5
7
6
8
</p>
<p>0
:9
</p>
<p>0
:5
9
0
6
2
9
</p>
<p>0
:5
9
5
4
4
5
</p>
<p>0
:6
0
0
2
1
7
</p>
<p>0
:6
0
4
9
4
4
</p>
<p>0
:6
0
9
6
2
6
</p>
<p>0
:6
1
4
2
6
3
</p>
<p>0
:6
1
8
8
5
5
</p>
<p>0
:6
2
3
4
0
4
</p>
<p>0
:6
2
7
9
0
8
</p>
<p>0
:6
3
2
3
6
7
</p>
<p>1
:0
</p>
<p>0
:6
3
6
7
8
3
</p>
<p>0
:6
4
1
1
5
4
</p>
<p>0
:6
4
5
4
8
2
</p>
<p>0
:6
4
9
7
6
7
</p>
<p>0
:6
5
4
0
0
7
</p>
<p>0
:6
5
8
2
0
5
</p>
<p>0
:6
6
2
3
5
9
</p>
<p>0
:6
6
6
4
7
1
</p>
<p>0
:6
7
0
5
3
9
</p>
<p>0
:6
7
4
5
6
6
</p>
<p>1
:1
</p>
<p>0
:6
7
8
5
4
9
</p>
<p>0
:6
8
2
4
9
1
</p>
<p>0
:6
8
6
3
9
1
</p>
<p>0
:6
9
0
2
4
9
</p>
<p>0
:6
9
4
0
6
6
</p>
<p>0
:6
9
7
8
4
1
</p>
<p>0
:7
0
1
5
7
6
</p>
<p>0
:7
0
5
2
7
0
</p>
<p>0
:7
0
8
9
2
3
</p>
<p>0
:7
1
2
5
3
6
</p>
<p>1
:2
</p>
<p>0
:7
1
6
1
0
9
</p>
<p>0
:7
1
9
6
4
3
</p>
<p>0
:7
2
3
1
3
6
</p>
<p>0
:7
2
6
5
9
1
</p>
<p>0
:7
3
0
0
0
7
</p>
<p>0
:7
3
3
3
8
4
</p>
<p>0
:7
3
6
7
2
3
</p>
<p>0
:7
4
0
0
2
3
</p>
<p>0
:7
4
3
2
8
6
</p>
<p>0
:7
4
6
5
1
2
</p>
<p>1
:3
</p>
<p>0
:7
4
9
7
0
0
</p>
<p>0
:7
5
2
8
5
1
</p>
<p>0
:7
5
5
9
6
5
</p>
<p>0
:7
5
9
0
4
3
</p>
<p>0
:7
6
2
0
8
5
</p>
<p>0
:7
6
5
0
9
2
</p>
<p>0
:7
6
8
0
6
2
</p>
<p>0
:7
7
0
9
9
8
</p>
<p>0
:7
7
3
8
9
9
</p>
<p>0
:7
7
6
7
6
5
</p>
<p>1
:4
</p>
<p>0
:7
7
9
5
9
6
</p>
<p>0
:7
8
2
3
9
4
</p>
<p>0
:7
8
5
1
5
8
</p>
<p>0
:7
8
7
8
8
9
</p>
<p>0
:7
9
0
5
8
7
</p>
<p>0
:7
9
3
2
5
2
</p>
<p>0
:7
9
5
8
8
5
</p>
<p>0
:7
9
8
4
8
5
</p>
<p>0
:8
0
1
0
5
4
</p>
<p>0
:8
0
3
5
9
1
</p>
<p>1
:5
</p>
<p>0
:8
0
6
0
9
7
</p>
<p>0
:8
0
8
5
7
2
</p>
<p>0
:8
1
1
0
1
6
</p>
<p>0
:8
1
3
4
3
0
</p>
<p>0
:8
1
5
8
1
4
</p>
<p>0
:8
1
8
1
6
8
</p>
<p>0
:8
2
0
4
9
3
</p>
<p>0
:8
2
2
7
8
9
</p>
<p>0
:8
2
5
0
5
6
</p>
<p>0
:8
2
7
2
9
5
</p>
<p>1
:6
</p>
<p>0
:8
2
9
5
0
5
</p>
<p>0
:8
3
1
6
8
8
</p>
<p>0
:8
3
3
8
4
3
</p>
<p>0
:8
3
5
9
7
0
</p>
<p>0
:8
3
8
0
7
1
</p>
<p>0
:8
4
0
1
4
5
</p>
<p>0
:8
4
2
1
9
2
</p>
<p>0
:8
4
4
2
1
3
</p>
<p>0
:8
4
6
2
0
9
</p>
<p>0
:8
4
8
1
7
9
</p>
<p>1
:7
</p>
<p>0
:8
5
0
1
2
4
</p>
<p>0
:8
5
2
0
4
3
</p>
<p>0
:8
5
3
9
3
8
</p>
<p>0
:8
5
5
8
0
9
</p>
<p>0
:8
5
7
6
5
5
</p>
<p>0
:8
5
9
4
7
8
</p>
<p>0
:8
6
1
2
7
7
</p>
<p>0
:8
6
3
0
5
3
</p>
<p>0
:8
6
4
8
0
5
</p>
<p>0
:8
6
6
5
3
5
</p>
<p>1
:8
</p>
<p>0
:8
6
8
2
4
3
</p>
<p>0
:8
6
9
9
2
8
</p>
<p>0
:8
7
1
5
9
1
</p>
<p>0
:8
7
3
2
3
3
</p>
<p>0
:8
7
4
8
5
3
</p>
<p>0
:8
7
6
4
5
2
</p>
<p>0
:8
7
8
0
3
0
</p>
<p>0
:8
7
9
5
8
7
</p>
<p>0
:8
8
1
1
2
4
</p>
<p>0
:8
8
2
6
4
0
</p>
<p>1
:9
</p>
<p>0
:8
8
4
1
3
7
</p>
<p>0
:8
8
5
6
1
4
</p>
<p>0
:8
8
7
0
7
2
</p>
<p>0
:8
8
8
5
1
0
</p>
<p>0
:8
8
9
9
3
0
</p>
<p>0
:8
9
1
3
3
0
</p>
<p>0
:8
9
2
7
1
2
</p>
<p>0
:8
9
4
0
7
6
</p>
<p>0
:8
9
5
4
2
2
</p>
<p>0
:8
9
6
7
5
0
</p>
<p>2
:0
</p>
<p>0
:8
9
8
0
6
1
</p>
<p>0
:8
9
9
3
5
4
</p>
<p>0
:9
0
0
6
3
0
</p>
<p>0
:9
0
1
8
8
9
</p>
<p>0
:9
0
3
1
3
2
</p>
<p>0
:9
0
4
3
5
8
</p>
<p>0
:9
0
5
5
6
7
</p>
<p>0
:9
0
6
7
6
1
</p>
<p>0
:9
0
7
9
3
8
</p>
<p>0
:9
0
9
1
0
1
</p>
<p>2
:1
</p>
<p>0
:9
1
0
2
4
7
</p>
<p>0
:9
1
1
3
7
8
</p>
<p>0
:9
1
2
4
9
5
</p>
<p>0
:9
1
3
5
9
6
</p>
<p>0
:9
1
4
6
8
3
</p>
<p>0
:9
1
5
7
5
5
</p>
<p>0
:9
1
6
8
1
3
</p>
<p>0
:9
1
7
8
5
7
</p>
<p>0
:9
1
8
8
8
7
</p>
<p>0
:9
1
9
9
0
4
</p>
<p>2
:2
</p>
<p>0
:9
2
0
9
0
6
</p>
<p>0
:9
2
1
8
9
6
</p>
<p>0
:9
2
2
8
7
2
</p>
<p>0
:9
2
3
8
3
5
</p>
<p>0
:9
2
4
7
8
6
</p>
<p>0
:9
2
5
7
2
3
</p>
<p>0
:9
2
6
6
4
9
</p>
<p>0
:9
2
7
5
6
2
</p>
<p>0
:9
2
8
4
6
2
</p>
<p>0
:9
2
9
3
5
1
</p>
<p>2
:3
</p>
<p>0
:9
3
0
2
2
8
</p>
<p>0
:9
3
1
0
9
3
</p>
<p>0
:9
3
1
9
4
7
</p>
<p>0
:9
3
2
7
8
9
</p>
<p>0
:9
3
3
6
2
0
</p>
<p>0
:9
3
4
4
4
0
</p>
<p>0
:9
3
5
2
4
9
</p>
<p>0
:9
3
6
0
4
8
</p>
<p>0
:9
3
6
8
3
5
</p>
<p>0
:9
3
7
6
1
3
</p>
<p>2
:4
</p>
<p>0
:9
3
8
3
8
0
</p>
<p>0
:9
3
9
1
3
6
</p>
<p>0
:9
3
9
8
8
3
</p>
<p>0
:9
4
0
6
2
0
</p>
<p>0
:9
4
1
3
4
7
</p>
<p>0
:9
4
2
0
6
4
</p>
<p>0
:9
4
2
7
7
2
</p>
<p>0
:9
4
3
4
7
0
</p>
<p>0
:9
4
4
1
5
9
</p>
<p>0
:9
4
4
8
3
9
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>300 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.1
9
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
4
5
5
1
0
</p>
<p>0
:9
4
6
1
7
2
</p>
<p>0
:9
4
6
8
2
6
</p>
<p>0
:9
4
7
4
7
0
</p>
<p>0
:9
4
8
1
0
7
</p>
<p>0
:9
4
8
7
3
4
</p>
<p>0
:9
4
9
3
5
4
</p>
<p>0
:9
4
9
9
6
5
</p>
<p>0
:9
5
0
5
6
8
</p>
<p>0
:9
5
1
1
6
4
</p>
<p>2
:6
</p>
<p>0
:9
5
1
7
5
1
</p>
<p>0
:9
5
2
3
3
1
</p>
<p>0
:9
5
2
9
0
3
</p>
<p>0
:9
5
3
4
6
7
</p>
<p>0
:9
5
4
0
2
4
</p>
<p>0
:9
5
4
5
7
4
</p>
<p>0
:9
5
5
1
1
6
</p>
<p>0
:9
5
5
6
5
2
</p>
<p>0
:9
5
6
1
8
0
</p>
<p>0
:9
5
6
7
0
2
</p>
<p>2
:7
</p>
<p>0
:9
5
7
2
1
6
</p>
<p>0
:9
5
7
7
2
4
</p>
<p>0
:9
5
8
2
2
5
</p>
<p>0
:9
5
8
7
2
0
</p>
<p>0
:9
5
9
2
0
8
</p>
<p>0
:9
5
9
6
9
0
</p>
<p>0
:9
6
0
1
6
6
</p>
<p>0
:9
6
0
6
3
5
</p>
<p>0
:9
6
1
0
9
8
</p>
<p>0
:9
6
1
5
5
6
</p>
<p>2
:8
</p>
<p>0
:9
6
2
0
0
7
</p>
<p>0
:9
6
2
4
5
2
</p>
<p>0
:9
6
2
8
9
2
</p>
<p>0
:9
6
3
3
2
6
</p>
<p>0
:9
6
3
7
5
4
</p>
<p>0
:9
6
4
1
7
7
</p>
<p>0
:9
6
4
5
9
4
</p>
<p>0
:9
6
5
0
0
6
</p>
<p>0
:9
6
5
4
1
2
</p>
<p>0
:9
6
5
8
1
4
</p>
<p>2
:9
</p>
<p>0
:9
6
6
2
1
0
</p>
<p>0
:9
6
6
6
0
1
</p>
<p>0
:9
6
6
9
8
7
</p>
<p>0
:9
6
7
3
6
8
</p>
<p>0
:9
6
7
7
4
4
</p>
<p>0
:9
6
8
1
1
5
</p>
<p>0
:9
6
8
4
8
2
</p>
<p>0
:9
6
8
8
4
3
</p>
<p>0
:9
6
9
2
0
0
</p>
<p>0
:9
6
9
5
5
3
</p>
<p>3
:0
</p>
<p>0
:9
6
9
9
0
1
</p>
<p>0
:9
7
0
2
4
5
</p>
<p>0
:9
7
0
5
8
4
</p>
<p>0
:9
7
0
9
1
9
</p>
<p>0
:9
7
1
2
5
0
</p>
<p>0
:9
7
1
5
7
6
</p>
<p>0
:9
7
1
8
9
8
</p>
<p>0
:9
7
2
2
1
7
</p>
<p>0
:9
7
2
5
3
1
</p>
<p>0
:9
7
2
8
4
1
</p>
<p>3
:1
</p>
<p>0
:9
7
3
1
4
7
</p>
<p>0
:9
7
3
4
5
0
</p>
<p>0
:9
7
3
7
4
8
</p>
<p>0
:9
7
4
0
4
3
</p>
<p>0
:9
7
4
3
3
4
</p>
<p>0
:9
7
4
6
2
1
</p>
<p>0
:9
7
4
9
0
5
</p>
<p>0
:9
7
5
1
8
5
</p>
<p>0
:9
7
5
4
6
2
</p>
<p>0
:9
7
5
7
3
5
</p>
<p>3
:2
</p>
<p>0
:9
7
6
0
0
5
</p>
<p>0
:9
7
6
2
7
2
</p>
<p>0
:9
7
6
5
3
5
</p>
<p>0
:9
7
6
7
9
5
</p>
<p>0
:9
7
7
0
5
1
</p>
<p>0
:9
7
7
3
0
5
</p>
<p>0
:9
7
7
5
5
5
</p>
<p>0
:9
7
7
8
0
2
</p>
<p>0
:9
7
8
0
4
6
</p>
<p>0
:9
7
8
2
8
7
</p>
<p>3
:3
</p>
<p>0
:9
7
8
5
2
5
</p>
<p>0
:9
7
8
7
6
0
</p>
<p>0
:9
7
8
9
9
2
</p>
<p>0
:9
7
9
2
2
1
</p>
<p>0
:9
7
9
4
4
8
</p>
<p>0
:9
7
9
6
7
2
</p>
<p>0
:9
7
9
8
9
3
</p>
<p>0
:9
8
0
1
1
1
</p>
<p>0
:9
8
0
3
2
6
</p>
<p>0
:9
8
0
5
3
9
</p>
<p>3
:4
</p>
<p>0
:9
8
0
7
4
9
</p>
<p>0
:9
8
0
9
5
7
</p>
<p>0
:9
8
1
1
6
2
</p>
<p>0
:9
8
1
3
6
5
</p>
<p>0
:9
8
1
5
6
5
</p>
<p>0
:9
8
1
7
6
3
</p>
<p>0
:9
8
1
9
5
8
</p>
<p>0
:9
8
2
1
5
1
</p>
<p>0
:9
8
2
3
4
2
</p>
<p>0
:9
8
2
5
3
0
</p>
<p>3
:5
</p>
<p>0
:9
8
2
7
1
6
</p>
<p>0
:9
8
2
9
0
0
</p>
<p>0
:9
8
3
0
8
1
</p>
<p>0
:9
8
3
2
6
1
</p>
<p>0
:9
8
3
4
3
8
</p>
<p>0
:9
8
3
6
1
3
</p>
<p>0
:9
8
3
7
8
6
</p>
<p>0
:9
8
3
9
5
7
</p>
<p>0
:9
8
4
1
2
6
</p>
<p>0
:9
8
4
2
9
2
</p>
<p>3
:6
</p>
<p>0
:9
8
4
4
5
7
</p>
<p>0
:9
8
4
6
2
0
</p>
<p>0
:9
8
4
7
8
1
</p>
<p>0
:9
8
4
9
4
0
</p>
<p>0
:9
8
5
0
9
7
</p>
<p>0
:9
8
5
2
5
2
</p>
<p>0
:9
8
5
4
0
6
</p>
<p>0
:9
8
5
5
5
7
</p>
<p>0
:9
8
5
7
0
7
</p>
<p>0
:9
8
5
8
5
5
</p>
<p>3
:7
</p>
<p>0
:9
8
6
0
0
1
</p>
<p>0
:9
8
6
1
4
6
</p>
<p>0
:9
8
6
2
8
8
</p>
<p>0
:9
8
6
4
2
9
</p>
<p>0
:9
8
6
5
6
9
</p>
<p>0
:9
8
6
7
0
7
</p>
<p>0
:9
8
6
8
4
3
</p>
<p>0
:9
8
6
9
7
7
</p>
<p>0
:9
8
7
1
1
1
</p>
<p>0
:9
8
7
2
4
2
</p>
<p>3
:8
</p>
<p>0
:9
8
7
3
7
2
</p>
<p>0
:9
8
7
5
0
0
</p>
<p>0
:9
8
7
6
2
7
</p>
<p>0
:9
8
7
7
5
3
</p>
<p>0
:9
8
7
8
7
7
</p>
<p>0
:9
8
7
9
9
9
</p>
<p>0
:9
8
8
1
2
0
</p>
<p>0
:9
8
8
2
4
0
</p>
<p>0
:9
8
8
3
5
9
</p>
<p>0
:9
8
8
4
7
5
</p>
<p>3
:9
</p>
<p>0
:9
8
8
5
9
1
</p>
<p>0
:9
8
8
7
0
5
</p>
<p>0
:9
8
8
8
1
8
</p>
<p>0
:9
8
8
9
3
0
</p>
<p>0
:9
8
9
0
4
1
</p>
<p>0
:9
8
9
1
5
0
</p>
<p>0
:9
8
9
2
5
8
</p>
<p>0
:9
8
9
3
6
4
</p>
<p>0
:9
8
9
4
7
0
</p>
<p>0
:9
8
9
5
7
4
</p>
<p>4
:0
</p>
<p>0
:9
8
9
6
7
7
</p>
<p>0
:9
8
9
7
7
9
</p>
<p>0
:9
8
9
8
8
0
</p>
<p>0
:9
8
9
9
7
9
</p>
<p>0
:9
9
0
0
7
8
</p>
<p>0
:9
9
0
1
7
5
</p>
<p>0
:9
9
0
2
7
1
</p>
<p>0
:9
9
0
3
6
6
</p>
<p>0
:9
9
0
4
6
1
</p>
<p>0
:9
9
0
5
5
4
</p>
<p>4
:1
</p>
<p>0
:9
9
0
6
4
6
</p>
<p>0
:9
9
0
7
3
7
</p>
<p>0
:9
9
0
8
2
6
</p>
<p>0
:9
9
0
9
1
5
</p>
<p>0
:9
9
1
0
0
3
</p>
<p>0
:9
9
1
0
9
0
</p>
<p>0
:9
9
1
1
7
6
</p>
<p>0
:9
9
1
2
6
1
</p>
<p>0
:9
9
1
3
4
5
</p>
<p>0
:9
9
1
4
2
9
</p>
<p>4
:2
</p>
<p>0
:9
9
1
5
1
1
</p>
<p>0
:9
9
1
5
9
2
</p>
<p>0
:9
9
1
6
7
3
</p>
<p>0
:9
9
1
7
5
2
</p>
<p>0
:9
9
1
8
3
1
</p>
<p>0
:9
9
1
9
0
9
</p>
<p>0
:9
9
1
9
8
6
</p>
<p>0
:9
9
2
0
6
2
</p>
<p>0
:9
9
2
1
3
7
</p>
<p>0
:9
9
2
2
1
1
</p>
<p>4
:3
</p>
<p>0
:9
9
2
2
8
5
</p>
<p>0
:9
9
2
3
5
8
</p>
<p>0
:9
9
2
4
3
0
</p>
<p>0
:9
9
2
5
0
1
</p>
<p>0
:9
9
2
5
7
2
</p>
<p>0
:9
9
2
6
4
1
</p>
<p>0
:9
9
2
7
1
0
</p>
<p>0
:9
9
2
7
7
8
</p>
<p>0
:9
9
2
8
4
6
</p>
<p>0
:9
9
2
9
1
3
</p>
<p>4
:4
</p>
<p>0
:9
9
2
9
7
9
</p>
<p>0
:9
9
3
0
4
4
</p>
<p>0
:9
9
3
1
0
8
</p>
<p>0
:9
9
3
1
7
2
</p>
<p>0
:9
9
3
2
3
6
</p>
<p>0
:9
9
3
2
9
8
</p>
<p>0
:9
9
3
3
6
0
</p>
<p>0
:9
9
3
4
2
1
</p>
<p>0
:9
9
3
4
8
2
</p>
<p>0
:9
9
3
5
4
2
</p>
<p>4
:5
</p>
<p>0
:9
9
3
6
0
1
</p>
<p>0
:9
9
3
6
6
0
</p>
<p>0
:9
9
3
7
1
8
</p>
<p>0
:9
9
3
7
7
5
</p>
<p>0
:9
9
3
8
3
2
</p>
<p>0
:9
9
3
8
8
8
</p>
<p>0
:9
9
3
9
4
4
</p>
<p>0
:9
9
3
9
9
9
</p>
<p>0
:9
9
4
0
5
3
</p>
<p>0
:9
9
4
1
0
7
</p>
<p>4
:6
</p>
<p>0
:9
9
4
1
6
0
</p>
<p>0
:9
9
4
2
1
3
</p>
<p>0
:9
9
4
2
6
5
</p>
<p>0
:9
9
4
3
1
7
</p>
<p>0
:9
9
4
3
6
8
</p>
<p>0
:9
9
4
4
1
8
</p>
<p>0
:9
9
4
4
6
8
</p>
<p>0
:9
9
4
5
1
8
</p>
<p>0
:9
9
4
5
6
7
</p>
<p>0
:9
9
4
6
1
5
</p>
<p>4
:7
</p>
<p>0
:9
9
4
6
6
3
</p>
<p>0
:9
9
4
7
1
1
</p>
<p>0
:9
9
4
7
5
8
</p>
<p>0
:9
9
4
8
0
4
</p>
<p>0
:9
9
4
8
5
0
</p>
<p>0
:9
9
4
8
9
6
</p>
<p>0
:9
9
4
9
4
1
</p>
<p>0
:9
9
4
9
8
6
</p>
<p>0
:9
9
5
0
3
0
</p>
<p>0
:9
9
5
0
7
3
</p>
<p>4
:8
</p>
<p>0
:9
9
5
1
1
7
</p>
<p>0
:9
9
5
1
6
0
</p>
<p>0
:9
9
5
2
0
2
</p>
<p>0
:9
9
5
2
4
4
</p>
<p>0
:9
9
5
2
8
5
</p>
<p>0
:9
9
5
3
2
7
</p>
<p>0
:9
9
5
3
6
7
</p>
<p>0
:9
9
5
4
0
8
</p>
<p>0
:9
9
5
4
4
7
</p>
<p>0
:9
9
5
4
8
7
</p>
<p>4
:9
</p>
<p>0
:9
9
5
5
2
6
</p>
<p>0
:9
9
5
5
6
5
</p>
<p>0
:9
9
5
6
0
3
</p>
<p>0
:9
9
5
6
4
1
</p>
<p>0
:9
9
5
6
7
8
</p>
<p>0
:9
9
5
7
1
5
</p>
<p>0
:9
9
5
7
5
2
</p>
<p>0
:9
9
5
7
8
9
</p>
<p>0
:9
9
5
8
2
5
</p>
<p>0
:9
9
5
8
6
0</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 301
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
0
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>=
10
</p>
<p>),
or
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
7
8
2
</p>
<p>0
:0
1
5
5
6
3
</p>
<p>0
:0
2
3
3
4
3
</p>
<p>0
:0
3
1
1
2
0
</p>
<p>0
:0
3
8
8
9
3
</p>
<p>0
:0
4
6
6
6
2
</p>
<p>0
:0
5
4
4
2
6
</p>
<p>0
:0
6
2
1
8
4
</p>
<p>0
:0
6
9
9
3
6
</p>
<p>0
:1
</p>
<p>0
:0
7
7
6
7
9
</p>
<p>0
:0
8
5
4
1
4
</p>
<p>0
:0
9
3
1
4
0
</p>
<p>0
:1
0
0
8
5
6
</p>
<p>0
:1
0
8
5
6
0
</p>
<p>0
:1
1
6
2
5
3
</p>
<p>0
:1
2
3
9
3
3
</p>
<p>0
:1
3
1
6
0
0
</p>
<p>0
:1
3
9
2
5
2
</p>
<p>0
:1
4
6
8
8
9
</p>
<p>0
:2
</p>
<p>0
:1
5
4
5
1
1
</p>
<p>0
:1
6
2
1
1
6
</p>
<p>0
:1
6
9
7
0
3
</p>
<p>0
:1
7
7
2
7
2
</p>
<p>0
:1
8
4
8
2
2
</p>
<p>0
:1
9
2
3
5
2
</p>
<p>0
:1
9
9
8
6
1
</p>
<p>0
:2
0
7
3
5
0
</p>
<p>0
:2
1
4
8
1
6
</p>
<p>0
:2
2
2
2
6
0
</p>
<p>0
:3
</p>
<p>0
:2
2
9
6
7
9
</p>
<p>0
:2
3
7
0
7
5
</p>
<p>0
:2
4
4
4
4
6
</p>
<p>0
:2
5
1
7
9
1
</p>
<p>0
:2
5
9
1
1
0
</p>
<p>0
:2
6
6
4
0
2
</p>
<p>0
:2
7
3
6
6
6
</p>
<p>0
:2
8
0
9
0
2
</p>
<p>0
:2
8
8
1
0
9
</p>
<p>0
:2
9
5
2
8
6
</p>
<p>0
:4
</p>
<p>0
:3
0
2
4
3
3
</p>
<p>0
:3
0
9
5
4
9
</p>
<p>0
:3
1
6
6
3
3
</p>
<p>0
:3
2
3
6
8
6
</p>
<p>0
:3
3
0
7
0
6
</p>
<p>0
:3
3
7
6
9
2
</p>
<p>0
:3
4
4
6
4
5
</p>
<p>0
:3
5
1
5
6
3
</p>
<p>0
:3
5
8
4
4
7
</p>
<p>0
:3
6
5
2
9
5
</p>
<p>0
:5
</p>
<p>0
:3
7
2
1
0
7
</p>
<p>0
:3
7
8
8
8
2
</p>
<p>0
:3
8
5
6
2
1
</p>
<p>0
:3
9
2
3
2
2
</p>
<p>0
:3
9
8
9
8
5
</p>
<p>0
:4
0
5
6
1
0
</p>
<p>0
:4
1
2
1
9
7
</p>
<p>0
:4
1
8
7
4
4
</p>
<p>0
:4
2
5
2
5
1
</p>
<p>0
:4
3
1
7
1
8
</p>
<p>0
:6
</p>
<p>0
:4
3
8
1
4
5
</p>
<p>0
:4
4
4
5
3
1
</p>
<p>0
:4
5
0
8
7
6
</p>
<p>0
:4
5
7
1
7
9
</p>
<p>0
:4
6
3
4
4
1
</p>
<p>0
:4
6
9
6
6
0
</p>
<p>0
:4
7
5
8
3
7
</p>
<p>0
:4
8
1
9
7
1
</p>
<p>0
:4
8
8
0
6
1
</p>
<p>0
:4
9
4
1
0
9
</p>
<p>0
:7
</p>
<p>0
:5
0
0
1
1
3
</p>
<p>0
:5
0
6
0
7
2
</p>
<p>0
:5
1
1
9
8
8
</p>
<p>0
:5
1
7
8
6
0
</p>
<p>0
:5
2
3
6
8
6
</p>
<p>0
:5
2
9
4
6
8
</p>
<p>0
:5
3
5
2
0
5
</p>
<p>0
:5
4
0
8
9
7
</p>
<p>0
:5
4
6
5
4
3
</p>
<p>0
:5
5
2
1
4
4
</p>
<p>0
:8
</p>
<p>0
:5
5
7
7
0
0
</p>
<p>0
:5
6
3
2
0
9
</p>
<p>0
:5
6
8
6
7
3
</p>
<p>0
:5
7
4
0
9
1
</p>
<p>0
:5
7
9
4
6
3
</p>
<p>0
:5
8
4
7
8
8
</p>
<p>0
:5
9
0
0
6
7
</p>
<p>0
:5
9
5
3
0
0
</p>
<p>0
:6
0
0
4
8
7
</p>
<p>0
:6
0
5
6
2
7
</p>
<p>0
:9
</p>
<p>0
:6
1
0
7
2
1
</p>
<p>0
:6
1
5
7
6
8
</p>
<p>0
:6
2
0
7
6
9
</p>
<p>0
:6
2
5
7
2
4
</p>
<p>0
:6
3
0
6
3
2
</p>
<p>0
:6
3
5
4
9
4
</p>
<p>0
:6
4
0
3
0
9
</p>
<p>0
:6
4
5
0
7
8
</p>
<p>0
:6
4
9
8
0
0
</p>
<p>0
:6
5
4
4
7
7
</p>
<p>1
:0
</p>
<p>0
:6
5
9
1
0
7
</p>
<p>0
:6
6
3
6
9
1
</p>
<p>0
:6
6
8
2
3
0
</p>
<p>0
:6
7
2
7
2
2
</p>
<p>0
:6
7
7
1
6
9
</p>
<p>0
:6
8
1
5
6
9
</p>
<p>0
:6
8
5
9
2
5
</p>
<p>0
:6
9
0
2
3
5
</p>
<p>0
:6
9
4
4
9
9
</p>
<p>0
:6
9
8
7
1
9
</p>
<p>1
:1
</p>
<p>0
:7
0
2
8
9
3
</p>
<p>0
:7
0
7
0
2
3
</p>
<p>0
:7
1
1
1
0
8
</p>
<p>0
:7
1
5
1
4
9
</p>
<p>0
:7
1
9
1
4
5
</p>
<p>0
:7
2
3
0
9
7
</p>
<p>0
:7
2
7
0
0
6
</p>
<p>0
:7
3
0
8
7
0
</p>
<p>0
:7
3
4
6
9
1
</p>
<p>0
:7
3
8
4
6
9
</p>
<p>1
:2
</p>
<p>0
:7
4
2
2
0
4
</p>
<p>0
:7
4
5
8
9
6
</p>
<p>0
:7
4
9
5
4
5
</p>
<p>0
:7
5
3
1
5
2
</p>
<p>0
:7
5
6
7
1
7
</p>
<p>0
:7
6
0
2
4
0
</p>
<p>0
:7
6
3
7
2
1
</p>
<p>0
:7
6
7
1
6
1
</p>
<p>0
:7
7
0
5
5
9
</p>
<p>0
:7
7
3
9
1
7
</p>
<p>1
:3
</p>
<p>0
:7
7
7
2
3
5
</p>
<p>0
:7
8
0
5
1
1
</p>
<p>0
:7
8
3
7
4
8
</p>
<p>0
:7
8
6
9
4
5
</p>
<p>0
:7
9
0
1
0
3
</p>
<p>0
:7
9
3
2
2
1
</p>
<p>0
:7
9
6
3
0
1
</p>
<p>0
:7
9
9
3
4
1
</p>
<p>0
:8
0
2
3
4
4
</p>
<p>0
:8
0
5
3
0
8
</p>
<p>1
:4
</p>
<p>0
:8
0
8
2
3
5
</p>
<p>0
:8
1
1
1
2
4
</p>
<p>0
:8
1
3
9
7
6
</p>
<p>0
:8
1
6
7
9
2
</p>
<p>0
:8
1
9
5
7
0
</p>
<p>0
:8
2
2
3
1
3
</p>
<p>0
:8
2
5
0
1
9
</p>
<p>0
:8
2
7
6
9
0
</p>
<p>0
:8
3
0
3
2
6
</p>
<p>0
:8
3
2
9
2
7
</p>
<p>1
:5
</p>
<p>0
:8
3
5
4
9
3
</p>
<p>0
:8
3
8
0
2
5
</p>
<p>0
:8
4
0
5
2
3
</p>
<p>0
:8
4
2
9
8
7
</p>
<p>0
:8
4
5
4
1
7
</p>
<p>0
:8
4
7
8
1
5
</p>
<p>0
:8
5
0
1
8
0
</p>
<p>0
:8
5
2
5
1
2
</p>
<p>0
:8
5
4
8
1
3
</p>
<p>0
:8
5
7
0
8
1
</p>
<p>1
:6
</p>
<p>0
:8
5
9
3
1
9
</p>
<p>0
:8
6
1
5
2
5
</p>
<p>0
:8
6
3
7
0
0
</p>
<p>0
:8
6
5
8
4
5
</p>
<p>0
:8
6
7
9
5
9
</p>
<p>0
:8
7
0
0
4
4
</p>
<p>0
:8
7
2
0
9
9
</p>
<p>0
:8
7
4
1
2
5
</p>
<p>0
:8
7
6
1
2
2
</p>
<p>0
:8
7
8
0
9
1
</p>
<p>1
:7
</p>
<p>0
:8
8
0
0
3
1
</p>
<p>0
:8
8
1
9
4
3
</p>
<p>0
:8
8
3
8
2
8
</p>
<p>0
:8
8
5
6
8
5
</p>
<p>0
:8
8
7
5
1
6
</p>
<p>0
:8
8
9
3
1
9
</p>
<p>0
:8
9
1
0
9
7
</p>
<p>0
:8
9
2
8
4
8
</p>
<p>0
:8
9
4
5
7
3
</p>
<p>0
:8
9
6
2
7
3
</p>
<p>1
:8
</p>
<p>0
:8
9
7
9
4
8
</p>
<p>0
:8
9
9
5
9
8
</p>
<p>0
:9
0
1
2
2
3
</p>
<p>0
:9
0
2
8
2
5
</p>
<p>0
:9
0
4
4
0
2
</p>
<p>0
:9
0
5
9
5
5
</p>
<p>0
:9
0
7
4
8
5
</p>
<p>0
:9
0
8
9
9
2
</p>
<p>0
:9
1
0
4
7
7
</p>
<p>0
:9
1
1
9
3
8
</p>
<p>1
:9
</p>
<p>0
:9
1
3
3
7
8
</p>
<p>0
:9
1
4
7
9
6
</p>
<p>0
:9
1
6
1
9
1
</p>
<p>0
:9
1
7
5
6
6
</p>
<p>0
:9
1
8
9
1
9
</p>
<p>0
:9
2
0
2
5
2
</p>
<p>0
:9
2
1
5
6
4
</p>
<p>0
:9
2
2
8
5
6
</p>
<p>0
:9
2
4
1
2
8
</p>
<p>0
:9
2
5
3
8
0
</p>
<p>2
:0
</p>
<p>0
:9
2
6
6
1
2
</p>
<p>0
:9
2
7
8
2
6
</p>
<p>0
:9
2
9
0
2
0
</p>
<p>0
:9
3
0
1
9
6
</p>
<p>0
:9
3
1
3
5
3
</p>
<p>0
:9
3
2
4
9
2
</p>
<p>0
:9
3
3
6
1
3
</p>
<p>0
:9
3
4
7
1
7
</p>
<p>0
:9
3
5
8
0
3
</p>
<p>0
:9
3
6
8
7
1
</p>
<p>2
:1
</p>
<p>0
:9
3
7
9
2
3
</p>
<p>0
:9
3
8
9
5
8
</p>
<p>0
:9
3
9
9
7
7
</p>
<p>0
:9
4
0
9
7
9
</p>
<p>0
:9
4
1
9
6
5
</p>
<p>0
:9
4
2
9
3
6
</p>
<p>0
:9
4
3
8
9
1
</p>
<p>0
:9
4
4
8
3
0
</p>
<p>0
:9
4
5
7
5
5
</p>
<p>0
:9
4
6
6
6
4
</p>
<p>2
:2
</p>
<p>0
:9
4
7
5
5
9
</p>
<p>0
:9
4
8
4
4
0
</p>
<p>0
:9
4
9
3
0
6
</p>
<p>0
:9
5
0
1
5
8
</p>
<p>0
:9
5
0
9
9
6
</p>
<p>0
:9
5
1
8
2
1
</p>
<p>0
:9
5
2
6
3
2
</p>
<p>0
:9
5
3
4
3
0
</p>
<p>0
:9
5
4
2
1
5
</p>
<p>0
:9
5
4
9
8
7
</p>
<p>2
:3
</p>
<p>0
:9
5
5
7
4
6
</p>
<p>0
:9
5
6
4
9
3
</p>
<p>0
:9
5
7
2
2
8
</p>
<p>0
:9
5
7
9
5
0
</p>
<p>0
:9
5
8
6
6
1
</p>
<p>0
:9
5
9
3
6
0
</p>
<p>0
:9
6
0
0
4
7
</p>
<p>0
:9
6
0
7
2
3
</p>
<p>0
:9
6
1
3
8
8
</p>
<p>0
:9
6
2
0
4
2
</p>
<p>2
:4
</p>
<p>0
:9
6
2
6
8
5
</p>
<p>0
:9
6
3
3
1
7
</p>
<p>0
:9
6
3
9
3
9
</p>
<p>0
:9
6
4
5
5
0
</p>
<p>0
:9
6
5
1
5
1
</p>
<p>0
:9
6
5
7
4
3
</p>
<p>0
:9
6
6
3
2
4
</p>
<p>0
:9
6
6
8
9
6
</p>
<p>0
:9
6
7
4
5
8
</p>
<p>0
:9
6
8
0
1
0
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>302 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
0
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
6
8
5
5
4
</p>
<p>0
:9
6
9
0
8
8
</p>
<p>0
:9
6
9
6
1
3
</p>
<p>0
:9
7
0
1
3
0
</p>
<p>0
:9
7
0
6
3
7
</p>
<p>0
:9
7
1
1
3
6
</p>
<p>0
:9
7
1
6
2
7
</p>
<p>0
:9
7
2
1
1
0
</p>
<p>0
:9
7
2
5
8
4
</p>
<p>0
:9
7
3
0
5
0
</p>
<p>2
:6
</p>
<p>0
:9
7
3
5
0
9
</p>
<p>0
:9
7
3
9
6
0
</p>
<p>0
:9
7
4
4
0
3
</p>
<p>0
:9
7
4
8
3
8
</p>
<p>0
:9
7
5
2
6
6
</p>
<p>0
:9
7
5
6
8
7
</p>
<p>0
:9
7
6
1
0
1
</p>
<p>0
:9
7
6
5
0
8
</p>
<p>0
:9
7
6
9
0
8
</p>
<p>0
:9
7
7
3
0
1
</p>
<p>2
:7
</p>
<p>0
:9
7
7
6
8
7
</p>
<p>0
:9
7
8
0
6
7
</p>
<p>0
:9
7
8
4
4
0
</p>
<p>0
:9
7
8
8
0
7
</p>
<p>0
:9
7
9
1
6
8
</p>
<p>0
:9
7
9
5
2
2
</p>
<p>0
:9
7
9
8
7
1
</p>
<p>0
:9
8
0
2
1
3
</p>
<p>0
:9
8
0
5
5
0
</p>
<p>0
:9
8
0
8
8
1
</p>
<p>2
:8
</p>
<p>0
:9
8
1
2
0
6
</p>
<p>0
:9
8
1
5
2
6
</p>
<p>0
:9
8
1
8
4
0
</p>
<p>0
:9
8
2
1
4
9
</p>
<p>0
:9
8
2
4
5
2
</p>
<p>0
:9
8
2
7
5
0
</p>
<p>0
:9
8
3
0
4
4
</p>
<p>0
:9
8
3
3
3
2
</p>
<p>0
:9
8
3
6
1
5
</p>
<p>0
:9
8
3
8
9
3
</p>
<p>2
:9
</p>
<p>0
:9
8
4
1
6
7
</p>
<p>0
:9
8
4
4
3
6
</p>
<p>0
:9
8
4
7
0
0
</p>
<p>0
:9
8
4
9
6
0
</p>
<p>0
:9
8
5
2
1
5
</p>
<p>0
:9
8
5
4
6
6
</p>
<p>0
:9
8
5
7
1
2
</p>
<p>0
:9
8
5
9
5
5
</p>
<p>0
:9
8
6
1
9
3
</p>
<p>0
:9
8
6
4
2
7
</p>
<p>3
:0
</p>
<p>0
:9
8
6
6
5
7
</p>
<p>0
:9
8
6
8
8
3
</p>
<p>0
:9
8
7
1
0
5
</p>
<p>0
:9
8
7
3
2
3
</p>
<p>0
:9
8
7
5
3
8
</p>
<p>0
:9
8
7
7
4
9
</p>
<p>0
:9
8
7
9
5
6
</p>
<p>0
:9
8
8
1
6
0
</p>
<p>0
:9
8
8
3
6
0
</p>
<p>0
:9
8
8
5
5
6
</p>
<p>3
:1
</p>
<p>0
:9
8
8
7
5
0
</p>
<p>0
:9
8
8
9
4
0
</p>
<p>0
:9
8
9
1
2
6
</p>
<p>0
:9
8
9
3
1
0
</p>
<p>0
:9
8
9
4
9
0
</p>
<p>0
:9
8
9
6
6
7
</p>
<p>0
:9
8
9
8
4
2
</p>
<p>0
:9
9
0
0
1
3
</p>
<p>0
:9
9
0
1
8
1
</p>
<p>0
:9
9
0
3
4
6
</p>
<p>3
:2
</p>
<p>0
:9
9
0
5
0
9
</p>
<p>0
:9
9
0
6
6
8
</p>
<p>0
:9
9
0
8
2
5
</p>
<p>0
:9
9
0
9
7
9
</p>
<p>0
:9
9
1
1
3
1
</p>
<p>0
:9
9
1
2
8
0
</p>
<p>0
:9
9
1
4
2
6
</p>
<p>0
:9
9
1
5
7
0
</p>
<p>0
:9
9
1
7
1
1
</p>
<p>0
:9
9
1
8
5
0
</p>
<p>3
:3
</p>
<p>0
:9
9
1
9
8
7
</p>
<p>0
:9
9
2
1
2
1
</p>
<p>0
:9
9
2
2
5
3
</p>
<p>0
:9
9
2
3
8
3
</p>
<p>0
:9
9
2
5
1
0
</p>
<p>0
:9
9
2
6
3
5
</p>
<p>0
:9
9
2
7
5
8
</p>
<p>0
:9
9
2
8
7
9
</p>
<p>0
:9
9
2
9
9
8
</p>
<p>0
:9
9
3
1
1
5
</p>
<p>3
:4
</p>
<p>0
:9
9
3
2
2
9
</p>
<p>0
:9
9
3
3
4
2
</p>
<p>0
:9
9
3
4
5
3
</p>
<p>0
:9
9
3
5
6
2
</p>
<p>0
:9
9
3
6
6
9
</p>
<p>0
:9
9
3
7
7
4
</p>
<p>0
:9
9
3
8
7
8
</p>
<p>0
:9
9
3
9
7
9
</p>
<p>0
:9
9
4
0
7
9
</p>
<p>0
:9
9
4
1
7
7
</p>
<p>3
:5
</p>
<p>0
:9
9
4
2
7
4
</p>
<p>0
:9
9
4
3
6
9
</p>
<p>0
:9
9
4
4
6
2
</p>
<p>0
:9
9
4
5
5
4
</p>
<p>0
:9
9
4
6
4
4
</p>
<p>0
:9
9
4
7
3
2
</p>
<p>0
:9
9
4
8
1
9
</p>
<p>0
:9
9
4
9
0
5
</p>
<p>0
:9
9
4
9
8
9
</p>
<p>0
:9
9
5
0
7
1
</p>
<p>3
:6
</p>
<p>0
:9
9
5
1
5
3
</p>
<p>0
:9
9
5
2
3
2
</p>
<p>0
:9
9
5
3
1
1
</p>
<p>0
:9
9
5
3
8
8
</p>
<p>0
:9
9
5
4
6
4
</p>
<p>0
:9
9
5
5
3
8
</p>
<p>0
:9
9
5
6
1
1
</p>
<p>0
:9
9
5
6
8
3
</p>
<p>0
:9
9
5
7
5
4
</p>
<p>0
:9
9
5
8
2
4
</p>
<p>3
:7
</p>
<p>0
:9
9
5
8
9
2
</p>
<p>0
:9
9
5
9
5
9
</p>
<p>0
:9
9
6
0
2
5
</p>
<p>0
:9
9
6
0
9
0
</p>
<p>0
:9
9
6
1
5
4
</p>
<p>0
:9
9
6
2
1
7
</p>
<p>0
:9
9
6
2
7
8
</p>
<p>0
:9
9
6
3
3
9
</p>
<p>0
:9
9
6
3
9
8
</p>
<p>0
:9
9
6
4
5
7
</p>
<p>3
:8
</p>
<p>0
:9
9
6
5
1
5
</p>
<p>0
:9
9
6
5
7
1
</p>
<p>0
:9
9
6
6
2
7
</p>
<p>0
:9
9
6
6
8
2
</p>
<p>0
:9
9
6
7
3
5
</p>
<p>0
:9
9
6
7
8
8
</p>
<p>0
:9
9
6
8
4
0
</p>
<p>0
:9
9
6
8
9
1
</p>
<p>0
:9
9
6
9
4
1
</p>
<p>0
:9
9
6
9
9
1
</p>
<p>3
:9
</p>
<p>0
:9
9
7
0
3
9
</p>
<p>0
:9
9
7
0
8
7
</p>
<p>0
:9
9
7
1
3
4
</p>
<p>0
:9
9
7
1
8
0
</p>
<p>0
:9
9
7
2
2
6
</p>
<p>0
:9
9
7
2
7
0
</p>
<p>0
:9
9
7
3
1
4
</p>
<p>0
:9
9
7
3
5
7
</p>
<p>0
:9
9
7
3
9
9
</p>
<p>0
:9
9
7
4
4
1
</p>
<p>4
:0
</p>
<p>0
:9
9
7
4
8
2
</p>
<p>0
:9
9
7
5
2
2
</p>
<p>0
:9
9
7
5
6
2
</p>
<p>0
:9
9
7
6
0
1
</p>
<p>0
:9
9
7
6
3
9
</p>
<p>0
:9
9
7
6
7
7
</p>
<p>0
:9
9
7
7
1
4
</p>
<p>0
:9
9
7
7
5
0
</p>
<p>0
:9
9
7
7
8
6
</p>
<p>0
:9
9
7
8
2
1
</p>
<p>4
:1
</p>
<p>0
:9
9
7
8
5
6
</p>
<p>0
:9
9
7
8
9
0
</p>
<p>0
:9
9
7
9
2
3
</p>
<p>0
:9
9
7
9
5
6
</p>
<p>0
:9
9
7
9
8
9
</p>
<p>0
:9
9
8
0
2
0
</p>
<p>0
:9
9
8
0
5
2
</p>
<p>0
:9
9
8
0
8
2
</p>
<p>0
:9
9
8
1
1
3
</p>
<p>0
:9
9
8
1
4
2
</p>
<p>4
:2
</p>
<p>0
:9
9
8
1
7
2
</p>
<p>0
:9
9
8
2
0
1
</p>
<p>0
:9
9
8
2
2
9
</p>
<p>0
:9
9
8
2
5
7
</p>
<p>0
:9
9
8
2
8
4
</p>
<p>0
:9
9
8
3
1
1
</p>
<p>0
:9
9
8
3
3
7
</p>
<p>0
:9
9
8
3
6
3
</p>
<p>0
:9
9
8
3
8
9
</p>
<p>0
:9
9
8
4
1
4
</p>
<p>4
:3
</p>
<p>0
:9
9
8
4
3
9
</p>
<p>0
:9
9
8
4
6
3
</p>
<p>0
:9
9
8
4
8
7
</p>
<p>0
:9
9
8
5
1
1
</p>
<p>0
:9
9
8
5
3
4
</p>
<p>0
:9
9
8
5
5
7
</p>
<p>0
:9
9
8
5
7
9
</p>
<p>0
:9
9
8
6
0
1
</p>
<p>0
:9
9
8
6
2
3
</p>
<p>0
:9
9
8
6
4
4
</p>
<p>4
:4
</p>
<p>0
:9
9
8
6
6
5
</p>
<p>0
:9
9
8
6
8
6
</p>
<p>0
:9
9
8
7
0
6
</p>
<p>0
:9
9
8
7
2
6
</p>
<p>0
:9
9
8
7
4
6
</p>
<p>0
:9
9
8
7
6
5
</p>
<p>0
:9
9
8
7
8
4
</p>
<p>0
:9
9
8
8
0
3
</p>
<p>0
:9
9
8
8
2
1
</p>
<p>0
:9
9
8
8
4
0
</p>
<p>4
:5
</p>
<p>0
:9
9
8
8
5
7
</p>
<p>0
:9
9
8
8
7
5
</p>
<p>0
:9
9
8
8
9
2
</p>
<p>0
:9
9
8
9
0
9
</p>
<p>0
:9
9
8
9
2
6
</p>
<p>0
:9
9
8
9
4
2
</p>
<p>0
:9
9
8
9
5
8
</p>
<p>0
:9
9
8
9
7
4
</p>
<p>0
:9
9
8
9
9
0
</p>
<p>0
:9
9
9
0
0
5
</p>
<p>4
:6
</p>
<p>0
:9
9
9
0
2
0
</p>
<p>0
:9
9
9
0
3
5
</p>
<p>0
:9
9
9
0
5
0
</p>
<p>0
:9
9
9
0
6
4
</p>
<p>0
:9
9
9
0
7
8
</p>
<p>0
:9
9
9
0
9
2
</p>
<p>0
:9
9
9
1
0
6
</p>
<p>0
:9
9
9
1
2
0
</p>
<p>0
:9
9
9
1
3
3
</p>
<p>0
:9
9
9
1
4
6
</p>
<p>4
:7
</p>
<p>0
:9
9
9
1
5
9
</p>
<p>0
:9
9
9
1
7
2
</p>
<p>0
:9
9
9
1
8
4
</p>
<p>0
:9
9
9
1
9
6
</p>
<p>0
:9
9
9
2
0
8
</p>
<p>0
:9
9
9
2
2
0
</p>
<p>0
:9
9
9
2
3
2
</p>
<p>0
:9
9
9
2
4
3
</p>
<p>0
:9
9
9
2
5
5
</p>
<p>0
:9
9
9
2
6
6
</p>
<p>4
:8
</p>
<p>0
:9
9
9
2
7
7
</p>
<p>0
:9
9
9
2
8
8
</p>
<p>0
:9
9
9
2
9
8
</p>
<p>0
:9
9
9
3
0
9
</p>
<p>0
:9
9
9
3
1
9
</p>
<p>0
:9
9
9
3
2
9
</p>
<p>0
:9
9
9
3
3
9
</p>
<p>0
:9
9
9
3
4
9
</p>
<p>0
:9
9
9
3
5
8
</p>
<p>0
:9
9
9
3
6
8
</p>
<p>4
:9
</p>
<p>0
:9
9
9
3
7
7
</p>
<p>0
:9
9
9
3
8
7
</p>
<p>0
:9
9
9
3
9
6
</p>
<p>0
:9
9
9
4
0
4
</p>
<p>0
:9
9
9
4
1
3
</p>
<p>0
:9
9
9
4
2
2
</p>
<p>0
:9
9
9
4
3
0
</p>
<p>0
:9
9
9
4
3
9
</p>
<p>0
:9
9
9
4
4
7
</p>
<p>0
:9
9
9
4
5
5</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 303
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
1
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
</p>
<p>=
20
</p>
<p>),
or
</p>
<p>pr
ob
</p>
<p>ab
il
</p>
<p>it
y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
8
8
0
</p>
<p>0
:0
1
5
7
5
8
</p>
<p>0
:0
2
3
6
3
6
</p>
<p>0
:0
3
1
5
1
0
</p>
<p>0
:0
3
9
3
8
2
</p>
<p>0
:0
4
7
2
4
9
</p>
<p>0
:0
5
5
1
1
1
</p>
<p>0
:0
6
2
9
6
8
</p>
<p>0
:0
7
0
8
1
8
</p>
<p>0
:1
</p>
<p>0
:0
7
8
6
6
0
</p>
<p>0
:0
8
6
4
9
4
</p>
<p>0
:0
9
4
3
2
0
</p>
<p>0
:1
0
2
1
3
5
</p>
<p>0
:1
0
9
9
4
0
</p>
<p>0
:1
1
7
7
3
3
</p>
<p>0
:1
2
5
5
1
4
</p>
<p>0
:1
3
3
2
8
2
</p>
<p>0
:1
4
1
0
3
6
</p>
<p>0
:1
4
8
7
7
6
</p>
<p>0
:2
</p>
<p>0
:1
5
6
5
0
0
</p>
<p>0
:1
6
4
2
0
8
</p>
<p>0
:1
7
1
8
9
9
</p>
<p>0
:1
7
9
5
7
2
</p>
<p>0
:1
8
7
2
2
7
</p>
<p>0
:1
9
4
8
6
3
</p>
<p>0
:2
0
2
4
7
8
</p>
<p>0
:2
1
0
0
7
3
</p>
<p>0
:2
1
7
6
4
7
</p>
<p>0
:2
2
5
1
9
9
</p>
<p>0
:3
</p>
<p>0
:2
3
2
7
2
7
</p>
<p>0
:2
4
0
2
3
2
</p>
<p>0
:2
4
7
7
1
3
</p>
<p>0
:2
5
5
1
6
9
</p>
<p>0
:2
6
2
5
9
9
</p>
<p>0
:2
7
0
0
0
3
</p>
<p>0
:2
7
7
3
7
9
</p>
<p>0
:2
8
4
7
2
8
</p>
<p>0
:2
9
2
0
4
9
</p>
<p>0
:2
9
9
3
4
1
</p>
<p>0
:4
</p>
<p>0
:3
0
6
6
0
4
</p>
<p>0
:3
1
3
8
3
6
</p>
<p>0
:3
2
1
0
3
7
</p>
<p>0
:3
2
8
2
0
7
</p>
<p>0
:3
3
5
3
4
5
</p>
<p>0
:3
4
2
4
5
0
</p>
<p>0
:3
4
9
5
2
2
</p>
<p>0
:3
5
6
5
6
1
</p>
<p>0
:3
6
3
5
6
5
</p>
<p>0
:3
7
0
5
3
4
</p>
<p>0
:5
</p>
<p>0
:3
7
7
4
6
8
</p>
<p>0
:3
8
4
3
6
6
</p>
<p>0
:3
9
1
2
2
8
</p>
<p>0
:3
9
8
0
5
3
</p>
<p>0
:4
0
4
8
4
1
</p>
<p>0
:4
1
1
5
9
1
</p>
<p>0
:4
1
8
3
0
2
</p>
<p>0
:4
2
4
9
7
5
</p>
<p>0
:4
3
1
6
0
8
</p>
<p>0
:4
3
8
2
0
2
</p>
<p>0
:6
</p>
<p>0
:4
4
4
7
5
6
</p>
<p>0
:4
5
1
2
6
9
</p>
<p>0
:4
5
7
7
4
2
</p>
<p>0
:4
6
4
1
7
4
</p>
<p>0
:4
7
0
5
6
3
</p>
<p>0
:4
7
6
9
1
1
</p>
<p>0
:4
8
3
2
1
7
</p>
<p>0
:4
8
9
4
8
0
</p>
<p>0
:4
9
5
7
0
0
</p>
<p>0
:5
0
1
8
7
7
</p>
<p>0
:7
</p>
<p>0
:5
0
8
0
1
0
</p>
<p>0
:5
1
4
0
9
9
</p>
<p>0
:5
2
0
1
4
5
</p>
<p>0
:5
2
6
1
4
6
</p>
<p>0
:5
3
2
1
0
2
</p>
<p>0
:5
3
8
0
1
3
</p>
<p>0
:5
4
3
8
7
9
</p>
<p>0
:5
4
9
7
0
0
</p>
<p>0
:5
5
5
4
7
6
</p>
<p>0
:5
6
1
2
0
6
</p>
<p>0
:8
</p>
<p>0
:5
6
6
8
8
9
</p>
<p>0
:5
7
2
5
2
7
</p>
<p>0
:5
7
8
1
1
9
</p>
<p>0
:5
8
3
6
6
4
</p>
<p>0
:5
8
9
1
6
2
</p>
<p>0
:5
9
4
6
1
4
</p>
<p>0
:6
0
0
0
1
9
</p>
<p>0
:6
0
5
3
7
8
</p>
<p>0
:6
1
0
6
8
9
</p>
<p>0
:6
1
5
9
5
3
</p>
<p>0
:9
</p>
<p>0
:6
2
1
1
7
1
</p>
<p>0
:6
2
6
3
4
1
</p>
<p>0
:6
3
1
4
6
3
</p>
<p>0
:6
3
6
5
3
9
</p>
<p>0
:6
4
1
5
6
7
</p>
<p>0
:6
4
6
5
4
8
</p>
<p>0
:6
5
1
4
8
2
</p>
<p>0
:6
5
6
3
6
8
</p>
<p>0
:6
6
1
2
0
7
</p>
<p>0
:6
6
5
9
9
9
</p>
<p>1
:0
</p>
<p>0
:6
7
0
7
4
4
</p>
<p>0
:6
7
5
4
4
1
</p>
<p>0
:6
8
0
0
9
1
</p>
<p>0
:6
8
4
6
9
4
</p>
<p>0
:6
8
9
2
5
0
</p>
<p>0
:6
9
3
7
5
9
</p>
<p>0
:6
9
8
2
2
2
</p>
<p>0
:7
0
2
6
3
7
</p>
<p>0
:7
0
7
0
0
6
</p>
<p>0
:7
1
1
3
2
8
</p>
<p>1
:1
</p>
<p>0
:7
1
5
6
0
4
</p>
<p>0
:7
1
9
8
3
3
</p>
<p>0
:7
2
4
0
1
6
</p>
<p>0
:7
2
8
1
5
4
</p>
<p>0
:7
3
2
2
4
5
</p>
<p>0
:7
3
6
2
9
1
</p>
<p>0
:7
4
0
2
9
1
</p>
<p>0
:7
4
4
2
4
5
</p>
<p>0
:7
4
8
1
5
5
</p>
<p>0
:7
5
2
0
1
9
</p>
<p>1
:2
</p>
<p>0
:7
5
5
8
3
9
</p>
<p>0
:7
5
9
6
1
4
</p>
<p>0
:7
6
3
3
4
4
</p>
<p>0
:7
6
7
0
3
1
</p>
<p>0
:7
7
0
6
7
3
</p>
<p>0
:7
7
4
2
7
1
</p>
<p>0
:7
7
7
8
2
6
</p>
<p>0
:7
8
1
3
3
8
</p>
<p>0
:7
8
4
8
0
7
</p>
<p>0
:7
8
8
2
3
3
</p>
<p>1
:3
</p>
<p>0
:7
9
1
6
1
6
</p>
<p>0
:7
9
4
9
5
7
</p>
<p>0
:7
9
8
2
5
6
</p>
<p>0
:8
0
1
5
1
3
</p>
<p>0
:8
0
4
7
2
8
</p>
<p>0
:8
0
7
9
0
3
</p>
<p>0
:8
1
1
0
3
6
</p>
<p>0
:8
1
4
1
2
9
</p>
<p>0
:8
1
7
1
8
1
</p>
<p>0
:8
2
0
1
9
3
</p>
<p>1
:4
</p>
<p>0
:8
2
3
1
6
5
</p>
<p>0
:8
2
6
0
9
8
</p>
<p>0
:8
2
8
9
9
2
</p>
<p>0
:8
3
1
8
4
6
</p>
<p>0
:8
3
4
6
6
2
</p>
<p>0
:8
3
7
4
4
0
</p>
<p>0
:8
4
0
1
8
0
</p>
<p>0
:8
4
2
8
8
2
</p>
<p>0
:8
4
5
5
4
6
</p>
<p>0
:8
4
8
1
7
4
</p>
<p>1
:5
</p>
<p>0
:8
5
0
7
6
5
</p>
<p>0
:8
5
3
3
1
9
</p>
<p>0
:8
5
5
8
3
7
</p>
<p>0
:8
5
8
3
2
0
</p>
<p>0
:8
6
0
7
6
7
</p>
<p>0
:8
6
3
1
7
9
</p>
<p>0
:8
6
5
5
5
6
</p>
<p>0
:8
6
7
8
9
9
</p>
<p>0
:8
7
0
2
0
7
</p>
<p>0
:8
7
2
4
8
2
</p>
<p>1
:6
</p>
<p>0
:8
7
4
7
2
3
</p>
<p>0
:8
7
6
9
3
2
</p>
<p>0
:8
7
9
1
0
7
</p>
<p>0
:8
8
1
2
5
0
</p>
<p>0
:8
8
3
3
6
1
</p>
<p>0
:8
8
5
4
4
0
</p>
<p>0
:8
8
7
4
8
7
</p>
<p>0
:8
8
9
5
0
3
</p>
<p>0
:8
9
1
4
8
9
</p>
<p>0
:8
9
3
4
4
4
</p>
<p>1
:7
</p>
<p>0
:8
9
5
3
6
9
</p>
<p>0
:8
9
7
2
6
4
</p>
<p>0
:8
9
9
1
3
0
</p>
<p>0
:9
0
0
9
6
7
</p>
<p>0
:9
0
2
7
7
5
</p>
<p>0
:9
0
4
5
5
4
</p>
<p>0
:9
0
6
3
0
5
</p>
<p>0
:9
0
8
0
2
9
</p>
<p>0
:9
0
9
7
2
5
</p>
<p>0
:9
1
1
3
9
4
</p>
<p>1
:8
</p>
<p>0
:9
1
3
0
3
6
</p>
<p>0
:9
1
4
6
5
1
</p>
<p>0
:9
1
6
2
4
1
</p>
<p>0
:9
1
7
8
0
4
</p>
<p>0
:9
1
9
3
4
2
</p>
<p>0
:9
2
0
8
5
5
</p>
<p>0
:9
2
2
3
4
3
</p>
<p>0
:9
2
3
8
0
6
</p>
<p>0
:9
2
5
2
4
5
</p>
<p>0
:9
2
6
6
6
0
</p>
<p>1
:9
</p>
<p>0
:9
2
8
0
5
2
</p>
<p>0
:9
2
9
4
2
0
</p>
<p>0
:9
3
0
7
6
5
</p>
<p>0
:9
3
2
0
8
8
</p>
<p>0
:9
3
3
3
8
8
</p>
<p>0
:9
3
4
6
6
6
</p>
<p>0
:9
3
5
9
2
2
</p>
<p>0
:9
3
7
1
5
7
</p>
<p>0
:9
3
8
3
7
0
</p>
<p>0
:9
3
9
5
6
3
</p>
<p>2
:0
</p>
<p>0
:9
4
0
7
3
5
</p>
<p>0
:9
4
1
8
8
6
</p>
<p>0
:9
4
3
0
1
8
</p>
<p>0
:9
4
4
1
3
0
</p>
<p>0
:9
4
5
2
2
2
</p>
<p>0
:9
4
6
2
9
5
</p>
<p>0
:9
4
7
3
5
0
</p>
<p>0
:9
4
8
3
8
5
</p>
<p>0
:9
4
9
4
0
2
</p>
<p>0
:9
5
0
4
0
2
</p>
<p>2
:1
</p>
<p>0
:9
5
1
3
8
3
</p>
<p>0
:9
5
2
3
4
7
</p>
<p>0
:9
5
3
2
9
3
</p>
<p>0
:9
5
4
2
2
2
</p>
<p>0
:9
5
5
1
3
5
</p>
<p>0
:9
5
6
0
3
1
</p>
<p>0
:9
5
6
9
1
1
</p>
<p>0
:9
5
7
7
7
4
</p>
<p>0
:9
5
8
6
2
2
</p>
<p>0
:9
5
9
4
5
5
</p>
<p>2
:2
</p>
<p>0
:9
6
0
2
7
2
</p>
<p>0
:9
6
1
0
7
4
</p>
<p>0
:9
6
1
8
6
1
</p>
<p>0
:9
6
2
6
3
4
</p>
<p>0
:9
6
3
3
9
2
</p>
<p>0
:9
6
4
1
3
6
</p>
<p>0
:9
6
4
8
6
6
</p>
<p>0
:9
6
5
5
8
3
</p>
<p>0
:9
6
6
2
8
6
</p>
<p>0
:9
6
6
9
7
6
</p>
<p>2
:3
</p>
<p>0
:9
6
7
6
5
3
</p>
<p>0
:9
6
8
3
1
7
</p>
<p>0
:9
6
8
9
6
9
</p>
<p>0
:9
6
9
6
0
8
</p>
<p>0
:9
7
0
2
3
5
</p>
<p>0
:9
7
0
8
5
0
</p>
<p>0
:9
7
1
4
5
3
</p>
<p>0
:9
7
2
0
4
4
</p>
<p>0
:9
7
2
6
2
4
</p>
<p>0
:9
7
3
1
9
4
</p>
<p>2
:4
</p>
<p>0
:9
7
3
7
5
2
</p>
<p>0
:9
7
4
2
9
9
</p>
<p>0
:9
7
4
8
3
5
</p>
<p>0
:9
7
5
3
6
1
</p>
<p>0
:9
7
5
8
7
7
</p>
<p>0
:9
7
6
3
8
3
</p>
<p>0
:9
7
6
8
7
9
</p>
<p>0
:9
7
7
3
6
5
</p>
<p>0
:9
7
7
8
4
2
</p>
<p>0
:9
7
8
3
0
9
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>304 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
1
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
7
8
7
6
7
</p>
<p>0
:9
7
9
2
1
6
</p>
<p>0
:9
7
9
6
5
6
</p>
<p>0
:9
8
0
0
8
7
</p>
<p>0
:9
8
0
5
1
0
</p>
<p>0
:9
8
0
9
2
4
</p>
<p>0
:9
8
1
3
3
0
</p>
<p>0
:9
8
1
7
2
7
</p>
<p>0
:9
8
2
1
1
7
</p>
<p>0
:9
8
2
4
9
9
</p>
<p>2
:6
</p>
<p>0
:9
8
2
8
7
3
</p>
<p>0
:9
8
3
2
4
0
</p>
<p>0
:9
8
3
5
9
9
</p>
<p>0
:9
8
3
9
5
1
</p>
<p>0
:9
8
4
2
9
6
</p>
<p>0
:9
8
4
6
3
4
</p>
<p>0
:9
8
4
9
6
5
</p>
<p>0
:9
8
5
2
8
9
</p>
<p>0
:9
8
5
6
0
7
</p>
<p>0
:9
8
5
9
1
8
</p>
<p>2
:7
</p>
<p>0
:9
8
6
2
2
2
</p>
<p>0
:9
8
6
5
2
1
</p>
<p>0
:9
8
6
8
1
3
</p>
<p>0
:9
8
7
0
9
9
</p>
<p>0
:9
8
7
3
8
0
</p>
<p>0
:9
8
7
6
5
4
</p>
<p>0
:9
8
7
9
2
3
</p>
<p>0
:9
8
8
1
8
6
</p>
<p>0
:9
8
8
4
4
4
</p>
<p>0
:9
8
8
6
9
6
</p>
<p>2
:8
</p>
<p>0
:9
8
8
9
4
3
</p>
<p>0
:9
8
9
1
8
5
</p>
<p>0
:9
8
9
4
2
2
</p>
<p>0
:9
8
9
6
5
4
</p>
<p>0
:9
8
9
8
8
1
</p>
<p>0
:9
9
0
1
0
3
</p>
<p>0
:9
9
0
3
2
1
</p>
<p>0
:9
9
0
5
3
4
</p>
<p>0
:9
9
0
7
4
3
</p>
<p>0
:9
9
0
9
4
7
</p>
<p>2
:9
</p>
<p>0
:9
9
1
1
4
6
</p>
<p>0
:9
9
1
3
4
2
</p>
<p>0
:9
9
1
5
3
3
</p>
<p>0
:9
9
1
7
2
1
</p>
<p>0
:9
9
1
9
0
4
</p>
<p>0
:9
9
2
0
8
3
</p>
<p>0
:9
9
2
2
5
9
</p>
<p>0
:9
9
2
4
3
1
</p>
<p>0
:9
9
2
5
9
9
</p>
<p>0
:9
9
2
7
6
4
</p>
<p>3
:0
</p>
<p>0
:9
9
2
9
2
5
</p>
<p>0
:9
9
3
0
8
2
</p>
<p>0
:9
9
3
2
3
6
</p>
<p>0
:9
9
3
3
8
7
</p>
<p>0
:9
9
3
5
3
5
</p>
<p>0
:9
9
3
6
7
9
</p>
<p>0
:9
9
3
8
2
0
</p>
<p>0
:9
9
3
9
5
9
</p>
<p>0
:9
9
4
0
9
4
</p>
<p>0
:9
9
4
2
2
6
</p>
<p>3
:1
</p>
<p>0
:9
9
4
3
5
6
</p>
<p>0
:9
9
4
4
8
2
</p>
<p>0
:9
9
4
6
0
6
</p>
<p>0
:9
9
4
7
2
7
</p>
<p>0
:9
9
4
8
4
6
</p>
<p>0
:9
9
4
9
6
2
</p>
<p>0
:9
9
5
0
7
5
</p>
<p>0
:9
9
5
1
8
6
</p>
<p>0
:9
9
5
2
9
4
</p>
<p>0
:9
9
5
4
0
0
</p>
<p>3
:2
</p>
<p>0
:9
9
5
5
0
4
</p>
<p>0
:9
9
5
6
0
6
</p>
<p>0
:9
9
5
7
0
5
</p>
<p>0
:9
9
5
8
0
2
</p>
<p>0
:9
9
5
8
9
7
</p>
<p>0
:9
9
5
9
9
0
</p>
<p>0
:9
9
6
0
8
1
</p>
<p>0
:9
9
6
1
6
9
</p>
<p>0
:9
9
6
2
5
6
</p>
<p>0
:9
9
6
3
4
1
</p>
<p>3
:3
</p>
<p>0
:9
9
6
4
2
4
</p>
<p>0
:9
9
6
5
0
5
</p>
<p>0
:9
9
6
5
8
5
</p>
<p>0
:9
9
6
6
6
2
</p>
<p>0
:9
9
6
7
3
8
</p>
<p>0
:9
9
6
8
1
2
</p>
<p>0
:9
9
6
8
8
5
</p>
<p>0
:9
9
6
9
5
6
</p>
<p>0
:9
9
7
0
2
5
</p>
<p>0
:9
9
7
0
9
3
</p>
<p>3
:4
</p>
<p>0
:9
9
7
1
5
9
</p>
<p>0
:9
9
7
2
2
4
</p>
<p>0
:9
9
7
2
8
7
</p>
<p>0
:9
9
7
3
4
9
</p>
<p>0
:9
9
7
4
1
0
</p>
<p>0
:9
9
7
4
6
9
</p>
<p>0
:9
9
7
5
2
7
</p>
<p>0
:9
9
7
5
8
3
</p>
<p>0
:9
9
7
6
3
8
</p>
<p>0
:9
9
7
6
9
2
</p>
<p>3
:5
</p>
<p>0
:9
9
7
7
4
5
</p>
<p>0
:9
9
7
7
9
7
</p>
<p>0
:9
9
7
8
4
7
</p>
<p>0
:9
9
7
8
9
7
</p>
<p>0
:9
9
7
9
4
5
</p>
<p>0
:9
9
7
9
9
2
</p>
<p>0
:9
9
8
0
3
8
</p>
<p>0
:9
9
8
0
8
3
</p>
<p>0
:9
9
8
1
2
7
</p>
<p>0
:9
9
8
1
7
0
</p>
<p>3
:6
</p>
<p>0
:9
9
8
2
1
2
</p>
<p>0
:9
9
8
2
5
3
</p>
<p>0
:9
9
8
2
9
3
</p>
<p>0
:9
9
8
3
3
3
</p>
<p>0
:9
9
8
3
7
1
</p>
<p>0
:9
9
8
4
0
8
</p>
<p>0
:9
9
8
4
4
5
</p>
<p>0
:9
9
8
4
8
1
</p>
<p>0
:9
9
8
5
1
6
</p>
<p>0
:9
9
8
5
5
0
</p>
<p>3
:7
</p>
<p>0
:9
9
8
5
8
3
</p>
<p>0
:9
9
8
6
1
6
</p>
<p>0
:9
9
8
6
4
8
</p>
<p>0
:9
9
8
6
7
9
</p>
<p>0
:9
9
8
7
0
9
</p>
<p>0
:9
9
8
7
3
9
</p>
<p>0
:9
9
8
7
6
8
</p>
<p>0
:9
9
8
7
9
7
</p>
<p>0
:9
9
8
8
2
4
</p>
<p>0
:9
9
8
8
5
1
</p>
<p>3
:8
</p>
<p>0
:9
9
8
8
7
8
</p>
<p>0
:9
9
8
9
0
4
</p>
<p>0
:9
9
8
9
2
9
</p>
<p>0
:9
9
8
9
5
4
</p>
<p>0
:9
9
8
9
7
8
</p>
<p>0
:9
9
9
0
0
2
</p>
<p>0
:9
9
9
0
2
5
</p>
<p>0
:9
9
9
0
4
7
</p>
<p>0
:9
9
9
0
6
9
</p>
<p>0
:9
9
9
0
9
1
</p>
<p>3
:9
</p>
<p>0
:9
9
9
1
1
2
</p>
<p>0
:9
9
9
1
3
2
</p>
<p>0
:9
9
9
1
5
2
</p>
<p>0
:9
9
9
1
7
2
</p>
<p>0
:9
9
9
1
9
1
</p>
<p>0
:9
9
9
2
1
0
</p>
<p>0
:9
9
9
2
2
8
</p>
<p>0
:9
9
9
2
4
6
</p>
<p>0
:9
9
9
2
6
3
</p>
<p>0
:9
9
9
2
8
0
</p>
<p>4
:0
</p>
<p>0
:9
9
9
2
9
7
</p>
<p>0
:9
9
9
3
1
3
</p>
<p>0
:9
9
9
3
2
9
</p>
<p>0
:9
9
9
3
4
5
</p>
<p>0
:9
9
9
3
6
0
</p>
<p>0
:9
9
9
3
7
5
</p>
<p>0
:9
9
9
3
8
9
</p>
<p>0
:9
9
9
4
0
3
</p>
<p>0
:9
9
9
4
1
7
</p>
<p>0
:9
9
9
4
3
0
</p>
<p>4
:1
</p>
<p>0
:9
9
9
4
4
4
</p>
<p>0
:9
9
9
4
5
6
</p>
<p>0
:9
9
9
4
6
9
</p>
<p>0
:9
9
9
4
8
1
</p>
<p>0
:9
9
9
4
9
3
</p>
<p>0
:9
9
9
5
0
5
</p>
<p>0
:9
9
9
5
1
6
</p>
<p>0
:9
9
9
5
2
8
</p>
<p>0
:9
9
9
5
3
9
</p>
<p>0
:9
9
9
5
4
9
</p>
<p>4
:2
</p>
<p>0
:9
9
9
5
6
0
</p>
<p>0
:9
9
9
5
7
0
</p>
<p>0
:9
9
9
5
8
0
</p>
<p>0
:9
9
9
5
9
0
</p>
<p>0
:9
9
9
5
9
9
</p>
<p>0
:9
9
9
6
0
8
</p>
<p>0
:9
9
9
6
1
7
</p>
<p>0
:9
9
9
6
2
6
</p>
<p>0
:9
9
9
6
3
5
</p>
<p>0
:9
9
9
6
4
3
</p>
<p>4
:3
</p>
<p>0
:9
9
9
6
5
2
</p>
<p>0
:9
9
9
6
6
0
</p>
<p>0
:9
9
9
6
6
7
</p>
<p>0
:9
9
9
6
7
5
</p>
<p>0
:9
9
9
6
8
3
</p>
<p>0
:9
9
9
6
9
0
</p>
<p>0
:9
9
9
6
9
7
</p>
<p>0
:9
9
9
7
0
4
</p>
<p>0
:9
9
9
7
1
1
</p>
<p>0
:9
9
9
7
1
8
</p>
<p>4
:4
</p>
<p>0
:9
9
9
7
2
4
</p>
<p>0
:9
9
9
7
3
1
</p>
<p>0
:9
9
9
7
3
7
</p>
<p>0
:9
9
9
7
4
3
</p>
<p>0
:9
9
9
7
4
9
</p>
<p>0
:9
9
9
7
5
5
</p>
<p>0
:9
9
9
7
6
0
</p>
<p>0
:9
9
9
7
6
6
</p>
<p>0
:9
9
9
7
7
1
</p>
<p>0
:9
9
9
7
7
6
</p>
<p>4
:5
</p>
<p>0
:9
9
9
7
8
2
</p>
<p>0
:9
9
9
7
8
7
</p>
<p>0
:9
9
9
7
9
2
</p>
<p>0
:9
9
9
7
9
6
</p>
<p>0
:9
9
9
8
0
1
</p>
<p>0
:9
9
9
8
0
6
</p>
<p>0
:9
9
9
8
1
0
</p>
<p>0
:9
9
9
8
1
5
</p>
<p>0
:9
9
9
8
1
9
</p>
<p>0
:9
9
9
8
2
3
</p>
<p>4
:6
</p>
<p>0
:9
9
9
8
2
7
</p>
<p>0
:9
9
9
8
3
1
</p>
<p>0
:9
9
9
8
3
5
</p>
<p>0
:9
9
9
8
3
9
</p>
<p>0
:9
9
9
8
4
2
</p>
<p>0
:9
9
9
8
4
6
</p>
<p>0
:9
9
9
8
5
0
</p>
<p>0
:9
9
9
8
5
3
</p>
<p>0
:9
9
9
8
5
6
</p>
<p>0
:9
9
9
8
6
0
</p>
<p>4
:7
</p>
<p>0
:9
9
9
8
6
3
</p>
<p>0
:9
9
9
8
6
6
</p>
<p>0
:9
9
9
8
6
9
</p>
<p>0
:9
9
9
8
7
2
</p>
<p>0
:9
9
9
8
7
5
</p>
<p>0
:9
9
9
8
7
8
</p>
<p>0
:9
9
9
8
8
1
</p>
<p>0
:9
9
9
8
8
4
</p>
<p>0
:9
9
9
8
8
6
</p>
<p>0
:9
9
9
8
8
9
</p>
<p>4
:8
</p>
<p>0
:9
9
9
8
9
1
</p>
<p>0
:9
9
9
8
9
4
</p>
<p>0
:9
9
9
8
9
6
</p>
<p>0
:9
9
9
8
9
9
</p>
<p>0
:9
9
9
9
0
1
</p>
<p>0
:9
9
9
9
0
3
</p>
<p>0
:9
9
9
9
0
6
</p>
<p>0
:9
9
9
9
0
8
</p>
<p>0
:9
9
9
9
1
0
</p>
<p>0
:9
9
9
9
1
2
</p>
<p>4
:9
</p>
<p>0
:9
9
9
9
1
4
</p>
<p>0
:9
9
9
9
1
6
</p>
<p>0
:9
9
9
9
1
8
</p>
<p>0
:9
9
9
9
2
0
</p>
<p>0
:9
9
9
9
2
2
</p>
<p>0
:9
9
9
9
2
3
</p>
<p>0
:9
9
9
9
2
5
</p>
<p>0
:9
9
9
9
2
7
</p>
<p>0
:9
9
9
9
2
8
</p>
<p>0
:9
9
9
9
3
0</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 305
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
2
</p>
<p>In
te
</p>
<p>gr
al
</p>
<p>of
St
</p>
<p>ud
en
</p>
<p>t&rsquo;s
fu
</p>
<p>nc
ti
</p>
<p>on
(f
D
</p>
<p>50
),
</p>
<p>or
pr
</p>
<p>ob
ab
</p>
<p>il
it
</p>
<p>y
p,
</p>
<p>as
fu
</p>
<p>nc
ti
</p>
<p>on
of
</p>
<p>cr
it
</p>
<p>ic
al
</p>
<p>va
lu
</p>
<p>e
T
cr
it
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>0
:0
</p>
<p>0
:0
0
0
0
0
0
</p>
<p>0
:0
0
7
9
3
9
</p>
<p>0
:0
1
5
8
7
7
</p>
<p>0
:0
2
3
8
1
4
</p>
<p>0
:0
3
1
7
4
8
</p>
<p>0
:0
3
9
6
7
8
</p>
<p>0
:0
4
7
6
0
5
</p>
<p>0
:0
5
5
5
2
7
</p>
<p>0
:0
6
3
4
4
3
</p>
<p>0
:0
7
1
3
5
3
</p>
<p>0
:1
</p>
<p>0
:0
7
9
2
5
6
</p>
<p>0
:0
8
7
1
5
0
</p>
<p>0
:0
9
5
0
3
6
</p>
<p>0
:1
0
2
9
1
2
</p>
<p>0
:1
1
0
7
7
8
</p>
<p>0
:1
1
8
6
3
2
</p>
<p>0
:1
2
6
4
7
4
</p>
<p>0
:1
3
4
3
0
4
</p>
<p>0
:1
4
2
1
2
0
</p>
<p>0
:1
4
9
9
2
2
</p>
<p>0
:2
</p>
<p>0
:1
5
7
7
0
8
</p>
<p>0
:1
6
5
4
7
9
</p>
<p>0
:1
7
3
2
3
3
</p>
<p>0
:1
8
0
9
7
0
</p>
<p>0
:1
8
8
6
8
9
</p>
<p>0
:1
9
6
3
8
8
</p>
<p>0
:2
0
4
0
6
9
</p>
<p>0
:2
1
1
7
2
9
</p>
<p>0
:2
1
9
3
6
7
</p>
<p>0
:2
2
6
9
8
5
</p>
<p>0
:3
</p>
<p>0
:2
3
4
5
7
9
</p>
<p>0
:2
4
2
1
5
1
</p>
<p>0
:2
4
9
6
9
8
</p>
<p>0
:2
5
7
2
2
1
</p>
<p>0
:2
6
4
7
1
9
</p>
<p>0
:2
7
2
1
9
1
</p>
<p>0
:2
7
9
6
3
7
</p>
<p>0
:2
8
7
0
5
5
</p>
<p>0
:2
9
4
4
4
5
</p>
<p>0
:3
0
1
8
0
7
</p>
<p>0
:4
</p>
<p>0
:3
0
9
1
4
0
</p>
<p>0
:3
1
6
4
4
3
</p>
<p>0
:3
2
3
7
1
5
</p>
<p>0
:3
3
0
9
5
7
</p>
<p>0
:3
3
8
1
6
7
</p>
<p>0
:3
4
5
3
4
5
</p>
<p>0
:3
5
2
4
9
0
</p>
<p>0
:3
5
9
6
0
2
</p>
<p>0
:3
6
6
6
8
0
</p>
<p>0
:3
7
3
7
2
3
</p>
<p>0
:5
</p>
<p>0
:3
8
0
7
3
2
</p>
<p>0
:3
8
7
7
0
5
</p>
<p>0
:3
9
4
6
4
2
</p>
<p>0
:4
0
1
5
4
2
</p>
<p>0
:4
0
8
4
0
6
</p>
<p>0
:4
1
5
2
3
2
</p>
<p>0
:4
2
2
0
2
0
</p>
<p>0
:4
2
8
7
7
0
</p>
<p>0
:4
3
5
4
8
1
</p>
<p>0
:4
4
2
1
5
2
</p>
<p>0
:6
</p>
<p>0
:4
4
8
7
8
4
</p>
<p>0
:4
5
5
3
7
6
</p>
<p>0
:4
6
1
9
2
7
</p>
<p>0
:4
6
8
4
3
7
</p>
<p>0
:4
7
4
9
0
6
</p>
<p>0
:4
8
1
3
3
3
</p>
<p>0
:4
8
7
7
1
7
</p>
<p>0
:4
9
4
0
6
0
</p>
<p>0
:5
0
0
3
5
9
</p>
<p>0
:5
0
6
6
1
6
</p>
<p>0
:7
</p>
<p>0
:5
1
2
8
2
9
</p>
<p>0
:5
1
8
9
9
8
</p>
<p>0
:5
2
5
1
2
3
</p>
<p>0
:5
3
1
2
0
4
</p>
<p>0
:5
3
7
2
4
0
</p>
<p>0
:5
4
3
2
3
1
</p>
<p>0
:5
4
9
1
7
7
</p>
<p>0
:5
5
5
0
7
7
</p>
<p>0
:5
6
0
9
3
2
</p>
<p>0
:5
6
6
7
4
2
</p>
<p>0
:8
</p>
<p>0
:5
7
2
5
0
5
</p>
<p>0
:5
7
8
2
2
2
</p>
<p>0
:5
8
3
8
9
2
</p>
<p>0
:5
8
9
5
1
6
</p>
<p>0
:5
9
5
0
9
3
</p>
<p>0
:6
0
0
6
2
3
</p>
<p>0
:6
0
6
1
0
6
</p>
<p>0
:6
1
1
5
4
2
</p>
<p>0
:6
1
6
9
3
1
</p>
<p>0
:6
2
2
2
7
2
</p>
<p>0
:9
</p>
<p>0
:6
2
7
5
6
6
</p>
<p>0
:6
3
2
8
1
2
</p>
<p>0
:6
3
8
0
1
0
</p>
<p>0
:6
4
3
1
6
1
</p>
<p>0
:6
4
8
2
6
3
</p>
<p>0
:6
5
3
3
1
8
</p>
<p>0
:6
5
8
3
2
6
</p>
<p>0
:6
6
3
2
8
5
</p>
<p>0
:6
6
8
1
9
6
</p>
<p>0
:6
7
3
0
5
9
</p>
<p>1
:0
</p>
<p>0
:6
7
7
8
7
5
</p>
<p>0
:6
8
2
6
4
2
</p>
<p>0
:6
8
7
3
6
2
</p>
<p>0
:6
9
2
0
3
3
</p>
<p>0
:6
9
6
6
5
7
</p>
<p>0
:7
0
1
2
3
3
</p>
<p>0
:7
0
5
7
6
2
</p>
<p>0
:7
1
0
2
4
3
</p>
<p>0
:7
1
4
6
7
6
</p>
<p>0
:7
1
9
0
6
2
</p>
<p>1
:1
</p>
<p>0
:7
2
3
4
0
0
</p>
<p>0
:7
2
7
6
9
1
</p>
<p>0
:7
3
1
9
3
5
</p>
<p>0
:7
3
6
1
3
2
</p>
<p>0
:7
4
0
2
8
2
</p>
<p>0
:7
4
4
3
8
6
</p>
<p>0
:7
4
8
4
4
2
</p>
<p>0
:7
5
2
4
5
2
</p>
<p>0
:7
5
6
4
1
6
</p>
<p>0
:7
6
0
3
3
4
</p>
<p>1
:2
</p>
<p>0
:7
6
4
2
0
6
</p>
<p>0
:7
6
8
0
3
2
</p>
<p>0
:7
7
1
8
1
2
</p>
<p>0
:7
7
5
5
4
7
</p>
<p>0
:7
7
9
2
3
7
</p>
<p>0
:7
8
2
8
8
2
</p>
<p>0
:7
8
6
4
8
2
</p>
<p>0
:7
9
0
0
3
7
</p>
<p>0
:7
9
3
5
4
8
</p>
<p>0
:7
9
7
0
1
5
</p>
<p>1
:3
</p>
<p>0
:8
0
0
4
3
8
</p>
<p>0
:8
0
3
8
1
8
</p>
<p>0
:8
0
7
1
5
4
</p>
<p>0
:8
1
0
4
4
7
</p>
<p>0
:8
1
3
6
9
7
</p>
<p>0
:8
1
6
9
0
4
</p>
<p>0
:8
2
0
0
7
0
</p>
<p>0
:8
2
3
1
9
3
</p>
<p>0
:8
2
6
2
7
4
</p>
<p>0
:8
2
9
3
1
4
</p>
<p>1
:4
</p>
<p>0
:8
3
2
3
1
2
</p>
<p>0
:8
3
5
2
7
0
</p>
<p>0
:8
3
8
1
8
7
</p>
<p>0
:8
4
1
0
6
4
</p>
<p>0
:8
4
3
9
0
1
</p>
<p>0
:8
4
6
6
9
8
</p>
<p>0
:8
4
9
4
5
5
</p>
<p>0
:8
5
2
1
7
4
</p>
<p>0
:8
5
4
8
5
3
</p>
<p>0
:8
5
7
4
9
5
</p>
<p>1
:5
</p>
<p>0
:8
6
0
0
9
8
</p>
<p>0
:8
6
2
6
6
3
</p>
<p>0
:8
6
5
1
9
0
</p>
<p>0
:8
6
7
6
8
1
</p>
<p>0
:8
7
0
1
3
5
</p>
<p>0
:8
7
2
5
5
2
</p>
<p>0
:8
7
4
9
3
3
</p>
<p>0
:8
7
7
2
7
8
</p>
<p>0
:8
7
9
5
8
7
</p>
<p>0
:8
8
1
8
6
2
</p>
<p>1
:6
</p>
<p>0
:8
8
4
1
0
1
</p>
<p>0
:8
8
6
3
0
6
</p>
<p>0
:8
8
8
4
7
7
</p>
<p>0
:8
9
0
6
1
4
</p>
<p>0
:8
9
2
7
1
8
</p>
<p>0
:8
9
4
7
8
8
</p>
<p>0
:8
9
6
8
2
6
</p>
<p>0
:8
9
8
8
3
1
</p>
<p>0
:9
0
0
8
0
5
</p>
<p>0
:9
0
2
7
4
6
</p>
<p>1
:7
</p>
<p>0
:9
0
4
6
5
6
</p>
<p>0
:9
0
6
5
3
5
</p>
<p>0
:9
0
8
3
8
3
</p>
<p>0
:9
1
0
2
0
1
</p>
<p>0
:9
1
1
9
8
9
</p>
<p>0
:9
1
3
7
4
7
</p>
<p>0
:9
1
5
4
7
6
</p>
<p>0
:9
1
7
1
7
6
</p>
<p>0
:9
1
8
8
4
7
</p>
<p>0
:9
2
0
4
9
0
</p>
<p>1
:8
</p>
<p>0
:9
2
2
1
0
5
</p>
<p>0
:9
2
3
6
9
3
</p>
<p>0
:9
2
5
2
5
3
</p>
<p>0
:9
2
6
7
8
6
</p>
<p>0
:9
2
8
2
9
3
</p>
<p>0
:9
2
9
7
7
3
</p>
<p>0
:9
3
1
2
2
7
</p>
<p>0
:9
3
2
6
5
6
</p>
<p>0
:9
3
4
0
6
0
</p>
<p>0
:9
3
5
4
3
9
</p>
<p>1
:9
</p>
<p>0
:9
3
6
7
9
3
</p>
<p>0
:9
3
8
1
2
3
</p>
<p>0
:9
3
9
4
2
9
</p>
<p>0
:9
4
0
7
1
1
</p>
<p>0
:9
4
1
9
7
0
</p>
<p>0
:9
4
3
2
0
6
</p>
<p>0
:9
4
4
4
2
0
</p>
<p>0
:9
4
5
6
1
1
</p>
<p>0
:9
4
6
7
8
0
</p>
<p>0
:9
4
7
9
2
7
</p>
<p>2
:0
</p>
<p>0
:9
4
9
0
5
3
</p>
<p>0
:9
5
0
1
5
8
</p>
<p>0
:9
5
1
2
4
3
</p>
<p>0
:9
5
2
3
0
6
</p>
<p>0
:9
5
3
3
5
0
</p>
<p>0
:9
5
4
3
7
4
</p>
<p>0
:9
5
5
3
7
8
</p>
<p>0
:9
5
6
3
6
3
</p>
<p>0
:9
5
7
3
2
9
</p>
<p>0
:9
5
8
2
7
6
</p>
<p>2
:1
</p>
<p>0
:9
5
9
2
0
5
</p>
<p>0
:9
6
0
1
1
6
</p>
<p>0
:9
6
1
0
0
9
</p>
<p>0
:9
6
1
8
8
4
</p>
<p>0
:9
6
2
7
4
2
</p>
<p>0
:9
6
3
5
8
4
</p>
<p>0
:9
6
4
4
0
8
</p>
<p>0
:9
6
5
2
1
6
</p>
<p>0
:9
6
6
0
0
8
</p>
<p>0
:9
6
6
7
8
4
</p>
<p>2
:2
</p>
<p>0
:9
6
7
5
4
4
</p>
<p>0
:9
6
8
2
8
9
</p>
<p>0
:9
6
9
0
1
9
</p>
<p>0
:9
6
9
7
3
4
</p>
<p>0
:9
7
0
4
3
4
</p>
<p>0
:9
7
1
1
2
0
</p>
<p>0
:9
7
1
7
9
1
</p>
<p>0
:9
7
2
4
4
9
</p>
<p>0
:9
7
3
0
9
3
</p>
<p>0
:9
7
3
7
2
4
</p>
<p>2
:3
</p>
<p>0
:9
7
4
3
4
1
</p>
<p>0
:9
7
4
9
4
6
</p>
<p>0
:9
7
5
5
3
8
</p>
<p>0
:9
7
6
1
1
7
</p>
<p>0
:9
7
6
6
8
4
</p>
<p>0
:9
7
7
2
3
9
</p>
<p>0
:9
7
7
7
8
2
</p>
<p>0
:9
7
8
3
1
3
</p>
<p>0
:9
7
8
8
3
3
</p>
<p>0
:9
7
9
3
4
2
</p>
<p>2
:4
</p>
<p>0
:9
7
9
8
4
0
</p>
<p>0
:9
8
0
3
2
7
</p>
<p>0
:9
8
0
8
0
3
</p>
<p>0
:9
8
1
2
6
9
</p>
<p>0
:9
8
1
7
2
5
</p>
<p>0
:9
8
2
1
7
1
</p>
<p>0
:9
8
2
6
0
7
</p>
<p>0
:9
8
3
0
3
3
</p>
<p>0
:9
8
3
4
5
0
</p>
<p>0
:9
8
3
8
5
7
</p>
<p>(c
on
</p>
<p>tin
ue
</p>
<p>d)</p>
<p/>
</div>
<div class="page"><p/>
<p>306 Appendix: Numerical Tables
</p>
<p>T
ab
</p>
<p>le
A
</p>
<p>.2
2
</p>
<p>(c
on
</p>
<p>ti
nu
</p>
<p>ed
)
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
5
</p>
<p>6
7
</p>
<p>8
9
</p>
<p>2
:5
</p>
<p>0
:9
8
4
2
5
5
</p>
<p>0
:9
8
4
6
4
5
</p>
<p>0
:9
8
5
0
2
6
</p>
<p>0
:9
8
5
3
9
8
</p>
<p>0
:9
8
5
7
6
1
</p>
<p>0
:9
8
6
1
1
7
</p>
<p>0
:9
8
6
4
6
4
</p>
<p>0
:9
8
6
8
0
4
</p>
<p>0
:9
8
7
1
3
5
</p>
<p>0
:9
8
7
4
5
9
</p>
<p>2
:6
</p>
<p>0
:9
8
7
7
7
6
</p>
<p>0
:9
8
8
0
8
5
</p>
<p>0
:9
8
8
3
8
7
</p>
<p>0
:9
8
8
6
8
3
</p>
<p>0
:9
8
8
9
7
1
</p>
<p>0
:9
8
9
2
5
2
</p>
<p>0
:9
8
9
5
2
7
</p>
<p>0
:9
8
9
7
9
6
</p>
<p>0
:9
9
0
0
5
8
</p>
<p>0
:9
9
0
3
1
4
</p>
<p>2
:7
</p>
<p>0
:9
9
0
5
6
4
</p>
<p>0
:9
9
0
8
0
7
</p>
<p>0
:9
9
1
0
4
6
</p>
<p>0
:9
9
1
2
7
8
</p>
<p>0
:9
9
1
5
0
5
</p>
<p>0
:9
9
1
7
2
6
</p>
<p>0
:9
9
1
9
4
2
</p>
<p>0
:9
9
2
1
5
3
</p>
<p>0
:9
9
2
3
5
9
</p>
<p>0
:9
9
2
5
6
0
</p>
<p>2
:8
</p>
<p>0
:9
9
2
7
5
6
</p>
<p>0
:9
9
2
9
4
7
</p>
<p>0
:9
9
3
1
3
3
</p>
<p>0
:9
9
3
3
1
5
</p>
<p>0
:9
9
3
4
9
3
</p>
<p>0
:9
9
3
6
6
6
</p>
<p>0
:9
9
3
8
3
5
</p>
<p>0
:9
9
3
9
9
9
</p>
<p>0
:9
9
4
1
6
0
</p>
<p>0
:9
9
4
3
1
6
</p>
<p>2
:9
</p>
<p>0
:9
9
4
4
6
9
</p>
<p>0
:9
9
4
6
1
8
</p>
<p>0
:9
9
4
7
6
3
</p>
<p>0
:9
9
4
9
0
4
</p>
<p>0
:9
9
5
0
4
2
</p>
<p>0
:9
9
5
1
7
6
</p>
<p>0
:9
9
5
3
0
7
</p>
<p>0
:9
9
5
4
3
5
</p>
<p>0
:9
9
5
5
5
9
</p>
<p>0
:9
9
5
6
8
1
</p>
<p>3
:0
</p>
<p>0
:9
9
5
7
9
9
</p>
<p>0
:9
9
5
9
1
4
</p>
<p>0
:9
9
6
0
2
6
</p>
<p>0
:9
9
6
1
3
5
</p>
<p>0
:9
9
6
2
4
2
</p>
<p>0
:9
9
6
3
4
5
</p>
<p>0
:9
9
6
4
4
7
</p>
<p>0
:9
9
6
5
4
5
</p>
<p>0
:9
9
6
6
4
1
</p>
<p>0
:9
9
6
7
3
4
</p>
<p>3
:1
</p>
<p>0
:9
9
6
8
2
5
</p>
<p>0
:9
9
6
9
1
4
</p>
<p>0
:9
9
7
0
0
0
</p>
<p>0
:9
9
7
0
8
4
</p>
<p>0
:9
9
7
1
6
5
</p>
<p>0
:9
9
7
2
4
5
</p>
<p>0
:9
9
7
3
2
3
</p>
<p>0
:9
9
7
3
9
8
</p>
<p>0
:9
9
7
4
7
1
</p>
<p>0
:9
9
7
5
4
3
</p>
<p>3
:2
</p>
<p>0
:9
9
7
6
1
2
</p>
<p>0
:9
9
7
6
8
0
</p>
<p>0
:9
9
7
7
4
6
</p>
<p>0
:9
9
7
8
1
0
</p>
<p>0
:9
9
7
8
7
2
</p>
<p>0
:9
9
7
9
3
3
</p>
<p>0
:9
9
7
9
9
2
</p>
<p>0
:9
9
8
0
5
0
</p>
<p>0
:9
9
8
1
0
6
</p>
<p>0
:9
9
8
1
6
0
</p>
<p>3
:3
</p>
<p>0
:9
9
8
2
1
3
</p>
<p>0
:9
9
8
2
6
4
</p>
<p>0
:9
9
8
3
1
4
</p>
<p>0
:9
9
8
3
6
3
</p>
<p>0
:9
9
8
4
1
1
</p>
<p>0
:9
9
8
4
5
7
</p>
<p>0
:9
9
8
5
0
1
</p>
<p>0
:9
9
8
5
4
5
</p>
<p>0
:9
9
8
5
8
7
</p>
<p>0
:9
9
8
6
2
8
</p>
<p>3
:4
</p>
<p>0
:9
9
8
6
6
9
</p>
<p>0
:9
9
8
7
0
7
</p>
<p>0
:9
9
8
7
4
5
</p>
<p>0
:9
9
8
7
8
2
</p>
<p>0
:9
9
8
8
1
8
</p>
<p>0
:9
9
8
8
5
3
</p>
<p>0
:9
9
8
8
8
6
</p>
<p>0
:9
9
8
9
1
9
</p>
<p>0
:9
9
8
9
5
1
</p>
<p>0
:9
9
8
9
8
2
</p>
<p>3
:5
</p>
<p>0
:9
9
9
0
1
2
</p>
<p>0
:9
9
9
0
4
2
</p>
<p>0
:9
9
9
0
7
0
</p>
<p>0
:9
9
9
0
9
8
</p>
<p>0
:9
9
9
1
2
5
</p>
<p>0
:9
9
9
1
5
1
</p>
<p>0
:9
9
9
1
7
6
</p>
<p>0
:9
9
9
2
0
1
</p>
<p>0
:9
9
9
2
2
5
</p>
<p>0
:9
9
9
2
4
8
</p>
<p>3
:6
</p>
<p>0
:9
9
9
2
7
0
</p>
<p>0
:9
9
9
2
9
2
</p>
<p>0
:9
9
9
3
1
4
</p>
<p>0
:9
9
9
3
3
4
</p>
<p>0
:9
9
9
3
5
4
</p>
<p>0
:9
9
9
3
7
4
</p>
<p>0
:9
9
9
3
9
3
</p>
<p>0
:9
9
9
4
1
1
</p>
<p>0
:9
9
9
4
2
9
</p>
<p>0
:9
9
9
4
4
7
</p>
<p>3
:7
</p>
<p>0
:9
9
9
4
6
3
</p>
<p>0
:9
9
9
4
8
0
</p>
<p>0
:9
9
9
4
9
6
</p>
<p>0
:9
9
9
5
1
1
</p>
<p>0
:9
9
9
5
2
6
</p>
<p>0
:9
9
9
5
4
0
</p>
<p>0
:9
9
9
5
5
4
</p>
<p>0
:9
9
9
5
6
8
</p>
<p>0
:9
9
9
5
8
1
</p>
<p>0
:9
9
9
5
9
4
</p>
<p>3
:8
</p>
<p>0
:9
9
9
6
0
7
</p>
<p>0
:9
9
9
6
1
9
</p>
<p>0
:9
9
9
6
3
1
</p>
<p>0
:9
9
9
6
4
2
</p>
<p>0
:9
9
9
6
5
3
</p>
<p>0
:9
9
9
6
6
4
</p>
<p>0
:9
9
9
6
7
4
</p>
<p>0
:9
9
9
6
8
4
</p>
<p>0
:9
9
9
6
9
4
</p>
<p>0
:9
9
9
7
0
4
</p>
<p>3
:9
</p>
<p>0
:9
9
9
7
1
3
</p>
<p>0
:9
9
9
7
2
2
</p>
<p>0
:9
9
9
7
3
1
</p>
<p>0
:9
9
9
7
3
9
</p>
<p>0
:9
9
9
7
4
7
</p>
<p>0
:9
9
9
7
5
5
</p>
<p>0
:9
9
9
7
6
3
</p>
<p>0
:9
9
9
7
7
0
</p>
<p>0
:9
9
9
7
7
7
</p>
<p>0
:9
9
9
7
8
4
</p>
<p>4
:0
</p>
<p>0
:9
9
9
7
9
1
</p>
<p>0
:9
9
9
7
9
8
</p>
<p>0
:9
9
9
8
0
4
</p>
<p>0
:9
9
9
8
1
0
</p>
<p>0
:9
9
9
8
1
6
</p>
<p>0
:9
9
9
8
2
2
</p>
<p>0
:9
9
9
8
2
8
</p>
<p>0
:9
9
9
8
3
3
</p>
<p>0
:9
9
9
8
3
9
</p>
<p>0
:9
9
9
8
4
4
</p>
<p>4
:1
</p>
<p>0
:9
9
9
8
4
9
</p>
<p>0
:9
9
9
8
5
4
</p>
<p>0
:9
9
9
8
5
8
</p>
<p>0
:9
9
9
8
6
3
</p>
<p>0
:9
9
9
8
6
7
</p>
<p>0
:9
9
9
8
7
1
</p>
<p>0
:9
9
9
8
7
5
</p>
<p>0
:9
9
9
8
7
9
</p>
<p>0
:9
9
9
8
8
3
</p>
<p>0
:9
9
9
8
8
7
</p>
<p>4
:2
</p>
<p>0
:9
9
9
8
9
1
</p>
<p>0
:9
9
9
8
9
4
</p>
<p>0
:9
9
9
8
9
8
</p>
<p>0
:9
9
9
9
0
1
</p>
<p>0
:9
9
9
9
0
4
</p>
<p>0
:9
9
9
9
0
7
</p>
<p>0
:9
9
9
9
1
0
</p>
<p>0
:9
9
9
9
1
3
</p>
<p>0
:9
9
9
9
1
6
</p>
<p>0
:9
9
9
9
1
9
</p>
<p>4
:3
</p>
<p>0
:9
9
9
9
2
1
</p>
<p>0
:9
9
9
9
2
4
</p>
<p>0
:9
9
9
9
2
6
</p>
<p>0
:9
9
9
9
2
9
</p>
<p>0
:9
9
9
9
3
1
</p>
<p>0
:9
9
9
9
3
3
</p>
<p>0
:9
9
9
9
3
6
</p>
<p>0
:9
9
9
9
3
8
</p>
<p>0
:9
9
9
9
4
0
</p>
<p>0
:9
9
9
9
4
2
</p>
<p>4
:4
</p>
<p>0
:9
9
9
9
4
4
</p>
<p>0
:9
9
9
9
4
5
</p>
<p>0
:9
9
9
9
4
7
</p>
<p>0
:9
9
9
9
4
9
</p>
<p>0
:9
9
9
9
5
1
</p>
<p>0
:9
9
9
9
5
2
</p>
<p>0
:9
9
9
9
5
4
</p>
<p>0
:9
9
9
9
5
5
</p>
<p>0
:9
9
9
9
5
7
</p>
<p>0
:9
9
9
9
5
8
</p>
<p>4
:5
</p>
<p>0
:9
9
9
9
6
0
</p>
<p>0
:9
9
9
9
6
1
</p>
<p>0
:9
9
9
9
6
2
</p>
<p>0
:9
9
9
9
6
4
</p>
<p>0
:9
9
9
9
6
5
</p>
<p>0
:9
9
9
9
6
6
</p>
<p>0
:9
9
9
9
6
7
</p>
<p>0
:9
9
9
9
6
8
</p>
<p>0
:9
9
9
9
6
9
</p>
<p>0
:9
9
9
9
7
0
</p>
<p>4
:6
</p>
<p>0
:9
9
9
9
7
1
</p>
<p>0
:9
9
9
9
7
2
</p>
<p>0
:9
9
9
9
7
3
</p>
<p>0
:9
9
9
9
7
4
</p>
<p>0
:9
9
9
9
7
5
</p>
<p>0
:9
9
9
9
7
6
</p>
<p>0
:9
9
9
9
7
7
</p>
<p>0
:9
9
9
9
7
7
</p>
<p>0
:9
9
9
9
7
8
</p>
<p>0
:9
9
9
9
7
9
</p>
<p>4
:7
</p>
<p>0
:9
9
9
9
8
0
</p>
<p>0
:9
9
9
9
8
0
</p>
<p>0
:9
9
9
9
8
1
</p>
<p>0
:9
9
9
9
8
2
</p>
<p>0
:9
9
9
9
8
2
</p>
<p>0
:9
9
9
9
8
3
</p>
<p>0
:9
9
9
9
8
3
</p>
<p>0
:9
9
9
9
8
4
</p>
<p>0
:9
9
9
9
8
5
</p>
<p>0
:9
9
9
9
8
5
</p>
<p>4
:8
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>0
:9
9
9
9
8
6
</p>
<p>0
:9
9
9
9
8
7
</p>
<p>0
:9
9
9
9
8
7
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
8
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>0
:9
9
9
9
8
9
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>4
:9
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
0
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
1
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
2
</p>
<p>0
:9
9
9
9
9
3</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 307
</p>
<p>Table A.23 Comparison of integrals of Student&rsquo;s function at different critical values.
</p>
<p>Critical value
</p>
<p>f T D 1 T D 2 T D 3 T D 4 T D 5
1 0:500000 0:704833 0:795168 0:844042 0:874334
</p>
<p>2 0:577351 0:816497 0:904534 0:942809 0:962251
</p>
<p>3 0:608998 0:860674 0:942332 0:971992 0:984608
</p>
<p>4 0:626099 0:883884 0:960058 0:983870 0:992510
</p>
<p>5 0:636783 0:898061 0:969901 0:989677 0:995896
</p>
<p>6 0:644083 0:907574 0:975992 0:992881 0:997548
</p>
<p>7 0:649384 0:914381 0:980058 0:994811 0:998435
</p>
<p>8 0:653407 0:919484 0:982929 0:996051 0:998948
</p>
<p>9 0:656564 0:923448 0:985044 0:996890 0:999261
</p>
<p>10 0:659107 0:926612 0:986657 0:997482 0:999463
</p>
<p>11 0:661200 0:929196 0:987921 0:997914 0:999598
</p>
<p>12 0:662951 0:931345 0:988934 0:998239 0:999691
</p>
<p>13 0:664439 0:933160 0:989762 0:998488 0:999757
</p>
<p>14 0:665718 0:934712 :990449 0:998684 0:999806
</p>
<p>15 0:666830 0:936055 0:991028 0:998841 0:999842
</p>
<p>16 0:667805 0:937228 0:991521 0:998968 0:999870
</p>
<p>17 0:668668 0:938262 0:991946 0:999073 0:999891
</p>
<p>18 0:669435 0:939179 0:992315 0:999161 0:999908
</p>
<p>19 0:670123 0:939998 0:992639 0:999234 0:999921
</p>
<p>20 0:670744 0:940735 0:992925 0:999297 0:999932
</p>
<p>21 0:671306 0:941400 0:993179 0:999351 0:999940
</p>
<p>22 0:671817 0:942005 0:993406 0:999397 0:999948
</p>
<p>23 0:672284 0:942556 0:993610 0:999438 0:999954
</p>
<p>24 0:672713 0:943061 0:993795 0:999474 0:999959
</p>
<p>25 0:673108 0:943524 0:993962 0:999505 0:999963
</p>
<p>26 0:673473 0:943952 0:994115 0:999533 0:999967
</p>
<p>27 0:673811 0:944348 0:994255 0:999558 0:999970
</p>
<p>28 0:674126 0:944715 0:994383 0:999580 0:999973
</p>
<p>29 0:674418 0:945057 0:994501 0:999600 0:999975
</p>
<p>30 0:674692 0:945375 0:994610 0:999619 0:999977
</p>
<p>31 0:674948 0:945673 0:994712 0:999635 0:999979
</p>
<p>32 0:675188 0:945952 0:994806 0:999650 0:999981
</p>
<p>33 0:675413 0:946214 0:994893 0:999664 0:999982
</p>
<p>34 0:675626 0:946461 0:994975 0:999677 0:999983
</p>
<p>35 0:675826 0:946693 0:995052 0:999688 0:999984
</p>
<p>36 0:676015 0:946912 0:995123 0:999699 0:999985
</p>
<p>37 0:676194 0:947119 0:995191 0:999709 0:999986
</p>
<p>38 0:676364 0:947315 0:995254 0:999718 0:999987
</p>
<p>39 0:676525 0:947501 0:995314 0:999727 0:999988
</p>
<p>40 0:676678 0:947678 0:995370 0:999735 0:999989
</p>
<p>(continued)</p>
<p/>
</div>
<div class="page"><p/>
<p>308 Appendix: Numerical Tables
</p>
<p>Table A.23 (continued)
</p>
<p>Critical value
</p>
<p>f T D 1 T D 2 T D 3 T D 4 T D 5
41 0:676824 0:947846 0:995424 0:999742 0:999989
</p>
<p>42 0:676963 0:948006 0:995474 0:999749 0:999990
</p>
<p>43 0:677095 0:948158 0:995522 0:999755 0:999990
</p>
<p>44 0:677222 0:948304 0:995568 0:999761 0:999991
</p>
<p>45 0:677343 0:948443 0:995611 0:999767 0:999991
</p>
<p>46 0:677458 0:948576 0:995652 0:999773 0:999992
</p>
<p>47 0:677569 0:948703 0:995691 0:999778 0:999992
</p>
<p>48 0:677675 0:948824 0:995729 0:999782 0:999992
</p>
<p>49 0:677777 0:948941 0:995765 0:999787 0:999993
</p>
<p>50 0:677875 0:949053 0:995799 0:999791 0:999993
</p>
<p>1 0:682690 0:954500 0:997301 0:999937 1:000000
</p>
<p>Table A.24 Critical values of the linear correlation coefficient
</p>
<p>Probability p to have an absolute value of r below the critical value
</p>
<p>f 0:50 0:60 0:70 0:80 0:90 0:95 0:99
</p>
<p>2 0:500 0:600 0:700 0:800 0:900 0:950 0:990
</p>
<p>3 0:404 0:492 0:585 0:687 0:805 0:878 0:959
</p>
<p>4 0:347 0:426 0:511 0:608 0:729 0:811 0:917
</p>
<p>5 0:309 0:380 0:459 0:551 0:669 0:754 0:875
</p>
<p>6 0:281 0:347 0:420 0:507 0:621 0:707 0:834
</p>
<p>7 0:260 0:321 0:390 0:472 0:582 0:666 0:798
</p>
<p>8 0:242 0:300 0:365 0:443 0:549 0:632 0:765
</p>
<p>9 0:228 0:282 0:344 0:419 0:521 0:602 0:735
</p>
<p>10 0:216 0:268 0:327 0:398 0:497 0:576 0:708
</p>
<p>20 0:152 0:189 0:231 0:284 0:360 0:423 0:537
</p>
<p>30 0:124 0:154 0:189 0:233 0:296 0:349 0:449
</p>
<p>40 0:107 0:133 0:164 0:202 0:257 0:304 0:393
</p>
<p>50 0:096 0:119 0:147 0:181 0:231 0:273 0:354
</p>
<p>60 0:087 0:109 0:134 0:165 0:211 0:250 0:325
</p>
<p>70 0:081 0:101 0:124 0:153 0:195 0:232 0:302
</p>
<p>80 0:076 0:094 0:116 0:143 0:183 0:217 0:283
</p>
<p>90 0:071 0:089 0:109 0:135 0:173 0:205 0:267
</p>
<p>100 0:068 0:084 0:104 0:128 0:164 0:195 0:254
</p>
<p>200 0:048 0:060 0:073 0:091 0:116 0:138 0:181
</p>
<p>300 0:039 0:049 0:060 0:074 0:095 0:113 0:148
</p>
<p>500 0:030 0:038 0:046 0:057 0:073 0:087 0:114
</p>
<p>1000 0:021 0:027 0:033 0:041 0:052 0:062 0:081</p>
<p/>
</div>
<div class="page"><p/>
<p>A.7 The Kolmogorov&ndash;Smirnov Test 309
</p>
<p>Table A.25 Critical values of the Kolmogorov&ndash;Smirnov statistic DN
</p>
<p>Probability p to have DN ï¿½
p
N below the critical value
</p>
<p>N 0:50 0:60 0:70 0:80 0:90 0:95 0:99
</p>
<p>1 0:750 0:800 0:850 0:900 0:950 0:975 0:995
</p>
<p>2 0:707 0:782 0:866 0:967 1:098 1:191 1:314
</p>
<p>3 0:753 0:819 0:891 0:978 1:102 1:226 1:436
</p>
<p>4 0:762 0:824 0:894 0:985 1:130 1:248 1:468
</p>
<p>5 0:765 0:827 0:902 0:999 1:139 1:260 1:495
</p>
<p>6 0:767 0:833 0:910 1:005 1:146 1:272 1:510
</p>
<p>7 0:772 0:838 0:914 1:009 1:154 1:279 1:523
</p>
<p>8 0:776 0:842 0:917 1:013 1:159 1:285 1:532
</p>
<p>9 0:779 0:844 0:920 1:017 1:162 1:290 1:540
</p>
<p>10 0:781 0:846 0:923 1:020 1:166 1:294 1:546
</p>
<p>15 0:788 0:855 0:932 1:030 1:177 1:308 1:565
</p>
<p>20 0:793 0:860 0:937 1:035 1:184 1:315 1:576
</p>
<p>25 0:796 0:863 0:941 1:039 1:188 1:320 1:583
</p>
<p>30 0:799 0:866 0:943 1:042 1:192 1:324 1:588
</p>
<p>35 0:801 0:868 0:946 1:045 1:194 1:327 1:591
</p>
<p>40 0:803 0:869 0:947 1:046 1:196 1:329 1:594
</p>
<p>45 0:804 0:871 0:949 1:048 1:198 1:331 1:596
</p>
<p>50 0:805 0:872 0:950 1:049 1:199 1:332 1:598
</p>
<p>60 0:807 0:874 0:952 1:051 1:201 1:335 1:601
</p>
<p>70 0:808 0:875 0:953 1:053 1:203 1:337 1:604
</p>
<p>80 0:810 0:877 0:955 1:054 1:205 1:338 1:605
</p>
<p>90 0:811 0:878 0:956 1:055 1:206 1:339 1:607
</p>
<p>100 0:811 0:879 0:957 1:056 1:207 1:340 1:608
</p>
<p>200 0:816 0:883 0:961 1:061 1:212 1:346 1:614
</p>
<p>300 0:818 0:885 0:964 1:063 1:214 1:348 1:617
</p>
<p>500 0:820 0:887 0:966 1:065 1:216 1:350 1:620
</p>
<p>1000 0:822 0:890 0:968 1:067 1:218 1:353 1:622
</p>
<p>1 0:828 0:895 0:973 1:073 1:224 1:358 1:628</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>1. Abramowitz, M., Stegun, I.A.: Handbook of Mathematical Functions: With Formulas, Graphs,
and Mathematical Tables. Dover, New York (1970)
</p>
<p>2. Akritas, M.G., Bershady, M.A.: Linear regression for astronomical data with measurement
errors and intrinsic scatter. Astrophys. J. 470, 706 (1996). doi:10.1086/177901
</p>
<p>3. Bayes, T., Price, R.: An essay towards solving a problem in the doctrine of chances. Philos.
Trans. R. Soc. Lond. 53 (1763)
</p>
<p>4. Bonamente, M., Swartz, D.A., Weisskopf, M.C., Murray, S.S.: Swift XRT observations
of the possible dark galaxy VIRGOHI 21. Astrophys. J. Lett. 686, L71&ndash;L74 (2008).
doi:10.1086/592819
</p>
<p>5. Bonamente, M., Hasler, N., Bulbul, E., Carlstrom, J.E., Culverhouse, T.L., Gralla, M., Greer,
C., Hawkins, D., Hennessy, R., Joy, M., Kolodziejczak, J., Lamb, J.W., Landry, D., Leitch,
E.M., Marrone, D.P., Miller, A., Mroczkowski, T., Muchovej, S., Plagge, T., Pryke, C., Sharp,
M., Woody, D.: Comparison of pressure profiles of massive relaxed galaxy clusters using the
Sunyaev&ndash;Zel&rsquo;dovich and X-ray data. New. J. Phys. 14(2), 025010 (2012). doi:10.1088/1367-
2630/14/2/025010
</p>
<p>6. Brooks, S.P., Gelman, A.: General methods for monitoring convergence of iterative simula-
tions. J. Comput. Graph. Stat. 7, 434&ndash;455 (1998)
</p>
<p>7. Bulmer, M.G.: Principles of Statistics. Dover, New York (1967)
8. Carlin, B., Gelfand, A., Smith, A.: Hierarchical Bayesian analysis for changepoint problems.
</p>
<p>Appl. Stat. 41, 389&ndash;405 (1992)
9. Cash, W.: Parameter estimation in astronomy through application of the likelihood ratio.
</p>
<p>Astrophys. J. 228, 939&ndash;947 (1979)
10. Cowan, G.: Statistical Data Analysis. Oxford University Press, Oxford (1998)
11. Cramer, H.: Mathematical Methods of Statistics. Princeton University Press, Princeton (1946)
12. Emslie, A.G., Massone, A.M.: Bayesian confidence limits of electron spectra obtained through
</p>
<p>regularized inversion of solar hard X-ray spectra. Astrophys. J. 759, 122 (2012)
13. Fisher, R.A.: On a distribution yielding the error functions of several well known statistics.
</p>
<p>Proc. Int. Congr. Math. 2, 805&ndash;813 (1924)
14. Fisher, R.A.: The use of multiple measurements in taxonomic problems. Ann. Eugenics 7,
</p>
<p>179&ndash;188 (1936)
15. Gamerman, D.: Markov Chain Monte Carlo. Chapman and Hall/CRC, London/New York
</p>
<p>(1997)
16. Gehrels, N.: Confidence limits for small numbers of events in astrophysical data. Astrophys. J.
</p>
<p>303, 336&ndash;346 (1986). doi:10.1086/164079
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4
</p>
<p>311</p>
<p/>
</div>
<div class="page"><p/>
<p>312 References
</p>
<p>17. Gelman, A., Rubin, D.: Inference from iterative simulation using multiple sequences. Stat. Sci.
7, 457&ndash;511 (1992)
</p>
<p>18. Gosset, W.S.: The probable error of a mean. Biometrika 6, 1&ndash;25 (1908)
19. Hastings, W.K.: Monte Carlo sampling methods using Markov chains and their applications.
</p>
<p>Biometrika 57(1), 97&ndash;109 (1970). doi:10.1093/biomet/57.1.97. http://biomet.oxfordjournals.
org/content/57/1/97.abstract
</p>
<p>20. Helmert, F.R.: Die genauigkeit der formel von peters zur berechnung des wahrscheinlichen
fehlers director beobachtungen gleicher genauigkeit. Astron. Nachr. 88, 192&ndash;218 (1876)
</p>
<p>21. Hubble, E., Humason, M.: The velocity-distance relation among extra-galactic nebulae.
Astrophys. J. 74, 43 (1931)
</p>
<p>22. Isobe, T., Feigelson, Isobe, T., Feigelson, E.D., Akritas, M.G., Babu, G.J.: Linear regression in
astronomy. Astrophys. J. 364, 104 (1990)
</p>
<p>23. Jeffreys, H.: Theory of Probability. Oxford University Press, London (1939)
24. Kelly, B.C.: Some aspects of measurement error in linear regression of astronomical data.
</p>
<p>Astrophys. J. 665, 1489&ndash;1506 (2007). doi: 10.1086/519947
25. Kolmogorov, A.: Sulla determinazione empirica di una legge di distribuzione. Giornale dell&rsquo;
</p>
<p>Istituto Italiano degli Attuari 4, 1&ndash;11 (1933)
26. Kolmogorov, A.N.: Foundations of the Theory of Probability. Chelsea, New York (1950)
27. Lampton, M., Margon, B., Bowyer, S.: Parameter estimation in X-ray astronomy. Astrophys.
</p>
<p>J. 208, 177&ndash;190 (1976). doi:10.1086/154592
28. Lewis, S.: gibbsit. http://lib.stat.cmu.edu/S/gibbsit
29. Madsen, R.W., Moeschberger, M.L.: Introductory Statistics for Business and Economics.
</p>
<p>Prentice-Hall, Englewood Cliffs (1983)
30. Marsaglia, G., Tsang, W., Wang, J.: Evaluating Kolmogorov&rsquo;s distribution. J. Stat. Softw. 8,
</p>
<p>1&ndash;4 (2003)
31. Mendel, G.: Versuche &uuml;ber plflanzenhybriden (experiments in plant hybridization). Verhand-
</p>
<p>lungen des naturforschenden Vereines in Br&uuml;nn, pp. 3&ndash;47 (1865)
32. Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E.: Equation of
</p>
<p>state calculations by fast computing machines. J. Chem. Phys. 21, 1087&ndash;1092 (1953). doi:
10.1063/1.1699114
</p>
<p>33. Pearson, K., Lee, A.: On the laws on inheritance in men. Biometrika 2, 357&ndash;462 (1903)
34. Plummer, M., Best, N., Cowles, K., Vines, K.: CODA: convergence diagnosis and output
</p>
<p>analysis for MCMC. R News 6(1), 7&ndash;11 (2006). http://CRAN.R-project.org/doc/Rnews/
35. Press, W., Teukolski, S., Vetterling, W., Flannery, B.: Numerical Recipes 3rd Edition: The Art
</p>
<p>of Scientific Computing. Cambridge University Press, Cambridge (2007)
36. Protassov, R., van Dyk, D.A., Connors, A., Kashyap, V.L., Siemiginowska, A.: Statistics, han-
</p>
<p>dle with care: detecting multiple model components with the likelihood ratio test. Astrophys.
J. 571, 545&ndash;559 (2002). doi:1086/339856
</p>
<p>37. Raftery, A., Lewis, S.: How many iterations in the gibbs sampler? Bayesian Stat. 4, 763&ndash;773
(1992)
</p>
<p>38. Ross, S.M.: Introduction to Probability Models. Academic, San Diego (2003)
39. Thomson, J.J.: Cathode rays. Philos. Mag. 44, 293 (1897)
40. Tremaine, S., Gebhardt, K., Bender, R., Bower, G., Dressler, A., Faber, S.M., Filippenko,
</p>
<p>A.V., Green, R., Grillmair, C., Ho, L.C., Kormendy, J., Lauer, T.R., Magorrian, J., Pinkney,
J., Richstone, D.: The slope of the black hole mass versus velocity dispersion correlation.
Astrophys. J. 574, 740&ndash;753 (2002). doi:10.1086/341002
</p>
<p>41. Von Eye, A., Schuster, C.: Regression Analysis for Social Sciences. Academic, New York
(1998)
</p>
<p>42. Wilks, S.S.: Mathematical Statistics. Princeton University Press, Princeton (1943)</p>
<p/>
<div class="annotation"><a href="http://biomet.oxfordjournals.org/content/57/1/97.ab stract">http://biomet.oxfordjournals.org/content/57/1/97.ab stract</a></div>
<div class="annotation"><a href="http://biomet.oxfordjournals.org/content/57/1/97.ab stract">http://biomet.oxfordjournals.org/content/57/1/97.ab stract</a></div>
<div class="annotation"><a href="http://lib.stat.cmu.edu/S/gibbsit">http://lib.stat.cmu.edu/S/gibbsit</a></div>
<div class="annotation"><a href="http://CRAN.R-project.org/doc/Rnews/">http://CRAN.R-project.org/doc/Rnews/</a></div>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>1=
p
N factor, 57
</p>
<p>DN statistic, 217
approximation, 218
</p>
<p>DNM statistic, 219
approximation, 220
</p>
<p>
C statistic, 186

ï¿½2 statistic, 182, 215
ï¿½2 distribution
</p>
<p>tables, 283
ï¿½2 distribution, 122
</p>
<p>degrees of freedom, 124
hypothesis testing, 126
mean, 125
moment generating function, 126
probability function, 124
reduced ï¿½2, 125
use of bivariate errors, 209
variance, 125
</p>
<p>ï¿½2min statistic, 149, 177

ï¿½2 statistic for confidence intervals, 182
hypothesis testing, 178
probability function, 178
</p>
<p>	 , simulation of number, 228
OR statistic, 266
</p>
<p>acceptability of goodness of fit, 195
acceptability of null hypothesis, 120
acceptable region, 118
acceptance probability, 252
accessible state, 241
Akritas, M., 204
Anderson, E., 165
auxiliary distribution, 251
average
</p>
<p>linear, 107
relative-error weighted, 113
weighted, 107
</p>
<p>Bayes&rsquo; theorem, 10
Bayes, Rev. Thomas, 4
Bayesian method for Poisson mean, 102
Bayesian statistics, 12
Bershady, M., 204
Beta function, 133
binary experiment, 35
binning of data, 162
</p>
<p>Kolmogorov Smirnov test, 216
binomial distribution, 35
</p>
<p>comparison with Gaussian and Poisson, 51
Ehrenfest chain, 246
mean, 38
moments, 38
probability function, 38
variance, 39
</p>
<p>bisector model, 207
bivariate data, 203
</p>
<p>use of ï¿½2, 209
bootstrap simulation, 230
</p>
<p>synthetic dataset, 230
unbiased estimator, 232
</p>
<p>burn-in period, 259
</p>
<p>candidate of MCMC, 251
Cartesian coordinates, 65
</p>
<p>transformation to polar, 81
Cash statistic, 161, 180
</p>
<p>approximate distribution, 180
</p>
<p>&copy; Springer Science+Busines Media New York 2017
M. Bonamente, Statistics and Analysis of Scientific Data, Graduate Texts
in Physics, DOI 10.1007/978-1-4939-6572-4
</p>
<p>313</p>
<p/>
</div>
<div class="page"><p/>
<p>314 Index
</p>
<p>Cash, W., 161
central limit theorem, 61
central moments of random variable, 23
change of variables, method, 65
CODA software, 268
coefficient of determination, 174
coin toss experiment, 4
combinations, 37
complementary event, 2
conditional probability, 7
confidence intervals, 93
</p>
<p>all fit parameters, 183
central, 93
Gaussian variables, 94
model parameters, 181, 186
one-sided, 93
Poisson data, 186
Poisson mean, 97
reduced parameters, 184
significance, 93
</p>
<p>confidence levels, 118
contingency table, 31
convergence tests of MCMC, 259
convolution, 67
coordinate transformation, Cartesian to polar,
</p>
<p>65
correlation coefficient, 27
</p>
<p>sample, see sample correlation coefficient
correlation of random variables, 26
counting experiment and Poisson distribution,
</p>
<p>47
counting process, 48
covariance, 26, 27
cumulative distribution function, 19
</p>
<p>debiased variance, 197, 204
degrees of freedom, 124
ï¿½2 distribution, 124
F statistic, 134
sampling distribution of variance, 130
Student&rsquo;s t distribution, 139
</p>
<p>design matrix, 169
detailed balance, 254
deviation of random variable, 22
distribution function, 17, 19
</p>
<p>properties, 19
full conditional, 258
</p>
<p>Ehrenfest chain, 239
transition probability, 240
stationary distribution, 245, 247
</p>
<p>error function, 44, 273
</p>
<p>error matrix, 153, 169
error propagation, 55, 70
error propagation formula, 71
</p>
<p>exponential of variable, 75
logarithm of variable, 75
power of variable, 74
product and division, 73
sum of constant, 72
table of common functions, 76
three independent variables, 83
weighted sum of two variables, 72
</p>
<p>event, 1
expectation of random variable, 20
experiment, 1
explained variance, 173
exponential distribution, 19
</p>
<p>cumulative distribution function, 20
probability function, 19
simulation, 79
</p>
<p>F statistic, 131
approximations, 134
degrees of freedom, 134
distribution function, 132
hypothesis testing, 134
mean, 134
variance, 134
tables, 282
</p>
<p>F test, 211
for additional model component, 214
multi&ndash;variable linear regression, 172
degrees of freedom, 212
nested component, 214
two independent ï¿½2 measurements, 212
use on same dataset, 213
</p>
<p>factorial function, Stirling&rsquo;s approximation, 51
Fisher, R.A., 133, 165
fractional errors, 109
full conditional distribution, 258
full-width at half maximum (FWHM), 44
function of random variables, 64
</p>
<p>mean, 69
multi-dimensional method, 66
variance, 70
</p>
<p>gamma distribution, 123
Gamma function, 124, 134
</p>
<p>asymptotic expansion, 288
Gaussian distribution, 40
</p>
<p>comparison with binomial and Poisson, 51
confidence intervals, 94
cumulative distribution function, 44</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 315
</p>
<p>log-normal distribution, 110
mean, 43
moment generating function, 59
moments, 44
probability function, 43
simulation, 45, 79
upper and lower limits, 95
variance, 43
tables, 273
</p>
<p>Geherels approximation, 99
tables of upper and lower limits, 277
</p>
<p>Gelman Rubin statistic, 260, 264
between&ndash;chain variance, 265
within&ndash;chain variance, 265
</p>
<p>genes and genotypes, 8
Geweke Z score, 260, 263
Gibbs sampler, 258
gibbsit software, 268
goodness of fit
ï¿½2min statistic, 177
Cash statistic, 180
Gaussian data, 149
Poisson data, 161
</p>
<p>Gosset, W.S. (Student), 140
</p>
<p>half-width at half maximum (HWHM), 44
Hubble constant, 158
Hubble&rsquo;s law, 157
Hubble, E., 157
hypothesis testing, 117
ï¿½2 distribution, 126
ï¿½2min statistic, 178
acceptable region, 118
confidence level, 118
F statistic, 134
four&ndash;step process, 118
linear correlation coefficient, 190
rejection region, 118
sampling distribution of variance, 131
Student&rsquo;s t distribution, 141
acceptability of null hypothesis, 120
rejection, 120
</p>
<p>impossible event, 1
independent events, 5
independent variables, 28
</p>
<p>two&ndash;variable data, 187
interesting and uninteresting parameters, 184
intrinsic covariance, 204
intrinsic scatter, 196
</p>
<p>alternative method using ï¿½2red , 197
direct calculation, 196
parameter estimation, 200
</p>
<p>intrinsic variance, 197, 204
Iris dataset (Fisher and Anderson), 166
irreducible aperiodic Markov chains, 244
</p>
<p>jackknife simulation, 234
resampled dataset, 234
unbiased estimator, 234
pseudo-values, 234
resampled dataset, 234
</p>
<p>Jacobian of transformation, 66
Jeffreys priors, 252
Jeffreys, H., 252
joint distribution function, 26
</p>
<p>Kolmogorov axioms, 2
Kolmogorov Smirnov test, 216
</p>
<p>DN statistic, 217
DNM statistic, 219
approximation, 218
comparison of data with model, 216
non&ndash;parametric nature, 216
two-sample test, 219
tables, 290
</p>
<p>Law of heredity (Mendel experiment), 8
Law of independent assortment (Mendel
</p>
<p>experiment), 9
law of large numbers, 68, 226
least&ndash;squares method, 150
likelihood, 11, 250
</p>
<p>Gaussian data, 85
Poisson data, 50
</p>
<p>linear average, 107
linear combination of random variables, 55
linear correlation coefficient, 187
</p>
<p>hypothesis testing, 190
probability function, 188
tables, 286, 308
</p>
<p>linear regression, 150
error matrix, 153
identical errors, 151
multi&ndash;variable, 168
identical errors or no errors, 155
model sample variance, 156
parameter errors and covariance, 154
</p>
<p>log-normal distribution, 110</p>
<p/>
</div>
<div class="page"><p/>
<p>316 Index
</p>
<p>logarithmic average, 109
weighted, 110
</p>
<p>marginalization of random variables, 29, 31
marginalization of uninteresting parameters,
</p>
<p>185
Markov chains, 237
</p>
<p>accessible states, 241
dependence of samples, 250
Markovian property, 238
recurrent and transient states, 240
short memory property, 238
state of system, 238
communication of states, 243
detailed balance, 254
irreducible aperiodic chains, 244
limiting probability, 243
periodicity, 244
recurrent and transient states, 242
stationary distribution, 243
time reversible, 253
</p>
<p>Markovian property, 238
mass distribution function, see probability
</p>
<p>mass function
maximum likelihood method
</p>
<p>bivariate data, 204
fit to non-linear functions, 160
fit to two&ndash;dimensional data, 149
Gaussian data, 149
Gaussian variable, 85
</p>
<p>estimate of mean, 86
estimate of sample variance, 87
estimate of variance, 87
</p>
<p>other variables, 90
Poisson data, 160
Poisson variable, 90
</p>
<p>MCMC, see Monte Carlo Markov chains
mean, 21
</p>
<p>function of random variables, 69
linear combination of variables, 55
weighted, 89
Bayesian expectation for Poisson mean,
</p>
<p>102
non-uniform errors, 88
</p>
<p>median, 22, 109
insensitivity to outliers, 109
</p>
<p>Mendel, G., 8
method of moments, 91
Metropolis Hastings algorithm, 251
</p>
<p>case of uniform priors and proposals,
253
</p>
<p>justification of method, 253
proposal (auxiliary) distribution, 251
</p>
<p>mixing properties of MCMC, 262
mode, 22
model sample variance, 156
moment generating function, 58
</p>
<p>properties, 59
Gaussian distribution, 59
Poisson distribution, 60
sum of Poisson variables, 60
sum of uniform distributions, 63
sum of uniform variables, 62
</p>
<p>moments of distribution function, 20
Monte Carlo, 225
</p>
<p>function evaluation, 227
integration, 226
dart method, 227
multi-dimensional integration, 227
simulation of variables, 228
</p>
<p>Monte Carlo Markov chains, 249
acceptance probability, 252
candidates, 251
prior distribution, 251
burn-in period, 259, 263
convergence tests, 259
correlation of links, 261
mixing, 262
posterior distribution, 250, 252
stopping time, 260
thinning, 263
</p>
<p>multi&ndash;variable dataset, 165
multi&ndash;variable linear regression, 168
</p>
<p>coefficient of determination, 174
design matrix, 169
error matrix, 169
F test, 172
Iris data, 171
T test, 170
tests for significance, 170
</p>
<p>multiple linear regression, 151
best-fit parameters, 152
error matrix, 153
parameter errors, 153
</p>
<p>multiplicative errors, 109
mutually exclusive events, 2
</p>
<p>nested component, 211, 214
normal distribution, 40
null hypothesis, 118
</p>
<p>Occam&rsquo;s razor, 216
Occam, William of, 216
orthonormal transformation, 129
overbooking, probability, 39</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 317
</p>
<p>parent distribution, 18
parent mean, 22
</p>
<p>comparison with sample mean, 137
partition of sample space, 2
Pearson, K., 30
</p>
<p>data on biometric characteristics,
30
</p>
<p>periodicity of Markov chains, 244
permutations, 36
photon counting experiment, 29
Poisson distribution, 45
</p>
<p>Bayesian expectation of mean, 103
Baysian upper and lower limits, 103
comparison with binomial and Gaussian,
</p>
<p>51
likelihood, 49
mean, 46
moment generating function, 60
posterior distribution, 50
posterior probability, 49, 102
S parameter, 100
upper and lower limits, 98
variance, 47
</p>
<p>Poisson process, 48
polar coordinates, 65
</p>
<p>transformation to Cartesian, 81
posterior distribution, 241
</p>
<p>Markov chains, 250
posterior probability, 11, 13
</p>
<p>Poisson mean, 102
prior distribution, 250
</p>
<p>MCMC, 251
prior probability, 11
probability
</p>
<p>Bayesian method, 1, 4
classical method, 3
empirical method, 4
frequentist method, 1, 3
fundamental properties, 4
</p>
<p>probability distribution function, 19
probability mass function, 19
probability of event, 2
proposal distribution, 251
</p>
<p>quantile, 93, 268
quantile function, 76
</p>
<p>exponential distribution, 77
uniform distribution, 77
</p>
<p>Raftery Lewis test, 267
random error, 198
random variables, 17
</p>
<p>random walk, 237, 239
recurrence of states, 241
transition probability, 239
recurrent states, 247
</p>
<p>Rayleigh distribution, 66
cumulative distribution, 80
quantile function, 80
</p>
<p>reduced ï¿½2, 125
rejection of hypothesis, 120
rejection region, 118
</p>
<p>one&ndash;sided, 119
two&ndash;sided, 119
</p>
<p>relative uncertainty of random variable, 57, 72
relative-error weighted average, 113
resampled dataset, 234
residual variance, 172
</p>
<p>sample correlation coefficient, 27, 28
sample covariance, 27, 28
sample distribution, 18
sample mean, 21
</p>
<p>comparison of two means, 141
comparison with parent mean, 137
</p>
<p>sample space, 1
partition, 2
</p>
<p>sample variance, 23
sampling distribution of mean, 137
sampling distribution of variance, 127
</p>
<p>degrees of freedom, 130
probability function, 130
</p>
<p>sequence of events, 36
sequence of random variables, 237
signal&ndash;to&ndash;noise ratio, 72
simulation of number 	 , 228
simulation of random variables, 78
</p>
<p>exponential, 79
Gaussian, 79, 229
Monte Carlo methods, 228
square of uniform distribution, 79
</p>
<p>standard deviation, 22
standard error, 22
standard Gaussian, 45
stationary distribution, 237
statistic, 117
statistical error, 108, 199
statistical independence, 5
</p>
<p>necessary conditions for, 6
statistical independence of random variables,
</p>
<p>28
Stirling&rsquo;s approximation, 51
stochastic processes, 237
stopping time of MCMC, 260
strong law of large numbers, 68</p>
<p/>
</div>
<div class="page"><p/>
<p>318 Index
</p>
<p>Student&rsquo;s t distribution, 137
comparison of two sample means, 142
degrees of freedom, 139
hypothesis testing, 141
mean, 140
probability function, 139
tables, 285
</p>
<p>sum of random variables, 62
synthetic dataset, 230
systematic error, 108, 199
</p>
<p>additive, 199
multiplicative, 199
parameter estimation, 200
</p>
<p>T test, 170
thinning of MCMC, 263
Thomson, J.J., 23
</p>
<p>analysis of experimental data, 222
discovery of electron, 23
</p>
<p>Total probability theorem, 10
transition kernel, 239
transition probability, 239
triangular distribution, 64, 67
two&ndash;variable dataset, 147
</p>
<p>bivariate errors, 203
independent variable, 147, 187
Monte Carlo estimates of errors, 230
</p>
<p>uncorrelated variables, 27, 56
uniform distribution, 67
</p>
<p>probability function, 67
simulation, 79
</p>
<p>square, 69
sum of two variables, 67
</p>
<p>upper and lower limits, 94
Bayesian method for Poisson mean, 103
Gaussian variables, 95
Geherels approximation, 99
Poisson variable, 98, 99
</p>
<p>upper limit to non detection, 96
Bayesian, 103
Gaussian, 96
Poisson, 101
</p>
<p>variance
debiased, 204
explained, 173
intrinsic, 204
linear combination of variables, 56
residual, 172
weighted mean, 90
anti-correlated variables, 56
correlated variables, 57
function of random variables, 70
</p>
<p>variance of random variable, 22
variance of sum of variables, 28
</p>
<p>weighted logarithmic average, 110
weighted mean, 89, 107
</p>
<p>variance, 90
</p>
<p>z&ndash;score, 45, 138</p>
<p/>
</div>
<ul>	<li>Preface to the First Edition</li>
	<li>Preface to the Second Edition</li>
	<li>Acknowledgments</li>
	<li>Contents</li>
	<li>1 Theory of Probability </li>
<ul>	<li>1.1 Experiments, Events, and the Sample Space</li>
	<li>1.2 Probability of Events</li>
<ul>	<li>1.2.1 The Kolmogorov Axioms</li>
	<li>1.2.2 Frequentist or Classical Method</li>
	<li>1.2.3 Bayesian or Empirical Method</li>
</ul>
	<li>1.3 Fundamental Properties of Probability</li>
	<li>1.4 Statistical Independence</li>
	<li>1.5 Conditional Probability</li>
	<li>1.6 A Classic Experiment: Mendel's Law of Heredity and the Independent Assortment of Species</li>
	<li>1.7 The Total Probability Theorem and Bayes' Theorem</li>
	<li>Problems</li>
</ul>
	<li>2 Random Variables and Their Distributions </li>
<ul>	<li>2.1 Random Variables</li>
	<li>2.2 Probability Distribution Functions</li>
	<li>2.3 Moments of a Distribution Function</li>
<ul>	<li>2.3.1 The Mean and the Sample Mean</li>
	<li>2.3.2 The Variance and the Sample Variance</li>
</ul>
	<li>2.4 A Classic Experiment: J.J. Thomson's Discoveryof the Electron</li>
	<li>2.5 Covariance and Correlation Between Random Variables</li>
<ul>	<li>2.5.1 Joint Distribution and Moments of Two Random Variables</li>
	<li>2.5.2 Statistical Independence of Random Variables</li>
</ul>
	<li>2.6 A Classic Experiment: Pearson's Collection of Data on Biometric Characteristics</li>
	<li>Problems</li>
</ul>
	<li>3 Three Fundamental Distributions: Binomial, Gaussian, and Poisson </li>
<ul>	<li>3.1 The Binomial Distribution</li>
<ul>	<li>3.1.1 Derivation of the Binomial Distribution</li>
	<li>3.1.2 Moments of the Binomial Distribution</li>
</ul>
	<li>3.2 The Gaussian Distribution</li>
<ul>	<li>3.2.1 Derivation of the Gaussian Distribution from the Binomial Distribution</li>
	<li>3.2.2 Moments and Properties of the Gaussian Distribution</li>
	<li>3.2.3 How to Generate a Gaussian Distribution from a Standard Normal</li>
</ul>
	<li>3.3 The Poisson Distribution</li>
<ul>	<li>3.3.1 Derivation of the Poisson Distribution</li>
	<li>3.3.2 Properties and Interpretation of the PoissonDistribution</li>
	<li>3.3.3 The Poisson Distribution and the Poisson Process</li>
	<li>3.3.4 An Example on Likelihood and Posterior Probability of a Poisson Variable</li>
</ul>
	<li>3.4 Comparison of Binomial, Gaussian, and Poisson Distributions</li>
	<li>Problems</li>
</ul>
	<li>4 Functions of Random Variables and Error Propagation </li>
<ul>	<li>4.1 Linear Combination of Random Variables</li>
<ul>	<li>4.1.1 General Mean and Variance Formulas</li>
	<li>4.1.2 Uncorrelated Variables and the 1/N Factor</li>
</ul>
	<li>4.2 The Moment Generating Function</li>
<ul>	<li>4.2.1 Properties of the Moment Generating Function</li>
	<li>4.2.2 The Moment Generating Function of the Gaussian and Poisson Distribution</li>
</ul>
	<li>4.3 The Central Limit Theorem</li>
	<li>4.4 The Distribution of Functions of Random Variables</li>
<ul>	<li>4.4.1 The Method of Change of Variables</li>
	<li>4.4.2 A Method for Multi-dimensional Functions</li>
</ul>
	<li>4.5 The Law of Large Numbers</li>
	<li>4.6 The Mean of Functions of Random Variables</li>
	<li>4.7 The Variance of Functions of Random Variables and Error Propagation Formulas</li>
<ul>	<li>4.7.1 Sum of a Constant</li>
	<li>4.7.2 Weighted Sum of Two Variables</li>
	<li>4.7.3 Product and Division of Two Random Variables</li>
	<li>4.7.4 Power of a Random Variable</li>
	<li>4.7.5 Exponential of a Random Variable</li>
	<li>4.7.6 Logarithm of a Random Variable</li>
</ul>
	<li>4.8 The Quantile Function and Simulation of Random Variables</li>
<ul>	<li>4.8.1 General Method to Simulate a Variable</li>
	<li>4.8.2 Simulation of a Gaussian Variable</li>
</ul>
	<li>Problems</li>
</ul>
	<li>5 Maximum Likelihood and Other Methods to Estimate Variables </li>
<ul>	<li>5.1 The Maximum Likelihood Method for Gaussian Variables</li>
<ul>	<li>5.1.1 Estimate of the Mean</li>
	<li>5.1.2 Estimate of the Variance</li>
	<li>5.1.3 Estimate of Mean for Non-uniform Uncertainties</li>
</ul>
	<li>5.2 The Maximum Likelihood Method for Other Distributions</li>
	<li>5.3 Method of Moments</li>
	<li>5.4 Quantiles and Confidence Intervals</li>
<ul>	<li>5.4.1 Confidence Intervals for a Gaussian Variable</li>
	<li>5.4.2 Confidence Intervals for the Mean of a PoissonVariable</li>
</ul>
	<li>5.5 Bayesian Methods for the Poisson Mean</li>
<ul>	<li>5.5.1 Bayesian Expectation of the Poisson Mean</li>
	<li>5.5.2 Bayesian Upper and Lower Limits for a Poisson Variable</li>
</ul>
	<li>Problems</li>
</ul>
	<li>6 Mean, Median, and Average Values of Variables </li>
<ul>	<li>6.1 Linear and Weighted Average</li>
	<li>6.2 The Median</li>
	<li>6.3 The Logarithmic Average and Fractionalor Multiplicative Errors</li>
<ul>	<li>6.3.1 The Weighted Logarithmic Average</li>
	<li>6.3.2 The Relative-Error Weighted Average</li>
</ul>
	<li>Problems</li>
</ul>
	<li>7 Hypothesis Testing and Statistics </li>
<ul>	<li>7.1 Statistics and Hypothesis Testing</li>
	<li>7.2 The Ï2 Distribution</li>
<ul>	<li>7.2.1 The Probability Distribution Function</li>
	<li>7.2.2 Moments and Other Properties</li>
	<li>7.2.3 Hypothesis Testing</li>
</ul>
	<li>7.3 The Sampling Distribution of the Variance</li>
	<li>7.4 The F Statistic</li>
<ul>	<li>7.4.1 The Probability Distribution Function</li>
	<li>7.4.2 Moments and Other Properties</li>
	<li>7.4.3 Hypothesis Testing</li>
</ul>
	<li>7.5 The Sampling Distribution of the Mean and the Student's t Distribution</li>
<ul>	<li>7.5.1 Comparison of Sample Mean with Parent Mean</li>
<ul>	<li>7.5.1.1 Hypothesis Testing</li>
</ul>
	<li>7.5.2 Comparison of Two Sample Means and Hypothesis Testing</li>
</ul>
	<li>Problems</li>
</ul>
	<li>8 Maximum Likelihood Methods for Two-Variable Datasets </li>
<ul>	<li>8.1 Measurement of Pairs of Variables</li>
	<li>8.2 Maximum Likelihood Method for Gaussian Data</li>
	<li>8.3 Least-Squares Fit to a Straight Line, or Linear Regression</li>
	<li>8.4 Multiple Linear Regression</li>
<ul>	<li>8.4.1 Best-Fit Parameters for Multiple Regression</li>
	<li>8.4.2 Parameter Errors and Covariances for Multiple Regression</li>
	<li>8.4.3 Errors and Covariance for Linear Regression</li>
</ul>
	<li>8.5 Special Cases: Identical Errors or No Errors Available</li>
	<li>8.6 A Classic Experiment: Edwin Hubble's Discovery of the Expansion of the Universe</li>
	<li>8.7 Maximum Likelihood Method for Non-linear Functions</li>
	<li>8.8 Linear Regression with Poisson Data</li>
	<li>Problems</li>
</ul>
	<li>9 Multi-Variable Regression </li>
<ul>	<li>9.1 Multi-Variable Datasets</li>
	<li>9.2 A Classic Experiment: The R.A. Fisher and E. Anderson Measurements of Iris Characteristics</li>
	<li>9.3 The Multi-Variable Linear Regression</li>
	<li>9.4 Tests for Significance of the Multiple Regression Coefficients</li>
<ul>	<li>9.4.1 T-Test for the Significance of Model Components</li>
	<li>9.4.2 F-Test for Goodness of Fit</li>
	<li>9.4.3 The Coefficient of Determination</li>
</ul>
	<li>Problems</li>
</ul>
	<li>10 Goodness of Fit and Parameter Uncertainty </li>
<ul>	<li>10.1 Goodness of Fit for the Ïmin2 Fit Statistic</li>
	<li>10.2 Goodness of Fit for the Cash C Statistic</li>
	<li>10.3 Confidence Intervals of Parameters for Gaussian Data</li>
<ul>	<li>10.3.1 Confidence Interval on All Parameters</li>
	<li>10.3.2 Confidence Intervals on Reduced Numberof Parameters</li>
</ul>
	<li>10.4 Confidence Intervals of Parameters for Poisson Data</li>
	<li>10.5 The Linear Correlation Coefficient</li>
<ul>	<li>10.5.1 The Probability Distribution Function</li>
	<li>10.5.2 Hypothesis Testing</li>
</ul>
	<li>Problems</li>
</ul>
	<li>11 Systematic Errors and Intrinsic Scatter </li>
<ul>	<li>11.1 What to Do When the Goodness-of-Fit Test Fails</li>
	<li>11.2 Intrinsic Scatter and Debiased Variance</li>
<ul>	<li>11.2.1 Direct Calculation of the Intrinsic Scatter</li>
	<li>11.2.2 Alternative Method to Estimate the Intrinsic Scatter</li>
</ul>
	<li>11.3 Systematic Errors</li>
	<li>11.4 Estimate of Model Parameters with Systematic Errors or Intrinsic Scatter</li>
	<li>Problems</li>
</ul>
	<li>12 Fitting Two-Variable Datasets with Bivariate Errors </li>
<ul>	<li>12.1 Two-Variable Datasets with Bivariate Errors</li>
	<li>12.2 Generalized Least-Squares Linear Fit to Bivariate Data</li>
	<li>12.3 Linear Fit Using Bivariate Errors in the Ï2 Statistic</li>
	<li>Problems</li>
</ul>
	<li>13 Model Comparison </li>
<ul>	<li>13.1 The F Test</li>
<ul>	<li>13.1.1 F-Test for Two Independent Ï2 Measurements</li>
	<li>13.1.2 F-Test for an Additional Model Component</li>
</ul>
	<li>13.2 Kolmogorov&ndash;Smirnov Tests</li>
<ul>	<li>13.2.1 Comparison of Data to a Model</li>
	<li>13.2.2 Two-Sample Kolmogorov&ndash;Smirnov Test</li>
</ul>
	<li>Problems</li>
</ul>
	<li>14 Monte Carlo Methods </li>
<ul>	<li>14.1 What is a Monte Carlo Analysis?</li>
	<li>14.2 Traditional Monte Carlo Integration</li>
	<li>14.3 Dart Monte Carlo Integration and Function Evaluation</li>
	<li>14.4 Simulation of Random Variables</li>
	<li>14.5 Monte Carlo Estimates of Errors for Two-Variable Datasets</li>
<ul>	<li>14.5.1 The Bootstrap Method</li>
	<li>14.5.2 The Jackknife Method</li>
</ul>
	<li>Problems</li>
</ul>
	<li>15 Introduction to Markov Chains </li>
<ul>	<li>15.1 Stochastic Processes and Markov Chains</li>
	<li>15.2 Mathematical Properties of Markov Chains</li>
	<li>15.3 Recurrent and Transient States</li>
	<li>15.4 Limiting Probabilities and Stationary Distribution</li>
	<li>Problems</li>
</ul>
	<li>16 Monte Carlo Markov Chains </li>
<ul>	<li>16.1 Introduction to Monte Carlo Markov chains</li>
	<li>16.2 Requirements and Goals of a Monte Carlo Markov Chain</li>
	<li>16.3 The Metropolis&ndash;Hastings Algorithm</li>
	<li>16.4 The Gibbs Sampler</li>
	<li>16.5 Tests of Convergence</li>
<ul>	<li>16.5.1 The Geweke Z Score Test</li>
	<li>16.5.2 The Gelman&ndash;Rubin Test</li>
	<li>16.5.3 The Raftery&ndash;Lewis Test</li>
</ul>
	<li>Problems</li>
</ul>
	<li>Appendix: Numerical Tables </li>
<ul>	<li>A.1 The Gaussian Distribution and the Error Function</li>
	<li>A.2 Upper and Lower Limits for a Poisson Distribution</li>
	<li>A.3 The Ï2 Distribution</li>
	<li>A.4 The F Distribution</li>
	<li>A.5 The Student's t Distribution</li>
	<li>A.6 The Linear Correlation Coefficient r</li>
	<li>A.7 The Kolmogorov&ndash;Smirnov Test</li>
</ul>
	<li>References</li>
	<li>Index</li>
</ul>
</body></html>