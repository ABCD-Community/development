<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Untitled</title>
</head>
<body><div class="page"><p/>
<p>Springer Texts in Business and Economics
</p>
<p>Uwe Hassler
</p>
<p>An Elementary Introduction 
with Applications
</p>
<p>Stochastic 
Processes and 
Calculus</p>
<p/>
</div>
<div class="page"><p/>
<p>Springer Texts in Business and Economics</p>
<p/>
</div>
<div class="page"><p/>
<p>More information about this series at http://www.springer.com/series/10099</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/10099">http://www.springer.com/series/10099</a></div>
</div>
<div class="page"><p/>
<p>Uwe Hassler
</p>
<p>Stochastic Processes
and Calculus
</p>
<p>An Elementary Introduction
with Applications
</p>
<p>123</p>
<p/>
</div>
<div class="page"><p/>
<p>Uwe Hassler
Faculty of Economics and Business
</p>
<p>Administration
Goethe University Frankfurt
Frankfurt, Germany
</p>
<p>ISSN 2192-4333 ISSN 2192-4341 (electronic)
Springer Texts in Business and Economics
ISBN 978-3-319-23427-4 ISBN 978-3-319-23428-1 (eBook)
DOI 10.1007/978-3-319-23428-1
</p>
<p>Library of Congress Control Number: 2015957196
</p>
<p>Springer Cham Heidelberg New York Dordrecht London
&copy; Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)</p>
<p/>
<div class="annotation"><a href="www.springer.com">www.springer.com</a></div>
</div>
<div class="page"><p/>
<p>I do not know what I may appear to the
</p>
<p>world, but to myself I seem to have been only
</p>
<p>like a boy playing on the sea-shore, and
</p>
<p>diverting myself in now and then finding a
</p>
<p>smoother pebble or a prettier shell than
</p>
<p>ordinary, whilst the great ocean of truth lay
</p>
<p>all undiscovered before me.
</p>
<p>ISAAC NEWTON
</p>
<p>Quoted from the novel Beyond Sleep by
Willem Frederik Hermans</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>Over the past decades great importance has been placed on stochastic calculus
</p>
<p>and processes in mathematics, finance, and econometrics. This book addresses
</p>
<p>particularly readers from these fields, although students of other subjects as biology,
</p>
<p>engineering, or physics may find it useful, too.
</p>
<p>Scope of the Book
</p>
<p>By now there exist a number of books describing stochastic integrals and stochastic
</p>
<p>calculus in an accessible manner. Such introductory books, however, typically
</p>
<p>address an audience having previous knowledge about and interest in one of the
</p>
<p>following three fields exclusively: finance, econometrics, or mathematics. The
</p>
<p>textbook at hand attempts to provide an introduction into stochastic calculus and
</p>
<p>processes for students from each of these fields. Obviously, this can on no account
</p>
<p>be an exhaustive treatment. In the next chapter a survey of the topics covered
</p>
<p>is given. In particular, the book does neither deal with finance theory nor with
</p>
<p>statistical methods from the time series econometrician&rsquo;s toolkit; it rather provides
</p>
<p>a mathematical background for those readers interested in these fields.
</p>
<p>The first part of this book is dedicated to discrete-time processes for modeling
</p>
<p>temporal dependence in time series. We begin with some basic principles of
</p>
<p>stochastics enabling us to define stochastic processes as families of random variables
</p>
<p>in general. We discuss models for short memory (so-called ARMA models), for
</p>
<p>long memory (fractional integration), and for conditional heteroscedasticity (so-
</p>
<p>called ARCH models) in respective chapters. One further chapter is concerned
</p>
<p>with the so-called frequency domain or spectral analysis that is often neglected in
</p>
<p>introductory books. Here, however, we propose an approach that is not technically
</p>
<p>too demanding. Throughout, we restrict ourselves to the consideration of stochastic
</p>
<p>properties and interpretation. The statistical issues of parameter estimation, testing,
</p>
<p>and model specification are not addressed due to space limitations; instead, we refer
</p>
<p>to, e.g., Mills and Markellos (2008), Kirchg&auml;ssner, Wolters, and Hassler (2013), or
</p>
<p>Tsay (2005).
</p>
<p>The second part contains an introduction to stochastic integration. We start with
</p>
<p>elaborations on the Wiener process W.t/ as we will define (almost) all integrals in
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Preface
</p>
<p>terms of Wiener processes. In one chapter we consider Riemann integrals of the
</p>
<p>form
R
</p>
<p>f .t/W.t/dt, where f is a deterministic function. In another chapter Stieltjes
</p>
<p>integrals are constructed as
R
</p>
<p>f .t/dW.t/. More specifically, stochastic integrals as
</p>
<p>such result when a stochastic process is integrated with respect to the Wiener
</p>
<p>process, e.g., the Ito integral
R
</p>
<p>W.t/dW.t/. Solving stochastic differential equations
</p>
<p>is one task of stochastic integration for which we will need to use Ito&rsquo;s lemma. Our
</p>
<p>description aims at a similar compromise between concreteness and mathematical
</p>
<p>rigor as, e.g., Mikosch (1998). If the reader wants to address this matter more
</p>
<p>rigorously, we recommend Klebaner (2005) or &Oslash;ksendal (2003).
</p>
<p>The third part of the book applies previous results. The chapter on stochastic
</p>
<p>differential equations consists basically of applications of Ito&rsquo;s lemma. Concrete
</p>
<p>differential equations, as they are used, e.g., when modeling interest rate dynamics,
</p>
<p>will be covered in a separate chapter. The second area of application concerns
</p>
<p>certain limiting distributions of time series econometrics. A separate chapter on the
</p>
<p>asymptotics of integrated processes covers weak convergence to Wiener processes.
</p>
<p>The final two chapters contain applications for nonstationary processes without
</p>
<p>cointegration on the one hand and for the analysis of cointegrated processes on the
</p>
<p>other. Further details regarding econometric application can be found in the books
</p>
<p>by Banerjee, Dolado, Galbraith and Hendry (1993), Hamilton (1994), or Tanaka
</p>
<p>(1996).
</p>
<p>The exposition in this book is elementary in the sense that knowledge of measure
</p>
<p>theory is neither assumed nor used. Consequently, mathematical foundations cannot
</p>
<p>be treated rigorously which is why, e.g., proofs of existence are omitted. Rather I
</p>
<p>had two goals in mind when writing this book. On the one hand, I wanted to give a
</p>
<p>basic and illustrative presentation of the relevant topics without many &ldquo;troublesome&rdquo;
</p>
<p>derivations. On the other hand, in many parts a technically advanced level has
</p>
<p>been aimed at: procedures are not only presented in form of recipes but are to
</p>
<p>be understood as far as possible which means they are to be proven. In order to
</p>
<p>meet both requirements jointly, this book is equipped with a lot of challenging
</p>
<p>problems at the end of each chapter as well as with the corresponding detailed
</p>
<p>solutions. Thus the virtual text &ndash; augmented with more than 60 basic examples and
</p>
<p>45 illustrative figures &ndash; is rather easy to read while a part of the technical arguments
</p>
<p>is transferred to the exercise problems and their solutions. This is why there are at
</p>
<p>least two possible ways to work with the book. For those who are merely interested
</p>
<p>in applying the methods introduced, the reading of the text is sufficient. However,
</p>
<p>for an in-depth knowledge of the theory and its application, the reader necessarily
</p>
<p>needs to study the problems and their solution extensively.
</p>
<p>Note to Students and Instructors
</p>
<p>I have taught the material collected here to master students (and diploma students
</p>
<p>in the old days) of economics and finance or students of mathematics with a minor
</p>
<p>in those fields. From my personal experience I may say that the material presented
</p>
<p>here is too vast to be treated in a course comprising 45 contact hours. I used the</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface ix
</p>
<p>textbook at hand for four slightly differing courses corresponding to four slightly
</p>
<p>differing routes through the parts of the book. Each of these routes consists of three
</p>
<p>stages: time series models, stochastic integration, and applications. After Part I on
</p>
<p>time series modeling, the different routes separate.
</p>
<p>The finance route: When teaching an audience with an exclusive interest in
</p>
<p>finance, one may simply drop the final three chapters. The second stage of the course
</p>
<p>then consists of Chaps. 7, 8, 9, 10, and 11. This Part II on stochastic integration is
</p>
<p>finally applied to the solution of stochastic differential equations and interest rate
</p>
<p>modeling in Chaps. 12 and 13, respectively.
</p>
<p>The mathematics route: There is a slight variant of the finance route for the
</p>
<p>mathematically inclined audience with an equal interest in finance or econometrics.
</p>
<p>One simply replaces Chap. 13 on interest rate modeling by Chap. 14 on weak con-
</p>
<p>vergence on function spaces, which is relevant for modern time series asymptotics.
</p>
<p>The econometrics route: After Part I on time series modeling, the students from
</p>
<p>a class on time series econometrics should be exposed to Chaps. 7, 8, 9, and 10 on
</p>
<p>Wiener processes and stochastic integrals. The three chapters (Chaps. 11, 12, and
</p>
<p>13) on Ito&rsquo;s lemma and its applications may be skipped to conclude the course
</p>
<p>with the last three chapters (Chaps. 14, 15, and 16) culminating in the topic of
</p>
<p>&ldquo;cointegration.&rdquo;
</p>
<p>The nontechnical route: Finally, the entire content of the textbook at hand can
</p>
<p>still be covered in one single semester; however, this comes with the cost of omitting
</p>
<p>technical aspects for the most part. Each chapter contains a rather technical section
</p>
<p>which in principle can be skipped without leading to a loss in understanding. When
</p>
<p>omitting these potentially difficult sections, it is possible to go through all the
</p>
<p>chapters in a single course. The following sections should be skipped for a less
</p>
<p>technical route:
</p>
<p>3.3 &amp; 4.3 &amp; 5.4 &amp; 6.4 &amp; 7.3 &amp; 8.4 &amp; 9.4
</p>
<p>&amp; 10.4 &amp; 11.4 &amp; 12.2 &amp; 13.4 &amp; 14.3 &amp; 15.4 &amp; 16.4 .
</p>
<p>It has been mentioned that each chapter concludes with problems and solutions.
</p>
<p>Some of them are clearly too hard or lengthy to be dealt with in exams, while others
</p>
<p>are questions from former exams of my own or are representative of problems to be
</p>
<p>solved in my exams.
</p>
<p>Frankfurt, Germany Uwe Hassler
</p>
<p>July 2015
</p>
<p>References
</p>
<p>Banerjee, A., Dolado, J. J., Galbraith, J. W., &amp; Hendry, D. F. (1993). Co-integration, error
correction, and the econometric analysis of non-stationary data. Oxford/New York: Oxford
University Press.
</p>
<p>Hamilton, J. (1994). Time series analysis. Princeton: Princeton University Press.
Kirchg&auml;ssner, G., Wolters, J., &amp; Hassler, U. (2013). Introduction to modern time series analysis
</p>
<p>(2nd ed.). Berlin/New York: Springer.</p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface
</p>
<p>Klebaner, F. C. (2005). Introduction to stochastic calculus with applications (2nd ed.). London:
Imperical College Press.
</p>
<p>Mikosch, Th. (1998). Elementary stochastic calculus with finance in view. Singapore: World
Scientific Publishing.
</p>
<p>Mills, T. C., &amp; Markellos, R. N. (2008). The econometric modelling of financial time series (3rd
ed.). Cambridge/New York: Cambridge University Press.
</p>
<p>&Oslash;ksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.).
Berlin/New York: Springer.
</p>
<p>Tanaka, K. (1996). Time series analysis: Nonstationary and noninvertible distribution theory.
New York: Wiley.
</p>
<p>Tsay, R. S. (2005). Analysis of financial time series (2nd ed.). New York: Wiley.</p>
<p/>
</div>
<div class="page"><p/>
<p>Acknowledgments
</p>
<p>This textbook grew out of lecture notes from which I taught over 15 years. Without
</p>
<p>my students&rsquo; thirst for knowledge and their critique, I would not even have started
</p>
<p>the project. In particular, I thank Bal&aacute;zs Cserna, Matei Demetrescu, Eduard Dubin,
</p>
<p>Mehdi Hosseinkouchack, Vladimir Kuzin, Maya Olivares, Marc Pohle, Adina
</p>
<p>Tarcolea, and Mu-Chun Wang who corrected numerous errors in the manuscript.
</p>
<p>Originally, large parts of this text had been written in German, and I thank Verena
</p>
<p>Werkmann for her help when translating into English. Last but not least I am
</p>
<p>indebted to Goethe University Frankfurt for allowing me to take sabbatical leave.
</p>
<p>Without this support I would not have been able to complete this book at a time
</p>
<p>when academics are under pressure to publish in the first place primary research.
</p>
<p>xi</p>
<p/>
</div>
<div class="page"><p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.2 Finance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
</p>
<p>1.3 Econometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.4 Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
</p>
<p>1.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
</p>
<p>Part I Time Series Modeling
</p>
<p>2 Basic Concepts from Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.2 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
</p>
<p>2.3 Joint and Conditional Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
</p>
<p>2.4 Stochastic Processes (SP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>2.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
</p>
<p>3 Autoregressive Moving Average Processes (ARMA) . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.2 Moving Average Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
</p>
<p>3.3 Lag Polynomials and Invertibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>3.4 Autoregressive and Mixed Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
</p>
<p>3.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
</p>
<p>4 Spectra of Stationary Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
</p>
<p>4.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
</p>
<p>4.2 Definition and Interpretation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
</p>
<p>4.3 Filtered Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>4.4 Examples of ARMA Spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
</p>
<p>4.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>5 Long Memory and Fractional Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>5.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>5.2 Persistence and Long Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
</p>
<p>5.3 Fractionally Integrated Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
</p>
<p>5.4 Generalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
</p>
<p>5.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
</p>
<p>6 Processes with Autoregressive Conditional
</p>
<p>Heteroskedasticity (ARCH) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
</p>
<p>6.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
</p>
<p>6.2 Time-Dependent Heteroskedasticity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
</p>
<p>6.3 ARCH Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
</p>
<p>6.4 Generalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
</p>
<p>6.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
</p>
<p>Part II Stochastic Integrals
</p>
<p>7 Wiener Processes (WP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>7.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>7.2 From Random Walk to Wiener Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
</p>
<p>7.3 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
</p>
<p>7.4 Functions of Wiener Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
</p>
<p>7.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
</p>
<p>8 Riemann Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>8.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>8.2 Definition and Fubini&rsquo;s Theorem .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>8.3 Riemann Integration of Wiener Processes . . . . . . . . . . . . . . . . . . . . . . . . . . 183
</p>
<p>8.4 Convergence in Mean Square . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
</p>
<p>8.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
</p>
<p>9 Stieltjes Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>9.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>9.2 Definition and Partial Integration .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>9.3 Gaussian Distribution and Autocovariances . . . . . . . . . . . . . . . . . . . . . . . . 202
</p>
<p>9.4 Standard Ornstein-Uhlenbeck Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
</p>
<p>9.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
</p>
<p>Reference .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
</p>
<p>10 Ito Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>10.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>10.2 A Special Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
</p>
<p>10.3 General Ito Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>10.4 (Quadratic) Variation .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
</p>
<p>10.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>11 Ito&rsquo;s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>11.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>11.2 The Univariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>11.3 Bivariate Diffusions with One WP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>11.4 Generalization for Independent WP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
</p>
<p>11.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
</p>
<p>Reference .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
</p>
<p>Part III Applications
</p>
<p>12 Stochastic Differential Equations (SDE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>12.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>12.2 Definition and Existence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
</p>
<p>12.3 Linear Stochastic Differential Equations .. . . . . . . . . . . . . . . . . . . . . . . . . . . 265
</p>
<p>12.4 Numerical Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
</p>
<p>12.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
</p>
<p>13 Interest Rate Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
</p>
<p>13.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
</p>
<p>13.2 Ornstein-Uhlenbeck Process (OUP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
</p>
<p>13.3 Positive Linear Interest Rate Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
</p>
<p>13.4 Nonlinear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
</p>
<p>13.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
</p>
<p>14 Asymptotics of Integrated Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
</p>
<p>14.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
</p>
<p>14.2 Limiting Distributions of Integrated Processes . . . . . . . . . . . . . . . . . . . . . 303
</p>
<p>14.3 Weak Convergence of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
</p>
<p>14.4 Multivariate Limit Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
</p>
<p>14.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
</p>
<p>15 Trends, Integration Tests and Nonsense Regressions . . . . . . . . . . . . . . . . . . . 331
</p>
<p>15.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
</p>
<p>15.2 Trend Regressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
</p>
<p>15.3 Integration Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
</p>
<p>15.4 Nonsense Regression .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
</p>
<p>15.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>16 Cointegration Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>16.1 Summary.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>16.2 Error-Correction and Cointegration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
</p>
<p>16.3 Cointegration Regressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
</p>
<p>16.4 Cointegration Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
</p>
<p>16.5 Problems and Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
</p>
<p>References .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389</p>
<p/>
</div>
<div class="page"><p/>
<p>List of Figures
</p>
<p>Fig. 3.1 Simulated MA(1) processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
</p>
<p>Fig. 3.2 Simulated AR(1) processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
</p>
<p>Fig. 3.3 Stationarity triangle for AR(2) processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>Fig. 3.4 Autocorrelograms for AR(2) processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
</p>
<p>Fig. 3.5 Autocorrelograms for ARMA(1,1) processes. . . . . . . . . . . . . . . . . . . . . . . 67
</p>
<p>Fig. 4.1 Cosine cycle with different frequencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
</p>
<p>Fig. 4.2 Spectra of MA(S) Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>Fig. 4.3 Business Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
</p>
<p>Fig. 4.4 AR(1) spectra (2� f .�/) with positive autocorrelation . . . . . . . . . . . . . 91
</p>
<p>Fig. 4.5 AR(1) spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
</p>
<p>Fig. 4.6 AR(2) spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
</p>
<p>Fig. 4.7 ARMA(1,1) spectra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
</p>
<p>Fig. 4.8 Spectra of multiplicative seasonal AR processes . . . . . . . . . . . . . . . . . . . 95
</p>
<p>Fig. 5.1 Hyperbolic decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
</p>
<p>Fig. 5.2 Exponential decay .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
</p>
<p>Fig. 5.3 Autocorrelogram of fractional noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
</p>
<p>Fig. 5.4 Spectrum of fractional noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
</p>
<p>Fig. 5.5 Simulated fractional noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
</p>
<p>Fig. 5.6 Nonstationary fractional noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
</p>
<p>Fig. 6.1 ARCH(1) with ˛0 D 1 and ˛1 D 0:5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Fig. 6.2 ARCH(1) with ˛0 D 1 and ˛1 D 0:9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Fig. 6.3 GARCH(1,1) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:3 . . . . . . . . . . . . . . . . 137
Fig. 6.4 GARCH(1,1) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:5 . . . . . . . . . . . . . . . . 138
Fig. 6.5 IGARCH(1,1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
</p>
<p>Fig. 6.6 GARCH(1,1)-M .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
</p>
<p>Fig. 6.7 EGARCH(1,1). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
</p>
<p>Fig. 7.1 Step function on the interval [0,1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
</p>
<p>Fig. 7.2 Simulated paths of the WP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
</p>
<p>Fig. 7.3 WP and Brownian motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
</p>
<p>Fig. 7.4 WP and Brownian motion with drift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
</p>
<p>Fig. 7.5 WP and Brownian bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
</p>
<p>Fig. 7.6 WP and reflected WP along with expectation . . . . . . . . . . . . . . . . . . . . . . 165
xvii</p>
<p/>
</div>
<div class="page"><p/>
<p>xviii List of Figures
</p>
<p>Fig. 7.7 Geometric Brownian motion along with expectation.. . . . . . . . . . . . . . 166
</p>
<p>Fig. 7.8 WP and geometric Brownian motion.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
</p>
<p>Fig. 7.9 WP and maximum process along with expectation.. . . . . . . . . . . . . . . . 168
</p>
<p>Fig. 7.10 WP and integrated WP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
</p>
<p>Fig. 9.1 Standard Ornstein-Uhlenbeck processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
</p>
<p>Fig. 10.1 Sine cycles of different frequencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
</p>
<p>Fig. 13.1 OUP with Starting Values X.0/ D � D 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
Fig. 13.2 OUP with Starting Value X.0/ D 5:1 including
</p>
<p>Expected Value Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
</p>
<p>Fig. 13.3 Interest Rate Dynamics According to Dothan . . . . . . . . . . . . . . . . . . . . . . 291
</p>
<p>Fig. 13.4 Interest Rate Dynamics According to Brennan-Schwartz .. . . . . . . . . 292
</p>
<p>Fig. 13.5 Interest Rate Dynamics According to CKLS . . . . . . . . . . . . . . . . . . . . . . . 294
</p>
<p>Fig. 13.6 OUP and CIR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
</p>
<p>Fig. 15.1 Linear Time Trend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333</p>
<p/>
</div>
<div class="page"><p/>
<p>1Introduction
</p>
<p>1.1 Summary
</p>
<p>Stochastic calculus is used in finance and econom(etr)ics for instance for solving
</p>
<p>stochastic differential equations and handling stochastic integrals. This requires
</p>
<p>stochastic processes. Although stemming from a rather recent area of mathematics,
</p>
<p>the methods of stochastic calculus have shortly come to be widely spread not only
</p>
<p>in finance and economics. Moreover, these techniques &ndash; along with methods of time
</p>
<p>series modeling &ndash; are central in the contemporary econometric tool box. In this
</p>
<p>introductory chapter some motivating questions are brought up being answered in
</p>
<p>the course of the book, thus providing a brief survey of the topics treated.
</p>
<p>1.2 Finance
</p>
<p>The names of two Nobel prize winners1 dealing with finance are closely connected
</p>
<p>to one field of applications treated in the textbook at hand. The analysis and the
</p>
<p>modeling of stock prices and returns is central to this work.
</p>
<p>Stock Prices
</p>
<p>Let S.t/, t � 0, be the continuous stock price of a stock with return R.t/ D S0.t/=S.t/
expressed as growth rate. We assume constant returns,
</p>
<p>R.t/ D c &rdquo; S0.t/ D c S.t/ &rdquo; dS.t/
dt
</p>
<p>D cS.t/:
</p>
<p>1In 1997, R.C. Merton and M.S. Scholes were awarded the Nobel prize jointly, &ldquo;for a new method
to determine the value of derivatives&rdquo; (according to the official statement of the Nobel Committee).
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_1
</p>
<p>1</p>
<p/>
</div>
<div class="page"><p/>
<p>2 1 Introduction
</p>
<p>This differential equation for the stock price is usually also written as follows:
</p>
<p>dS.t/ D c S.t/ dt : (1.1)
</p>
<p>The corresponding solution is (see Problem 1.1)
</p>
<p>S.t/ D S.0/ ec t ; (1.2)
</p>
<p>i.e. if c &gt; 0 the exponential process is explosive. The assumption of a deterministic
</p>
<p>stock price movement is of course unrealistic which is why a stochastic differential
</p>
<p>equation consistent with (1.1) is often assumed since Black and Scholes (1973) and
</p>
<p>Merton (1973),
</p>
<p>dS.t/ D c S.t/ dt C � S.t/ dW.t/ ; (1.3)
</p>
<p>where dW.t/ are the increments of a so-called Wiener process W.t/ (also referred to
</p>
<p>as Brownian motion, cf. Chap. 7). This is a stochastic process, i.e. a random process.
</p>
<p>Thus, for a fixed point in time t, S.t/ is a random variable. How does this random
</p>
<p>variable behave on average? How do the parameters c and � affect the expected
</p>
<p>value and the variance as time passes by? We will find answers to these questions in
</p>
<p>Chap. 12 on stochastic differential equations.
</p>
<p>Interest Rates
</p>
<p>Next, r.t/ denotes an interest rate for t � 0. Assume it is given by the differential
equation
</p>
<p>dr.t/ D c .r.t/ � �/ dt (1.4)
</p>
<p>with c 2 R or equivalently by
</p>
<p>r0.t/ D dr.t/
dt
</p>
<p>D c .r.t/ � �/:
</p>
<p>Expression (1.4) can alternatively be written as the following integral equation:
</p>
<p>r.t/ D r.0/C c
Z t
</p>
<p>0
</p>
<p>.r.s/ � �/ ds: (1.5)
</p>
<p>The solution to this reads (see Problem 1.2)
</p>
<p>r.t/ D �C ec t .r.0/ � �/ : (1.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Econometrics 3
</p>
<p>For c &lt; 0 therefore it holds that the interest rate converges to � as time goes
</p>
<p>by. Again, a deterministic movement is not realistic. This is why Vasicek (1977)
</p>
<p>specified a stochastic differential equation consistent with (1.4):
</p>
<p>dr.t/ D c .r.t/ � �/ dt C � dW.t/ : (1.7)
</p>
<p>As aforementioned, dW.t/ denotes the increments of a Wiener process. How is the
</p>
<p>interest rate movement (on average) affected by the parameter c? Which kind of
</p>
<p>stochastic process is described by (1.7)? The answers to these and similar questions
</p>
<p>will be obtained in Chap. 13 on interest rate models.
</p>
<p>Empirical Returns
</p>
<p>Looking at return time series one can observe that the variance (or volatility)
</p>
<p>fluctuates a lot as time passes by. Long quiet market phases characterized by only
</p>
<p>mild variation are followed by short periods characterized by extreme observations
</p>
<p>where extreme amplitudes again tend to entail extreme observations. Such a
</p>
<p>behavior is in conflict with the assumption of normally distributed data. It is an
</p>
<p>empirically well confirmed law (&ldquo;stylized fact&rdquo;) that financial market data in general
</p>
<p>and returns in particular produce &ldquo;outliers&rdquo; with larger probability than it would be
</p>
<p>expected under normality.
</p>
<p>It is crucial, however, that extreme observations occur in clusters (volatility
</p>
<p>clusters). Even though returns are not correlated over time in efficient markets, they
</p>
<p>are not independent as there exists a systematic time dependence of volatility. Engle
</p>
<p>(1982) suggested the so-called ARCH model (see Chap. 6) in order to capture the
</p>
<p>outlined effects. His work constituted an entire field of research known nowadays
</p>
<p>under the keyword &ldquo;financial econometrics&rdquo;, and consequently he was awarded the
</p>
<p>Nobel prize in 2003.2
</p>
<p>1.3 Econometrics
</p>
<p>Clive Granger (1934&ndash;2009) was a British econometrician who created the concept
</p>
<p>of cointegration (Granger, 1981). He shared the Nobel prize &ldquo;for methods of
</p>
<p>analyzing economic time series with common trends (cointegration)&rdquo; (official
</p>
<p>statement of the Nobel Committee) with R.F. Engle. The leading example of
</p>
<p>trending time series he considered is the random walk.
</p>
<p>2R.F. Engle shared the Nobel prize &ldquo;for methods of analyzing economic time series with time-
varying volatility (ARCH)&rdquo; (official statement of the Nobel Committee) with C.W.J. Granger.</p>
<p/>
</div>
<div class="page"><p/>
<p>4 1 Introduction
</p>
<p>RandomWalks
</p>
<p>In econometrics, we are often concerned with time series not fluctuating with
</p>
<p>constant variance around a fixed level. A widely-used model for accounting for
</p>
<p>this nonstationarity are so-called integrated processes. They form the basis for the
</p>
<p>cointegration approach that has become an integral part of common econometric
</p>
<p>methodology since Engle and Granger (1987). Let&rsquo;s consider a special case &ndash; the
</p>
<p>random walk &ndash; as a preliminary model,
</p>
<p>xt D
tX
</p>
<p>jD1
"j ; t D 1; : : : ; n ; (1.8)
</p>
<p>where f"tg is a random process, i.e. "t and "s, t &curren; s, are uncorrelated or even
independent with zero expected value and constant variance �2. For a random walk
</p>
<p>with zero starting value x0 D 0 it holds by definition that:
</p>
<p>xt D xt�1 C "t ; t D 1; : : : ; n ; with Var.xt/ D �2 t : (1.9)
</p>
<p>The increments can also be written using the difference operator�,
</p>
<p>�xt D xt � xt�1 D "t :
</p>
<p>Regressing two stochastically independent random walks on each other, a statisti-
</p>
<p>cally significant relationship is identified which is a statistical artefact and therefore
</p>
<p>nonsense (see Chap. 15). Two random walks following a common trend, however,
</p>
<p>are called cointegrated. In this case the regression on each other does not only
</p>
<p>give the consistent estimation of the true relationship but the estimator is even
</p>
<p>&ldquo;superconsistent&rdquo; (cf. Chap. 16).
</p>
<p>Dickey-Fuller Distribution
</p>
<p>If one wants to test whether a given time series indeed follows a random walk, then
</p>
<p>equation (1.9) suggests to estimate the regression
</p>
<p>xt D Oa xt�1 C O"t ; t D 1; : : : ; n :
</p>
<p>From this, the (ordinary) least squares (LS) estimator under the null hypothe-
</p>
<p>sis (1.9), i.e. under a D 1, is obtained as
</p>
<p>Oa D
Pn
</p>
<p>tD1 xt xt�1Pn
tD1 x
</p>
<p>2
t�1
</p>
<p>D 1C
Pn
</p>
<p>tD1 xt�1 "tPn
tD1 x
</p>
<p>2
t�1
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>1.3 Econometrics 5
</p>
<p>This constitutes the basic ingredient for the test by Dickey and Fuller (1979). Under
</p>
<p>the null hypothesis of a random walk (a D 1) it holds asymptotically (n ! 1)
</p>
<p>n .Oa � 1/ d! DFa ; (1.10)
</p>
<p>where &ldquo;
d!&rdquo; stands for convergence in distribution and DFa denotes the so-called
</p>
<p>Dickey-Fuller distribution. Corresponding modes of convergence will be explained
</p>
<p>in Chap. 14. Since Phillips (1987) an elegant way for expressing the Dickey-Fuller
</p>
<p>distribution by stochastic integrals is known (again, W.t/ denotes a Wiener process):
</p>
<p>DFa D
R 1
0
</p>
<p>W.t/ dW.t/
R 1
0
</p>
<p>W2.t/ dt
: (1.11)
</p>
<p>Note (and enjoy!) the formal correspondence of the sum of squares
Pn
</p>
<p>tD1 x
2
t�1 in
</p>
<p>the denominator of Oa � 1 and the integral over the squared Wiener process in the
denominator of (1.11),
</p>
<p>R 1
0
</p>
<p>W2.t/ dt (this is a Riemann integral, cf. Chap. 8). Just
</p>
<p>as well the sum
Pn
</p>
<p>tD1 xt�1 "t D
Pn
</p>
<p>tD1 xt�1�xt resembles the so-called Ito integralR 1
0
</p>
<p>W.t/ dW.t/. But how are these integrals defined, what are they about? How is this
</p>
<p>distribution (and similar ones) attained? And why does there exist another equivalent
</p>
<p>representation,
</p>
<p>DFa D
W2.1/ � 1
2
R 1
0
</p>
<p>W2.t/ dt
; (1.12)
</p>
<p>of the Dickey-Fuller distribution? We concern ourselves with these questions in
</p>
<p>connection with Ito&rsquo;s lemma in Chap. 11.
</p>
<p>Autocorrelation
</p>
<p>The assumption of the increments �xt D xt � xt�1 of economic times series being
free from serial (temporal) correlation &ndash; as it is true for the random walk &ndash; is too
</p>
<p>restrictive in practice. Thus, we have to learn how the Dickey-Fuller distribution is
</p>
<p>generalized with autocorrelated (i.e. serially correlated) increments. In practise, so-
</p>
<p>called ARMA models are used most frequently in order to model autocorrelation.
</p>
<p>This class of models will be discussed intuitively as well as rigorously in Chap. 3.
</p>
<p>The so-called spectral analysis translates autocorrelation patterns in oscillation
</p>
<p>patterns. In Chap. 4 we learn to determine which frequency&rsquo;s or period&rsquo;s oscillations
</p>
<p>add particularly intensely to a time series&rsquo; variation. Often economists are refused
</p>
<p>access to spectral analysis because of the extensive use of complex numbers.
</p>
<p>Therefore, we suggest an approach that avoids complex numbers. Finally, Chap. 5
</p>
<p>introduces a model where the temporal dependence is particularly persistent such
</p>
<p>that the autocorrelations die out more slowly than in the ARMA case. Such a feature</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Introduction
</p>
<p>has been called &ldquo;long memory&rdquo; and is observed with many economic and financial
</p>
<p>series.
</p>
<p>1.4 Mathematics
</p>
<p>Stochastic calculus, which will be applied here, is a rather recent area in mathemat-
</p>
<p>ics. It was pioneered by Kiyoshi Ito3 in a sequence of pathbreaking papers published
</p>
<p>in Japanese starting from the forties of the last century.4 The Ito integral as a special
</p>
<p>case of stochastic integration is introduced in Chap. 10.
</p>
<p>Ito Integrals
</p>
<p>The aforementioned interest rate model by Vasicek (1977) leads to a stochastic
</p>
<p>process given by an integral constructed as
R t
0
</p>
<p>f .s/ dW.s/ where f is a deterministic
</p>
<p>function and again dW denotes the increments of a Wiener process. Such integrals &ndash;
</p>
<p>being in a sense classical integrals &ndash; will be defined as Stieltjes integrals in Chap. 9.
</p>
<p>Ito integrals are a generalization of these. At first glance, the deterministic function
</p>
<p>f is replaced by a stochastic process X,
R t
0
</p>
<p>X.s/ dW.s/. Mathematically, this results
</p>
<p>in a considerably more complicated object, the definition thereof being a problem
</p>
<p>on its own, cf. Chap. 10.
</p>
<p>Ito&rsquo;s Lemma
</p>
<p>At this point, the idea of Ito&rsquo;s lemma is briefly conveyed. For the moment, assume
</p>
<p>a deterministic (differentiable) function f .t/. Using the chain rule it holds for the
</p>
<p>derivative of the square f 2:
</p>
<p>df 2.t/
</p>
<p>dt
D 2 f .t/ f 0.t/
</p>
<p>or rather
</p>
<p>df 2.t/
</p>
<p>2
D f .t/ f 0.t/ dt D f .t/ df .t/ : (1.13)
</p>
<p>3Alternative transcriptions of his name into the Latin alphabet, It&ocirc; or Itō, are frequently used in
the literature and are equally accepted. In this textbook we follow the spelling of Ito&rsquo;s compatriot
(Tanaka, 1996).
4In 2006, Ito received the inaugural Gauss Prize for Applied Mathematics by the International
Mathematical Union, which is awarded every fourth year since then.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Problems and Solutions 7
</p>
<p>Thus, for the ordinary integral it follows
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ df .s/ D
Z t
</p>
<p>0
</p>
<p>f .s/ f 0.s/ ds D 1
2
</p>
<p>f 2.s/
</p>
<p>ˇ̌
ˇ̌
t
</p>
<p>0
</p>
<p>D 1
2
</p>
<p>�
f 2.t/ � f 2.0/
</p>
<p>�
:
</p>
<p>However, among other things, we will learn that the Wiener process is not a
</p>
<p>differentiable function with respect to time t. The ordinary chain rule does not apply
</p>
<p>and for the according Ito integral one obtains
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ D 1
2
</p>
<p>�
W2.s/ � s
</p>
<p>�ˇ̌ˇ̌
t
</p>
<p>0
</p>
<p>D 1
2
</p>
<p>�
W2.t/ � W2.0/� t
</p>
<p>�
: (1.14)
</p>
<p>This result follows from the famous and fundamental lemma by Ito being a kind of
</p>
<p>&ldquo;stochastified chain rule&rdquo; for Wiener processes in its simplest case. Instead of (1.13)
</p>
<p>for Wiener processes it holds that
</p>
<p>dW2.t/
</p>
<p>2
D W.t/ dW.t/C 1
</p>
<p>2
dt : (1.15)
</p>
<p>Substantial generalizations and multivariate extensions will be discussed in
</p>
<p>Chap. 11. In particular, Ito&rsquo;s lemma will enable us to solve stochastic differential
</p>
<p>equations in Chap. 12, and it will turn out that S.t/ solving (1.3) is a so-called
</p>
<p>geometric Brownian motion. In Chap. 13 we will look in greater detail in models
</p>
<p>for interest rates as e.g. given by Eq. (1.7).
</p>
<p>Starting point for all the considerations outlined is the Wiener process &ndash; often
</p>
<p>also called Brownian motion. Before turning to it and its properties, general
</p>
<p>stochastic processes need to be defined and classified beforehand. This is done &ndash;
</p>
<p>among other things &ndash; in the following chapter on basic concepts from probability
</p>
<p>theory.
</p>
<p>1.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>1.1 Solve the differential equation (1.1), i.e. obtain the solution (1.2).
</p>
<p>1.2 Verify that r.t/ from (1.6) solves the differential equation (1.4).
</p>
<p>1.3 Consider a simple regression model,
</p>
<p>yi D ˛ C ˇ xi C "i ; i D 1; : : : ; n ;</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Introduction
</p>
<p>with OLS estimator Ǒ. Check that:
</p>
<p>Ǒ � ˇ D
Pn
</p>
<p>iD1 .xi � x/ "iPn
iD1 .xi � x/2
</p>
<p>;
</p>
<p>with arithmetic mean x.
</p>
<p>Hint:
Pn
</p>
<p>iD1 .xi � x/ D 0.
</p>
<p>1.4 Let f n denote n-th power of a function f with derivative f 0, n 2 N. Show that:
</p>
<p>f n.t/ D f n.0/C n
Z t
</p>
<p>0
</p>
<p>f n�1.s/ f 0.s/ ds :
</p>
<p>Hint: Chain rule as in (1.13).
</p>
<p>Solutions
</p>
<p>1.1 Using equation (1.1) we get by integration
</p>
<p>Z t
</p>
<p>0
</p>
<p>S0.r/
</p>
<p>S.r/
dr D
</p>
<p>Z t
</p>
<p>0
</p>
<p>c dr D ct:
</p>
<p>Since5
</p>
<p>d log.S.t//
</p>
<p>dt
D S
</p>
<p>0.t/
</p>
<p>S.t/
</p>
<p>this implies
</p>
<p>log.S.t// � log.S.0// D ct ;
</p>
<p>or
</p>
<p>S.t/ D elog.S.0// ect
</p>
<p>D S.0/ ect ;
</p>
<p>which is the required solution.
</p>
<p>5By &ldquo;log&rdquo; we denote the natural logarithm to the base e.</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Problems and Solutions 9
</p>
<p>1.2 Taking the derivative of (1.6) yields:
</p>
<p>dr.t/
</p>
<p>dt
D c ect .r.0/� �/
</p>
<p>D c .r.t/ � �/ ;
</p>
<p>where again the given form of r.t/ was used. By purely symbolically multiplying
</p>
<p>by dt the equation (1.4) is obtained. Hence, the problem is already solved.
</p>
<p>1.3 It is well known that the OLS estimator is given by &ldquo;covariance divided by
</p>
<p>variance of the regressor&rdquo;, i.e. it holds that:
</p>
<p>Ǒ D
1
n
</p>
<p>Pn
iD1.xi � Nx/.yi � Ny/
1
n
</p>
<p>Pn
iD1.xi � Nx/2
</p>
<p>:
</p>
<p>Because of
Pn
</p>
<p>iD1.xi � Nx/ D 0 this simplifies to
</p>
<p>Ǒ D
1
n
</p>
<p>Pn
iD1.xi � Nx/yi
</p>
<p>1
n
</p>
<p>Pn
iD1.xi � Nx/2
</p>
<p>:
</p>
<p>Assuming the model to be correct and substituting yi D ˛ C ˇ xi C "i, one obtains
</p>
<p>Ǒ D
Pn
</p>
<p>iD1.xi � Nx/.˛ C ˇ xi C "i/Pn
iD1.xi � Nx/2
</p>
<p>:
</p>
<p>Again applying the argument
Pn
</p>
<p>iD1.xi � Nx/ D 0 yields
</p>
<p>Ǒ D
Pn
</p>
<p>iD1.xi � Nx/.ˇ.xi � Nx/C "i/Pn
iD1.xi � Nx/2
</p>
<p>D ˇ C
Pn
</p>
<p>iD1.xi � Nx/"iPn
iD1.xi � Nx/2
</p>
<p>:
</p>
<p>This was exactly the claim.
</p>
<p>1.4 We address the problem in a slightly more general way. Let g be a differentiable
</p>
<p>function with derivative g0. By the fundamental theorem of calculus it holds that6
</p>
<p>tZ
</p>
<p>0
</p>
<p>g0.s/ds D g.t/ � g.0/;
</p>
<p>6For an introduction to calculus we recommend Trench (2013); this book is available electronically
for free as a textbook approved by the American Institute of Mathematics.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Introduction
</p>
<p>or
</p>
<p>g.t/ D g.0/C
tZ
</p>
<p>0
</p>
<p>g0.s/ds:
</p>
<p>If g describes a process over time, this last relation can be interpreted the following
</p>
<p>way: The value at time t is made up by the starting value g.0/ plus the sum or
</p>
<p>integral over all changes occurring between 0 and t. Now, choosing in particular
</p>
<p>g.t/ D f n.t/ with
</p>
<p>g0.t/ D nf n�1.t/f 0.t/ ;
</p>
<p>we obtain the required result.
</p>
<p>References
</p>
<p>Black, F., &amp; Scholes, M. (1973). The pricing of options and corporate liabilities. The Journal of
Political Economy, 81, 637&ndash;654.
</p>
<p>Dickey, D. A., &amp; Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series
with a unit root. Journal of the American Statistical Association, 74, 427&ndash;431.
</p>
<p>Engle, R. F. (1982). Autoregressive conditional heteroskedasticity with estimates of the variance
of U.K. inflation. Econometrica, 50, 987&ndash;1008.
</p>
<p>Engle, R. F., &amp; Granger, C. W. J. (1987). Co-integration and error correction: Representation,
estimation, and testing. Econometrica, 55, 251&ndash;276.
</p>
<p>Granger, C. W. J. (1981). Some properties of time series data and their use in econometric model
specification. Journal of Econometrics, 16, 121&ndash;130.
</p>
<p>Merton, R. C. (1973). Theory of rational option pricing. The Bell Journal of Economics and
Management Science, 4, 141&ndash;183.
</p>
<p>Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica, 55, 277&ndash;301.
Tanaka, K. (1996). Time series analysis: Nonstationary and noninvertible distribution theory.
</p>
<p>New York: Wiley.
Trench, W. F. (2013). Introduction to real analysis. Free Hyperlinked Edition 2.04 December 2013.
</p>
<p>Downloaded on 10th May 2014 from http://digitalcommons.trinity.edu/mono/7.
Vasicek, O. (1977). An equilibrium characterization of the term structure. Journal of Financial
</p>
<p>Economics, 5, 177&ndash;188.</p>
<p/>
<div class="annotation"><a href="http://digitalcommons.trinity.edu/mono/7">http://digitalcommons.trinity.edu/mono/7</a></div>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Time Series Modeling</p>
<p/>
</div>
<div class="page"><p/>
<p>2Basic Concepts from Probability Theory
</p>
<p>2.1 Summary
</p>
<p>This chapter reviews some basic material. We collect some elementary concepts
</p>
<p>and properties in connection with random variables, expected values, multivariate
</p>
<p>and conditional distributions. Then we define stochastic processes, both discrete and
</p>
<p>continuous in time, and discuss some fundamental properties. For a successful study
</p>
<p>of the remainder of this book, the reader is required to be familiar with all of these
</p>
<p>principles.
</p>
<p>2.2 Random Variables
</p>
<p>Stochastic processes are defined as families of random variables. This is why
</p>
<p>related concepts will be recapitulated to facilitate the definition of random variables.
</p>
<p>Measure theoretical aspects, however, will not be touched.1
</p>
<p>Probability Space
</p>
<p>We denote the possible set of outcomes of a random experiment by ˝ . Subsets
</p>
<p>A, A � ˝ , are called events. These events are assigned probabilities to. The
probability is a mapping
</p>
<p>A 7! P.A/ 2 Œ0; 1&#141; ; A � ˝ ;
</p>
<p>1Ross (2010) provides a nice introduction to probability, and so do Grimmett and Stirzaker (2001)
with a focus on stochastic processes. For a short reference and refreshing e.g. the shorter appendix
in Bickel and Doksum (2001) is recommended.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_2
</p>
<p>13</p>
<p/>
</div>
<div class="page"><p/>
<p>14 2 Basic Concepts from Probability Theory
</p>
<p>which fulfills the axioms of probability,
</p>
<p>&bull; P.A/ � 0,
&bull; P.˝/ D 1,
&bull; P
</p>
<p>�S
i
</p>
<p>Ai
</p>
<p>�
D
P
</p>
<p>i
</p>
<p>P.Ai/ for Ai \ Aj D ; with i &curren; j,
</p>
<p>where fAig may be a possibly infinite sequence of pairwise disjoint events. For a
well-defined mapping, we do not consider every possible event but in particular
</p>
<p>only those being contained in �-algebras. A � -algebra2 F of ˝ is defined as a
</p>
<p>system of subsets containing
</p>
<p>&bull; the empty set ;,
&bull; the complement Ac of every subset A 2 F (this is the set ˝ without A, Ac D
˝ n A),
</p>
<p>&bull; and the union
S
</p>
<p>i
</p>
<p>Ai of a possibly infinite sequence of elements Ai 2 F .
</p>
<p>Of course, a �-algebra is not unique but can be constructed according to problems
</p>
<p>of interest. The interrelated triple of set of outcomes, �-algebra and probability
</p>
<p>measure, .˝; F ; P/, is also called a probability space.
</p>
<p>Example 2.1 (Game of Dice) Consider a fair hexagonal die with the set of outcomes
</p>
<p>˝ D f1; 2; 3; 4; 5; 6g;
</p>
<p>where each elementary event f!g � ˝ is assigned the same probability to:
</p>
<p>P.f1g/ D : : : D P.f6g/ D 1
6
:
</p>
<p>When #.A/ denotes the number of elements of A � ˝ , it holds in the example of
the die that
</p>
<p>P.A/ D #.A/
#.˝/
</p>
<p>D #.A/
6
:
</p>
<p>The probability for the occurrence of A hence equals the number of outcomes
</p>
<p>leading to A divided by the number of possible outcomes. If one is only interested
</p>
<p>in the event whether an even or an odd number occurs,
</p>
<p>E D f2; 4; 6g; Ec D ˝ n E D f1; 3; 5g;
</p>
<p>2Sometimes also called a � -field, which motivates the symbol F .</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Random Variables 15
</p>
<p>then the �-algebra obviously reads
</p>
<p>F1 D f;;E;Ec;˝g:
</p>
<p>If one is interested in all possible outcomes without any qualification, then the
</p>
<p>�-algebra chosen will be the power set of ˝ , P.˝/. This is the set of all subsets
</p>
<p>of ˝:
</p>
<p>F2 D P.˝/ D f;; f1g; : : : ; f6g; f1; 2g; : : : ; f5; 6g; f1; 2; 3g; : : : ;˝g:
</p>
<p>Systematic counting shows that P.˝/ contains exactly 2#.˝/ D 26 D 64 elements.
With one and the same probability mapping one obtains for different �-algebras
</p>
<p>different probability spaces:
</p>
<p>.˝;F1;P/ and .˝;F2;P/ : �
</p>
<p>RandomVariable
</p>
<p>Often not the events themselves are of interest but some values associated with them,
</p>
<p>that is to say random variables. A real-valued one-dimensional random variable X
</p>
<p>maps the set of outcomes˝ of the space .˝; F ; P/ to the real numbers:
</p>
<p>X W ˝ ! R
! 7! X.!/ :
</p>
<p>Again, however, not all such possible mappings can be considered. In particular, a
</p>
<p>random variable is required to have the property of measurability (more precisely:
</p>
<p>F -measurability). This implies the following: A subset B � R defines an event of
˝ in such a way that:
</p>
<p>X�1.B/ WD f! 2 ˝ j X.!/ 2 Bg :
</p>
<p>This so-called inverse image X�1.B/ � ˝ of B contains exactly the very elements
of ˝ which are mapped by X to B. Let B be a family of sets consisting of subsets
</p>
<p>of R. Then as measurability it is required from a random variable X that for all
</p>
<p>B 2 B all inverse images are contained in the �-algebra F : X�1.B/ 2 F . Thereby
the probability measure P on F is conveyed to B, i.e. the probability function Px
assigning values to X is induced as follows:
</p>
<p>Px.X 2 B/ D P
�
X�1.B/
</p>
<p>�
; B 2 B :</p>
<p/>
</div>
<div class="page"><p/>
<p>16 2 Basic Concepts from Probability Theory
</p>
<p>Thus, strictly speaking, X does not map from˝ to R but from one probability space
</p>
<p>to another:
</p>
<p>X W .˝; F ; P/ ! .R; B; Px/ ;
</p>
<p>where B now denotes a �-algebra named after Emile Borel. This Borel algebra B is
</p>
<p>the smallest �-algebra over R containing all real intervals. In particular, for x 2 R
the event X � x has an induced probability leading to the distribution function of
X defined as follows:
</p>
<p>Fx.x/ WD Px.X � x/ D Px .X 2 .�1; x&#141;/ D P
�
X�1 ..�1; x&#141;/
</p>
<p>�
; x 2 R :
</p>
<p>Example 2.2 (Game of Dice) Let us continue the example of dice and let us define
</p>
<p>a random variable X assigning a gain of 50 monetary units to an even number and
</p>
<p>assigning a loss of 50 monetary units to an odd number,
</p>
<p>1 7! �50
2 7! C50
</p>
<p>X W 3 7! �50
4 7! C50
5 7! �50
6 7! C50
</p>
<p>The random variable X operates on the probability space .˝;F1;P/ known from
</p>
<p>Example 2.1. For arbitrary real intervals probabilities Px with F1 D f;;E;Ec;˝g
are induced, e.g.:
</p>
<p>Px .X 2 Œ�100;�50&#141;/ D P
�
X�1 .Œ�100;�50&#141;/
</p>
<p>�
D P.Ec/ D 1
</p>
<p>2
;
</p>
<p>Fx.60/ D Px .X 2 .�1; 60&#141;/ D P
�
X�1 ..�1; 60&#141;/
</p>
<p>�
D P.˝/ D 1:
</p>
<p>Let a second random variable Y model the following gain or loss function:
</p>
<p>1 7! �10
2 7! �20
</p>
<p>Y W 3 7! �30
4 7! �40
5 7! 0
6 7! 100
</p>
<p>As in this case each outcome leads to another value of the random variable, the
</p>
<p>probability space chosen is .˝;F2;P/ with the power set F2 D P.˝/ being the</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Random Variables 17
</p>
<p>�-algebra. Then we obtain for Y for instance the following probabilities:
</p>
<p>Fy.0/ D Py .Y � 0/ D P
�
Y�1.�1; 0&#141;
</p>
<p>�
D P .f1; 2; 3; 4; 5g/ D 5
</p>
<p>6
;
</p>
<p>Py .Y 2 Œ�20; 20// D P
�
Y�1Œ�20; 20/
</p>
<p>�
D P .f1; 2; 5g/ D 1
</p>
<p>2
:
</p>
<p>For another probability space the mapping Y is possibly not measurable and
</p>
<p>therefore it cannot be a random variable. E.g. Y is not F1-measurable. This is due
</p>
<p>to the fact that the image Y D 0 has the inverse image Y�1.0/ D f5g � ˝ which is
not contained in F1 as an elementary event: f5g &hellip; F1. �
</p>
<p>Continuous RandomVariables
</p>
<p>For most of all problems in practice we do not explicitly construct a random
</p>
<p>experiment with probability P in order to derive probabilities Px of a random
</p>
<p>variable X. Typically we start directly with the quantity of interest X modeling a
</p>
<p>probability distribution without inducing it. In particular, this is the case for so-
</p>
<p>called continuous variables. For a continuous random variable every value taken
</p>
<p>from a real interval is a possible realization. As a continuous random variable can
</p>
<p>therefore take uncountably many values it is not possible to calculate a probability
</p>
<p>P.x1 &lt; X � x2/ by summing up the individual probabilities. Instead, probabilities
are calculated by integrating a probability density. We assume the function f .x/ to
</p>
<p>be continuous (or at least Riemann-integrable) and to be nonnegative for all x 2 R.
Then f is called (probability) density (or density function) of X if it holds for
</p>
<p>arbitrary numbers x1 &lt; x2 that
</p>
<p>P.x1 &lt; X � x2/ D
Z x2
</p>
<p>x1
</p>
<p>f .x/ dx:
</p>
<p>The area beneath the density function therefore measures the probability with
</p>
<p>which the continuous random variable takes on values of the interval considered.
</p>
<p>In general, a density is defined by two properties:
</p>
<p>1. f .x/ � 0 ,
</p>
<p>2.
</p>
<p>Z C1
</p>
<p>�1
f .x/ dx D 1 .
</p>
<p>Thus, the distribution function F.x/ D P.X � x/ of a continuous random variable X
is calculated as follows:
</p>
<p>F.x/ D
Z x
</p>
<p>�1
f .t/ dt:</p>
<p/>
</div>
<div class="page"><p/>
<p>18 2 Basic Concepts from Probability Theory
</p>
<p>If there is the danger of a confusion, we sometimes subscript the distribution
</p>
<p>function, e.g. Fx.0/ D P.X � 0/.
</p>
<p>Expected Value and Higher Moments
</p>
<p>As is well known, the expected value E.X/ (also called expectation) of a
</p>
<p>continuous random variable X with continuous density f is defined as follows:
</p>
<p>E.X/ D
Z 1
</p>
<p>�1
xf .x/ dx:
</p>
<p>For (measurable) mappings g, transformations g.X/ are again random variables, and
</p>
<p>the expected value is given by:
</p>
<p>E Œg.X/&#141; D
Z 1
</p>
<p>�1
g.x/f .x/ dx:
</p>
<p>In particular, for each power of X so-called moments are defined for k D 1; 2; : : ::
</p>
<p>�k D E
�
Xk
�
:
</p>
<p>Note that this term represents integrals which are not necessarily finite (then one
</p>
<p>says: the respective moments do not exist). There are even random variables whose
</p>
<p>density f allows for very large observations in absolute value with such a high
</p>
<p>probability that even the expected value�1 is not finite.
3 If nothing else is suggested,
</p>
<p>we will always assume random variables with finite moments without pointing out
</p>
<p>explicitly.
</p>
<p>Often we consider so-called centered moments where g.X/ is chosen as .X �
E.X//k. For k D 2 the variance is obtained (often denoted by �2)4:
</p>
<p>�2 D Var.X/ D
Z 1
</p>
<p>�1
.x � E.X//2f .x/ dx:
</p>
<p>Elementarily, the following additive decomposition is shown:
</p>
<p>Var.X/ D E.X2/ � .E.X//2 D �2 � �21: (2.1)
</p>
<p>3An example for this is the Cauchy distribution, i.e. the t-distribution with one degree of freedom.
For the Pareto distribution, as well, the existence of moments is dependent on the parameter value;
this is shown in Problem 2.2.
4Then � describes the square root of Var.X/ with positive sign.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Random Variables 19
</p>
<p>Since Var.X/ � 0 by construction, this gives rise to the following inequality:
</p>
<p>.E.X//2 � E
�
X2
�
: (2.2)
</p>
<p>In addition to centering, for higher moments a standardization is typically consid-
</p>
<p>ered. The following measures of skewness and kurtosis with k D 3 and k D 4,
respectively, are widely used:
</p>
<p>&#13;1 D
EŒ.X � �1/3&#141;
</p>
<p>�3
; &#13;2 D
</p>
<p>EŒ.X � �1/4&#141;
�4
</p>
<p>:
</p>
<p>The skewness coefficient is used to measure deviations from symmetry. If X
</p>
<p>exhibits a density f which is symmetric around the expected value, it obviously
</p>
<p>follows that &#13;1 D 0. The interpretation of the kurtosis coefficient is more difficult.
Generally, &#13;2 is taken as a measure for a distribution&rsquo;s &ldquo;peakedness&rdquo;, or alternatively,
</p>
<p>for how probable extreme observations (&ldquo;outliers&rdquo;) are. Frequently, the normal
</p>
<p>distribution is taken as a reference. For every normal distribution (also called
</p>
<p>Gaussian distribution, see Example 2.4) it holds that the kurtosis takes the value 3.
</p>
<p>Furthermore, it can be shown that it holds always true that
</p>
<p>&#13;2 � 1 ;
</p>
<p>which is verified in Problem 2.1.
</p>
<p>Example 2.3 (Kurtosis of a Continuous Uniform Distribution) The random variable
</p>
<p>X is assumed to be uniformly distributed on Œ0; b&#141; with density
</p>
<p>f .x/ D
�
1
b
; x 2 Œ0; b&#141;
0; else
</p>
<p>:
</p>
<p>As is well known, it then holds that
</p>
<p>�1 D E .X/ D
b
</p>
<p>2
; �2 D Var .X/ D b
</p>
<p>2
</p>
<p>12
:
</p>
<p>In order to calculate the kurtosis &#13;2 we are interested in the fourth centered moment:
</p>
<p>E
h
.X � �1/4
</p>
<p>i
D
</p>
<p>bZ
</p>
<p>0
</p>
<p>�
x � b
</p>
<p>2
</p>
<p>�4
1
</p>
<p>b
dx:</p>
<p/>
</div>
<div class="page"><p/>
<p>20 2 Basic Concepts from Probability Theory
</p>
<p>For this we determine (binomial theorem):
</p>
<p>�
x � b
</p>
<p>2
</p>
<p>�4
D x4 C 4x3
</p>
<p>�
�b
2
</p>
<p>�
C 6x2 b
</p>
<p>2
</p>
<p>4
C 4x
</p>
<p>�
�b
2
</p>
<p>�3
C
�
</p>
<p>b
</p>
<p>2
</p>
<p>�4
</p>
<p>D x4 � 2x3b C 3
2
</p>
<p>x2b2 � 1
2
</p>
<p>xb3 C b
4
</p>
<p>16
:
</p>
<p>From this it is obtained that
</p>
<p>bZ
</p>
<p>0
</p>
<p>�
x � b
</p>
<p>2
</p>
<p>�4
dx D b
</p>
<p>5
</p>
<p>5
� b
</p>
<p>5
</p>
<p>2
C b
</p>
<p>5
</p>
<p>2
� b
</p>
<p>5
</p>
<p>4
C b
</p>
<p>5
</p>
<p>16
</p>
<p>D b
5
</p>
<p>80
;
</p>
<p>and hence
</p>
<p>E
h
.X � �1/4
</p>
<p>i
D b
</p>
<p>4
</p>
<p>80
:
</p>
<p>The kurtosis coefficient is therefore determined as
</p>
<p>&#13;2 D
E
h
.X � �1/4
</p>
<p>i
</p>
<p>�4
D b
</p>
<p>4
</p>
<p>80
</p>
<p>�
12
</p>
<p>b2
</p>
<p>�2
D 1:8:
</p>
<p>It is obvious that the kurtosis is independent of b. The value 1:8 is clearly smaller
</p>
<p>than 3 indicating that the uniform distribution&rsquo;s curve exhibits a flatter behavior than
</p>
<p>that of the normal distribution. �
</p>
<p>Markov&rsquo;s and Chebyshev&rsquo;s Inequality
</p>
<p>Consider again the random variable X with variance �2 D Var.X/. Depending on
�2, Chebyshev&rsquo;s inequality allows to bound the probability with which the random
</p>
<p>variable is distributed around its expected value. In fact, this result is a special case
</p>
<p>of the more general Markov&rsquo;s inequality, see (2.3), which is established e.g. in Ross
</p>
<p>(2010, Sect. 8.2). A proof of Chebyshev&rsquo;s result given in (2.4) will be provided in
</p>
<p>Problem 2.3.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.2 Random Variables 21
</p>
<p>Lemma 2.1 (Markov&rsquo;s and Chebyshev&rsquo;s Inequality) Let X be a random vari-
</p>
<p>able.
</p>
<p>(a) If X takes only nonnegative values, then it holds for any real constant a &gt; 0:
</p>
<p>P.X � a/ � E.X/
a
</p>
<p>I (2.3)
</p>
<p>(b) with �2 D Var.X/ &lt;1 it holds that
</p>
<p>P.jX � E.X/j � "/ � �
2
</p>
<p>"2
; (2.4)
</p>
<p>where " &gt; 0 is an arbitrary real constant.
</p>
<p>Example 2.4 (Normal Distribution) The density of a random variable X with
</p>
<p>normal or Gaussian distribution with parameters � and � &gt; 0 goes back to Gauss5
</p>
<p>and is, as is well known,
</p>
<p>f .x/ D 1p
2��
</p>
<p>exp
</p>
<p>�
�1
2
</p>
<p>�x � �
�
</p>
<p>�2�
; x 2 R;
</p>
<p>with
</p>
<p>E.X/ D � and Var.X/ D �2:
</p>
<p>In symbols we also write X � N .�; �2/. As the density function is symmetric
around � it follows that &#13;1 D 0. The kurtosis we adopt from the literature without
calculation as &#13;2 D 3. Sometimes we use this result for determining the fourth
centered moment. Under normality it holds that:
</p>
<p>EŒ.X � �1/4&#141; D 3 .Var.X//2 :
</p>
<p>We want to use this example to show that Chebyshev&rsquo;s inequality may be not very
</p>
<p>sharp. For example,
</p>
<p>P.jX � �j � 2 �/ � �
2
</p>
<p>4 �2
D 0:25:
</p>
<p>5The traditional German spelling is Gau&szlig;. Carl Friedrich Gau&szlig; lived from 1777 to 1855 and was a
professor in G&ouml;ttingen. His name is connected to many discoveries and inventions in theoretical and
applied mathematics. His portrait and a graph of the density of the normal distribution decorated
the 10-DM-bill in Germany prior to the Euro.</p>
<p/>
</div>
<div class="page"><p/>
<p>22 2 Basic Concepts from Probability Theory
</p>
<p>When using the standard normal distribution, however, one obtains a much smaller
</p>
<p>probability than the bound due to (2.4):
</p>
<p>P .jX � �j � 2 �/ D P
� jX � �j
</p>
<p>�
� 2
</p>
<p>�
D 2 P
</p>
<p>�
X � �
�
</p>
<p>� �2
�
� 0:044: �
</p>
<p>2.3 Joint and Conditional Distributions
</p>
<p>In this section we first recapitulate some widely known results. At the end we
</p>
<p>introduce the more involved theory of conditional expectation.
</p>
<p>Joint Distribution and Independence
</p>
<p>In order to restrict the notational burden, we only consider the three-dimensional
</p>
<p>case of continuous random variables X, Y and Z with the joint density function
</p>
<p>fx;y;z mapping from R
3 to R. For arbitrary real numbers a, b and c, probabilities are
</p>
<p>defined as multiple (or iterated) integrals:
</p>
<p>P.X � a;Y � b;Z � c/ D
Z c
</p>
<p>�1
</p>
<p>Z b
</p>
<p>�1
</p>
<p>Z a
</p>
<p>�1
fx;y;z.x; y; z/dxdydz:
</p>
<p>As long as f is a continuous function, the order of integration does not matter, i.e.
</p>
<p>one obtains e.g.
</p>
<p>P.X � a;Y � b;Z � c/ D
Z a
</p>
<p>�1
</p>
<p>Z b
</p>
<p>�1
</p>
<p>Z c
</p>
<p>�1
fx;y;z.x; y; z/dzdydx
</p>
<p>D
Z b
</p>
<p>�1
</p>
<p>Z a
</p>
<p>�1
</p>
<p>Z c
</p>
<p>�1
fx;y;z.x; y; z/dzdxdy :
</p>
<p>This reversibility is sometimes called Fubini&rsquo;s theorem.6
</p>
<p>Univariate and bivariate marginal distributions arise from integrating the respec-
</p>
<p>tive variable:
</p>
<p>fx.x/ D
Z 1
</p>
<p>�1
</p>
<p>Z 1
</p>
<p>�1
fx;y;z.x; y; z/dydz;
</p>
<p>fx;y.x; y/ D
Z 1
</p>
<p>�1
fx;y;z.x; y; z/dz:
</p>
<p>6 Cf. Syds&aelig;ter, Str&oslash;m, and Berck (1999, p. 53). A proof is contained e.g. in the classical textbook
by Rudin (1976, Thm. 10.2), or in Trench (2013, Coro. 7.2.2); the latter book may be recommended
since it is downloadable free of charge.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Joint and Conditional Distributions 23
</p>
<p>The variables are called stochastically independent if, for arbitrary arguments, the
</p>
<p>joint distribution is given as the product of the marginal densities:
</p>
<p>fx;y;z.x; y; z/ D fx.x/ fy.y/ fz.z/ ;
</p>
<p>which implies pairwise independence:
</p>
<p>fx;y.x; y/ D fx.x/ fy.y/ :
</p>
<p>The joint probability
</p>
<p>P.X � a;Y � b;Z � c/ D
Z c
</p>
<p>�1
</p>
<p>Z b
</p>
<p>�1
</p>
<p>Z a
</p>
<p>�1
fx.x/ fy.y/ fz.z/dxdydz
</p>
<p>is, under independence, factorized to
</p>
<p>P.X � a;Y � b;Z � c/ D
Z c
</p>
<p>�1
</p>
<p>Z b
</p>
<p>�1
fy.y/ fz.z/
</p>
<p>�Z a
</p>
<p>�1
fx.x/ dx
</p>
<p>�
dydz
</p>
<p>D
Z c
</p>
<p>�1
fz.z/
</p>
<p>�Z b
</p>
<p>�1
fy.y/ dy
</p>
<p>� �Z a
</p>
<p>�1
fx.x/ dx
</p>
<p>�
dz
</p>
<p>D
Z a
</p>
<p>�1
fx.x/dx
</p>
<p>Z b
</p>
<p>�1
fy.y/dy
</p>
<p>Z c
</p>
<p>�1
fz.z/dz
</p>
<p>D P.X � a/ P.Y � b/ P.Z � c/:
</p>
<p>Covariance
</p>
<p>In particular for only two variables a generalization of the expectation operator is
</p>
<p>considered. Let h be a real-valued function of two variables, h: R2 ! R, then we
define as a double integral:
</p>
<p>EŒh.X;Y/&#141; D
Z 1
</p>
<p>�1
</p>
<p>Z 1
</p>
<p>�1
h.x; y/fx;y.x; y/dxdy:
</p>
<p>Hence, the covariance between X and Y can be defined as follows:
</p>
<p>Cov.X;Y/ WD EŒ.X � E.X//.Y � E.Y//&#141;
D E.XY/ � E.X/E.Y/ ;
</p>
<p>where the finiteness of these integrals is again assumed tacitly. It can be easily
</p>
<p>shown that the independence of two variables implies their uncorrelatedness, i.e.
</p>
<p>Cov.X;Y/ D 0, whereas the reverse does not generally hold true. In particular, the</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 Basic Concepts from Probability Theory
</p>
<p>covariance only measures the linear relation between two variables. In order to have
</p>
<p>the measure independent of the units, it is usually standardized as follows:
</p>
<p>�xy D
Cov.X;Y/p
</p>
<p>Var.X/
p
</p>
<p>Var.Y/
:
</p>
<p>The correlation coefficient �xy is smaller than or equal to one in absolute value, see
</p>
<p>Problem 2.7.
</p>
<p>Example 2.5 (Bivariate Normal Distribution) Let X and Y be two Gaussian
</p>
<p>random variables,
</p>
<p>X � N .�x; �2x /; Y � N .�y; �2y /;
</p>
<p>with correlation coefficient �. We talk about a bivariate normal distribution if the
</p>
<p>joint density takes the following form:
</p>
<p>fx;y.x; y/ D
1
</p>
<p>2��x�y
p
1 � �2
</p>
<p>'x;y.x; y/
</p>
<p>with 'x;y.x; y/ equal to
</p>
<p>exp
</p>
<p>(
� 1
2.1� �2/
</p>
<p>"�
x � �x
�x
</p>
<p>�2
� 2�
</p>
<p>�
x � �x
�x
</p>
<p>��
y � �y
�y
</p>
<p>�
C
�
</p>
<p>y � �y
�y
</p>
<p>�2#)
:
</p>
<p>Symbolically, we denote the vector as
</p>
<p>�
X
</p>
<p>Y
</p>
<p>�
� N2.�;˙/;
</p>
<p>where � is a vector and ˙ stands for a symmetric matrix:
</p>
<p>� D
�
�x
�y
</p>
<p>�
; ˙ D
</p>
<p> 
�2x Cov.X;Y/
</p>
<p>Cov.X;Y/ �2y
</p>
<p>!
:
</p>
<p>In general, the covariance matrix is defined as follows:
</p>
<p>˙ D E
��
</p>
<p>X � E.X/
Y � E.Y/
</p>
<p>�
.X � E.X/; Y � E.Y//
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Joint and Conditional Distributions 25
</p>
<p>Note that in the case of uncorrelatedness .� D 0/ it holds that
</p>
<p>fx;y.x; y/ D
1p
2��x
</p>
<p>exp
</p>
<p>�
� .x � �x/
</p>
<p>2
</p>
<p>2�2x
</p>
<p>�
1p
2��y
</p>
<p>exp
</p>
<p>(
� .y � �y/
</p>
<p>2
</p>
<p>2�2y
</p>
<p>)
</p>
<p>D fx.x/ fy.y/:
</p>
<p>The joint density function is then determined as the product of the individual
</p>
<p>densities. Consequently, the random variables X and Y are independent. Therefore it
</p>
<p>follows, in particular for the normal distribution, that uncorrelatedness is equivalent
</p>
<p>to stochastic independence. Furthermore, bivariate Gaussian random variables have
</p>
<p>the property that each linear combination is univariate normally distributed. More
</p>
<p>precisely, it holds for � 2 R2 with7 �0 D .�1; �2/ that:
</p>
<p>�0
�
</p>
<p>X
</p>
<p>Y
</p>
<p>�
D �1X C �2Y � N .�0�; �0˙�/:
</p>
<p>Interesting special cases are obtained with �0 D .1; 1/ and �0 D .1; �1/ for
sums and differences. Note that furthermore for multivariate normal distributions
</p>
<p>necessarily all marginal distributions are normal (with �0 D .1; 0/ and �0 D .0; 1/).
The reverse does not hold. A bivariate example for Gaussian marginal distributions
</p>
<p>without joint normal distributions is given by Bickel and Doksum (2001, p. 533). �
</p>
<p>Cauchy-Schwarz Inequality
</p>
<p>The inequality by Cauchy and Schwarz is the reason why j�xyj � 1 applies. The
following statement is verified in Problem 2.6.
</p>
<p>Lemma 2.2 (Cauchy-Schwarz Inequality) For arbitrary random variables Y and
</p>
<p>Z it holds that
</p>
<p>jE.YZ/j �
p
</p>
<p>E.Y2/
p
</p>
<p>E.Z2/; (2.5)
</p>
<p>where finite moments are assumed.
</p>
<p>We want to supplement the Cauchy-Schwarz inequality by an intermediate inequal-
</p>
<p>ity, see (2.8). For this purpose we remember the so-called triangle inequality for
</p>
<p>7Up to this point a superscript prime at a function has denoted its derivative. In the rare cases
in which we are concerned with matrices or vectors, the symbol will also be used to indicate
transposition. Bearing in mind the respective context, there should not occur any ambiguity.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 Basic Concepts from Probability Theory
</p>
<p>two real numbers:
</p>
<p>ja1 C a2j � ja1j C ja2j:
</p>
<p>Obviously, this can be generalized to:
</p>
<p>ˇ̌
ˇ̌
ˇ
</p>
<p>nX
</p>
<p>iD1
ai
</p>
<p>ˇ̌
ˇ̌
ˇ �
</p>
<p>nX
</p>
<p>iD1
jaij:
</p>
<p>If the sequence is absolutely summable, it is allowed to set n D 1. This suggests
that an analogous inequality also applies for integrals. If the function g is continuous,
</p>
<p>this implies continuity of jgj and one obtains:
ˇ̌
ˇ̌
Z
</p>
<p>g.x/dx
</p>
<p>ˇ̌
ˇ̌ �
</p>
<p>Z
jg.x/jdx:
</p>
<p>This implies for the expected value of a random variable X:
</p>
<p>jE .X/j � E .jXj/: (2.6)
</p>
<p>This relation resembles (2.2); in fact, both relations are special cases of Jensen&rsquo;s
</p>
<p>inequality.8 A random variable is called integrable if E.jXj/ &lt; 1. Of course this
implies a finite expected value. For integrability a finite second moment is sufficient,
</p>
<p>which follows from (2.5) with Y D jXj and Z D 1:
</p>
<p>E .jXj/ �
p
</p>
<p>EjXj2
p
12 D
</p>
<p>p
E.X/2:
</p>
<p>Now, if setting X D YZ in (2.6), it follows that: jE.YZ/j � E.jYjjZj/. This is the
bound added to (2.5):
</p>
<p>jE .YZ/j � E .jYjjZj/ �
p
</p>
<p>E .Y2/
p
</p>
<p>E .Z2/: (2.8)
</p>
<p>The first inequality follows from (2.6). The second one will be verified in the
</p>
<p>problem section.
</p>
<p>8The general statement is: for a convex function g it holds
</p>
<p>g .E .X// � E .g.X// I (2.7)
</p>
<p>see e.g. Syds&aelig;ter et al. (1999, p. 181), while a proof is given e.g. in Davidson (1994, Ch. 9) or
Ross (2010, p. 409).</p>
<p/>
</div>
<div class="page"><p/>
<p>2.3 Joint and Conditional Distributions 27
</p>
<p>Conditional Distributions
</p>
<p>Conditional distributions and densities, respectively, are defined as the ratio of the
</p>
<p>joint density and the &ldquo;conditioning density&rdquo;, i.e. they are defined by the following
</p>
<p>density functions (where positive denominators are assumed):
</p>
<p>fxjy.x/ D
fx;y.x; y/
</p>
<p>fy.y/
;
</p>
<p>fxjy;z.x/ D
fx;y;z.x; y; z/
</p>
<p>fy;z.y; z/
;
</p>
<p>fx;yjz.x; y/ D
fx;y;z.x; y; z/
</p>
<p>fz.z/
:
</p>
<p>It should be clear that these conditional densities are in fact density functions. In
</p>
<p>case of independence it holds by definition that the conditional and the unconditional
</p>
<p>densities are equal, e.g.
</p>
<p>fxjy.x/ D fx.x/ :
</p>
<p>This is very intuitive: In case of two independent random variables, one does not
</p>
<p>have any influence on the probability with which the other takes on values.
</p>
<p>Conditional Expectation
</p>
<p>If the random variables X and Y are not independent and if the realization of Y is
</p>
<p>known, Y D y, then the expectation of X will be affected:
</p>
<p>E.XjY D y/ D
Z 1
</p>
<p>�1
xfxjy.x/dx:
</p>
<p>Analogously, we define the conditional expectation of a random variable Z, Z D
h.X;Y/, h W R2 ! R, given Y D y as:
</p>
<p>E.ZjY D y/ D E .h.X;Y/ j Y D y/
</p>
<p>D
Z 1
</p>
<p>�1
h.x; y/fxjy.x/ dx:
</p>
<p>In particular, for h.X;Y/ D X g.Y/ with g W R ! R one therefore obtains
</p>
<p>E.X g.Y/ j Y D y/ D g.y/
Z 1
</p>
<p>�1
xfxjy.x/ dx
</p>
<p>D g.y/E.XjY D y/:</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 Basic Concepts from Probability Theory
</p>
<p>Here, the marginal density of X is replaced by the conditional density conditioned
</p>
<p>on the value Y D y.
Technically, we can calculate the density conditioned on the random variable Y
</p>
<p>instead of conditioned on a value9 Y D y:
</p>
<p>fxjY .x/ D
fx;y.x;Y/
</p>
<p>fy.Y/
:
</p>
<p>By fxjY.x/ a transformation of the random variable Y and consequently a new random
variable is obtained. This is also true for the related conditional expectations:
</p>
<p>E.XjY/ D
Z 1
</p>
<p>�1
xfxjY .x/dx ;
</p>
<p>E.h.X;Y/jY/ D
Z 1
</p>
<p>�1
h.x;Y/fxjY .x/dx:
</p>
<p>As this is about random variables, it is absolutely reasonable to determine the
</p>
<p>expected value over the conditional expectation. This calculation can be carried out
</p>
<p>applying a rule called the &ldquo;law of iterated expectations (LIE)&rdquo; in the literature; it is
</p>
<p>given in Proposition 2.1. In order to prevent confusion whether X or Y is integrated,
</p>
<p>it is advisable to subscript the expectation operator accordingly:
</p>
<p>EyŒEx.XjY/&#141; D
Z 1
</p>
<p>�1
ŒEx.Xjy/&#141; fy.y/dy D
</p>
<p>Z 1
</p>
<p>�1
</p>
<p>�Z 1
</p>
<p>�1
x fxjy.x/dx
</p>
<p>�
fy.y/dy:
</p>
<p>Although Y and g.Y/ are random variables, after conditioning on Y they can be
</p>
<p>treated as constants and in case of a multiplicative composition, they can be put in
</p>
<p>front of the expected value when integration is with respect to X. This is the second
</p>
<p>statement in the following proposition, also cf. Davidson (1994, Theorem 10.10).
</p>
<p>The first statement will be derived in Problem 2.9.
</p>
<p>Proposition 2.1 (Conditional Expectation) With the notation introduced above, it
</p>
<p>holds that:
</p>
<p>(a) EyŒEx.XjY/&#141; D Ex.X/,
(b) Eh.g.Y/XjY/ D g.Y/Ex.XjY/ for h.X;Y/ D X g.Y/.
</p>
<p>9This is not a really rigorous way of introducing expectations conditioned on random variables.
A mathematically correct exposition, however, requires measure theoretical arguments not being
available at this point; cf. for example Davidson (1994, Ch. 10), or Klebaner (2005, Ch. 2). More
generally, one may define expectations conditioned on a � -algebra, E.XjG/, where G could be the
� -algebra generated by Y: G D �.Y/.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Stochastic Processes (SP) 29
</p>
<p>Frequently, we formulate these statements in a shorter way,
</p>
<p>EŒE.XjY/&#141; D E.X/;
E .g.Y/XjY/ D g.Y/E.XjY/;
</p>
<p>if there is no risk of misunderstanding.
</p>
<p>2.4 Stochastic Processes (SP)
</p>
<p>In this section stochastic processes are defined and classified. In the following
</p>
<p>chapters we will be confronted with concrete types of stochastic processes.
</p>
<p>Definition
</p>
<p>A univariate stochastic process (SP) is a family of (real-valued) random variables,
</p>
<p>fX.tI!/gt2T, for a given index set T:
</p>
<p>X W T �˝ ! R
.t I !/ 7! X.tI!/ :
</p>
<p>The subscript t 2 T is always to be interpreted as &ldquo;time&rdquo;. At a fixed point in time t0
the stochastic process is therefore simply a random variable,
</p>
<p>X W ˝ ! R
! 7! X.t0I!/ :
</p>
<p>A fixed !0, however, results in a path, a trajectory or a realization of a process which
</p>
<p>is also often referred to as time series,
</p>
<p>X W T ! R
t 7! X.tI!0/ :
</p>
<p>In fact, a stochastic process is a rather complex object. In order to characterize it
</p>
<p>mathematically, random vectors of arbitrary, finite length n at arbitrary points in
</p>
<p>time t1 &lt; � � � &lt; tn have to be considered:
</p>
<p>Xn.ti/ WD .X.t1I!/; : : : ; X.tnI!//0 ; t1 &lt; � � � &lt; tn :</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 Basic Concepts from Probability Theory
</p>
<p>The multivariate distribution of such an arbitrary random vector characterizes
</p>
<p>a stochastic process. In particular, certain minimal requirements for the finite-
</p>
<p>dimensional distribution of Xn.ti/ guarantee that a stochastic process exists at all.
10
</p>
<p>Depending on the countability or non-countability of the index set T, discrete-
</p>
<p>time and continuous-time SPs are distinguished. In the case of sequences of random
</p>
<p>variables, we talk about discrete-time processes, where the index set consists of
</p>
<p>integers , T � N or T � Z. For discrete-time processes we agree upon lower case
letters as an abbreviation without explicitly denoting the dependence on !,
</p>
<p>fxtg ; t 2 T for fX.tI!/gt2T :
</p>
<p>For so-called continuous-time processes the index set T is a real interval, T D
Œa; b&#141; � R, frequently T D Œ0;T&#141; or T D Œ0; 1&#141;, however, open intervals are also
admitted. For continuous-time processes we also suppress the dependence on !
</p>
<p>notationally and write in a shorter way11
</p>
<p>X.t/ ; t 2 T for fX.tI!/gt2T :
</p>
<p>Stationary and Gaussian Processes
</p>
<p>Consider again generally an arbitrary vector of the length n,
</p>
<p>Xn.ti/ D .X.t1I!/; : : : ; X.tnI!//0 :
</p>
<p>If Xn.ti/ is jointly normally distributed for all n and ti, then X.tI!/ is called a normal
process (also: Gaussian process). Furthermore, we talk about a strictly stationary
</p>
<p>process if the distribution is invariant over time. More precisely, Xn.ti/ follows the
</p>
<p>same distribution as a vector which is shifted by s units on the time axis.
</p>
<p>X0n.ti C s/ D .X.t1 C sI!/; : : : ; X.tn C sI!// :
</p>
<p>The distributional properties of a strictly stationary process do not depend on
</p>
<p>the location on the time axis but only on how far the individual components
</p>
<p>X.tiI!/ are apart from each other temporally. Strict stationarity therefore implies
</p>
<p>10These &ldquo;consistency&rdquo; requirements due to Kolmogorov are found e.g. in Brockwell and Davis
(1991, p. 11) or Grimmett and Stirzaker (2001, p. 372). A proof of Kolmogorov&rsquo;s existence theorem
can be found e.g. in Billingsley (1986, Sect. 36).
11The convention of using upper case letters for continuous-time process is not universal.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Stochastic Processes (SP) 31
</p>
<p>the expected value and the variance to be constant (assuming they are finite) and the
</p>
<p>autocovariance for two points in time to depend only on the temporal interval:
</p>
<p>1. E .X.tI!// D �x for t 2 T ,
2. Cov .X.tI!/; X.t C hI!// D &#13;x.h/ for all t; t C h 2 T ,
</p>
<p>and therefore in particular
</p>
<p>Var .X.tI!// D &#13;x.0/ for all t 2 T :
</p>
<p>A process (with finite second moments EŒ.X.tI!//2&#141;) fulfilling these two conditions
(without necessarily being strictly stationary) is also called weakly stationary
</p>
<p>(or: second-order stationary). Under stationarity, we define as autocorrelation
</p>
<p>coefficient also independent of t:
</p>
<p>�x.h/ D
&#13;x.h/
</p>
<p>&#13;x.0/
:
</p>
<p>Synonymously to autocorrelation we also speak of serial or temporal correlation.
</p>
<p>For weak stationarity not necessarily the whole distribution is invariant over time,
</p>
<p>however, at least the expected value and the autocorrelation structure are constant.
</p>
<p>In the following, the term &ldquo;stationarity&rdquo; always refers to the weak form unless
</p>
<p>stated otherwise.
</p>
<p>Example 2.6 (White Noise Process) In the following chapters, f"tg often denotes
a discrete-time process f".tI!/g free of serial correlation. In addition we assume a
mean of zero and a constant variance �2 &gt; 0, i.e.
</p>
<p>E."t/ D 0 and E."t"s/ D
�
�2 ; t D s
0 ; t &curren; s :
</p>
<p>By definition such a process is weakly stationary. We typically denote it as
</p>
<p>f"tg � WN.0; �2/:
</p>
<p>The reason why such a process is called white noise will be provided in Chap. 4. �
</p>
<p>Example 2.7 (Pure Random Process) Sometimes f"tg from Example 2.6 will
meet the stronger requirements of being identically and independently distributed.
</p>
<p>Identically distributed implies that the marginal distribution
</p>
<p>Fi."/ D P ."ti � "/ D F."/ ; i D 1; : : : ; n ;</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 Basic Concepts from Probability Theory
</p>
<p>does not vary over time. Independence means that the joint distribution of the vector
</p>
<p>"0n;ti D ."t1 ; : : : ; "tn/
</p>
<p>equals the product of the marginal distributions. As the marginal distributions are
</p>
<p>invariant with respect to time, this also holds for their product. Thus, f"tg is strictly
stationary. In the following it is furthermore assumed that "t has zero expectation
</p>
<p>and the finite variance �2. Symbolically, we also write12:
</p>
<p>f"tg � iid.0; �2/:
</p>
<p>A stochastic process with these properties is frequently called a pure random
</p>
<p>process. Clearly, an iid (or pure random) process is white noise. �
</p>
<p>Markov Processes andMartingales
</p>
<p>A SP is called a Markov process if all information of the past about its future
</p>
<p>behavior is entirely concentrated in the present. In order to capture this concept
</p>
<p>more rigorosly, the set of information about the past of the process available up to
</p>
<p>time t is denoted by It. Frequently, the information set is also referred to as
</p>
<p>It D � .X.rI!/; r � t/ ;
</p>
<p>because it is the smallest �-algebra generated by the past and presence of the process
</p>
<p>X.rI!/ up to time t.13 The entire information about the process up to time t is
contained in It. A Markov process, so to speak, does not remember how it arrived
</p>
<p>at the present state: The probability that the process takes on a certain value at time
</p>
<p>tCs depends only on the value at time t (&ldquo;present&rdquo;) and does not depend on the past
behavior. In terms of conditional probabilities, for s &gt; 0 the corresponding property
</p>
<p>reads:
</p>
<p>P .X.t C sI!/ � x j It / D P .X.t C sI!/ � x j X.tI!// : (2.9)
</p>
<p>A process is called a martingale if the present value is the best prediction for
</p>
<p>the future. A martingale technically fulfills two properties. In the first place, it has
</p>
<p>to be (absolutely) integrable, i.e. it is required that (2.10) holds. Secondly, given all
</p>
<p>12The acronym stands for &ldquo;independently identically distributed&rdquo;.
13By assumption, the information at an earlier point in time is contained in the information set at a
subsequent point in time: It � ItCs for s � 0. A family of such nested � -algebras is also called
&ldquo;filtration&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.4 Stochastic Processes (SP) 33
</p>
<p>information It, the conditional expectation only uses the information at time t. More
</p>
<p>precisely, the expected value for the future is equal to today&rsquo;s value. Technically, this
</p>
<p>amounts to:
</p>
<p>E .jX.tI!/j/ &lt;1 ; (2.10)
</p>
<p>E .X.t C sI!/ j It/ D X.tI!/ ; s � 0: (2.11)
</p>
<p>Note that the conditional expectation is a random variable. Therefore, strictly
</p>
<p>speaking, equation (2.11) only holds with probability one.
</p>
<p>Martingale Differences
</p>
<p>Now, let us focus on the discrete-time case. A discrete-time martingale is defined by
</p>
<p>the expectation at time t for tC1 being given by the value at time t. This is equivalent
to expecting a zero increment from t to t C 1. Therefore, this concept is frequently
expressed in form of differences. We then talk about martingale differences. As
</p>
<p>we will see, in a sense, such a property is settled between uncorrelatedness and
</p>
<p>independence and is interesting from both an economic and a statistical point of
</p>
<p>view.
</p>
<p>We again assume an integrable process, i.e. fxtg fulfills (2.10). It is called
a martingale difference (or martingale difference sequences) if the conditional
</p>
<p>expectation (given its own past) is zero:
</p>
<p>E.xtC1j�.xt; xt�1; : : :// D 0:
</p>
<p>This condition states concretely that the past does not have any influence on
</p>
<p>predictions (conditional expectation); i.e. knowing the past does not lead to an
</p>
<p>improvement of the prediction, the forecast is always zero. Not surprisingly, this
</p>
<p>also applies if only one single past observation is known (see Proposition 2.2(a)).
</p>
<p>Two further conclusions for unconditional moments contained in the proposition
</p>
<p>can be verified,14 see Problem 2.10: martingale differences are zero on average and
</p>
<p>free of serial correlation. In spite of serial uncorrelatedness, martingale differences
</p>
<p>in general are on no account independent over time. What is more, they do not even
</p>
<p>have to be stationary as it is not ruled out that their variance function depends on t.
</p>
<p>14We cannot prove the first statement rigorously, which would require a generalization of
Proposition 2.1(a). The more general statement taken e.g. from Breiman (1992, Prop. 4.20) or
Davidson (1994, Thm. 10.26) reads in our setting as
</p>
<p>E ŒE.xtjIt�1/jxt�h&#141; D E.xtjxt�h/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2 Basic Concepts from Probability Theory
</p>
<p>Proposition 2.2 (Martingale Differences) For a martingale difference sequence
</p>
<p>fxtg with It D �.xs ; s � t/ it holds that
</p>
<p>(a) E.xtjxt�h/ D 0 for h &gt; 0,
(b) E.xt/ D 0,
(c) Cov.xt; xtCh/ D E.xtxtCh/ D 0 for h &curren; 0
</p>
<p>for all t 2 T.
</p>
<p>Note that a stationary martingale difference sequence has a constant variance and
</p>
<p>is thus white noise by Proposition 2.2. The concept should be further clarified by
</p>
<p>means of an example.
</p>
<p>Example 2.8 (Martingale Difference) Consider the process given by
</p>
<p>xt D xt�1
"t
</p>
<p>"t�2
; t 2 f2; : : : ; ng ; f"tg � iid.0; �2/ ;
</p>
<p>with x1 D "1 and "0 D 1. From this it follows that x2 D x1 "2"0 D "1"2 and by
continued substitution:
</p>
<p>xt D "t�1"t ; t D 2; : : : ; n:
</p>
<p>We want to show that this is a martingale difference sequence. Therefore, we note
</p>
<p>that the past of the pure random process can be reconstructed from the past of xt:
</p>
<p>"2 D
x2
</p>
<p>"1
D x2
</p>
<p>x1
; "3 D
</p>
<p>x3
</p>
<p>"2
; : : : ; "t D
</p>
<p>xt
</p>
<p>"t�1
:
</p>
<p>Therefore, the information set It constructed from fxt; : : : ; x1g contains not only the
past values of xtC1, but also the ones of the iid process up to time t. Thus, it holds
that
</p>
<p>E.xtC1jIt/ D E."t"tC1jIt/
D "tE."tC1jIt/
D "tE."tC1/
D 0 :
</p>
<p>The first equality follows from the definition of the process. The second equality
</p>
<p>is accounted for by Proposition 2.1(b). The third step is due to the independence
</p>
<p>of "tC1 of the past up to t, that is why conditional and unconditional expectation
coincide. Finally, by assumption, "tC1 is zero on average. All in all, by this the
property of martingale differences is established. Therefore, fxtg is free of serial</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Problems and Solutions 35
</p>
<p>correlation, however, it is serially (i.e. temporally) dependent, which is obvious from
</p>
<p>the recursive definition. �
</p>
<p>A prominent class of martingale differences are the ARCH processes treated in
</p>
<p>Chap. 6.
</p>
<p>2.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>2.1 Prove for the kurtosis coefficient: &#13;2 � 1.
</p>
<p>2.2 Let X follow a Pareto distribution with
</p>
<p>f .x/ D � x���1; x � 1; � &gt; 0:
</p>
<p>Prove that X has finite k-th moments if and only if � &gt; k.
</p>
<p>2.3 Prove Chebyshev&rsquo;s inequality (2.4).
</p>
<p>2.4 Consider a bivariate distribution with:
</p>
<p>fx;y.x; y/ D
(
</p>
<p>1
ab
; .x; y/ 2 Œ0; a&#141; � Œ0; b&#141;
</p>
<p>0 ; else
:
</p>
<p>Prove that X and Y are stochastically independent.
</p>
<p>2.5 Calculate the expected values, variances and the correlation of X and Y from
</p>
<p>Example 2.2.
</p>
<p>2.6 Prove the second inequality from (2.8).
</p>
<p>2.7 Prove for the correlation coefficient that j�xyj � 1.
</p>
<p>2.8 Consider a bivariate logistic distribution function for X and Y:
</p>
<p>Fx;y.x; y/ D .1C e�x C e�y/�1;
</p>
<p>where x and y from R are arbitrary. What does the conditional density function of X
</p>
<p>given Y D y look like?
</p>
<p>2.9 Prove statement (a) from Proposition 2.1.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 2 Basic Concepts from Probability Theory
</p>
<p>2.10 Derive the properties (b) and (c) from Proposition 2.2.
</p>
<p>Hint: Use statement (a).
</p>
<p>Solutions
</p>
<p>2.1 Assuming finite fourth moments we define for a random variable X with �1 D
E.X/:
</p>
<p>&#13;2 D
E.X � �1/4
</p>
<p>�4
:
</p>
<p>Consider the standardized random variable Z with expectation 0 and variance 1:
</p>
<p>Z D X � �1
�
</p>
<p>with E.Z2/ D 1:
</p>
<p>For this random variable, it holds that &#13;2 D E.Z4/. Replacing X by Z2 in (2.2), it
follows
</p>
<p>1 D
�
E.Z2/
</p>
<p>�2 � E
�
Z4
�
D &#13;2 ;
</p>
<p>which proves the claim.
</p>
<p>2.2 For the k-th moment it holds:
</p>
<p>E.Xk/ D
Z 1
</p>
<p>�1
xkf .x/dx D
</p>
<p>Z 1
</p>
<p>1
</p>
<p>� xk���1dx:
</p>
<p>1. case: If � &curren; k, then the antiderivative results in
Z
� xk���1 dx D �
</p>
<p>k � � x
k�� :
</p>
<p>The corresponding improper integral is defined as limit:
</p>
<p>Z 1
</p>
<p>1
</p>
<p>� xk���1dx D lim
M!1
</p>
<p>�
</p>
<p>k � �
�
xk��
</p>
<p>�M
1
:
</p>
<p>For � &gt; k it follows that
</p>
<p>Z 1
</p>
<p>1
</p>
<p>� xk���1dx D 0 � �
k � � D
</p>
<p>�
</p>
<p>� � k &lt;1:
</p>
<p>For � &lt; k, however, no finite value is obtained as Mk�� goes off to infinity.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Problems and Solutions 37
</p>
<p>2. case: For � D k the antiderivative takes on another form:
Z
� xk���1dx D
</p>
<p>Z
� x�1 dx D � log.x/:
</p>
<p>As the logarithm is unbounded, or log.M/! 1 for M ! 1, one cannot obtain
a finite expectation either, as the upper bound of integration is 1.
</p>
<p>Both cases jointly prove the claim.
</p>
<p>2.3 We provide two proofs. The first one builds on the fact that (2.4) is a special
</p>
<p>case of (2.3). The second one is less abstract and more elementary, and hence
</p>
<p>instructive, too.
</p>
<p>1. Note that .X � �/2 is a nonnegative random variable. Therefore, (2.3) applies
with a D "2:
</p>
<p>P..X � �/2 � "2/ � E..X � �/
2/
</p>
<p>"2
:
</p>
<p>The event .X � �/2 � "2, however, is equivalent to jX � �j � ", which
establishes (2.4).
</p>
<p>2. Elementarily, we prove the claim for the case that X is a continuous random vari-
</p>
<p>able with density function f ; the discrete case can be accomplished analogously.
</p>
<p>Note the following sequence of inequalities:
</p>
<p>Var.X/ D
Z 1
</p>
<p>�1
.x � �/2f .x/dx
</p>
<p>�
Z ��"
</p>
<p>�1
.x � �/2f .x/dx C
</p>
<p>Z 1
</p>
<p>�C"
.x � �/2f .x/dx
</p>
<p>�
Z ��"
</p>
<p>�1
"2f .x/dx C
</p>
<p>Z 1
</p>
<p>�C"
"2f .x/dx:
</p>
<p>The first inequality is of course due to the omittance of
</p>
<p>Z �C"
</p>
<p>��"
.x � �/2f .x/dx � 0:
</p>
<p>The second one is accounted for by the fact that for the integrands of the
</p>
<p>respective integrals it holds that:
</p>
<p>x � � &lt; �" for x &lt; � � "</p>
<p/>
</div>
<div class="page"><p/>
<p>38 2 Basic Concepts from Probability Theory
</p>
<p>and
</p>
<p>x � � &gt; " for x &gt; �C ":
</p>
<p>Up to this point, it is therefore shown that:
</p>
<p>Var.X/ � "2P.X � � � "/C "2P.X � �C "/
D "2P.jX � �j � "/:
</p>
<p>This is equivalent to the claim.
</p>
<p>2.4 The marginal density is obtained as follows:
</p>
<p>fx.x/ D
Z 1
</p>
<p>�1
fx;y.x; y/dy
</p>
<p>D
Z b
</p>
<p>0
</p>
<p>1
</p>
<p>ab
dy
</p>
<p>D b � 0
ab
</p>
<p>D 1
a
</p>
<p>for x 2 Œ0; a&#141; ;
</p>
<p>and fx.x/ D 0 for x &hellip; Œ0; a&#141;. It also holds that
</p>
<p>fy.y/ D
(
1
b
; y 2 Œ0; b&#141;
</p>
<p>0 ; else
:
</p>
<p>Hence, one immediately obtains for all x and y:
</p>
<p>fx;y.x; y/ D fx.x/fy.y/;
</p>
<p>which was to be proved.
</p>
<p>2.5 Obviously, the expected value of X is zero,
</p>
<p>E.X/ D 50 � Px.X D 50/� 50 � Px.X D �50/ D 0:
</p>
<p>Therefore, it holds for the variance that:
</p>
<p>Var.X/ D EŒ.X � E.X//2&#141; D E.X2/
D 502 � Px.X D 50/C .�50/2 � Px.X D �50/
</p>
<p>D 2500
2
</p>
<p>C 2500
2
</p>
<p>D 2500:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Problems and Solutions 39
</p>
<p>Also Y is zero on average:
</p>
<p>E.Y/ D �10 � Py.Y D �10/� 20 � Py.Y D �20/ � 30 � Py.Y D �30/
� 40 � Py.Y D �40/C 0 � Py.Y D 0/C 100 � Py.Y D 100/
</p>
<p>D 1
6
.�10� 20 � 30 � 40C 100/ D 0 :
</p>
<p>Hence, the variance reads
</p>
<p>Var.Y/ D 1
6
</p>
<p>�
.�10/2 C .�20/2 C .�30/2 C .�40/2 C 02 C 1002
</p>
<p>�
D 2166:67:
</p>
<p>For the covariance we obtain
</p>
<p>Cov.X;Y/ D E Œ.X � E.X// .Y � E.Y//&#141;
D E.X Y/
</p>
<p>D
2X
</p>
<p>iD1
</p>
<p>6X
</p>
<p>jD1
xiyjPx;y.X D xi;Y D yj/:
</p>
<p>In order to compute it, the entire joint probability distribution is to be established:
</p>
<p>Px;y.X D �50;Y D �40/ D P.f1; 3; 5g \ f4g/ D P.;/ D 0;
</p>
<p>Px;y.X D 50;Y D �40/ D P.f2; 4; 6g \ f4g/ D P.f4g/ D
1
</p>
<p>6
;
</p>
<p>Px;y.X D �50;Y D �30/ D P.Ec \ f3g/ D P.f3g/ D
1
</p>
<p>6
;
</p>
<p>Px;y.X D 50;Y D �30/ D P.E \ f3g/ D P.;/ D 0:
</p>
<p>We may collect those numbers in a table:
</p>
<p>Y D �40 �30 �20 �10 0 100
X D �50 0 1
</p>
<p>6
0 1
</p>
<p>6
1
6
</p>
<p>0
</p>
<p>X D 50 1
6
</p>
<p>0 1
6
</p>
<p>0 0 1
6
</p>
<p>Plugging in yields
</p>
<p>E.X Y/ D 1
6
Œ�50 � 40C 50 � 30� 50 � 20C 50 � 10C 50 � 0C 50 � 100&#141; D 666:67:</p>
<p/>
</div>
<div class="page"><p/>
<p>40 2 Basic Concepts from Probability Theory
</p>
<p>Therefore one obtains for the correlation coefficient apart from rounding errors
</p>
<p>�xy D 0:286.
2.6 It only remains to be shown that:
</p>
<p>E.jYjjZj/ �
p
</p>
<p>E.Y2/
p
</p>
<p>E.Z2/:
</p>
<p>In order to see that we use the binomial formula and obtain
</p>
<p>Y2
</p>
<p>E.Y2/
� 2jYjjZjp
</p>
<p>E.Y2/
p
</p>
<p>E.Z2/
C Z
</p>
<p>2
</p>
<p>E.Z2/
D
 
</p>
<p>jYjp
E.Y2/
</p>
<p>� jZjp
E.Z2/
</p>
<p>!2
� 0:
</p>
<p>Therefore, the expectation of the left hand side cannot become negative, which
</p>
<p>yields:
</p>
<p>1 � 2E.jYjjZj/p
E.Y2/
</p>
<p>p
E.Z2/
</p>
<p>C 1 D 2
 
1 � E.jYjjZj/p
</p>
<p>E.Y2/
p
</p>
<p>E.Z2/
</p>
<p>!
� 0:
</p>
<p>In particular, it can be observed that the expression is always positive except for the
</p>
<p>case Y D Z. Rearranging terms verifies the second inequality from (2.8).
2.7 Plugging in X � E.X/ and Y � E.Y/ instead of Y and Z in (2.5) by
Cauchy-Schwarz it follows that
</p>
<p>jEŒ.X � E.X//.Y � E.Y//&#141;j �
p
</p>
<p>EŒ.X � E.X//2&#141;
p
</p>
<p>EŒ.Y � E.Y//2&#141;;
</p>
<p>which is the same as:
</p>
<p>jCov.X;Y/j �
p
</p>
<p>Var.X/
p
</p>
<p>Var.Y/:
</p>
<p>This verifies the claim.
</p>
<p>2.8 Due to
</p>
<p>Fx;y.x; y/ D
Z y
</p>
<p>�1
</p>
<p>Z x
</p>
<p>�1
fx;y.r; s/dr ds;
</p>
<p>fx;y is determined by taking the partial derivative of Fx;y with respect to both
</p>
<p>arguments:
</p>
<p>@2Fx;y.x; y/
</p>
<p>@x@y
D @.1C e
</p>
<p>�x C e�y/�2e�x
@y
</p>
<p>D 2e
�xe�y
</p>
<p>.1C e�x C e�y/3
</p>
<p>D fx;y.x; y/:</p>
<p/>
</div>
<div class="page"><p/>
<p>2.5 Problems and Solutions 41
</p>
<p>The marginal distribution of Y is determined by
</p>
<p>Fy.y/ D
Z y
</p>
<p>�1
</p>
<p>Z 1
</p>
<p>�1
fx;y.x; s/dxds
</p>
<p>D lim
x!1
</p>
<p>Fx;y.x; y/ D .1C e�y/�1:
</p>
<p>The marginal density therefore reads
</p>
<p>fy.y/ D
e�y
</p>
<p>.1C e�y/2 :
</p>
<p>Division yields the conditional density:
</p>
<p>fxjy.x/ D
fx;y.x; y/
</p>
<p>fy.y/
</p>
<p>D 2e
�x.1C e�y/2
</p>
<p>.1C e�x C e�y/3 :
</p>
<p>2.9 The following sequence of equalities holds and will be justified in detail. The
</p>
<p>first two equations define exactly the corresponding (conditional) expectations. For
</p>
<p>the third equality, the order of integration is reversed; this is due to Fubini&rsquo;s theorem.
</p>
<p>The fourth equation is again by definition (conditional density), whereas in the fifth
</p>
<p>equation only the density of Y is cancelled out. In the sixth equation, the influence of
</p>
<p>Y on the joint density is integrated out such that the marginal density of X remains.
</p>
<p>This again yields the expectation of X by definition. Therefore, it holds that
</p>
<p>Ey .Ex .XjY// D
1Z
</p>
<p>� 1
</p>
<p>Ex .Xjy/ fy .y/ dy
</p>
<p>D
1Z
</p>
<p>� 1
</p>
<p>2
4
</p>
<p>1Z
</p>
<p>� 1
</p>
<p>x fxjy .x/ dx
</p>
<p>3
5 fy .y/ dy
</p>
<p>D
1Z
</p>
<p>� 1
</p>
<p>x
</p>
<p>2
4
</p>
<p>1Z
</p>
<p>� 1
</p>
<p>fxjy .x/ fy .y/ dy
</p>
<p>3
5 dx
</p>
<p>D
1Z
</p>
<p>� 1
</p>
<p>x
</p>
<p>2
4
</p>
<p>1Z
</p>
<p>� 1
</p>
<p>fx;y .x; y/
</p>
<p>fy.y/
fy .y/ dy
</p>
<p>3
5 dx</p>
<p/>
</div>
<div class="page"><p/>
<p>42 2 Basic Concepts from Probability Theory
</p>
<p>D
1Z
</p>
<p>� 1
</p>
<p>x
</p>
<p>2
4
</p>
<p>1Z
</p>
<p>� 1
</p>
<p>fx;y .x; y/ dy
</p>
<p>3
5 dx
</p>
<p>D
1Z
</p>
<p>� 1
</p>
<p>x fx.x/ dx
</p>
<p>D Ex .X/ ;
</p>
<p>which was to be verified.
</p>
<p>2.10 We use statement (a), E.xtjxt�h/ D 0 for h &gt; 0, connected with the law of
iterated expectations:
</p>
<p>E.xt/ D EŒE.xtjxt�h/&#141; D E.0/ D 0:
</p>
<p>This proves (b), that martingale differences are also unconditionally zero on average.
</p>
<p>By applying both results of Proposition 2.1 for h &gt; 0 again with (a), one arrives at:
</p>
<p>E.xt xtCh/ D EŒE.xt xtChjxt/&#141;
D EŒxtE.xtChjxt/&#141;
D EŒxt � 0&#141;
D 0:
</p>
<p>Therefore, Cov.xt; xtCh/ D 0 for h &gt; 0. However, as the covariance function is
symmetric in h, the result holds for arbitrary h &curren; 0 which was to be verified to
show (c).
</p>
<p>References
</p>
<p>Bickel, P. J., &amp; Doksum, K. A. (2001). Mathematical statistics: Basic ideas and selected topics,
volume 1 (2nd ed.). Upper Saddle River: Prentice-Hall.
</p>
<p>Billingsley, P. (1986). Probability and measure (2nd ed.). New York: Wiley.
Breiman, L. (1992). Probability (2nd ed.). Philadelphia: Society for Industrial and Applied
</p>
<p>Mathematics.
Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
</p>
<p>Springer.
Davidson, J. (1994). Stochastic limit theory: An introduction for econometricians.
</p>
<p>Oxford/New York: Oxford University Press.
Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes (3rd ed.). Oxford:
</p>
<p>Oxford University Press.
Klebaner, F. C. (2005). Introduction to stochastic calculus with applications (2nd ed.). London:
</p>
<p>Imperical College Press.
Ross, S. (2010). A first course in probability (8th ed.). Upper Saddle River: Prentice-Hall.
Rudin, W. (1976). Principles of mathematical analyis (3rd ed.). New York: McGraw-Hill.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 43
</p>
<p>Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
Berlin/New York: Springer.
</p>
<p>Trench, W. F. (2013). Introduction to real analysis. Free Hyperlinked Edition 2.04 December 2013.
Downloaded on 10th May 2014 from http://digitalcommons.trinity.edu/mono/7.</p>
<p/>
<div class="annotation"><a href="http://digitalcommons.trinity.edu/mono/7">http://digitalcommons.trinity.edu/mono/7</a></div>
</div>
<div class="page"><p/>
<p>3Autoregressive Moving Average Processes(ARMA)
</p>
<p>3.1 Summary
</p>
<p>This chapter is concerned with the modelling of serial correlation (or autocorrela-
</p>
<p>tion) that is characteristic of many time series dynamics. To that end we cover a class
</p>
<p>of stochastic processes widely used in practice. They are discrete-time processes:
</p>
<p>fxtgt2T with T � Z. Throughout this chapter, the innovations or shocks f"tg behind
fxtg are assumed to form a white noise sequence as defined in Example 2.6. The
next section treats the rather simple moving average structure. The third section
</p>
<p>addresses the inversion of lag polynomials at a general level. The fourth section
</p>
<p>breaks down the technical aspects to the application with ARMA processes.
</p>
<p>3.2 Moving Average Processes
</p>
<p>We define the moving average process of order q (MA(q)) as
</p>
<p>xt D �C b0 "t C b1 "t�1 C � � � C bq "t�q ; b0 D 1 ; t 2 T : (3.1)
</p>
<p>In the following, we assume a white noise process for the so-called innovation f"tg
governing the processes considered. In general, we assume bq &curren; 0. Let us consider
a special case before we enter the general discussion of the model.
</p>
<p>MA(1)
</p>
<p>We set � to zero and q to one in (3.1) and thereby obtain
</p>
<p>xt D "t C b "t�1:
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_3
</p>
<p>45</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>As the innovations are WN.0; �2/, the moments are independent of time: Firstly, it
</p>
<p>holds that E.xt/ D � D 0. Secondly, one obtains from
</p>
<p>&#13;.h/ D Cov.xt; xtCh/ D E .."t C b "t�1/ ."tCh C b "tCh�1//
</p>
<p>immediately
</p>
<p>&#13;.0/ D �2.1C b2/ ; &#13;.1/ D �2b ;
</p>
<p>and
</p>
<p>&#13;.h/ D 0 for h &gt; 1 :
</p>
<p>For the MA(1) process considered the autocorrelation function �.h/ D &#13;.h/
&#13;.0/
</p>
<p>therefore is:
</p>
<p>�.1/ D b
1C b2 ; �.h/ D 0; h &gt; 1:
</p>
<p>This is why the process is always (weakly) stationary without any assumptions with
</p>
<p>respect to b or to the index set T. Elementary curve sketching shows that �.1/
</p>
<p>considered with respect to b,
</p>
<p>�.1I b/ D b
1C b2 ;
</p>
<p>becomes extremal for b D ˙1. For b D �1, �.1/ takes the minimum � 1
2
, and for
</p>
<p>b D 1 it takes the maximum 1
2
</p>
<p>(see Problem 3.1).
</p>
<p>In Fig. 3.1 realizations were simulated by means of so-called pseudo random
</p>
<p>number with each 50 observations of moving average processes. All three graphs
</p>
<p>were generated by the same realizations of f"tg. The first graph with b1 D b D 0
shows the simulation of a pure random process f"tg: The times series oscillates
arbitrarily around the expected value zero. The third graph with b1 D b D 0:9
depicts the case of (strong) positive autocorrelation of first order: Positive values are
</p>
<p>followed by positive values whereas negative values tend to entail negative values,
</p>
<p>i.e. the zero line is less often crossed than in the first case. Finally, the graph in the
</p>
<p>middle lies in between both extreme cases as weak positive autocorrelation (b1 D
b D 0:3) is present.
</p>
<p>MA(q)
</p>
<p>The results obtained for the MA(1) process can be generalized. Every MA process
</p>
<p>is &ndash; for all parameter values independent of starting value conditions &ndash; always
</p>
<p>stationary. For the MA(q) process, it holds that its autocorrelation sequence vanishes
</p>
<p>from the order q on (i.e. it becomes zero). The proof is elementary and is therefore
</p>
<p>omitted.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Moving Average Processes 47
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>b1 = 0.0
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>b1 = 0.3
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
2
</p>
<p>0
2
</p>
<p>4
</p>
<p>b1 = 0.9
</p>
<p>Fig. 3.1 Simulated MA(1) processes with �2 D 1 and � D 0
</p>
<p>Proposition 3.1 (MA(q)) Assume an MA(q) process from (3.1).
</p>
<p>(a) The process is stationary with expectation �.
</p>
<p>(b) For the autocovariances it holds that,
</p>
<p>&#13;.h/ D �2.bh C bhC1b1 C : : :C bqbq�h/ ; h D 0; 1; : : : q ;
</p>
<p>and &#13;.h/ D 0 for h &gt; q.
</p>
<p>Example 3.1 (Seasonal MA) Let S denote the seasonality, e.g. S D 4 or S D 12 for
quarterly or monthly data, or S D 5 for (work) daily observations. Hence, we define
as a special MA process1
</p>
<p>xt D "t C b "t�S:
</p>
<p>1For S D 1 we obtain the MA(1) case, however, without a seasonal interpretation.</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>In this context, q D S and bq D b hold and
</p>
<p>b1 D b2 D : : : D bq�1 D 0 :
</p>
<p>Proposition 3.1 (b) yields:
</p>
<p>&#13;.0/ D �2.b0 C b21 C � � � C b2q/ D �2.1C b2/ ;
</p>
<p>&#13;.1/ D �2.b1 C b2 b1 C � � � C bq bq�1/ D 0 ;
</p>
<p>and also
</p>
<p>&#13;.h/ D 0 for h D 1; 2; : : : ; S � 1 :
</p>
<p>The case h D S, however, yields
</p>
<p>&#13;.S/ D �2bSb0 D �2b :
</p>
<p>For h &gt; S, according to Proposition 3.1 it holds that &#13;.h/ D 0. Hence, the process at
hand is exclusively autocorrelated at lag S, which is why it is also called a seasonal
</p>
<p>MA process. �
</p>
<p>MA(1) Processes
</p>
<p>We now let q go off to infinity. For reasons that become obvious in the fourth section,
</p>
<p>the MA coefficients, however, are not denoted by bj anymore. Instead, consider the
</p>
<p>infinite real sequence fcjg, j 2 N, to define:
</p>
<p>xt D �C
1X
</p>
<p>jD0
cj "t�j ;
</p>
<p>1X
</p>
<p>jD0
jcjj &lt;1 ; c0 D 1 : (3.2)
</p>
<p>Sometimes the process from (3.2) is called &ldquo;causal&rdquo; as there are only past or
</p>
<p>contemporaneous random variables "t�j, j � 0, entering the process at time t. The
condition on the coefficients fcjg of being absolutely summable guarantees thatP1
</p>
<p>jD0 cj "t�j in (3.2) is a well-defined random variable, which then can be called
xt, see e.g. Fuller (1996, Theorem 2.2.1). Absolute summability naturally implies
</p>
<p>square summability,
</p>
<p>1X
</p>
<p>jD0
c2j &lt;1 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Moving Average Processes 49
</p>
<p>and, indeed, sometimes we define the MA(1) process upon this weaker assumption.
Square summability of fcjg is sufficient for stationarity with E.xt/ D � and
</p>
<p>&#13;.h/ D �2
1X
</p>
<p>jD0
cj cjCh ;
</p>
<p>see Fuller (1996, Theorem 2.2.3), which is the first result of the follow proposition.
</p>
<p>The second one is established in Problem 3.3.
</p>
<p>Proposition 3.2 (Infinite MA) Assume an MA(1) process,
</p>
<p>xt D �C
1X
</p>
<p>jD0
cj "t�j ; f"tg � WN.0; �2/ ; c0 D 1 ;
</p>
<p>with
P1
</p>
<p>jD0 c
2
j &lt;1.
</p>
<p>(a) The process is stationary with expected value �, and for the autocovariances it
</p>
<p>holds that
</p>
<p>&#13;.h/ D �2
1X
</p>
<p>jD0
cj cjCh ; h D 0; 1; : : : :
</p>
<p>(b) Under absolute summability,
P1
</p>
<p>jD0 jcjj &lt; 1, the sequence of autocovariances
is absolutely summable:
</p>
<p>1X
</p>
<p>hD0
j&#13;.h/j &lt;1 :
</p>
<p>The fact that the sequence of autocovariances is absolutely summable under (3.2),
</p>
<p>see (b) of the proposition, has an immediate implication: the autocovariances tend
</p>
<p>to zero with growing lag:
</p>
<p>&#13;.h/! 0 as h ! 1 :
</p>
<p>This means that the correlation between xt and xt�h tends to decrease with growing
lag h.
</p>
<p>Note that xt from (3.2) or Proposition 3.2 is defined as a linear combination of
</p>
<p>"t�j, j � 0. Therefore, one sometimes speaks of a linear process. Other authors
reserve this label for the more restricted case of iid innovations, where all temporal
</p>
<p>dependence of fxtg arises exclusively from the MA coefficients fcjg. The results of
Proposition 3.2, however, hold under the weaker assumption that f"tg is white noise.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>Impulse Responses
</p>
<p>The MA(1) coefficients are often called impulse responses, since they measure
the effect of a shock j periods ago on xt:
</p>
<p>@xt
</p>
<p>@"t�j
D cj :
</p>
<p>Square summability implies that shocks are transient in that cj ! 0 as j ! 1. The
speed at which the impulse responses converge to zero characterizes the dynamics.
</p>
<p>In particular, Campbell and Mankiw (1987) popularized the cumulated impulse
</p>
<p>responses as measure of persistence, where the cumulated effect is defined as
</p>
<p>CIR.J/ D
JX
</p>
<p>jD0
cj :
</p>
<p>This measure quantifies the total effect up to J periods back, if there occurred a unit
</p>
<p>shock in each past period, including the present period at time t. Asymptotically,
</p>
<p>one obtains the so-called long-run effect, often called total multiplier in economics.
</p>
<p>This measure is defined as
</p>
<p>CIR WD lim
J!1
</p>
<p>CIR.J/ ; (3.3)
</p>
<p>provided that this quantity exists. Clearly, under absolute summability of the
</p>
<p>impulse responses, CIR is well defined. Andrews and Chen (1994) advocated CIR
</p>
<p>as being superior to alternative measures of persistence. We will return to the
</p>
<p>measurement and interpretation of persistence in the next chapter.
</p>
<p>The MA(1) process is of considerable generality. In fact, Wold (1938) showed
that every stationary process with expected value zero can be decomposed into a
</p>
<p>square summable, purely non-deterministic MA(1) component and an uncorrelated
component, say fıtg, which is deterministic in the sense that it is perfectly
predictable from its past:2
</p>
<p>1X
</p>
<p>jD0
cj "t�j C ıt ;
</p>
<p>1X
</p>
<p>jD0
c2j &lt;1 ; E."t�jıt/ D 0 ; (3.4)
</p>
<p>2Typically, one assumes that ıt is identically equal to zero for all t, since &ldquo;perfectly predictable&rdquo;
only allows for trivial processes like e.g. ıt D .�1/tA or ıt D A, where A is some random variable,
such that ıt D �ıt�1 or ıt D ıt�1, respectively. Of course, this does not rule out the case of a
constant mean � different from zero as assumed in (3.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Lag Polynomials and Invertibility 51
</p>
<p>where E."t/ D E.ıt/ D 0. For details on this Wold decomposition see Fuller (1996,
Theorem 2.10.2) or Brockwell and Davis (1991, Theorem 5.7.1).
</p>
<p>In practice it is not reasonable to construct a model with infinitely many
</p>
<p>parameters cj as they cannot be estimated from a finite number of observations
</p>
<p>without further restrictions. In the fourth section of this chapter, however, we will
</p>
<p>learn that very simple, so-called autoregressive processes with a finite number of
</p>
<p>parameters possess an MA(1) representation. In order to discuss autoregressive
processes rigorously, we need to concern ourselves with polynomials in the lag
</p>
<p>operator and their invertibility.
</p>
<p>3.3 Lag Polynomials and Invertibility
</p>
<p>Frequently, time series models are written by means of the lag operator3 L. The
</p>
<p>lag operator shifts the process fxtg by one unit in time: Lxt D xt�1. By the inverse
operator L�1 the shift is just reversed, L�1Lxt D xt, or L�1xt D xtC1. Successive use
of the operator is denoted by its power, L jxt D xt�j, j 2 Z. The identity is described
with L0, L0xt D xt. Applied to a constant c, the operator leaves the value unchanged,
Lc D c.
</p>
<p>Causal Linear Filters
</p>
<p>Let us consider an input process fxtg, t 2 T, which is transferred into an output
process fytg by linear filtering,
</p>
<p>yt D
pX
</p>
<p>jD0
wj xt�j ; (3.5)
</p>
<p>with the real filter coefficients fwjg (which need not to add up to one). Therefore, yt
is a linear combination of values of xt�j where we assume constant filters, i.e. the
weights do not depend on t. In particular, the filter is called causal as yt is defined
</p>
<p>by past and contemporaneous values of fxtg only.
The general linear causal filter from (3.5) can be formulated as polynomial in
</p>
<p>the lag operator:
</p>
<p>F.L/ D
pX
</p>
<p>jD0
wj L
</p>
<p>j with yt D F.L/ xt :
</p>
<p>3Many authors also speak of the backshift operator and write B.</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>Occasionally, two filters are put in a row:
</p>
<p>yt D F1.L/ xt ; zt D F2.L/ yt D F2.L/F1.L/ xt ;
</p>
<p>F1.L/ D
p1X
</p>
<p>jD0
w1; j L
</p>
<p>j ; F2.L/ D
p2X
</p>
<p>jD0
w2; j L
</p>
<p>j :
</p>
<p>Then, the filter F.L/ transforming fxtg into fztg is defined by multiplying the filters,
F.L/ D F2.L/F1.L/, which is called a &ldquo;convolution&rdquo;:
</p>
<p>F2.L/F1.L/ D
p1Cp2X
</p>
<p>kD0
vkL
</p>
<p>k ; vk D
kX
</p>
<p>jD0
w2; jw1;k�j D
</p>
<p>kX
</p>
<p>jD0
w2;k�jw1; j :
</p>
<p>This convolution is commutative:
</p>
<p>F1.L/F2.L/ D F2.L/F1.L/ :
</p>
<p>By expressing filters by means of the lag operator, we can manipulate them just
</p>
<p>as ordinary (complex-valued) polynomials. As an example we consider so-called
</p>
<p>difference filters.
</p>
<p>Example 3.2 (Difference Filters) By means of the lag operator, filters can be
</p>
<p>constructed, for example the difference filter � D 1 � L or the difference of the
previous year for quarterly data �4 D 1 � L4:
</p>
<p>�xt D xt � xt�1 ; �4xt D xt � xt�4 :
</p>
<p>The seasonal difference filter for monthly observations (S D 12) as well as for daily
observations (S D 5) are defined analogously:
</p>
<p>�S D 1 � LS :
</p>
<p>Instead of extensively calculating the double difference,
</p>
<p>�.�xt/ D �.xt � xt�1/ D .xt � xt�1/� L.xt � xt�1/
D .xt � xt�1/ � .xt�1 � xt�2/ D xt � 2 xt�1 C xt�2 ;
</p>
<p>we write in short by expanding .1 � L/2:
</p>
<p>�2xt D .1 � L/2xt D .1 � 2 L C L2/xt D xt � 2 xt�1 C xt�2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Lag Polynomials and Invertibility 53
</p>
<p>While the ordinary difference operator� eliminates a linear time trend from a time
</p>
<p>series, the second differences naturally remove a quadratic time trend:
</p>
<p>�2.a C bt C ct2/ D c.t2 � 2.t � 1/2 C .t � 2/2/ D 2c:
</p>
<p>The fact that the order of filtering is exchangeable is well demonstrated by means
</p>
<p>of the example of seasonal and ordinary differences:
</p>
<p>��S D .1 � L/ .1 � LS/ D 1 � L � LS C LSC1
</p>
<p>D .1 � LS/ .1 � L/ D �S� : �
</p>
<p>Invertibility of Lag Polynomials
</p>
<p>We define as a polynomial of degree p (also of order p) in the lag operator
</p>
<p>P.L/ D 1C b1 L C � � � C bp L p ; bp &curren; 0 ; (3.6)
</p>
<p>with the real coefficients b1 to bp. For brevity, P.L/ is also called lag polynomial.
</p>
<p>Consider a first degree polynomial as a special case of (3.6),
</p>
<p>A1.L/ D 1 � a L ;
</p>
<p>where the reason for the negative sign will immediately be obvious. When and how
</p>
<p>can this polynomial be inverted? A comparison of coefficients (&ldquo;method of unde-
</p>
<p>termined coefficients&rdquo;) results in the following series expansion (see Problem 3.4):
</p>
<p>.1 � aL/�1 D 1
A1.L/
</p>
<p>D
1X
</p>
<p>jD0
a jL j :
</p>
<p>As is well known, it holds that (infinite geometric series, see Problem 3.2)
</p>
<p>1X
</p>
<p>jD0
a j D 1
</p>
<p>1 � a &lt;1 &rdquo; jaj &lt; 1 : (3.7)
</p>
<p>Hence, it holds that
</p>
<p>.1 � aL/�1 D 1
A1.L/
</p>
<p>D
1X
</p>
<p>jD0
a jL j with
</p>
<p>1X
</p>
<p>jD0
ja jj D 1
</p>
<p>1 � jaj &lt;1
</p>
<p>if and only if jaj &lt; 1. This condition of invertibility is frequently reformulated. In
order to do this, we determine the so-called z-transform of the lag polynomial with
</p>
<p>z being an element of the complex numbers (z 2 C): A1.z/ D 1 � az. Now, jaj &lt; 1</p>
<p/>
</div>
<div class="page"><p/>
<p>54 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>implies that the z-transform A1.z/ D 1 � az exhibits only roots outside the unit
circle, i.e. roots being greater than one in absolute value:
</p>
<p>jaj &lt; 1 &rdquo;
�
</p>
<p>A1.z/ D 0 ) jzj D
1
</p>
<p>jaj &gt; 1
�
: (3.8)
</p>
<p>Causally Invertible Polynomials
</p>
<p>This condition of invertibility for A1.L/ from (3.8) is easily conveyed to a polyno-
</p>
<p>mial P.L/ of the order p. We say P.L/ is causally invertible if there exists a power
</p>
<p>series expansion with non-negative powers and absolutely summable coefficients:
</p>
<p>.P.L//�1 D 1
P.L/
</p>
<p>D
1X
</p>
<p>jD0
˛jL
</p>
<p>j with
</p>
<p>1X
</p>
<p>jD0
j˛jj &lt;1 :
</p>
<p>The invertibility depends on the z-transform
</p>
<p>P.z/ D 1C b1 z C � � � C bp z p ; z 2 C ; (3.9)
</p>
<p>or rather on the absolute value of its roots. The following condition of invertibility
</p>
<p>is adopted from Brockwell and Davis (1991, Thm. 3.1.1), and it is discussed as an
</p>
<p>exercise (Problem 3.5).
</p>
<p>Proposition 3.3
</p>
<p>(a) The polynomial 1 � aL is causally invertible,
</p>
<p>1
</p>
<p>1 � aL D
1X
</p>
<p>jD0
a jL j with
</p>
<p>1X
</p>
<p>jD0
ja jj &lt;1 ;
</p>
<p>if and only if jaj &lt; 1.
(b) The polynomial P.L/ from (3.6) is causally invertible, i.e. for .P.L//�1 there
</p>
<p>exits the absolutely summable series expansion,
</p>
<p>.P.L//�1 D 1
P.L/
</p>
<p>D
1X
</p>
<p>jD0
˛jL
</p>
<p>j with
</p>
<p>1X
</p>
<p>jD0
j˛jj &lt;1 ;
</p>
<p>if and only if it holds for all roots of P.z/ that they are greater than one in
</p>
<p>absolute value:
</p>
<p>P.z/ D 0 ) jzj &gt; 1 : (3.10)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Lag Polynomials and Invertibility 55
</p>
<p>Example 3.3 (Invertible MA Processes) The MA(1) process, xt D "t C b"t�1, can
now be formulated alternatively by applying the MA(1) polynomial B.L/ D 1Cb L.
Although not required for stationarity, it is usually assumed that jbj &lt; 1. What for?
This implies for the MA polynomial that:
</p>
<p>B.z/ D 0 ) jzj D 1jbj &gt; 1:
</p>
<p>According to Proposition 3.3 the condition of invertibility is fulfilled and there exists
</p>
<p>1
</p>
<p>B.L/
D 1
1C b L D
</p>
<p>1X
</p>
<p>jD0
˛j L
</p>
<p>j;
</p>
<p>where the coefficients f˛jg are absolutely summable. The f˛jg are obtained explicitly
by comparison of coefficients in
</p>
<p>1 D .1C b L/
1X
</p>
<p>jD0
˛j L
</p>
<p>j
</p>
<p>D ˛0 C ˛1 L C ˛2 L2 C ˛3 L3 C � � �
Cb .˛0 L C ˛1 L2 C ˛2 L3 C � � � /;
</p>
<p>yielding
</p>
<p>1 D ˛0; 0 D ˛1 C b˛0; 0 D ˛2 C b˛1; etc.;
</p>
<p>or
</p>
<p>˛0 D 1; ˛1 D �b; ˛2 D b2; and ˛j D .�1/ j b j; j � 0:
</p>
<p>Hence, the MA(1) process xt D B.L/ "t can be reformulated as follows:
</p>
<p>"t D
xt
</p>
<p>1C b L D xt � b xt�1 C b
2 xt�2 � b3 xt�3 ˙ : : : ;
</p>
<p>or
</p>
<p>xt D b xt�1 � b2 xt�2 C b3 xt�3 ˙ : : :C "t
</p>
<p>D
1X
</p>
<p>jD1
.�1/ j�1 b j xt�j C "t ;
</p>
<p>1X
</p>
<p>jD0
jb jj &lt;1:</p>
<p/>
</div>
<div class="page"><p/>
<p>56 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>In other words: The invertible MA(1) process (for jbj &lt; 1) can be expressed (in
an absolutely summable manner) as a process depending on its own infinitely many
</p>
<p>lags. Such a process is called autoregressive (of infinite order). �
</p>
<p>Frequently, one is interested in the special case of a quadratic polynomial, p D 2. In
this case, the so-called Schur criterion provides an equivalent reformulation of the
</p>
<p>condition of invertibility, rephrasing (3.10) in terms of the polynomial coefficients
</p>
<p>bi directly. These have to fulfill three conditions simultaneously. We take the
</p>
<p>corresponding statement from e.g. Syds&aelig;ter, Str&oslash;m, and Berck (1999, p. 58).
</p>
<p>Corollary 3.1 For p D 2 the polynomial from (3.6),
</p>
<p>P.L/ D 1C b1L C b2L2 ;
</p>
<p>is causally invertible with absolutely summable series expansion .P.L//�1 if and
only if:
</p>
<p>.i/ 1 � b2 &gt; 0 ;
and .ii/ 1 � b1 C b2 &gt; 0 ;
and .iii/ 1C b1 C b2 &gt; 0 :
</p>
<p>Instead of checking jz1;2j &gt; 1 for
</p>
<p>z1;2 D
�b1 ˙
</p>
<p>q
b21 � 4b2
</p>
<p>2b2
;
</p>
<p>it may in practice be simpler to check (i) through (iii) from Corollary 3.1.
</p>
<p>3.4 Autoregressive andMixed Processes
</p>
<p>Let fxtg be given by the following stochastic difference equation,
</p>
<p>xt D � C a1 xt�1 C � � � C ap xt�p C "t ; ap &curren; 0 ; t 2 T ;
</p>
<p>defining an autoregressive process of the order p, AR(p). The properties of the
</p>
<p>general AR process can be illustrated well at the example p D 1.
</p>
<p>AR(1)
</p>
<p>Particularly, let p D 1:
</p>
<p>xt D � C a xt�1 C "t ; t 2 T : (3.11)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 57
</p>
<p>When replacing xt�1 by this defining equation, one obtains:
</p>
<p>xt D � C a. � C a xt�2 C "t�1/C "t
D � C a � C a2 xt�2 C a "t�1 C "t:
</p>
<p>Hence, xt and xt�2 prove to be correlated. Continued substitution yields
</p>
<p>xt D � C a � C a2 � C a3 xt�3 C a2 "t�2 C a "t�1 C "t ;
</p>
<p>or for any h � 0:
</p>
<p>xt D .1C a C : : :C ah�1/ � C ah xt�h C ah�1 "t�hC1 C : : :C a "t�1 C "t:
</p>
<p>Now, let us suppose that the index set T does not have a lower bound at zero but
</p>
<p>includes an infinite past, then h can be arbitrarily large and the substitution can be
</p>
<p>repeated ad infinitum. If it furthermore holds that
</p>
<p>jaj &lt; 1;
</p>
<p>then the geometric series yields (h ! 1)
</p>
<p>1C a C : : :C ah�1 D 1 � a
h
</p>
<p>1 � a !
1
</p>
<p>1 � a ;
</p>
<p>and ah xt�h ! 0 in a sense that can be made rigoros, see Brockwell and Davis
(1991, p. 71) or Fuller (1996, p. 39). In this manner, it follows for h ! 1 under the
aforementioned conditions that:
</p>
<p>xt D
�
</p>
<p>1 � a C 0C
1X
</p>
<p>jD0
a j "t�j:
</p>
<p>This way one obtains an infinite MA representation with geometrically decaying
</p>
<p>coefficients, cj D a j in (3.2). In fact, this representation can formally be obtained
by inverting 1� aL, see Proposition 3.3 (a). The process is therefore stationary with
(see Proposition 3.2)
</p>
<p>E.xt/ D � D
�
</p>
<p>1 � a ;
</p>
<p>Var.xt/ D �2
1X
</p>
<p>jD0
a2j D �
</p>
<p>2
</p>
<p>1 � a2 D &#13;.0/</p>
<p/>
</div>
<div class="page"><p/>
<p>58 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>and
</p>
<p>Cov.xt ; xtCh/ D �2
1X
</p>
<p>jD0
a jajCh
</p>
<p>D ah &#13;.0/:
</p>
<p>It follows
</p>
<p>�.h/ D ah:
</p>
<p>Note that these results are obtained for jaj &lt; 1. Furthermore, stationarity also
depends on the index set. That is to say, if it holds that T D f0; 1; 2; : : :g then the
above-mentioned repeated substitution cannot be performed infinitely often. From
</p>
<p>xt D .1C a C : : :C at�1/ � C at x0 C at�1 "1 C � � � C a "t�1 C "t
</p>
<p>the expected value follows as time-dependent:
</p>
<p>E.xt/ D .1C a C : : :C at�1/ � C atE.x0/; t 2 f0; 1; : : :g:
</p>
<p>In particular, this example shows that the stationarity behavior of a process can
</p>
<p>depend on the index set T. Therefore, in general a stochastic process is not
</p>
<p>completely characterized without specifying T.
</p>
<p>Example 3.4 (AR(1)) Figure 3.2 displays 50 realizations each of AR(1) processes
</p>
<p>obtained by simulation. However, in the first case a1 D a D 0 such that the graph
depicts the realizations of f"tg. On the right the theoretical autocorrelogram, �.h/ D
0 for h &gt; 0, is shown. In the second panel, the case of a positive autocorrelation
</p>
<p>(a1 D a D 0:75) is illustrated: Positive values tend to be followed by positive values
(and vice versa for negative values), such that phases of positive realizations tend to
</p>
<p>alternate with phases of negative values. The corresponding autocorrelogram shows
</p>
<p>the geometrically decaying positive autocorrelations up to the order 10. In the last
</p>
<p>case, there is a negative autocorrelation (a1 D a D �0:75); consistently, negative
values tend to be followed by positive ones and vice versa positive values tend to be
</p>
<p>followed by negative ones. Therefore, the zero line is more often crossed than in the
</p>
<p>first case of no serial correlation. The corresponding autocorrelogram is alternating:
</p>
<p>�.h/ D jajh.�1/h for a D �0:75. Qualitatively different patterns of autocorrelation
cannot be generated by the simple AR(1) model. Note that the impulse responses or
</p>
<p>MA(1) coefficients of the AR(1) model are cj D a j. Consequently, the cumulated
effect defined in (3.3) becomes for jaj &lt; 1:
</p>
<p>CIR D
1X
</p>
<p>jD0
a j D 1
</p>
<p>1 � a :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 59
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
1
</p>
<p>0
1
</p>
<p>2
</p>
<p>a1 = 0.0
</p>
<p>0 2 4 6 8 10
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>autocorrelogram
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
3
</p>
<p>&minus;
1
</p>
<p>1
3
</p>
<p>a1 = 0.75
</p>
<p>0 2 4 6 8 10
</p>
<p>0
.2
</p>
<p>0
.6
</p>
<p>1
.0
</p>
<p>autocorrelogram
</p>
<p>0 10 20 30 40 50
</p>
<p>&minus;
2
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>4
</p>
<p>a1 = &minus;0.75
</p>
<p>0 2 4 6 8 10
</p>
<p>&minus;
0
.5
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>autocorrelogram
</p>
<p>Fig. 3.2 Simulated AR(1) processes with �2 D 1 and � D 0
</p>
<p>The larger a, the larger is CIR, which hence quantifies the persistence described
</p>
<p>above for positive a. �
</p>
<p>We have seen that the stationarity of an AR(1) process depends essentially on
</p>
<p>the absolute value of a. For the Markov property, however, this value is irrelevant.
</p>
<p>We talk about a Markov process if the entire past information It up to time t is
</p>
<p>concentrated in the last observation xt:
</p>
<p>P .xtCs � xjIt/ D P .xtCs � xjxt/
</p>
<p>for all s &gt; 0 and x 2 R. Assuming a normal distribution, we show in Problem 3.7
that every AR(1) process is a Markov process.
</p>
<p>AR(p)
</p>
<p>In general, the AR(p) process can be formulated equivalently by means of a lag
</p>
<p>polynomial:
</p>
<p>A.L/ xt D � C "t ; A.L/ D 1 � a1 L � � � � � ap L p ; t 2 T : (3.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>60 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>Under the condition
</p>
<p>A.z/ D 0 ) jzj &gt; 1
</p>
<p>the autoregressive polynomial A.L/ is causally invertible with absolutely summable
</p>
<p>coefficients according to Proposition 3.3:
</p>
<p>1
</p>
<p>A.L/
D
</p>
<p>1X
</p>
<p>jD0
˛j L
</p>
<p>j ;
</p>
<p>1X
</p>
<p>jD0
j˛jj &lt;1 :
</p>
<p>Therefore, under the condition of invertibility it holds that
</p>
<p>xt D
� C "t
A.L/
</p>
<p>D �
A.1/
</p>
<p>C
1X
</p>
<p>jD0
˛j "t�j ;
</p>
<p>and fxtg is a stationary MA(1) process, see Proposition 3.2. Hence, the autocovari-
ance sequence is absolutely summable. Furthermore, in this case it holds for h &gt; 0
</p>
<p>(w.l.o.g.4 we set � D 0 for simplification),
</p>
<p>&#13;.h/ D Cov.xt; xtCh/
D E.xt xtCh/
D E
</p>
<p>�
xt.a1 xtCh�1 C : : :C ap xtCh�p C "tCh/
</p>
<p>�
</p>
<p>D a1&#13;.h � 1/C : : :C ap&#13;.h � p/C 0 :
</p>
<p>Dividing by &#13;.0/ yields the recursive relation from the subsequent proposition: The
</p>
<p>autocorrelations are given by a deterministic difference equation of order p. The still
</p>
<p>missing necessary condition of stationarity from Proposition 3.4 (A.1/ &gt; 0) will be
</p>
<p>derived in Problem 3.6.
</p>
<p>Proposition 3.4 (AR(p)) Let fxtg be an AR(p) process from (3.12) with index set5
T D f�1; : : : ;Tg.
</p>
<p>4The abbreviation stands for &ldquo;without loss of generality&rdquo;. It is frequently used for assumptions that
are substantially not necessary and that are only made to simplify the argument or the notation. In
the example at hand, generally it would have to be written .xt�j ��/ for all j; just as well, one can
set � D �=A.1/ equal to zero and simply write xt�j.
5Also in the following, the notation f�1; : : : ; Tg is always to denote the set of all integers without
fT C 1; T C 2; : : :g.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 61
</p>
<p>(a) The process has an absolutely summable MA(1) representation according
to (3.2) if and only if it holds that
</p>
<p>A.z/ D 0 ) jzj &gt; 1 :
</p>
<p>Then the process is stationary with expectation � D �=A.1/. The condition
A.1/ D 1 �
</p>
<p>Pp
jD1 aj &gt; 0 is necessary for this.
</p>
<p>(b) For stationary processes the autocorrelation sequence is absolutely summable
</p>
<p>where it holds that �.h/ &curren; 0 for all integer numbers h and
</p>
<p>�.h/ D a1 �.h � 1/C : : :C ap �.h � p/ ; h &gt; 0 :
</p>
<p>Again note that the absolute summability of �.h/ implies: �.h/ ! 0 for h ! 1.
The farther xt and xtCh are apart from each other the weaker tends to be their
correlation.
</p>
<p>Certain properties of the AR(1) process are lost for p &gt; 1. In Problem 3.8 we
</p>
<p>show for a special case (p D 2) that the AR(p) process, p &gt; 1, is not a Markov
process in general.
</p>
<p>AR(2)
</p>
<p>Let p D 2,
</p>
<p>xt D � C a1 xt�1 C a2 xt�2 C "t ;
</p>
<p>with the autoregressive polynomial
</p>
<p>A.L/ D 1 � a1L � a2L2 :
</p>
<p>From Corollary 3.1, we know the conditions under which .A.L//�1 can be expanded
as an absolutely summable filter:
</p>
<p>.i/ 1C a2 &gt; 0 ;
and .ii/ 1C a1 � a2 &gt; 0 ;
and .iii/ 1 � a1 � a2 &gt; 0 :
</p>
<p>Consequently, under these three parameter restrictions the AR(2) process is sta-
</p>
<p>tionary. The restrictions become even more obvious if they are solved for a2 and
</p>
<p>depicted in a coordinate system:
</p>
<p>.i/ a2 &gt; �1 ;
and .ii/ a2 &lt; 1C a1 ;
and .iii/ a2 &lt; 1 � a1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>62 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>&minus;2 &minus;1 0 1 2
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>a1
</p>
<p>a
2
</p>
<p>Fig. 3.3 Stationarity triangle for AR(2) processes
</p>
<p>So, three lines are given, above or below which a2 has to be, respectively. This is
</p>
<p>why one also talks about the stability or stationarity triangle, see Fig. 3.3. Within the
</p>
<p>triangle lies the stationarity region.
</p>
<p>The autocorrelation series of the AR(2) case is determined from Proposition 3.4.
</p>
<p>For h D 0, it naturally holds that �.0/ D 1. For h D 1, one obtains
</p>
<p>�.1/ D a1 �.0/C a2 �.�1/:
</p>
<p>Because of the symmetry, �.�h/ D �.h/, it follows that
</p>
<p>�.1/ D a1
1 � a2
</p>
<p>:
</p>
<p>Similarly, it follows that
</p>
<p>�.2/ D a1 �.1/C a2 �.0/
</p>
<p>D a
2
1
</p>
<p>1 � a2
C a2:
</p>
<p>By repeated insertion into the second order difference equation,
</p>
<p>�.h/ D a1 �.h � 1/C a2 �.h � 2/ ; h � 2 ;
</p>
<p>the entire autocorrelation sequence is determined. Next, four numerical examples
</p>
<p>will be considered.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 63
</p>
<p>0 5 10 15
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>a1 = 0.7, a2 = 0.1
</p>
<p>0 5 10 15
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>a1 = &minus;0.4, a2 = 0.4
</p>
<p>0 5 10 15
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>a1 = &minus;1.0, a2 = &minus;0.8
</p>
<p>0 5 10 15
</p>
<p>&minus;
0
</p>
<p>.2
0
</p>
<p>.2
0
</p>
<p>.6
1
</p>
<p>.0
</p>
<p>a1 = 1.0, a2 = &minus;0.5
</p>
<p>Fig. 3.4 Autocorrelograms for AR(2) processes
</p>
<p>Example 3.5 (AR(2)) We now consider four numerical examples for AR(2) pro-
</p>
<p>cesses in order to characterize typical patterns of autocorrelation. In all cases, the
</p>
<p>stability conditions can be proven to be fulfilled. The corresponding autocorrelo-
</p>
<p>grams can be found in Fig. 3.4.
</p>
<p>(i) a1 D 0:7, a2 D 0:1: In this case, all the autocorrelations are positive and they
converge to zero with h; their behavior is similar to the autocorrelogram of a
</p>
<p>AR(1) process with a1 &gt; 0, see Fig. 3.2.
</p>
<p>(ii) a1 D �0:4, a2 D 0:4: Starting with �.1/ &lt; 0, the autocorrelations alternate
similarly to an AR(1) process with a1 &lt; 0, cf. Fig. 3.2.
</p>
<p>(iii) a1 D �1:0, a2 D �0:8: In this case, we find a dynamic which cannot be
generated by an AR(1) model; two negative autocorrelations of the first and
</p>
<p>second order are followed by a seemingly irregularly alternating pattern.
</p>
<p>(iv) a1 D 1:0, a2 D �0:5: In the last case, the autocorrelations swing from the
positive area to the negative area, then to the positive one and again to the
</p>
<p>negative one whereas the last-mentioned can hardly be perceived because of
</p>
<p>the small absolute values.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>Therefore, the cases (iii) and (iv) show that the AR(2) process allows for richer
</p>
<p>dynamics and dependence structures than the simpler AR(1) process. �
</p>
<p>AutoregressiveMoving Average Processes
</p>
<p>Now, we consider a combination of AR and MA processes. Again, fxtg is given by
a stochastic difference equation of order p, only that "t from (3.12) is replaced by an
</p>
<p>MA(q) process:
</p>
<p>xt D � C a1 xt�1 C � � � C ap xt�p C "t C b1 "t�1 C � � � C bq "t�q ; t 2 T :
</p>
<p>Abbreviating, we talk about ARMA(p; q) processes assuming ap &curren; 0 and bq &curren; 0.
Again, a more compact representation follows by using lag polynomials,
</p>
<p>A.L/ xt D � C B.L/ "t ; t 2 T ; (3.13)
</p>
<p>where it is assumed that both polynomials
</p>
<p>A.L/ D 1 � a1 L � � � � � ap L p and B.L/ D 1C b1 L C � � � C bq Lq
</p>
<p>do not have common roots.
</p>
<p>A stationary MA(1) representation hinges on the autoregressive polynomial
such that the stationarity condition can be adopted from the pure AR(p) case in
</p>
<p>Proposition 3.4. It amounts to an absolutely summable expansion of .A.L//�1
</p>
<p>such that the process possesses an absolutely summable representation as MA(1)
process, see (3.2). If stationarity is given, the absolutely summable autocorrelation
</p>
<p>sequence can again be determined from a stable difference equation (see e.g.
</p>
<p>Brockwell &amp; Davis, 1991, p. 93)
</p>
<p>Proposition 3.5 (ARMA(p; q)) Let fxtg be an ARMA(p; q) process from (3.13) with
T D f�1; : : : ;Tg.
</p>
<p>(a) The process has an absolutely summable MA(1) representation according
to (3.2) if and only if it holds that
</p>
<p>A.z/ D 0 ) jzj &gt; 1 :
</p>
<p>Then the process is stationary with expectation � D �=A.1/. The condition
A.1/ D 1 �
</p>
<p>Pp
jD1 aj &gt; 0 is necessary for this.
</p>
<p>(b) For stationary processes the autocorrelation sequence is absolutely summable
</p>
<p>where it holds that �.h/ &curren; 0 for all integer numbers h and
</p>
<p>�.h/ D a1 �.h � 1/C : : :C ap �.h � p/ ; h � max.p; q C 1/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 65
</p>
<p>For h � max.p; q C 1/ the autocorrelations satisfy the same difference equation as
in the pure AR(p) case. It is known that the solution to such a difference equation
</p>
<p>is bounded exponentially. Consequently, we have the following result, see e.g.
</p>
<p>Brockwell and Davis (1991, Prob. 3.11): For a stationary ARMA process there exist
</p>
<p>positive constants c and g with 0 &lt; g &lt; 1, such that
</p>
<p>j�.h/j � c gh (3.14)
</p>
<p>for all h D 1; 2; : : :. Hence, the decay rate of the autocorrelations is bounded
exponentially. This shows again that stationary ARMA process are characterized
</p>
<p>be absolutely summable autocorrelations, see Problem 3.2. Proposition 3.5 will be
</p>
<p>discussed below for p D q D 1.
Before turning to the ARMA(1,1) case, we note that an analogous result to
</p>
<p>Proposition 3.5 (a) is available for an AR(1) representation according to Propo-
sition 3.3: The ARMA process has an absolutely summable AR(1) representation
(see Example 3.3) if and only if
</p>
<p>B.z/ D 0 ) jzj &gt; 1 :
</p>
<p>In this case the ARMA process is called invertible.
</p>
<p>ARMA(1,1)
</p>
<p>We now wish to obtain the autocorrelation structure of the ARMA(1,1) process,
</p>
<p>xt D a xt�1 C "t C b "t�1; jaj &lt; 1; jbj &lt; 1 ;
</p>
<p>where jaj &lt; 1 for stationarity, and jbj &lt; 1 to ensure invertibility (see Example 3.3).
The condition of no common roots of 1� aL and 1C bL is given for a &curren; �b. In the
case of common roots, the lag polynomials could be reduced and one would obtain
</p>
<p>xt D
1C bL
1C bL "t D "t for a D �b :
</p>
<p>Due to the invertibility of 1 � aL, the process can be formulated as an infinite MA
process,
</p>
<p>xt D
"t C b "t�1
1 � aL
</p>
<p>D
1X
</p>
<p>jD0
a j "t�j C b
</p>
<p>1X
</p>
<p>jD0
a j "t�1�j
</p>
<p>D "t C
1X
</p>
<p>jD1
</p>
<p>�
a j C b aj�1
</p>
<p>�
"t�j ;</p>
<p/>
</div>
<div class="page"><p/>
<p>66 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>where a shift of subscripts was carried out. In the notation of (3.2) the MA(1)
coefficients result as
</p>
<p>cj D aj�1.a C b/ ; j � 1 :
</p>
<p>In this way Proposition 3.2 yields for the variance:
</p>
<p>&#13;.0/ D �2
0
@1C
</p>
<p>1X
</p>
<p>jD1
a2j�2 .a C b/2
</p>
<p>1
A D : : : D �2 .1C b
</p>
<p>2 C 2ab/
1 � a2 :
</p>
<p>The autocovariance at lag one follows in the same way,
</p>
<p>&#13;.1/ D �2 .a C b/.1C ab/
1 � a2 ;
</p>
<p>such that it holds:
</p>
<p>�.1/ D .a C b/.1C ab/
1C b2 C 2ab :
</p>
<p>Furthermore we learn from Proposition 3.5:
</p>
<p>�.h/ D a �.h � 1/; h � 2:
</p>
<p>Hence, the MA(1) component (that is b) influences directly only �.1/ having only
</p>
<p>an indirect effect beyond the autocorrelation of the first order: For h � 2 a recursive
relation between �.h/ and �.h � 1/ holds true just as it applies to the pure AR(1)
process. Thus, this yields four typical patterns. In order to identify these, it suffices
</p>
<p>entirely to concentrate on the numerator of �.1/ as well as on the sign of a as the
</p>
<p>denominator of �.1/ is always positive because it is a multiple of the variance. As
</p>
<p>1C ab is positive due to the stationarity and the invertibility of the MA polynomial,
the behavior of the autocorrelogram depends on the signs of a C b and a only. The
exponential bound for the autocorrelations is easily verified:
</p>
<p>�.h/ D a �.h � 1/ D � � � D ah�1�.1/ : h � 2:
</p>
<p>Therefore, (3.14) applies with g D jaj and c D j�.1/j=jaj.
</p>
<p>Example 3.6 (ARMA(1,1)) The four possible patterns of the autocorrelogram of a
</p>
<p>stationary and invertible ARMA(1,1) model will be discussed and illustrated by
</p>
<p>numerical examples, cf. Fig. 3.5.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.4 Autoregressive and Mixed Processes 67
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>a1 = 0.75, b1 = 0.75
</p>
<p>0 8 10 062 4 2 4 6 8 10
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>a1 = 0.75, b1 = &minus;0.90
</p>
<p>0 2 4 6 8 10
</p>
<p>0
.0
</p>
<p>0
.4
</p>
<p>0
.8
</p>
<p>a1 = &minus;0.5, b1 = 0.90
</p>
<p>0 2 4 6 8 10
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>a1 = &minus;0.50, b1 = &minus;0.90
</p>
<p>Fig. 3.5 Autocorrelograms for ARMA(1,1) processes
</p>
<p>Case 1: Obviously, for a C b &gt; 0 it holds that �.1/ &gt; 0. If, furthermore, a &gt; 0
then the entire autocorrelogram proceeds above the zero line.
</p>
<p>Case 2: For a C b &lt; 0 and a &gt; 0 one obtains exclusively negative autocorrela-
tions.
</p>
<p>Case 3: An alternating pattern, starting with �.1/ &gt; 0, is obtained for a &lt; 0 and
</p>
<p>a &gt; �b.
Case 4: For a &lt; 0 and a &lt; �b the autocorrelation series is as well alternating but
</p>
<p>starts with a negative value.
</p>
<p>Note that cases 1 and 4 can be generated qualitatively by a pure AR(1) process as
</p>
<p>well. For cases 2 and 3, however, there occur patterns which cannot be produced
</p>
<p>by an AR(1) process. Therefore, the ARMA(1,1) process allows for richer dynamic
</p>
<p>modeling than the AR(1) model does. Also when comparing with the AR(2) case,
</p>
<p>we find that the ARMA(1,1) model allows for additional dynamics. �</p>
<p/>
</div>
<div class="page"><p/>
<p>68 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>3.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>3.1 Where does the first order autocorrelation of an MA(1) process (�.1/ D b
1Cb2 )
</p>
<p>have its maximum and minimum?
</p>
<p>3.2 Show for g 2 R n f1g (geometric series):
</p>
<p>nX
</p>
<p>iD0
gi D 1 � g
</p>
<p>nC1
</p>
<p>1 � g :
</p>
<p>Conclusion: For jgj &lt; 1 and n ! 1 it holds that:
</p>
<p>1X
</p>
<p>iD0
gi D 1
</p>
<p>1 � g :
</p>
<p>3.3 Prove part (b) from Proposition 3.2.
</p>
<p>3.4 Derive the series expansion
</p>
<p>.1 � aL/�1 D
1X
</p>
<p>jD0
a jL j
</p>
<p>for real a with jaj &lt; 1.
</p>
<p>3.5 Prove Proposition 3.3 (b).
</p>
<p>3.6 Prove the necessary condition of causal invertibility from Proposition 3.4, that
</p>
<p>is:
</p>
<p>fA.z/ D 0) jzj &gt; 1g H) fA.1/ &gt; 0g ;
</p>
<p>where A.z/ D 1 � a1 z � : : : � ap z p.
</p>
<p>3.7 Let f"tg � WN.0; �2/ be a Gaussian process. Show that fxtg with xt D a1 xt�1C
"t is a Markov process.
</p>
<p>3.8 Let f"tg � WN.0; �2/. Show that fxtg with xt D a2 xt�2 C "t is not a Markov
process (a2 &curren; 0).</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Problems and Solutions 69
</p>
<p>Solutions
</p>
<p>3.1 The traditional way to solve the problem is curve sketching. We consider the
</p>
<p>first order autocorrelation to be a function of b:
</p>
<p>f .b/ D �.1/ D b
1C b2 :
</p>
<p>Then the quotient rule for the first order derivative yields
</p>
<p>f 0.b/ D 1C b
2 � 2b2
</p>
<p>.1C b2/2 D
.1 � b/.1C b/
.1C b2/2 :
</p>
<p>The roots of the derivative are given by jbj D 1. In b D �1 there is a change
of sign of f 0.b/, namely from a negative to a positive slope. Hence, in b D �1
there is a relative (and also an absolute) minimum. Because of f .b/ being an odd
</p>
<p>function (symmetric about the origin), there is a maximum in b D 1. Therefore, the
maximum possible correlation in absolute value is
</p>
<p>j f .�1/j D f .1/ D 1
2
:
</p>
<p>One may also tackle the problem by more elementary means. Note that for b &curren; 0
</p>
<p>1
</p>
<p>jbj � 2C jbj D
 
</p>
<p>1p
jbj
</p>
<p>�
p
jbj
!2
</p>
<p>� 0 ;
</p>
<p>which is equivalent to
</p>
<p>1
</p>
<p>2
� 1
</p>
<p>1
jbj C jbj
</p>
<p>D jbj
1C b2 D jf .b/j ;
</p>
<p>with f .b/ D �.1/ defined above. Since j f .�1/j D f .1/ D 1
2
, this solves the problem.
</p>
<p>3.2 By Sn we denote the following sum for finite n:
</p>
<p>Sn D
nX
</p>
<p>iD0
gi D 1C g C : : :C gn�1 C gn:
</p>
<p>Multiplication by g yields
</p>
<p>g Sn D g C g2 C : : :C gn C gnC1:</p>
<p/>
</div>
<div class="page"><p/>
<p>70 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>Therefore, it holds that
</p>
<p>Sn � g Sn D 1 � gnC1:
</p>
<p>By ordinary factorization the formula
</p>
<p>Sn D
1 � gnC1
1 � g
</p>
<p>and therefore the claim is verified.
</p>
<p>3.3 The absolute summability of &#13;.h/ follows from the absolute summability of the
</p>
<p>linear coefficients fcjg allowing for a change of the order of summation. In order to
do so, we first apply the triangle inequality:
</p>
<p>1
</p>
<p>�2
</p>
<p>1X
</p>
<p>hD0
j&#13;.h/j D
</p>
<p>1X
</p>
<p>hD0
</p>
<p>ˇ̌
ˇ̌
ˇ̌
1X
</p>
<p>jD0
cj cjCh
</p>
<p>ˇ̌
ˇ̌
ˇ̌
</p>
<p>�
1X
</p>
<p>hD0
</p>
<p>1X
</p>
<p>jD0
</p>
<p>ˇ̌
cj cjCh
</p>
<p>ˇ̌
D
</p>
<p>1X
</p>
<p>hD0
</p>
<p>1X
</p>
<p>jD0
</p>
<p>ˇ̌
cj
ˇ̌ ˇ̌
</p>
<p>cjCh
ˇ̌
</p>
<p>D
1X
</p>
<p>jD0
</p>
<p>ˇ̌
cj
ˇ̌
 1X
</p>
<p>hD0
</p>
<p>ˇ̌
cjCh
</p>
<p>ˇ̌
!
;
</p>
<p>where at the end round brackets were placed for reasons of clarity. The final term is
</p>
<p>further bounded by enlarging the expression in brackets:
</p>
<p>1X
</p>
<p>jD0
</p>
<p>ˇ̌
cj
ˇ̌
 1X
</p>
<p>hD0
</p>
<p>ˇ̌
cjCh
</p>
<p>ˇ̌
!
�
</p>
<p>1X
</p>
<p>jD0
</p>
<p>ˇ̌
cj
ˇ̌
 1X
</p>
<p>hD0
jchj
</p>
<p>!
:
</p>
<p>Therefore, the claim follows indeed from the absolute summability of fcjg.
3.4 For the proof we denote .1 � aL/�1 as
</p>
<p>P1
jD0 ˛jL
</p>
<p>j,
</p>
<p>1
</p>
<p>1 � aL D
1X
</p>
<p>jD0
˛jL
</p>
<p>j ;</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Problems and Solutions 71
</p>
<p>and determine the coefficients ˛j. By multiplying this equation with 1 � aL, we
obtain
</p>
<p>1 D .1 � aL/
1X
</p>
<p>jD0
˛jL
</p>
<p>j
</p>
<p>D ˛0 C ˛1L1 C ˛2L2 C : : :
�a ˛0L1 � a ˛1L2 � a ˛2L3 � : : : :
</p>
<p>Now, we compare the coefficients associated with L j on the left- and on the right-
</p>
<p>hand side:
</p>
<p>1 D ˛0 ;
0 D ˛1 � a ˛0 ;
0 D ˛2 � a ˛1 ;
:::
</p>
<p>0 D ˛j � a ˛j�1 ; j � 1 :
</p>
<p>As claimed, the solution of the difference equation obtained in this way, (˛j D
a ˛j�1), is obviously ˛j D a j.
3.5 We factorize P.z/ D 1Cb1zC : : :Cbpz p with roots z1; : : : ; zp of this polynomial
(fundamental theorem of algebra):
</p>
<p>P.z/ D bp .z � z1/ : : : .z � zp/ :
</p>
<p>From each bracket we factorize �zj out such that
</p>
<p>P.z/ D bp .�1/p z1 : : : zp
�
1 � z
</p>
<p>z1
</p>
<p>�
: : :
</p>
<p>�
1 � z
</p>
<p>zp
</p>
<p>�
:
</p>
<p>Because of P.0/ D 1, we obtain bp .�1/p z1 : : : zp D 1. Therefore the factorization
simplifies to
</p>
<p>P.z/ D
�
1 � z
</p>
<p>z1
</p>
<p>�
: : :
</p>
<p>�
1 � z
</p>
<p>zp
</p>
<p>�
</p>
<p>D P1.z/ � � � Pp.z/ ;
</p>
<p>with
</p>
<p>Pk.z/ D 1 �
z
</p>
<p>zk
D 1 � �kz ; k D 1; : : : ; p ;</p>
<p/>
</div>
<div class="page"><p/>
<p>72 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>where �k D 1=zk. From part a) we know that
</p>
<p>1
</p>
<p>Pk.L/
D
</p>
<p>1X
</p>
<p>jD0
�
</p>
<p>j
</p>
<p>kL
j with
</p>
<p>1X
</p>
<p>jD0
j� jkj &lt;1
</p>
<p>if and only if
</p>
<p>jzkj D
1
</p>
<p>j�kj
&gt; 1 :
</p>
<p>Now, consider the convolution (sometimes called Cauchy product) for k &curren; `:
</p>
<p>1
</p>
<p>Pk.L/
</p>
<p>1
</p>
<p>P`.L/
D
</p>
<p>1X
</p>
<p>jD0
cjL
</p>
<p>j
</p>
<p>with
</p>
<p>cj WD
jX
</p>
<p>iD0
� ik�
</p>
<p>j�i
` :
</p>
<p>We have
P1
</p>
<p>jD0 jcjj &lt; 1 if and only if both P�1k .L/ and P�1` .L/ are absolutely
summable, which holds true if and only if
</p>
<p>jzkj &gt; 1 and jz`j &gt; 1 :
</p>
<p>Repeating this argument we obtain that
</p>
<p>1
</p>
<p>P.L/
D 1
</p>
<p>P1.L/
� � � 1
</p>
<p>Pp.L/
D
</p>
<p>1X
</p>
<p>jD0
cjL
</p>
<p>j with
</p>
<p>1X
</p>
<p>jD0
jcjj &lt;1
</p>
<p>if and only if (3.10) holds. Quod erat demonstrandum.
</p>
<p>3.6 At first we reformulate the autoregressive polynomial A.z/ D 1�a1z� : : :�apz p
in its factorized form with roots z1; : : : ; zp (again by the fundamental theorem of
</p>
<p>algebra):
</p>
<p>A.z/ D �ap.z � z1/ : : : .z � zp/ :
</p>
<p>For z D 1 this amounts to
</p>
<p>A.1/ D �ap.1 � z1/ : : : .1 � zp/ : (3.15)</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Problems and Solutions 73
</p>
<p>Because of A.0/ D 1 we obtain as well:
</p>
<p>1 D �ap.�1/pz1 : : : zp : (3.16)
</p>
<p>Now we proceed in two steps, treating the cases of complex and real roots separately.
</p>
<p>(A) Complex roots: Note that for a root z1 2 C it holds that the complex conjugate,
z2 D z1; is a root as well. Then calculating with complex numbers yields for
the product
</p>
<p>.1 � z1/.1 � z2/ D .1 � z1/.1� z1/
D .1 � z1/.1� z1/
D j1� z1j2 &gt; 0 :
</p>
<p>Hence, for p &gt; 2, complex roots contribute positively to A.1/ in (3.15). If
</p>
<p>p D 2, the roots are only complex if a2 &lt; 0, since the discriminant is a21C 4a2;
hence, A.1/ &gt; 0 by (3.15).
</p>
<p>(B) Since the effect of complex roots is positive, we now concentrate on real roots
</p>
<p>zi, for which it holds that jzij &gt; 1 by assumption. So, we assume without loss
of generality that the polynomial has no complex roots, or that all complex
</p>
<p>roots have been factored out. Two sub-cases have to be distinguished. (1) Even
</p>
<p>degree: For an even p we again distinguish between two cases. Case 1, ap &gt; 0:
</p>
<p>Because of (3.16) there has to be an odd number of negative roots and therefore
</p>
<p>there has to be an odd number of positive roots as well. For the latter it holds
</p>
<p>that .1� zi/ &lt; 0 while the first naturally fulfill .1� zi/ &gt; 0. Hence, as claimed,
it follows from (3.15) that A.1/ is positive. Case 2, ap &lt; 0: In this case one
</p>
<p>argues quite analogously. Because of (3.16) there is an even number of positive
</p>
<p>and negative roots such that the requested claim follows from (3.15) as well. (2)
</p>
<p>Odd degree: For an odd p one obtains the requested result as well by distinction
</p>
<p>of the two cases for the sign of ap. We omit details.
</p>
<p>Hence, the proof is complete.
</p>
<p>3.7 The normality of f"tg implies a multivariate Gaussian distribution of
0
B@
"tC1
:::
</p>
<p>"tCs
</p>
<p>1
CA � iiNs
</p>
<p>0
B@
</p>
<p>0
B@
0
:::
</p>
<p>0
</p>
<p>1
CA ; �2 Is
</p>
<p>1
CA
</p>
<p>with the identity matrix Is of dimension s. The s-fold substitution yields
</p>
<p>xtCs D as1xt C as�11 "tC1 C : : :C a1"tCs�1 C "tCs
</p>
<p>D as1xt C
s�1X
</p>
<p>iD0
ai1"tCs�i:</p>
<p/>
</div>
<div class="page"><p/>
<p>74 3 Autoregressive Moving Average Processes (ARMA)
</p>
<p>The sum over the white noise process has the moments
</p>
<p>E
</p>
<p> 
s�1X
</p>
<p>iD0
ai1"tCs�i
</p>
<p>!
D 0; Var
</p>
<p> 
s�1X
</p>
<p>iD0
ai1"tCs�i
</p>
<p>!
D �2
</p>
<p>s�1X
</p>
<p>iD0
a2i1 ;
</p>
<p>and, furthermore, it is normally distributed:
</p>
<p>s�1X
</p>
<p>iD0
ai1"tCs�i � N
</p>
<p> 
0; �2
</p>
<p>s�1X
</p>
<p>iD0
a2i1
</p>
<p>!
:
</p>
<p>Hence, xtCs given xt follows a Gaussian distribution with the corresponding
moments:
</p>
<p>xtCsj xt � N
 
</p>
<p>as1xt; �
2
</p>
<p>s�1X
</p>
<p>iD0
a2i1
</p>
<p>!
:
</p>
<p>As xtCs can be expressed as a function of xt and "tC1; : : : ; "tCs alone, the further past
of the process does not matter for the conditional distribution of xtCs. Therefore, for
the entire information It up to time t it holds that:
</p>
<p>xtCsj It � N
 
</p>
<p>as1xt; �
2
</p>
<p>s�1X
</p>
<p>iD0
a2i1
</p>
<p>!
:
</p>
<p>Hence, the Markov property (2.9) has been shown. It holds independently of the
</p>
<p>concrete value of a1.
</p>
<p>3.8 For
</p>
<p>xt D a2xt�2 C "t;
</p>
<p>we obtain for s D 1 the conditional expectations E.xtC1j It/ D a2 xt�1; and
</p>
<p>E.xtC1j xt/ D E.a2xt�1 C "tC1j xt/ D a2 E.xt�1j xt/ ;
</p>
<p>with It D �.xt; xt�1; : : : ; x1/: As the conditional expectations are not equivalent, the
conditional distributions are not the same. Hence, it generally holds that
</p>
<p>P.xtC1 � xj xt/ &curren; P.xtC1 � xj It/ ;
</p>
<p>which proves that fxtg is not a Markov process.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 75
</p>
<p>References
</p>
<p>Andrews, D. W. K., &amp; Chen, H.-Y. (1994). Approximately median-unbiased estimation of
autoregressive models. Journal of Business &amp; Economic Statistics, 12, 187&ndash;204.
</p>
<p>Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
Springer.
</p>
<p>Campbell, J. Y., &amp; Mankiw, N. G. (1987). Are output fluctuations transitory? Quarterly Journal of
Economics, 102, 857&ndash;880.
</p>
<p>Fuller, W. A. (1996). Introduction to statistical time series (2nd ed.). New York: Wiley.
Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
</p>
<p>Berlin/New York: Springer.
Wold, H. O. A. (1938). A study in the analysis of stationary time series. Stockholm: Almquist &amp;
</p>
<p>Wiksell.</p>
<p/>
</div>
<div class="page"><p/>
<p>4Spectra of Stationary Processes
</p>
<p>4.1 Summary
</p>
<p>Spectral analysis (or analysis in the frequency domain) aims at detecting cycli-
</p>
<p>cal movements in a time series. These may originate from seasonality, a trend
</p>
<p>component or from a business cycle. The theoretical spectrum of a stationary
</p>
<p>process is the quantity measuring how strongly cycles with a certain period, or
</p>
<p>frequency, account for total variance. Typically, elaborations on spectral analysis
</p>
<p>are formally demanding requiring e.g. knowledge of complex numbers and Fourier
</p>
<p>transformations. In this textbook we have tried for a way of presenting and deriving
</p>
<p>the relevant results being less elegant but in return managing with less mathematical
</p>
<p>burden. The next section provides the definitions and intuition behind spectral
</p>
<p>analysis. Section 4.3 is analytically more demanding containing some general
</p>
<p>theory. This theory is exemplified with the discussion of spectra from particular
</p>
<p>ARMA processes, hence building on the previous chapter.
</p>
<p>4.2 Definition and Interpretation
</p>
<p>In this chapter we assume the most general case considered previously, i.e. the
</p>
<p>infinite MA process that is only square summable, fxtgt2T, T � Z,
</p>
<p>xt D �C
1X
</p>
<p>jD0
cj "t�j ;
</p>
<p>1X
</p>
<p>jD0
c2j &lt;1 ; c0 D 1 ; (4.1)
</p>
<p>with f"tg � WN.0; �2/. The autocovariances,
</p>
<p>&#13;.h/ D Cov.xt; xtCh/ D &#13;.�h/ ; h 2 Z ;
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_4
</p>
<p>77</p>
<p/>
</div>
<div class="page"><p/>
<p>78 4 Spectra of Stationary Processes
</p>
<p>are given in Proposition 3.2 (a). We do not assume that fcjg and hence f&#13;.h/g are
absolutely summable,1 simply because this will not hold under long memory treated
</p>
<p>in the next chapter. We wish to construct a function f that allows to express the
</p>
<p>autocovariances as weighted cosine waves of different periodicity,2
</p>
<p>&#13;.h/ D
Z �
</p>
<p>��
cos.�h/ f .�/ d� :
</p>
<p>The basic ingredient of an analysis of periodicity is the cosine cycle whose
</p>
<p>properties we want to recall as an introduction.
</p>
<p>Periodic Cycles
</p>
<p>By c�.t/ we denote the cycle based on the cosine,
3
</p>
<p>c�.t/ D cos .�t/ ; t 2 R;
</p>
<p>where � with � &gt; 0 is called frequency. The frequency is inversely related to the
</p>
<p>period P,
</p>
<p>P D 2�
�
:
</p>
<p>For � D 1 one obtains the cosine function which is 2��periodic and even
(symmetric about the ordinate):
</p>
<p>c1 .t/ D cos .t/ D cos .t C 2�/ D c1 .t C 2�/ ;
</p>
<p>c1 .�t/ D cos .�t/ D cos.t/ D c1.t/:
</p>
<p>More generally, it holds with P D 2�=� that:
</p>
<p>c�.t/ D cos .�t/ D cos .�t C 2�/ D cos .�.t C P// D c� .t C P/ :
</p>
<p>Therefore the cosine cycle c� .t/ with frequency � has the period P D 2�=�: Of
course, the symmetry of c1.t/ carries over:
</p>
<p>c� .t/ D c� .�t/ :
</p>
<p>1The assumption of absolute summability underlies most textbooks when it comes to spectral
analysis, see e.g. Hamilton (1994) or Fuller (1996).
2From Brockwell and Davis (1991, Coro. 4.3.1) in connection with Brockwell and Davis (Thm.
5.7.2) one knows that such an expression exists.
3Here, the so-called amplitude is equal to one (jc�.t/j� 1/, and the phase shift is zero .c� .0/ D 1/.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Definition and Interpretation 79
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>&minus;
1
.0
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>cosine wave (lambda = 1 and lambda = 2)
</p>
<p>[&minus;2pi, 2pi]
</p>
<p>&minus;6 &minus;4 &minus;2 0 2 4 6
</p>
<p>&minus;
1
.0
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>cosine wave (lambda = 1 and lambda = 0.5)
</p>
<p>[&minus;2pi, 2pi]
</p>
<p>Fig. 4.1 Cosine cycle with different frequencies
</p>
<p>For � D 1; � D 2 and � D 0:5 these properties are graphically illustrated in Fig. 4.1.
Finally, remember the derivative of the cosine,
</p>
<p>d c�.t/
</p>
<p>d t
D c0�.t/ D �� sin.�t/ ;
</p>
<p>which we will use repeatedly.
</p>
<p>Definition
</p>
<p>For convenience, we now rephrase the MA(1) process in terms of the lag
polynomial C.L/ of infinite order,
</p>
<p>xt D �C C.L/ "t with C.L/ D
1X
</p>
<p>jD0
cjL
</p>
<p>j :</p>
<p/>
</div>
<div class="page"><p/>
<p>80 4 Spectra of Stationary Processes
</p>
<p>Next, we define the so-called power transfer function TC .�/ of this polynomial:
4
</p>
<p>TC .�/ D
1X
</p>
<p>jD0
c2j C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>1X
</p>
<p>jD0
cjcjCh cos .�h/ ; � 2 R n f��g : (4.2)
</p>
<p>Note that TC may not exist everywhere, there may be singularities at some frequency
</p>
<p>�� such that TC .�/ goes off to infinity as � ! ��; but at least the power transfer
function is integrable. The key result in Proposition 4.1 (e) is from from Brockwell
</p>
<p>and Davis (1991, Coro. 4.3.1, Thm. 5.7.2); it will be proved explicitly in Problem 4.1
</p>
<p>under the simplifying assumption of absolute summability. The first four statements
</p>
<p>in the following proposition are rather straightforward and will be justified below.
</p>
<p>Proposition 4.1 (Spectrum) Define for fxtg from (4.1) the spectrum
</p>
<p>f .�/ D TC.�/
�2
</p>
<p>2�
:
</p>
<p>It has the following properties:
</p>
<p>(a) f .��/ D f .�/,
(b) f .�/ D f .�C 2�/,
(c) f .�/ � 0,
(d) f .�/ is continuous in � under absolute summability,
</p>
<p>P
j jcjj &lt;1.
</p>
<p>(e) For all h 2 Z:
</p>
<p>&#13;.h/ D
Z �
</p>
<p>��
f .�/ cos.�h/ d� D 2
</p>
<p>Z �
</p>
<p>0
</p>
<p>f .�/ cos.�h/ d� :
</p>
<p>Substituting the autocovariance expression from Proposition 3.2 into (4.2), the
</p>
<p>following representation of the spectrum exists:
</p>
<p>f .�/ D &#13;.0/
2�
</p>
<p>C 2
2�
</p>
<p>1X
</p>
<p>hD1
&#13;.h/ cos.�h/ D 1
</p>
<p>2�
</p>
<p>1X
</p>
<p>hD�1
&#13;.h/ cos.�h/ : (4.3)
</p>
<p>The symmetry of the spectrum in Proposition 4.1 (a) immediately follows from the
</p>
<p>symmetry of the cosine function. From the periodicity of the cosine, (b) follows as
</p>
<p>well. Both results jointly explain why the spectrum is normally considered on the
</p>
<p>restricted domain Œ0; �&#141; only. Property (c) follows from the definition of the power
</p>
<p>transfer function, see Footnote 6 below. Finally, the continuity of f .�/ claimed in
</p>
<p>4A more detailed and technical exposition is reserved for the next section. Our expression in (4.2)
can be derived from the expression in Brockwell and Davis (1991, eq. 5.7.9), which is given in
terms of complex numbers.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Definition and Interpretation 81
</p>
<p>(d) under absolute summability results from uniform convergence, see Fuller (1996,
</p>
<p>Thm. 3.1.9).
</p>
<p>We call the function f (or fx, if we want to emphasize that fxtg is the underlying
process) the spectrum of fxtg. Frequently, one also talks about spectral density or
spectral density function as f is a non-negative function which could be standardized
</p>
<p>in such a way that the area beneath it would be equal to one.
</p>
<p>Interpretation
</p>
<p>The usual interpretation of the spectrum is based on Proposition 4.1. Result (e)
</p>
<p>and (4.3) jointly show the spectrum and the autocovariance series to result from
</p>
<p>each other. In a sense, spectrum and autocovariances are two sides of the same coin.
</p>
<p>The spectrum can be determined from the autocovariances by definition and having
</p>
<p>the spectrum, Proposition 4.1 provides the autocovariances. The case h D 0 with
</p>
<p>Var.xt/ D &#13;.0/ D
Z �
</p>
<p>��
f .�/ d� D 2
</p>
<p>Z �
</p>
<p>0
</p>
<p>f .�/ d�
</p>
<p>is particularly interesting. This equation implies: The spectrum at �0 measures how
</p>
<p>strongly the cycle with frequency �0 and therefore of period P0 D 2�=�0 adds to
the variance of the process. If f has a maximum at �0, then the dynamics of fxtg
is dominated by the corresponding cycle or period; inversely, if the spectrum has a
</p>
<p>minimum at �0, then the corresponding cycle is of less relevance for the behavior
</p>
<p>of fxtg than all other cycles. For �! 0, period P converges to infinity. A cycle with
an infinitely long period is interpreted as a trend or a long-run component. Hence,
</p>
<p>f .0/ indicates how strongly the process is dominated by a trend component.
</p>
<p>Frequently, the analysis of the autocovariance structure or the autocorrelation
</p>
<p>structure of a process is called &ldquo;analysis in the time domain&rdquo; as &#13;.h/ measures the
</p>
<p>direct temporary dependence between xt and xtCh. Correspondingly, the spectral
analysis is often referred to as &ldquo;analysis in the frequency domain&rdquo;. Proposition 4.1
</p>
<p>and the definition in (4.3) show how to move back and forth between time and
</p>
<p>frequency domain.
</p>
<p>Examples
</p>
<p>Example 4.1 (White Noise) Let us consider the white noise process xt D "t being
free from serial correlation. By definition it immediately follows that the spectrum
</p>
<p>is constant:
</p>
<p>f".�/ D �2=2� ; � 2 Œ0; �&#141; :
</p>
<p>According to Proposition 4.1 all frequencies account equally strongly for the
</p>
<p>variance of the process. Analogously to the perspective in optics that the &ldquo;color&rdquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>82 4 Spectra of Stationary Processes
</p>
<p>white results if all frequencies are present equally strongly, serially uncorrelated
</p>
<p>processes are also often called &ldquo;white noise&rdquo;. �
</p>
<p>Example 4.2 (Season) Let us consider the ordinary seasonal MA process from
</p>
<p>Example 3.1,
</p>
<p>xt D "t C b"t�S
</p>
<p>with
</p>
<p>&#13; .0/ D �2
�
1C b2
</p>
<p>�
; &#13; .S/ D �2b
</p>
<p>and &#13; .h/ D 0 else. By definition we obtain for the spectrum from (4.3)
</p>
<p>2� f .�/ D &#13; .0/C 2&#13; .S/ cos .�S/
</p>
<p>or
</p>
<p>f .�/ D
�
1C b2 C 2b cos .�S/
</p>
<p>�
�2=2�:
</p>
<p>In Problem 4.2 we determine that there are extrema at
</p>
<p>0;
�
</p>
<p>S
;
2�
</p>
<p>S
; : : : ;
</p>
<p>.S � 1/�
S
</p>
<p>; � :
</p>
<p>The corresponding values are
</p>
<p>f .0/ D f
�
2�
</p>
<p>S
</p>
<p>�
D : : : D .1C b/2 �2=2� ;
</p>
<p>f
��
</p>
<p>S
</p>
<p>�
D f
</p>
<p>�
3�
</p>
<p>S
</p>
<p>�
D : : : D .1 � b/2 �2=2�:
</p>
<p>Depending on the sign of b, maxima and minima are followed by each other,
</p>
<p>respectively. In Fig. 4.2 we find two typical shapes of the spectrum of the seasonal
</p>
<p>MA process for5 S D 4 (quarterly data) with b D 0:7 and b D �0:5. First, let
us interpret the case b &gt; 0. There are maxima at the frequencies 0; �=2 and � .
</p>
<p>Corresponding cycles are of the period
</p>
<p>P0 D
2�
</p>
<p>0
D 1; P1 D
</p>
<p>2�
</p>
<p>�=2
D 4; P2 D
</p>
<p>2�
</p>
<p>�
D 2:
</p>
<p>5The variance of the white noise is set to one , �2 D 1. This is also true for all spectra of this
chapter depicted in the following.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Definition and Interpretation 83
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>2
.0
</p>
<p>3
.0
</p>
<p>MA(4) with b=0.7 and b=&minus;0.5
</p>
<p>[0, pi]
</p>
<p>b&gt;0
b&lt;0
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>2
.0
</p>
<p>3
.0
</p>
<p>MA(1) with b=0.7 and b=&minus;0.5
</p>
<p>[0, pi]
</p>
<p>b&gt;0
b&lt;0
</p>
<p>Fig. 4.2 Spectra (2� f .�/) of the MA(S) process from Example 4.2
</p>
<p>The trend is the first infinitely long &ldquo;period&rdquo;. The second cycle has the period P1 D
4, i.e. four quarters which is why this is the annual cycle. The third cycle with
</p>
<p>P2 D 2 is the semi-annual cycle with only two quarters. These three cycles dominate
the process for b &gt; 0. Inversely, for b &lt; 0 it holds that these very cycles add
</p>
<p>particularly little to the variance of the process. �
</p>
<p>Example 4.3 (MA(1)) Specifically for S D 1 the seasonal MA process passes into
the MA(1) process. Accordingly, one obtains two extrema at zero and �:
</p>
<p>f .0/ D .1C b/2 �2=2� ; f .�/ D .1 � b/2 �2=2�:
</p>
<p>In between the spectrum reads
</p>
<p>f .�/ D
�
1C b2 C 2b cos .�/
</p>
<p>�
�2=2�:
</p>
<p>For b D 0:7 and b D �0:5, respectively, the spectra were calculated, see Fig. 4.2.
For b &lt; 0 one spots the relative absence of a trend (frequency zero matters least)
</p>
<p>while for b &gt; 0 precisely the long-run component as a trend dominates the process.
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>84 4 Spectra of Stationary Processes
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>8
</p>
<p>maximum at frequency lambda = 0.73
</p>
<p>[0, pi]
</p>
<p>A
R
</p>
<p>(2
) 
</p>
<p>s
p
</p>
<p>e
c
tr
</p>
<p>u
m
</p>
<p>Fig. 4.3 Spectrum (2� f .�/) of business cycle with a period of 8.6 years
</p>
<p>Example 4.4 (Business Cycle) The spectrum is not only used for modelling sea-
</p>
<p>sonal patterns but as well for determining the length of a typical business cycle.
</p>
<p>Let us assume a process with annual observations having the spectrum depicted in
</p>
<p>Fig. 4.3. The maximum is at � D 0:73. How do we interpret this fact with regard
to contents? The dominating frequency � D 0:73 corresponds to a period of about
8.6 (years). A frequency of this magnitude is often called &ldquo;business cycle frequency&rdquo;
</p>
<p>being interpreted as the frequency which corresponds to the business cycle. In fact,
</p>
<p>Fig. 4.3 does not comprise an empirical spectrum. Rather, one detects the theoretical
</p>
<p>spectrum of the AR(2) model whose autocorrelogram is depicted in Fig. 3.4 down
</p>
<p>to the right. The cycle, which can be seen in the autocorrelogram there, translates
</p>
<p>into the spectral maximum from Fig. 4.3. �
</p>
<p>4.3 Filtered Processes
</p>
<p>The ARMA process or more generally the infinite MA process have been defined as
</p>
<p>filtered white noise. In order to systematically derive a formal expression for their
</p>
<p>spectra, we start quite generally with the relation between input and output of a filter
</p>
<p>in the frequency domain.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Filtered Processes 85
</p>
<p>Filtered Processes
</p>
<p>As in the previous chapter, we consider the causal, time-invariant, linear filter F.L/,
</p>
<p>F .L/ D
pX
</p>
<p>jD0
wjL
</p>
<p>j;
</p>
<p>where L again denotes the lag operator and p D 1 is allowed for. The filter is
assumed to be absolutely summable, which trivially holds true for finite order p. Let
</p>
<p>the process fxtg be generated by filtering of the stationary process fetg,
</p>
<p>xt D F .L/ et:
</p>
<p>Then, how does the corresponding spectrum of fxtg for a given spectrum fe of fetg
read? The answer is based on the power transfer function TF .�/ that we briefly
</p>
<p>touched upon in the previous section6:
</p>
<p>TF .�/ D
1X
</p>
<p>jD0
w2j C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>1X
</p>
<p>jD0
wjwjCh cos .�h/ : (4.4)
</p>
<p>At a first glance, this expression appears cumbersome. However, in the next section
</p>
<p>we will see that for concrete ARMA processes it simplifies radically. If F .L/ is a
</p>
<p>finite filter (i.e. with finite p), then the sums of TF .�/ are truncated accordingly,
</p>
<p>see (4.8) in the next section. With TF .�/ the following proposition considerably
</p>
<p>simplifies the calculation of theoretical spectra (for a proof of an even more general
</p>
<p>result see Brockwell and Davis (1991, Thm. 4.4.1), while Fuller (1996, Thm. 4.3.1)
</p>
<p>covers our case where fetg has absolutely summable autocovariances).
</p>
<p>6 The mathematically experienced reader will find the expression in (4.4) to be unnecessarily
</p>
<p>complicated as the transformation TF .�/ can be written considerably more compactly by using
the exponential function in the complex space. It holds that
</p>
<p>TF .�/ D
ˇ̌
F.e�i�/
</p>
<p>ˇ̌2 D F.ei�/F.e�i�/;
</p>
<p>where Euler&rsquo;s formula allows for expressing the complex-valued exponential function by sine and
cosine,
</p>
<p>ei� D cos �C i sin�; i2 D �1;
</p>
<p>with the conjugate complex number e�i� D cos � � i sin� ; where i denotes the imaginary unit.
Instead of burdening the reader with complex numbers and functions, we rather expect him or her to
handle the more cumbersome definition from (4.4). By the way, the term &ldquo;power transfer function&rdquo;
</p>
<p>stems from calling F.e�i�/ alone transfer function of the filter F.L/, and TF.�/ D
ˇ̌
F.e�i�/
</p>
<p>ˇ̌2
being the power thereof.</p>
<p/>
</div>
<div class="page"><p/>
<p>86 4 Spectra of Stationary Processes
</p>
<p>Proposition 4.2 (Spectra of Filtered Processes) Let fetg be a stationary process
with spectrum fe .�/. The filter
</p>
<p>F .L/ D
1X
</p>
<p>jD0
wjL
</p>
<p>j
</p>
<p>be absolutely summable,
1P
</p>
<p>jD0
j wj j&lt;1, and fxtg be
</p>
<p>xt D F .L/ et:
</p>
<p>Then, fxtg is stationary with spectrum
</p>
<p>fx .�/ D TF .�/ fe .�/ ; � 2 Œ0; �&#141; ;
</p>
<p>where TF .�/ is defined in (4.4).
</p>
<p>Example 4.5 (Infinite MA) Let et D "t from Proposition 4.2 be white noise with
</p>
<p>f" .�/ D �2=2� ;
</p>
<p>and consider an absolutely summable MA(1) process,
</p>
<p>xt D C .L/ "t D
1X
</p>
<p>jD0
cj"t�j :
</p>
<p>Then Proposition 4.2 kicks in:
</p>
<p>fx .�/ D
</p>
<p>0
@
</p>
<p>1X
</p>
<p>jD0
c2j C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>1X
</p>
<p>jD0
cjcjCh cos .�h/
</p>
<p>1
A �
</p>
<p>2
</p>
<p>2�
; � 2 Œ0; �&#141; : (4.5)
</p>
<p>This special case of Proposition 4.2 will be verified in Problem 4.3. Note that the
</p>
<p>spectrum given in (4.5) equals of course the result from Proposition 4.1 with (4.2),
</p>
<p>which continues to hold without absolute summability. �
</p>
<p>Persistence
</p>
<p>We now return more systematically to the issue of persistence that we have touched
</p>
<p>upon in the example of the AR(1) process in the previous chapter. Loosely speaking,
</p>
<p>we understand by persistence the degree of (positive) autocorrelation such that
</p>
<p>subsequent observations form clusters: positive observations tend to be followed</p>
<p/>
</div>
<div class="page"><p/>
<p>4.3 Filtered Processes 87
</p>
<p>by positive ones, while negative observations tend to induce negative ones. With
</p>
<p>persistence we try to capture the strength of such a tendency, which depends not
</p>
<p>only on the autocorrelation coefficient at lag one but also on higher order lags. In
</p>
<p>the previous chapter we mentioned that it has been suggested to measure persistence
</p>
<p>by means of the cumulated impulse responses CIR defined in (3.3). This quantity
</p>
<p>shows up in the spectrum at frequency zero by Proposition 4.2. Assume that fxtg is
an MA(1) process with absolutely summable impulse response sequence fcjg. We
then have:
</p>
<p>fx.0/ D TC.0/
�2
</p>
<p>2�
D
</p>
<p>0
@
</p>
<p>1X
</p>
<p>jD0
c2j C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>1X
</p>
<p>jD0
cjcjCh
</p>
<p>1
A �
</p>
<p>2
</p>
<p>2�
D
</p>
<p>0
@
</p>
<p>1X
</p>
<p>jD0
cj
</p>
<p>1
A
2
</p>
<p>�2
</p>
<p>2�
;
</p>
<p>or
</p>
<p>fx.0/ D .CIR/2
�2
</p>
<p>2�
:
</p>
<p>Hence, the larger CIR, the stronger is the contribution of the trend component at
</p>
<p>frequency zero to the variance of the process, which formalizes our concept of
</p>
<p>persistence. Cogley and Sargent (2005) applied as relative spectral measure for
</p>
<p>persistence the ratio of 2�fx.0/=&#13;x.0/ pioneered previously by Cochrane (1988);
</p>
<p>it can be interpreted as a variance ratio and is hence abbreviated as VR:
</p>
<p>VR WD 2�fx.0/
&#13;x.0/
</p>
<p>D
</p>
<p>�P1
jD0 cj
</p>
<p>�2
</p>
<p>P1
jD0 c
</p>
<p>2
j
</p>
<p>: (4.6)
</p>
<p>In the case of a stationary AR(1) process, xt D a1 xt�1 C "t, it holds that (see
Problem 4.6)
</p>
<p>VR D 1 � a
2
1
</p>
<p>.1 � a1/2
D 1C a1
1 � a1
</p>
<p>8
&lt;
:
</p>
<p>&gt; 1 if a1 &gt; 0
</p>
<p>D 1 if a1 D 0
&lt; 1 if a1 &lt; 0
</p>
<p>: (4.7)
</p>
<p>In the case of a1 D 0 (white noise) we have no persistence, and VR D 1. For a1 &gt; 0
the process is all the more persistent the larger a1 is. Following Hassler (2014), one
</p>
<p>may say that a process has negative persistence if VR &lt; 1. The plot of a series
</p>
<p>under negative persistence will typically display a zigzag pattern as observed in the
</p>
<p>last plot in Fig. 3.2. The limiting cases of VR D 0 (also called antipersistent) and
VR D 1 (also called strongly persistent) will be dealt with in Chap. 5.</p>
<p/>
</div>
<div class="page"><p/>
<p>88 4 Spectra of Stationary Processes
</p>
<p>ARMA Spectra
</p>
<p>As a consequence of the previous proposition, we can derive what the spectrum of
</p>
<p>a stationary ARMA process fxtg looks like. Remember the definition from (3.13),
</p>
<p>A .L/ xt D B .L/ "t :
</p>
<p>Now, define
</p>
<p>yt D A .L/ xt D B .L/ "t :
</p>
<p>By Proposition 4.2 one obtains for the spectra
</p>
<p>fy.�/ D TA .�/ fx.�/ D TB .�/ �2=2� :
</p>
<p>The assumption of a stationary MA(1) representation7 implies TA .�/ &gt; 0.
Consequently, one may solve for fx rendering the following corollary.
</p>
<p>Corollary 4.1 (ARMA Spectra) Let fxtg be a stationary ARMA(p; q) process
</p>
<p>A .L/ xt D � C B .L/ "t:
</p>
<p>Its spectrum is given by
</p>
<p>fx .�/ D
TB .�/
</p>
<p>TA .�/
</p>
<p>�2
</p>
<p>2�
; � 2 Œ0; �&#141; ;
</p>
<p>where TB .�/ and TA .�/ are the power transfer functions of B .L/ and A .L/.
</p>
<p>Often, we restrict the class of stationary ARMA processes to the invertible
</p>
<p>ones, meaning we assume that the moving average polynomial B.L/ satisfies the
</p>
<p>invertibility condition of Proposition 3.3: All solutions of B.z/ D 0 are larger than
1 in absolute value. This implies as in Footnote 7 that TB.�/ &gt; 0, such that the
</p>
<p>invertible ARMA spectrum is strictly positive for all �.
</p>
<p>7 According to Proposition 3.5 we rule out autoregressive roots on the unit circle, such that
</p>
<p>A.e�i�/ &curren; 0, and
ˇ̌
A.e�i�/
</p>
<p>ˇ̌2
&gt; 0. By assumption, jzj D 1 implies A.z/ &curren; 0, and here, z D e�i�
</p>
<p>with
</p>
<p>je�i�j2 D .cos �/2 C .sin �/2 D 1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Examples of ARMA Spectra 89
</p>
<p>In the next section we will learn that the calculation of the functions TA .�/
</p>
<p>and TB .�/ and thereby the calculation of the spectra do not pose any problem, cf.
</p>
<p>Eq. (4.8).
</p>
<p>4.4 Examples of ARMA Spectra
</p>
<p>The ARMA filters A.L/ and B.L/ are assumed to be of finite order. In order to
</p>
<p>calculate the spectrum, the power transfer function is needed due to Corollary 4.1.
</p>
<p>Thus, next we will get to know a simple trick allowing for quickly calculating the
</p>
<p>power transfer function of a finite filter.
</p>
<p>Summation over the Diagonal
</p>
<p>We consider for finite p the filter F.L/ with the coefficients w0;w1; : : : ;wp being
</p>
<p>collected in a vector:
</p>
<p>w D
</p>
<p>0
BBBBB@
</p>
<p>w0
</p>
<p>w1
:::
</p>
<p>wp�1
wp
</p>
<p>1
CCCCCA
:
</p>
<p>The outer product yields a matrix where w0 stands for the transposition of the
column w:
</p>
<p>ww0 D
</p>
<p>0
BBBBB@
</p>
<p>w0
</p>
<p>w1
:::
</p>
<p>wp�1
wp
</p>
<p>1
CCCCCA
</p>
<p>�
w0;w1; : : : ;wp�1;wp
</p>
<p>�
</p>
<p>D
</p>
<p>0
BBBBBB@
</p>
<p>w20
w1w0
:::
</p>
<p>wp�1w0
wpw0
</p>
<p>w0w1
</p>
<p>w21
:::
</p>
<p>wp�1w1
wpw1
</p>
<p>� � �
� � �
� � �
� � �
� � �
</p>
<p>w0wp�1
w1wp�1
:::
</p>
<p>w2p�1
wpwp�1
</p>
<p>w0wp
</p>
<p>w1wp
:::
</p>
<p>wp�1wp
w2p
</p>
<p>1
CCCCCCA
:
</p>
<p>Obviously, the matrix is symmetric. Now, we add the cosine as function of j j � ij,
cos .� j j � i j/, to the entries wiwj. Let the resulting matrix be called MF .�/. It</p>
<p/>
</div>
<div class="page"><p/>
<p>90 4 Spectra of Stationary Processes
</p>
<p>becomes:
</p>
<p>0
BBBBBB@
</p>
<p>w20 cos.0/
</p>
<p>w0w1 cos .�/
:::
</p>
<p>w0wp�1 cos .� .p � 1//
w0wp cos .�p/
</p>
<p>w0w1 cos .�/
</p>
<p>w21 cos.0/
:::
</p>
<p>w1wp�1 cos .� .p � 2//
w1wp cos .� .p � 1//
</p>
<p>� � �
� � �
� � �
� � �
� � �
</p>
<p>w0wp cos .�p/
</p>
<p>w1wp cos .� .p � 1//
:::
</p>
<p>wp�1wp cos .�/
w2p cos.0/
</p>
<p>1
CCCCCCA
</p>
<p>The rule for calculating TF .�/ reads in words: &ldquo;Add up the sums over all diagonals
</p>
<p>of MF .�/&rdquo;:
</p>
<p>�
w20 C : : :w2p
</p>
<p>�
C 2
</p>
<p>�
w0w1 C � � �wp�1wp
</p>
<p>�
cos .�/C � � � C 2
</p>
<p>�
w0wp
</p>
<p>�
cos .�p/ :
</p>
<p>This corresponds exactly to (4.4) for finite p:
</p>
<p>TF .�/ D
pX
</p>
<p>jD0
w2j C 2
</p>
<p>pX
</p>
<p>hD1
</p>
<p>2
4
</p>
<p>p�hX
</p>
<p>jD0
wjwjCh
</p>
<p>3
5 cos .�h/ : (4.8)
</p>
<p>AR(1) Spectra
</p>
<p>The autoregressive polynomial of order one reads
</p>
<p>A .L/ D 1 � a1L;
</p>
<p>i.e. the filter coefficients are
</p>
<p>w0 D 1 and w1 D �a1:
</p>
<p>Hence, for the power transfer function, (4.8) provides us with
</p>
<p>TA.�/ D 1C a21 � 2a1 cos.�/ ;
</p>
<p>and Corollary 4.1 yields for the spectrum
</p>
<p>2� f .�/ D �
2
</p>
<p>1C a21 � 2a1 cos.�/
:
</p>
<p>In Problem 4.4 we will show that there are extrema at � D 0 and � D � , where
the slope of the spectrum is zero. For a1 &gt; 0 the spectrum decreases on Œ0; �&#141;,
</p>
<p>i.e. the most significant frequency is � D 0: The process is dominated by trending
behavior. Figure 4.4 shows that this is the more true the greater a1 is: The greater a1;
</p>
<p>the steeper and higher grows the spectrum in the area around zero. Mirror-inversely,</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Examples of ARMA Spectra 91
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
2
</p>
<p>0
4
</p>
<p>0
6
</p>
<p>0
8
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>[0, pi]
</p>
<p>a1 = 0.60
a1 = 0.75
a1 = 0.90
</p>
<p>Fig. 4.4 AR(1) spectra (2� f .�/) with positive autocorrelation
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>[0, pi]
</p>
<p>a1 = 0.75
a1 = &minus;0.75
</p>
<p>Fig. 4.5 AR(1) spectra (2� f .�/), cf. Fig. 3.2
</p>
<p>for a1 &lt; 0 it holds that the trend component matters least, see Fig. 4.5. The direct
</p>
<p>comparison to the time domain in Fig. 3.2 is also interesting. The case in which
</p>
<p>a1 &gt; 0 with the spectral maximum at � D 0 translates in persistence of the process:</p>
<p/>
</div>
<div class="page"><p/>
<p>92 4 Spectra of Stationary Processes
</p>
<p>Observations temporarily lying close together have similar numerical values, i.e. the
</p>
<p>autocorrelation function is positive. For a1 &lt; 0, however, observations following
</p>
<p>each other have the tendency to change their sign as, in this case, there is just no
</p>
<p>trending behavior.
</p>
<p>AR(2) Spectra
</p>
<p>The AR(2) process is given by
</p>
<p>xt D
"t
</p>
<p>A.L/
with A.L/ D 1 � a1L � a2L2:
</p>
<p>In Problem 4.5 we recapitulate the principle of the &ldquo;summation over the diagonal&rdquo;
</p>
<p>and thus we show
</p>
<p>TA.�/ D 1C a21 C a22 C 2 Œa1 .a2 � 1/ cos.�/� a2 cos .2�/&#141; :
</p>
<p>Therefore, due to Corollary 4.1, the corresponding spectrum reads
</p>
<p>2� f .�/ D �
2
</p>
<p>TA.�/
:
</p>
<p>For a2 D 0 one obtains the AR(1) case.
In Fig. 4.6 spectra for four parameter constellations are depicted; these are
</p>
<p>exactly the four cases for which autocorrelograms are given in Fig. 3.4. The top
</p>
<p>left case could be well approximated by an AR(1) process. This is also roughly true
</p>
<p>for the top right case; however, closer inspection reveals that the AR(2) spectrum
</p>
<p>is not minimal at frequency zero. Both the lower spectra entirely burst the AR(1)
</p>
<p>scheme. On the bottom right we have the example of the business cycle, see Fig. 4.3.
</p>
<p>The spectrum on the bottom left is even more extreme: Except for a rather small
</p>
<p>area around � D 2, it is zero almost everywhere which is why there is no trend
component. The process is determined by almost only one cycle which can be seen
</p>
<p>in the autocorrelogram as well.
</p>
<p>ARMA(1,1) Spectra
</p>
<p>Now, let us consider the two filters
</p>
<p>A.L/ D 1 � a1L and B.L/ D 1C b1L :
</p>
<p>We know the filter transfer function of B.L/ from Example 4.3:
</p>
<p>TB.�/ D 1C b21 C 2 b1 cos.�/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Examples of ARMA Spectra 93
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>2
5
</p>
<p>a1 = 0.7, a2 = 0.1
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>2
0
</p>
<p>2
5
</p>
<p>a1 = &minus;0.4, a2 = 0.4
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
1
</p>
<p>0
2
</p>
<p>0
3
</p>
<p>0
</p>
<p>a1 = &minus;1.0, a2 = &minus;0.8
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>8
</p>
<p>a1 = 1.0, a2 = &minus;0.5
</p>
<p>Fig. 4.6 AR(2) spectra (2� f .�/), cf. Fig. 3.4
</p>
<p>The transformation of A.L/ was determined at the beginning of this section. Due to
</p>
<p>Corollary 4.1, we put the spectrum together as follows:
</p>
<p>2� f .�/ D 1C b
2
1 C 2 b1 cos.�/
</p>
<p>1C a21 � 2 a1 cos.�/
�2 ; � 2 Œ0; �&#141; :
</p>
<p>In order to have this illustrated, consider the examples from Fig. 4.7. The cases
</p>
<p>correspond in their graphical arrangement to the autocorrelograms from Fig. 3.5.
</p>
<p>The cases top right and bottom left are interesting. At the top on the right, the entire
</p>
<p>absence of a trend is reflected in a negative autocorrelogram close to zero. At the
</p>
<p>bottom on the left, beside the trend, cycles of higher frequencies add to the process
</p>
<p>as well, the process consequently being positively autocorrelated of the first order
</p>
<p>and then exhibiting an alternating pattern of autocorrelation.</p>
<p/>
</div>
<div class="page"><p/>
<p>94 4 Spectra of Stationary Processes
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
1
</p>
<p>0
2
</p>
<p>0
3
</p>
<p>0
4
</p>
<p>0
5
</p>
<p>0
</p>
<p>a1 = 0.75, b1 = 0.75
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.2
</p>
<p>0
.4
</p>
<p>0
.6
</p>
<p>0
.8
</p>
<p>1
.0
</p>
<p>1
.2
</p>
<p>a1 = 0.75, b1 = &minus;0.9
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>a1 = &minus;0.5, b1 = 0.9
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>a1 = &minus;0.5, b1 = &minus;0.9
</p>
<p>Fig. 4.7 ARMA(1,1) spectra (2� f .�/), cf. Fig. 3.5
</p>
<p>Multiplicative Seasonal AR Process
</p>
<p>If one wants to have a decaying autocorrelation function not dropping to zero,
</p>
<p>then one does not choose a pure MA model as in Example 4.2. The most basic
</p>
<p>seasonal autoregressive model is based on the filter .1�aSL S/. Frequently, the trend
component is to have an additional weight which is why one adds the AR(1) factor
</p>
<p>.1 � a1L/:
</p>
<p>A.L/ D .1 � a1L/ .1 � aSL S/
D 1 � a1L � aSL S C a1 aSL SC1 :
</p>
<p>Therefore, we have an AR(S C 1) model with parameter restrictions. The spectrum
is adopted from Problem 4.6 in which TA.�/ is given:
</p>
<p>2� f .�/ D �
2
</p>
<p>TA.�/
:</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Problems and Solutions 95
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
2
</p>
<p>0
6
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>a1 = 0.5, a4 = 0.8
</p>
<p>0.0 0.5 1.0 1.5 2.0 2.5 3.0
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>1
5
</p>
<p>a1 = 0.5, a4 = 0.5
</p>
<p>Fig. 4.8 Spectra (2� f .�/) of multiplicative seasonal AR processes (S D 4)
</p>
<p>In Fig. 4.8, we show two examples for the quarterly case (S D 4). With the
frequencies � D � and � D �=2 the semi-annual cycles with P D 2 quarters period
and the annual cycles with P D 4 quarters length are modelled (one also talks about
seasonal cycles). As a1 D 0:5 is positive in both the spectra, the trend (at frequency
zero) dominates the seasonal cycles. The annual and semi-annual cycles add both
</p>
<p>equally strongly to the variance of the process. However, in the case a4 D 0:8, the
seasonal component is more pronounced than in the case a4 D 0:5 as in the upper
spectrum both the seasonal peaks are not only higher than in the lower one (note the
</p>
<p>scale on the ordinate) but most of all steeper: In the upper graph, the area beneath the
</p>
<p>spectrum substantially concentrates on the three frequencies 0; �=2 and � , whereas
</p>
<p>it is more spread over all frequencies in the lower one.
</p>
<p>4.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>4.1 Prove Proposition 4.1 (e) under the additional assumption of absolute summa-
</p>
<p>bility.
</p>
<p>4.2 Determine the extrema in the spectrum of the seasonal MA process from
</p>
<p>Example 4.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>96 4 Spectra of Stationary Processes
</p>
<p>4.3 Prove the structure of the spectrum (4.5) for absolutely summable MA(1)
processes.
</p>
<p>4.4 Determine the extrema of the AR(1) spectrum.
</p>
<p>4.5 Determine the power transfer function TA.�/ of the filter A.L/ D 1�a1L�a2L2.
</p>
<p>4.6 Determine the power transfer function TA.�/ of the multiplicative quarterly AR
</p>
<p>filter A.L/ D .1 � a1 L/.1 � a4 L4/ D 1 � a1 L � a4 L4 C a1 a4 L5.
</p>
<p>4.7 Determine the persistence measure VR from (4.6) for a stationary and invertible
</p>
<p>ARMA(1,1) process. Discuss its behavior in particular for the MA(1) model (in
</p>
<p>comparison with the AR(1) case given in (4.7)).
</p>
<p>Solutions
</p>
<p>4.1 We define the entity Ah and will show that it equals &#13;.h/. Due to the symmetry
</p>
<p>of the cosine function and of the even spectrum it holds by definition that:
</p>
<p>Ah WD 2
�Z
</p>
<p>0
</p>
<p>f .�/ cos .�h/ d�
</p>
<p>D
�Z
</p>
<p>��
</p>
<p>f .�/ cos .�h/ d�
</p>
<p>D 1
2�
</p>
<p>�Z
</p>
<p>��
</p>
<p>1X
</p>
<p>lD�1
&#13; .l/ cos .�l/ cos .�h/ d�:
</p>
<p>Because of the absolute summability, the order of summation and integration is
</p>
<p>interchangeable:
</p>
<p>2� Ah D
1X
</p>
<p>lD�1
&#13; .l/
</p>
<p>�Z
</p>
<p>��
</p>
<p>cos .�l/ cos .�h/ d�
</p>
<p>D &#13; .0/
�Z
</p>
<p>��
</p>
<p>cos .�h/ d�C 2
1X
</p>
<p>lD1
&#13; .l/
</p>
<p>�Z
</p>
<p>��
</p>
<p>cos .�l/ cos .�h/ d� :</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Problems and Solutions 97
</p>
<p>For h D 0 it holds that8
</p>
<p>2� A0 D &#13; .0/ 2� C 2
1X
</p>
<p>lD1
&#13; .l/
</p>
<p>sin .�l/ � sin .��l/
l
</p>
<p>D 2� &#13; .0/C 0:
</p>
<p>Accordingly, for h &curren; 0 it holds that
</p>
<p>2� Ah D 0C 2
1X
</p>
<p>lD1
&#13; .l/
</p>
<p>�Z
</p>
<p>��
</p>
<p>cos .� .l � h//C cos .� .l C h//
2
</p>
<p>d�;
</p>
<p>where the trigonometric formula
</p>
<p>2 cos x cos y D cos .x � y/C cos .x C y/
</p>
<p>was used. By this we obtain
</p>
<p>2� Ah D &#13; .h/
�Z
</p>
<p>��
</p>
<p>.1C cos .2�h// d�
</p>
<p>as one can see that for k 2 Znf0g the integral is
</p>
<p>�Z
</p>
<p>��
</p>
<p>cos .�k/ d� D sin .�k/ � sin .��k/
k
</p>
<p>D 0 :
</p>
<p>So, we finally obtain
</p>
<p>2� Ah D &#13; .h/ .2� C 0/ D 2� &#13; .h/
</p>
<p>for h &curren; 0 as well. Hence, Ah D &#13;.h/ for all h, and the proof is complete.
4.2 The spectrum
</p>
<p>f .�/ D
�
1C b2 C 2b cos .�S/
</p>
<p>�
�2=2�
</p>
<p>8We use
Z
</p>
<p>cos.�`/d� D sin.�`/
`
</p>
<p>;
</p>
<p>and sin.�k/ D 0 for k 2 Z.</p>
<p/>
</div>
<div class="page"><p/>
<p>98 4 Spectra of Stationary Processes
</p>
<p>is given. In order to determine the extrema, we consider the derivative,
</p>
<p>f 0 .�/ D �2bS sin .�S/ �2=2�;
</p>
<p>with .S C 1/ zeros
</p>
<p>0;
�
</p>
<p>S
;
2�
</p>
<p>S
; : : : ;
</p>
<p>.S � 1/ �
S
</p>
<p>; �
</p>
<p>on the interval Œ0; �&#141;. The sign of the second derivative depends on b:
</p>
<p>f 00 .�/ D �2bS2 cos .�S/ �2=2�:
</p>
<p>One obtains
</p>
<p>f 00 .0/ D f 00
�
2�
</p>
<p>S
</p>
<p>�
D : : : D �2bS2 �2=2� ;
</p>
<p>f 00
��
</p>
<p>S
</p>
<p>�
D f 00
</p>
<p>�
3�
</p>
<p>S
</p>
<p>�
D : : : D C2bS2 �2=2�:
</p>
<p>Accordingly, maxima and minima follow each other. For b &gt; 0, the sequence
</p>
<p>of extrema begins with a maximum at zero; for b &lt; 0, one obtains a minimum,
</p>
<p>inversely.
</p>
<p>4.3 The autocovariances of
</p>
<p>xt D
1X
</p>
<p>jD0
cj"t�j
</p>
<p>are known from Proposition 3.2:
</p>
<p>&#13;x.h/ D �2
1X
</p>
<p>jD0
cjcjCh:
</p>
<p>For the spectrum fx .�/ it follows:
</p>
<p>2�
fx .�/
</p>
<p>�2
D &#13;x .0/
</p>
<p>�2
C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>&#13;x .h/
</p>
<p>�2
cos .�h/
</p>
<p>D
1X
</p>
<p>jD0
c2j C 2
</p>
<p>1X
</p>
<p>hD1
</p>
<p>1X
</p>
<p>jD0
cjcjCh cos .�h/ :
</p>
<p>Hence, the claim is verified.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Problems and Solutions 99
</p>
<p>4.4 For
</p>
<p>f .�/ D
�
1C a21 � 2 a1 cos.�/
</p>
<p>��1
�2=2�
</p>
<p>one obtains by differentiation
</p>
<p>f 0.�/ D �
�
1C a21 � 2 a1 cos.�/
</p>
<p>��2
.�2 a1/ .� sin.�// �2=2� :
</p>
<p>Obviously, candidates for extrema are � D 0 and � D �:
</p>
<p>f 0.0/ D f 0.�/ D 0 :
</p>
<p>The sign of the derivative depends on a1 only:
</p>
<p>f 0.�/ &lt; 0 ; � 2 Œ0; �&#141; &rdquo; a1 &gt; 0 :
</p>
<p>Accordingly,
</p>
<p>f .0/ D .1 � a1/�2 �2=2� and f .�/ D .1C a1/�2 �2=2�
</p>
<p>are maxima and minima, depending on the sign of a1.
</p>
<p>4.5 With the vector of coefficients
</p>
<p>a D
</p>
<p>0
@
</p>
<p>1
</p>
<p>�a1
�a2
</p>
<p>1
A
</p>
<p>we obtain as outer product
</p>
<p>a a0 D
</p>
<p>0
@
</p>
<p>1 �a1 �a2
�a1 a21 a1a2
�a2 a1a2 a22
</p>
<p>1
A :
</p>
<p>Adding the cosine, it follows that
</p>
<p>MA.�/ D
</p>
<p>0
@
</p>
<p>1 �a1 cos.�/ �a2 cos.2 �/
�a1 cos.�/ a21 a1a2 cos.�/
�a2 cos.2 �/ a1a2 cos.�/ a22
</p>
<p>1
A :
</p>
<p>By summation over the diagonal we obtain due to symmetry
</p>
<p>TA.�/ D 1C a21 C a22 C 2 Œ�a1 C a1 a2&#141; cos.�/C 2 Œ�a2&#141; cos.2�/ ;
</p>
<p>which results from (4.8) as well. This is in accordance with the result in the text.</p>
<p/>
</div>
<div class="page"><p/>
<p>100 4 Spectra of Stationary Processes
</p>
<p>4.6 Using (4.8) with p D 5 yields the following expression:
</p>
<p>TA.�/ D 1C a21 C a24 C a21 a24
C2
</p>
<p>�
�a1 � a1 a24
</p>
<p>�
cos.�/C 2 Œa1 a4&#141; cos.3 �/
</p>
<p>C2
�
�a4 � a21 a4
</p>
<p>�
cos.4 �/C 2 Œa1 a4&#141; cos.5 �/ :
</p>
<p>This is simply an exercise in concentration and is simplified by the following
</p>
<p>equalities:
</p>
<p>w0 D 1; w1 D �a1; w2 D w3 D 0; w4 D �a4; w5 D a1 a4 :
</p>
<p>4.7 In the previous section we discussed the ARMA(1,1) process with the
</p>
<p>polynomials
</p>
<p>A.L/ D 1 � a1L and B.L/ D 1C b1L :
</p>
<p>Evaluating the spectrum given there we have:
</p>
<p>2� f .0/ D 1C b
2
1 C 2 b1
</p>
<p>1C a21 � 2 a1
�2 D .1C b1/
</p>
<p>2
</p>
<p>.1� a1/2
�2 :
</p>
<p>The variance we copy from Chap. 3:
</p>
<p>&#13;.0/ D .1C b
2
1 C 2a1b1/
1 � a21
</p>
<p>�2 :
</p>
<p>By (4.6) we obtain
</p>
<p>VR D 1C a1
1 � a1
</p>
<p>.1C b1/2
1C b21 C 2a1b1
</p>
<p>:
</p>
<p>If b1 D 0, the AR(1) case from (4.7) is of course reproduced. If a1 D 0, the MA(1)
case results as
</p>
<p>VR D .1C b1/
2
</p>
<p>1C b21
</p>
<p>8
&lt;
:
</p>
<p>&gt; 1 if b1 &gt; 0
</p>
<p>D 1 if b1 D 0
&lt; 1 if b1 &lt; 0
</p>
<p>:
</p>
<p>We hence have negative persistence for b1 &lt; 0, which reflects the negative
</p>
<p>autocorrelation. For b1 &gt; 0, it is straightforward to verify that VR is growing with
</p>
<p>b1, reaching a maximum value of VR D 2 for b1 D 1. This corresponds to the
persistence of an AR(1) process with a1 D 1=3. Hence, the invertible MA(1) process
with jb1j &lt; 1 can only capture very moderate persistence in comparison with the
AR(1) case where VR grows with a1 beyond any limit.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 101
</p>
<p>References
</p>
<p>Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
Springer.
</p>
<p>Cochrane, J. H. (1988). How big is the random walk in GNP? Journal of Political Economy, 96,
893&ndash;920.
</p>
<p>Cogley, T., &amp; Sargent T. S. (2005). Drifts and volatilities: Monetary policies and outcomes in the
post WWII US. Review of Economic Dynamics, 8, 262&ndash;302.
</p>
<p>Fuller, W. A. (1996). Introduction to statistical time series (2nd ed.). New York: Wiley.
Hamilton, J. (1994). Time series analysis. Princeton: Princeton University Press.
Hassler, U. (2014). Persistence under temporal aggregation and differencing. Economics Letters,
</p>
<p>124, 318&ndash;322.</p>
<p/>
</div>
<div class="page"><p/>
<p>5LongMemory and Fractional Integration
</p>
<p>5.1 Summary
</p>
<p>Below Proposition 3.5 we saw that the autocorrelation sequence of any stationary
</p>
<p>ARMA process dies out at exponential rate: j�.h/j � c gh, see (3.14). This is
too restrictive for many time series of stronger persistence, which display long
</p>
<p>memory in that the autocovariance sequence vanishes at a slower rate. In some
</p>
<p>fields of economics and finance long memory is treated as an empirical stylized
</p>
<p>fact.1 Fractional integration as a model for long memory will be presented in
</p>
<p>this chapter. In the same paper where Granger (1981) introduced the Nobel prize
</p>
<p>winning concept of cointegration (see Chap. 16) he addressed the idea of fractional
</p>
<p>integration, too. For an early survey on fractional integration and applications see
</p>
<p>Baillie (1996).
</p>
<p>5.2 Persistence and LongMemory
</p>
<p>We have already briefly touched upon the so-called random walk, see Eq. (1.9). In
</p>
<p>terms of the difference operator this can be written as �xt D "t, i.e. the process has
to be differenced once to obtain stationarity. Alternatively, the process is given by
</p>
<p>a cumulation or summation over the shocks, xt D
Pt
</p>
<p>jD1 "j, see (1.8), which is the
reason to call the process fxtg integrated of order 1, see also Chap. 14. In this section,
differencing or integration of order 1 will be extended by introducing non-integer
</p>
<p>orders of differencing and integration.
</p>
<p>1See e.g. the special issue edited by Maasoumi and McAleer (2008) in Econometric Reviews on
&ldquo;Realized Volatility and Long Memory&rdquo;.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_5
</p>
<p>103</p>
<p/>
</div>
<div class="page"><p/>
<p>104 5 Long Memory and Fractional Integration
</p>
<p>Persistence
</p>
<p>By persistence we understand how strongly a past shock affects the presence
</p>
<p>of a stochastic process. We stick to the MA(1) representation behind the Wold
decomposition of a stationary process that we briefly touched upon in Chap. 3,
</p>
<p>xt D
1X
</p>
<p>jD0
cj"t�j ;
</p>
<p>1X
</p>
<p>jD0
c2j &lt;1 ;
</p>
<p>with f"tg forming a white noise sequence. The impulse responses coefficients fcjg
measures the response of xt on a shock j periods ago. With stationary processes,
</p>
<p>the shocks are transitory in that limj!1 cj D 0. In particular, for stationary
ARMA processes we know that the impulse responses die out so fast that they are
</p>
<p>summable in absolute value, see (3.2). To model a stronger degree of persistence
</p>
<p>and long memory, we require a slower convergence to zero. The model of fractional
</p>
<p>integration of order d will impose the so-called hyperbolic decay rate,
</p>
<p>cj D c jd�1 ; c &curren; 0 :
</p>
<p>Under d &lt; 1, the sequence fcjg converges to zero. Clearly, the larger d, the stronger
is the persistence in that jd�1 dies out more slowly. Hence, the parameter d measures
the strength of persistence. Contrary to the exponential case characteristic of ARMA
</p>
<p>processes, hyperbolic decay is so slow for positive d &gt; 0, that the impulse responses
</p>
<p>are not summable. In Problem 5.1 we will establish the following convergence result
</p>
<p>concerning the so-called (generalized) harmonic series, often also called p-series:
</p>
<p>1X
</p>
<p>jD1
j�p &lt;1 if and only if p &gt; 1 : (5.1)
</p>
<p>Moreover, we will show in Problem 5.2 that exponential decay to zero is faster than
</p>
<p>hyperbolic one:
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>jd�1
D 0 ; 0 &lt; g &lt; 1 ; jdj &lt; 1 :
</p>
<p>In order to illustrate the different decay rates, we display in Figs. 5.1 and 5.2
</p>
<p>sequences jd�1 and gj�1, respectively; by construction they all have the value 1 at
j D 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Persistence and Long Memory 105
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.85
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.65
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.45
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.25
</p>
<p>Fig. 5.1 jd�1 for d D 0:85; 0:65; 0:45; 0:25
</p>
<p>For a process to be stationary, we require the impulse response sequence to
</p>
<p>be square summable, see Proposition 3.2. From (5.1) we learn that
P1
</p>
<p>jD1 j
2d�2 is
</p>
<p>finite if and only if d &lt; 0:5, which hence turns out to be the stationarity condition
</p>
<p>for processes with impulse responses fjd�1g.The model of fractional integration,
however, does not directly assume cj D c jd�1; rather this power law will hold only
true for large j,
</p>
<p>cj � c jd�1 ; j ! 1 ;
</p>
<p>where &ldquo;� for j ! 1&rdquo; is to be understood as a proper limit in the following way:
</p>
<p>aj � bj &rdquo; lim
j!1
</p>
<p>aj
</p>
<p>bj
D 1 ; bj &curren; 0 : (5.2)</p>
<p/>
</div>
<div class="page"><p/>
<p>106 5 Long Memory and Fractional Integration
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>g=0.85
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>g=0.65
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>g=0.45
</p>
<p>5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>g=0.25
</p>
<p>Fig. 5.2 g j�1 for g D 0:85; 0:65; 0:45; 0:25
</p>
<p>Fractional Differencing and Integration
</p>
<p>With the usual difference operator � D .1 � L/, see Example 3.2, we define
fractional differences by binomial expansion2:
</p>
<p>�d D .1 � L/d D 1 � dL � d .1 � d/
2
</p>
<p>L2 � d .1 � d/ .2 � d/
6
</p>
<p>L3 � � � �
</p>
<p>D
1X
</p>
<p>jD0
�jL
</p>
<p>j ; d &gt; �1 :
</p>
<p>2For the rest of this chapter we maintain d &gt; �1, which guarantees that f�jg converges to 0 with
growing j, making the infinite expansion meaningful.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 Persistence and Long Memory 107
</p>
<p>Readers not be familiar with binomial series for d &hellip; N may wish to consult e.g.
Trench (2013, Sect. 4.5). The binomial series results from a Taylor expansion of
</p>
<p>.1 � z/d about z D 0, hence also called Maclaurin series. The coefficients f�jg are
given in terms of binomial coefficients
</p>
<p>�
d
</p>
<p>j
</p>
<p>�
D d .d � 1/ � � � .d � j C 1/
</p>
<p>jŠ
;
</p>
<p>yielding the recursion
</p>
<p>�j D
�
</p>
<p>d
</p>
<p>j
</p>
<p>�
.�1/ j D j � 1 � d
</p>
<p>j
�j�1; j � 1 ; �0 D 1 : (5.3)
</p>
<p>For natural numbers d one has the more familiar finite expansions,
</p>
<p>.1 � L/1 D 1 � L ; .1� L/2 D 1 � 2 L C L2 ;
</p>
<p>while the expansion in (5.3) holds for non-integer (or fractional) values of d, too. In
</p>
<p>Problem 5.3 we derive the behavior for large j,
</p>
<p>�j �
j�d�1
</p>
<p>&#13; .�d/ ; j ! 1 ; d &curren; 0; (5.4)
</p>
<p>where &#13; .�/ is the so-called Gamma function introduced at greater detail below its
definition in (5.18) in the Problem section.
</p>
<p>Similarly to fractional differences, we may define the fractional integration
</p>
<p>operator upon inversion,
</p>
<p>��d D .1 � L/�d D
1X
</p>
<p>jD0
 jL
</p>
<p>j ;  0 D 1 ;
</p>
<p>where the coefficients are given by simply replacing d by �d in (5.3):
</p>
<p> j D
�
�d
j
</p>
<p>�
.�1/j D j � 1C d
</p>
<p>j
 j�1; j � 1 : (5.5)
</p>
<p>The same arguments establishing (5.4) hence show
</p>
<p> j �
jd�1
</p>
<p>&#13; .d/
; j ! 1 ; d &curren; 0 : (5.6)</p>
<p/>
</div>
<div class="page"><p/>
<p>108 5 Long Memory and Fractional Integration
</p>
<p>Fractional integration thus imposes the hyperbolic decay rate discussed above,
</p>
<p>where the speed of convergence varies with d. From (5.1) we observe with (5.6)
</p>
<p>that f jg is summable if and only if d &lt; 0, in which case
</p>
<p>1X
</p>
<p>jD0
 j D .1 � z/�d
</p>
<p>ˇ̌
zD1 D 0 ; d &lt; 0 : (5.7)
</p>
<p>Further, f jg is square summable if and only if d &lt; 0:5. These are the ingredients
to define a fractionally integrated process.
</p>
<p>5.3 Fractionally Integrated Noise
</p>
<p>We now apply the above findings to the simplest case of fractional noise (which
</p>
<p>is short for: fractionally integrated noise), define long memory in the time domain
</p>
<p>in (5.9), and translate it into the frequency domain.
</p>
<p>Fractional Noise and LongMemory
</p>
<p>In case of fractionally integrated noise the fractional differencing filter �d has to
</p>
<p>be applied to fxtg in order to obtain white noise f"tg with variance �2: �dxt D "t.
Equivalently, we write after inverting the differences
</p>
<p>xt D .1 � L/�d "t
</p>
<p>D
1X
</p>
<p>jD0
 j"t�j ; t 2 Z ; d &lt; 0:5 ; (5.8)
</p>
<p>with f jg from (5.5) being the sequence of impulse response coefficients measuring
the effect of a past shock on the presence. As we have discussed above the impulse
</p>
<p>responses die out the more slowly the larger d is. In that sense we interpret d as
</p>
<p>measure of persistence or memory. The impulse responses die out so slowly that
</p>
<p>they are not absolutely summable for positive memory parameter d. Consequently
</p>
<p>for d &gt; 0, fxtg from (5.8) does not belong to the class of processes with
absolutely summable autocovariances characterized in (3.2), while all stationary
</p>
<p>ARMA processes belong to this class. Hence, fractional integration models for
</p>
<p>d &gt; 0 a feature that is not captured by traditional ARMA processes, which we
</p>
<p>call strong persistence; it is defined in the time domain by
</p>
<p>JX
</p>
<p>jD1
 j ! 1 ; J ! 1 ; if d &gt; 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Fractionally Integrated Noise 109
</p>
<p>Consequently, CIR or VR defined in (3.3) and (4.6), respectively, do not exist. Even
</p>
<p>though the MA coefficients are not summable for positive d, the process is still
</p>
<p>stationary as long as d &lt; 0:5 because
P
</p>
<p>j  
2
j &lt;1 due to (5.6) and (5.1). Further, the
</p>
<p>process is often called invertible for d &gt; �0:5 since an autoregressive representation
exists that is square summable3:
</p>
<p>1X
</p>
<p>jD0
�jxt�j D "t with
</p>
<p>1X
</p>
<p>jD0
�2j &lt;1 :
</p>
<p>Note that the existence of an autoregressive representation in the mean square sense
</p>
<p>does not require square summability; in fact, Bondon and Palma (2007) extend the
</p>
<p>range of invertibility in the mean square to d &gt; �1. Given the existence of the
MA(1) representation the following properties of fractionally integrated noise are
proven in the Problem section.
</p>
<p>Proposition 5.1 (Fractional noise, time domain) For fractionally integrated
</p>
<p>noise from (5.8) it holds with �1 &lt; d &lt; 0:5 that
</p>
<p>(a) the variance equals
</p>
<p>&#13;.0/ D &#13;.0I d/ D �2 &#13; .1� 2d/
.&#13; .1 � d//2
</p>
<p>;
</p>
<p>with &#13; .�/ being defined in (5.18), and &#13;.0I d/ achieves its minimum for d D 0;
(b) the autocovariances equal
</p>
<p>&#13;.h/ D h � 1C d
h � d &#13;.h � 1/ ; h D 1; 2; : : : ;
</p>
<p>� &#13;d �2h2d�1; h ! 1 ; d &curren; 0 ;
</p>
<p>with
</p>
<p>&#13;d D
&#13; .1 � 2d/
</p>
<p>&#13; .d/&#13; .1 � d/ ;
</p>
<p>where &#13;d &lt; 0 if and only if d &lt; 0;
</p>
<p>(c) the autocorrelations �.h/ D �.hI d/ grow with d for d &gt; 0.
</p>
<p>Let us briefly comment those results. First, since &#13; .1/ D 1, the minimum
variance obtained in the white noise case (d D 0) is of course &#13;.0I 0/ D �2. Second,
</p>
<p>3A more technical exposition can be found in Brockwell and Davis (1991, Thm. 13.2.1) or Giraitis,
Koul, and Surgailis (2012, Thm. 7.2.1), although they consider only the range jdj &lt; 0:5.</p>
<p/>
</div>
<div class="page"><p/>
<p>110 5 Long Memory and Fractional Integration
</p>
<p>from the hyperbolic decay of the autocovariance sequence we observe that &#13;.h/
</p>
<p>converges to zero with h as long as d &lt; 0:5, but for d &gt; 0 so slowly, that we have
</p>
<p>long memory defined as
</p>
<p>HX
</p>
<p>hD0
j &#13; .h/ j! 1 ; H ! 1 if d &gt; 0 : (5.9)
</p>
<p>In particular, the autocovariances die out the more slowly the larger the memory
</p>
<p>parameter d is. Obviously, the same feature can be rephrased in terms of autocor-
</p>
<p>relations. The recursion carries over to the autocorrelations, and Proposition 5.1 (b)
</p>
<p>yields
</p>
<p>�.h/ � &#13;d �
2
</p>
<p>&#13;.0/
h2d�1; h ! 1 :
</p>
<p>For a numerical and graphical illustration see Fig. 5.3. The asymptotic constant &#13;d
has the same sign as d, meaning that in case of long memory the autocovariances
</p>
<p>converge to zero from above, and vice versa from below zero for d &lt; 0, see again
</p>
<p>0 5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.45
</p>
<p>0 5 10 15
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>d=0.25
</p>
<p>0 5 10 15
&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>d=&minus;0.25
</p>
<p>0 5 10 15
&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>d=&minus;0.45
</p>
<p>Fig. 5.3 �.h/ from Proposition 5.1 for d D 0:45; 0:25;�0:25;�0:45</p>
<p/>
</div>
<div class="page"><p/>
<p>5.3 Fractionally Integrated Noise 111
</p>
<p>Fig. 5.3. Note, however, that &#13;d collapses to zero as d ! 0, simply meaning that the
hyperbolic decay rate does not hold for d D 0. Third, a similar effect that d has on
�.h/ at long lags, holds true for finite h. More precisely, Proposition 5.1 (c) says for
</p>
<p>each finite h that the autocorrelation grows with d (for d &gt; 0), which reinforces the
</p>
<p>interpretation of d as measure of persistence or long memory.4
</p>
<p>The case of negative d results in short memory in that the autocovariances are
</p>
<p>absolutely summable, which is clear again from the p-series in (5.1). This case is
</p>
<p>sometimes called antipersistent, the reason for that being
</p>
<p>1X
</p>
<p>jD1
 j D 0 ; if d &lt; 0 :
</p>
<p>This property translates into a special case of short memory, namely
</p>
<p>1X
</p>
<p>hD�1
�.h/ D 0 ; if d &lt; 0 ;
</p>
<p>as we will become obvious from the spectrum at frequency zero.
</p>
<p>LongMemory in the Frequency Domain
</p>
<p>It is obvious from the definition of the spectrum in (4.3) that it does not exist at the
</p>
<p>origin under long memory (d &gt; 0), because the autocovariances are not summable.
</p>
<p>Still, the previous chapter has been set up sufficiently general to cover long memory,
</p>
<p>see (4.1). Given a singularity at frequency � D 0, one still may determine the rate
at which f .�/ goes off to infinity as � approaches 0. To determine f , we have to
</p>
<p>evaluate the power transfer function of .1 � L/�d from Proposition 4.1 and obtain5
</p>
<p>T.1�L/�d .�/ D .2 � 2 cos.�//�d D
�
4 sin2
</p>
<p>�
�
</p>
<p>2
</p>
<p>���d
;
</p>
<p>4More complicated is the effect of changes in d if d &lt; 0, see Hassler (2014).
5Readers not familiar with complex numbers, i2 D �1, may skip the following equation, see also
Footnote 6 in Chap. 4:
</p>
<p>T.1�L/�d .�/ D .1� ei�/�d.1� e�i�/�d
</p>
<p>D .1� ei� � e�i� C 1/�d
</p>
<p>D .2� 2 cos.�//�d :</p>
<p/>
</div>
<div class="page"><p/>
<p>112 5 Long Memory and Fractional Integration
</p>
<p>where the trigonometric half-angle formula was used for the second equality:
</p>
<p>2 sin2.x/ D 1 � cos.2x/ : (5.10)
</p>
<p>We hence have the following result.
</p>
<p>Proposition 5.2 (Fractional noise, frequency domain) Under the assumptions of
</p>
<p>Proposition 5.1 it holds for the spectrum of fractional noise xt D .1 � L/�d"t that
</p>
<p>f .�/ D
�
4 sin2
</p>
<p>�
�
</p>
<p>2
</p>
<p>���d
�2
</p>
<p>2�
; � &gt; 0 ; (5.11)
</p>
<p>and
</p>
<p>f .�/ � ��2d �
2
</p>
<p>2�
; �! 0 : (5.12)
</p>
<p>The second statement in Proposition 5.2 is again understood to be asymptotic:
</p>
<p>Similarly to (5.2) we denote for two function a.x/ and b.x/ &curren; 0:
</p>
<p>a.x/ � b.x/ for x ! 0 &rdquo; lim
x!0
</p>
<p>a.x/
</p>
<p>b.x/
D 1 : (5.13)
</p>
<p>Since limx!0 sin.x/=x D 1 we write sin.x/ � x for x ! 0. Consequently, (5.12)
arises from (5.11).
</p>
<p>From Proposition 5.2 we learn that long memory (d &gt; 0) translates into a spectral
</p>
<p>singularity at frequency zero, and the negative slope is the steeper the larger d
</p>
<p>is. In other words: the longer the memory, the stronger is the contribution of the
</p>
<p>long-run trend to the variance of the process. The antipersistent case in contrast is
</p>
<p>characterized by the opposite extreme: f .0/ D 0. For an illustration, have a look at
Fig. 5.4.
</p>
<p>Example 5.1 (Fractionally Integrated Noise) Although the fractional noise is dom-
</p>
<p>inated by the trend component at frequency zero (strongly persistent) for d &gt; 0,
</p>
<p>the process is stationary as long as d &lt; 0:5. Consequently, a typical trajectory
</p>
<p>can not drift off but displays somehow reversing trends. In Fig. 5.5 we see from
</p>
<p>simulated data that the deviations from the zero line are stronger for d D 0:45
than for d D 0:25. The antipersistent series (d D �0:45), in contrast, displays an
oscillating behavior due to the negative autocorrelation. �</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Generalizations 113
</p>
<p>0 1 2 3
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>d=0.45
</p>
<p>0 1 2 3
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>d=0.25
</p>
<p>0 1 2 3
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>d=&minus;0.25
</p>
<p>0 1 2 3
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>d=&minus;0.45
</p>
<p>Fig. 5.4 2� f .�/ from Proposition 5.2 for d D 0:45; 0:25;�0:25;�0:45
</p>
<p>5.4 Generalizations
</p>
<p>On top of long memory as implied by fractional integration for 0 &lt; d &lt; 0:5,
</p>
<p>we now want to allow for additional short memory. We assume that �d has to be
</p>
<p>applied to fxtg in order to obtain a short memory process fetg: �dxt D et. At the
end of this section, the order of integration d will be extended beyond d D 0:5 to
cover nonstationary processes, too. Thus we define general fractionally integrated
</p>
<p>processes of order d, in short xt � I.d/.6
</p>
<p>6The use of &lsquo;�&rsquo; with a differing meaning from that one in (5.2) should not be a source for
confusion.</p>
<p/>
</div>
<div class="page"><p/>
<p>114 5 Long Memory and Fractional Integration
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>d=0.45
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>d=0.25
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>d=&minus;0.45
</p>
<p>Fig. 5.5 Simulated fractional noise for d D 0:45; 0:25;�0:45
</p>
<p>Fractionally Integrated ARMA Processes (ARFIMA)
</p>
<p>Since the papers by Granger and Joyeux (1980) and Hosking (1981), it is often
</p>
<p>assumed that fetg is a stationary and invertible ARMA(p, q) process, A.L/et D
B.L/"t, with spectrum
</p>
<p>fe.�/ D
TB .�/
</p>
<p>TA .�/
</p>
<p>�2
</p>
<p>2�
;
</p>
<p>see Corollary 4.1. An ARFIMA(p,d,q) process is defined by replacing "t in (5.8) by
</p>
<p>et, such that
</p>
<p>A.L/�dxt D B.L/"t :
</p>
<p>With the expansion from (5.5) one obtains
</p>
<p>xt D .1 � L/�d et D
1X
</p>
<p>jD0
 jet�j ; t 2 Z ; d &lt; 0:5 : (5.14)</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Generalizations 115
</p>
<p>Under stationarity and invertibility of the ARMA process, the spectrum fe of fetg is
bounded and bounded away from zero everywhere:
</p>
<p>0 &lt; fe .�/ &lt;1; � 2 Œ0; �&#141; : (5.15)
</p>
<p>The results from Propositions 5.1 and 5.2 carry over to the ARFIMA(p,d,q) process,
</p>
<p>see also Brockwell and Davis (1991, Sect. 13.2).
</p>
<p>Proposition 5.3 (ARFIMA) Let the ARMA process with A.L/et D B.L/"t be
stationary and invertible. Then the ARFIMA process with A.L/�dxt D B.L/"t and
�1 &lt; d &lt; 0:5 is stationary, and it holds that
</p>
<p>(a) the spectral density f .�/ is given as
</p>
<p>f .�/ D 4�d sin�2d
�
�
</p>
<p>2
</p>
<p>�
fe.�/ ; � &gt; 0
</p>
<p>� ��2dfe.0/ ; �! 0 I
</p>
<p>(b) the autocovariances satisfy
</p>
<p>&#13;.h/ � &#13;d fe.0/ 2� h2d�1; h ! 1 ; d &curren; 0 ;
</p>
<p>with &#13;d from Proposition 5.1.
</p>
<p>Hosking (1981, Thm. 2) and Brockwell and Davis (1991, Thm. 13.2.2) cover
</p>
<p>only jdj &lt; 0:5, but their proof carries over to �1 &lt; d � �0:5. Further, they state
only &#13;.h/ � C h2d�1 for some constant C &curren; 0; looking at the details of the proof
of Brockwell and Davis (Thm. 13.2.2), however, it turns out that C D &#13;d fe.0/ 2� ,
which of course covers the case of Proposition 5.1, too. In particular, we find again
</p>
<p>that long memory defined by a non-summable autocovariance sequence translates
</p>
<p>into a spectral peak at � D 0. This feature occurs for d &gt; 0.
</p>
<p>Semiparametric Models
</p>
<p>The parametric assumption that fetg is an ARMA process is by no means essential
for Proposition 5.3 to hold. More generally, we now define a stationary process fetg
to be integrated of order 0, et � I.0/, if
</p>
<p>et D
1X
</p>
<p>kD0
bk"t�k ; with
</p>
<p>1X
</p>
<p>kD0
jbkj &lt;1 and
</p>
<p>1X
</p>
<p>kD0
bk &curren; 0 ; (5.16)</p>
<p/>
</div>
<div class="page"><p/>
<p>116 5 Long Memory and Fractional Integration
</p>
<p>where b0 D 1 and f"tg � WN.0; �2/. The absolute summability of fbkg rules out
long memory (or d &gt; 0) in that the autocovariances of fetg are absolutely summable,
see Proposition 3.2; the second condition that the sequence fbkg does not sum up to
zero rules out that fetg is integrated of order d with d &lt; 0, see (5.7). This motivates
to call processes from (5.16) integrated of order 0. Consequently, fxtg from (5.14)
with fetg from (5.16) is called integrated of order d, xt � I.d/. The spectrum of fetg
is of course given by Proposition 4.2, see (4.5). With fe being the spectrum of the
</p>
<p>I.0/ process, Proposition 5.3 continues to hold without changes, provided 0 &lt; d
</p>
<p>(see Giraitis et al., 2012, Prop. 3.1.1).
</p>
<p>A further question is the behavior of the impulse responses of a general I.d/
</p>
<p>process without parametric model: Does the decay rate jd�1 of f jg from��d carry
over? The answer is almost yes, but mild additional assumptions have to be imposed
</p>
<p>on fbkg from (5.16). Denote
</p>
<p>xt D ��det D
1X
</p>
<p>jD0
 jet�j D
</p>
<p>1X
</p>
<p>jD0
cj"t�j;
</p>
<p>where the MA coefficients are given by convolution:
</p>
<p>cj D
jX
</p>
<p>kD0
bk j�k; j � 0:
</p>
<p>Hassler and Kokoszka (2010) prove that a necessary and sufficient condition for
</p>
<p>cj �
P1
</p>
<p>kD0 bk
&#13; .d/
</p>
<p>jd�1; d &gt; 0 ;
</p>
<p>is under long memory
</p>
<p>k1�dbk ! 0; k ! 1: (5.17)
</p>
<p>This is a very weak condition satisfied by all stationary ARMA models and most
</p>
<p>other processes of practical interest. Hassler (2012) proves that this condition
</p>
<p>remains necessary in the case of antipersistence, d &lt; 0, and establishes a mildly
</p>
<p>stronger sufficient condition.
</p>
<p>The statistical literature often refrains from a fractionally integrated model of the
</p>
<p>type xt D .1� L/�det, and directly assumes for the corresponding spectral behavior
in a vicinity of the origin:
</p>
<p>f .�/ � ��2d g .�/ ; �! 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Generalizations 117
</p>
<p>When it comes to estimation of d, technical smoothness restrictions are imposed on
</p>
<p>g, including of course the minimum assumption
</p>
<p>0 &lt; g.0/ &lt;1 ;
</p>
<p>which is required to identify d.
</p>
<p>Nonstationary Processes
</p>
<p>The simplest way to define a process that is integrated of a higher order than d &lt; 0:5
</p>
<p>is as follows. Consider a sequence fytg that has to be differenced once in order to
obtain an I.ı/ process, �yt D xt, xt � I.ı/ or yt D yt�1 C xt. Given a starting value
y0 D 0, the solution of this difference equation for t 2 f1; 2; : : : ; ng is
</p>
<p>yt D
tX
</p>
<p>jD1
xj ; t D 1; 2; : : : ; n :
</p>
<p>Since fytg is given by integration over an I.ı/ process, we say that fytg is integrated
of order d, yt � I.d/, with d D ı C 1. For d � 0:5, i.e. ı � �0:5, the process
fytg is necessarily nonstationary. We illustrate this type of nonstationarity or drift by
means of an example.
</p>
<p>Example 5.2 (Nonstationary Fractional Noise) The middle graph in Fig. 5.6 dis-
</p>
<p>plays a realization of a random walk (d D 1). It drifts off from the zero line for very
long time spans and crosses only a few times. The I.1:45/ process drifts even more
</p>
<p>pronouncedly displaying a much smoother trajectory than the random walk. The
</p>
<p>I.0:55/ process does not drift as strongly, hitting the zero line much more often.
</p>
<p>In fact, comparing the I.0:55/ series with the I.0:45/ case from Fig. 5.5, one can
</p>
<p>imagine that it may be hard to tell apart stationarity and nonstationarity in finite
</p>
<p>samples. �
</p>
<p>The case of ı D 0 or yt � I.1/ is of particular interest in many financial and
economic applications. Hence, one may wish to test whether a process is I.1/ or
</p>
<p>not,
</p>
<p>H0 W d D 1 vs. H1 W d &curren; 1 :
</p>
<p>One method to discriminate more specifically between d D 1 and d D 0 is
the celebrated test by Dickey and Fuller (1979), see Chap. 15. In a fractionally
</p>
<p>integrated framework it is more generally possible to decide e.g. whether a process
</p>
<p>is nonstationary or not,
</p>
<p>H0 W d � 0:5 vs. H1 W d &lt; 0:5 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>118 5 Long Memory and Fractional Integration
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;100
</p>
<p>&minus;50
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>d=1.45
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;20
</p>
<p>&minus;10
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>d=1
</p>
<p>0 20 40 60 80 100 120 140 160 180 200
&minus;5
</p>
<p>0
</p>
<p>5
</p>
<p>d=0.55
</p>
<p>Fig. 5.6 Nonstationary fractional noise for d D 1:45; 1:0; 0:55
</p>
<p>or whether a process has short memory or not,
</p>
<p>H0 W d � 0 vs. H1 W d &gt; 0 :
</p>
<p>Demetrescu, Kuzin, and Hassler (2008) suggested a simple procedure similar to the
</p>
<p>Dickey-Fuller test, to test for arbitrary values d0.
</p>
<p>5.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>5.1 Consider the p-series
PJ
</p>
<p>jD1 j
�p for J ! 1. Show that the limit is finite if and
</p>
<p>only if p &gt; 1, see (5.1).
</p>
<p>5.2 Show that exponential decay is faster than hyperbolic decay, i.e.
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>jd�1
D 0 for 0 &lt; g &lt; 1 ; jdj &lt; 1 :
</p>
<p>5.3 Show (5.4).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Problems and Solutions 119
</p>
<p>Hint: Use properties of the Gamma function
</p>
<p>&#13; .x/ D
</p>
<p>8
&lt;
:
</p>
<p>R1
0
</p>
<p>tx�1e�tdt ; x &gt; 0
1 ; x D 0
</p>
<p>&#13; .x C 1/=x ; x &lt; 0 ; x &curren; �1;�2; : : :
: (5.18)
</p>
<p>5.4 Establish the following expression for the autocovariance &#13;.h/ of fractionally
</p>
<p>integrated noise:
</p>
<p>&#13;.h/ D �2 &#13; .1� 2d/&#13; .d C h/
&#13; .d/&#13; .1 � d/&#13; .1 � d C h/ : (5.19)
</p>
<p>Hint: Use Proposition 4.1 with (5.11), and apply the following identity from
</p>
<p>Gradshteyn and Ryzhik (2000, 3.631.8):
</p>
<p>�Z
</p>
<p>0
</p>
<p>sin��1.x/ cos.ax/dx D
� cos. a�
</p>
<p>2
/&#13; .� C 1/
</p>
<p>2��1� &#13; . �CaC1
2
</p>
<p>/&#13; . ��aC1
2
</p>
<p>/
; where � &gt; 0 :
</p>
<p>5.5 Show Proposition 5.1 (a).
</p>
<p>Hint: Use (5.19).
</p>
<p>5.6 Show Proposition 5.1 (b).
</p>
<p>Hint: Use (5.19).
</p>
<p>5.7 Show Proposition 5.1 (c).
</p>
<p>5.8 Consider the ARFIMA(0,d,1) model�dxt D B.L/ "t, B.L/ D 1Cb L. Show for
this special case that the proportionality constant from Proposition 5.3 (b) is indeed
</p>
<p>&#13;d fe.0/ 2� .
</p>
<p>Solutions
</p>
<p>5.1 We define the p-series of the first J terms,
</p>
<p>SJ. p/ D
JX
</p>
<p>jD1
</p>
<p>1
</p>
<p>jp
:</p>
<p/>
</div>
<div class="page"><p/>
<p>120 5 Long Memory and Fractional Integration
</p>
<p>First, we discuss the case separating the convergence region from the divergent one
</p>
<p>.p D 1/. For convenience choose J D 2n, such that
</p>
<p>S2n.1/ D
�
1C 1
</p>
<p>2
</p>
<p>�
C
�
1
</p>
<p>3
C 1
4
</p>
<p>�
C � � � C
</p>
<p>�
1
</p>
<p>2n�1 C 1 C � � � C
1
</p>
<p>2n
</p>
<p>�
</p>
<p>&gt;
1
</p>
<p>2
C 2
4
C 4
8
C � � � C 2
</p>
<p>n�1
</p>
<p>2n
D n
2
:
</p>
<p>Since the lower bound of S2n.1/ diverges to infinity with n, SJ.1/ must diverge, too.
</p>
<p>Second, consider the case where p &lt; 1, such that jp &lt; j, or j�p &gt; j�1. Hence,
SJ. p/ &gt; SJ.1/, and divergence of SJ.1/ implies divergence for p &lt; 1.
</p>
<p>Third, for p &gt; 1, we group the terms for J D 2n � 1 as follows.
</p>
<p>S2n�1. p/ D 1C
�
1
</p>
<p>2p
C 1
3p
</p>
<p>�
C � � � C
</p>
<p>�
1
</p>
<p>.2n�1/p
C � � � C 1
</p>
<p>.2n � 1/p
�
</p>
<p>&lt; 1C 2
2p
</p>
<p>C 4
4p
</p>
<p>C � � � C 2
n�1
</p>
<p>.2n�1/p
</p>
<p>D 1C 1
2p�1
</p>
<p>C 1
4p�1
</p>
<p>C � � � C 1
.2n�1/p�1
</p>
<p>:
</p>
<p>We now abbreviate g D 1
2p�1
</p>
<p>with 0 &lt; g &lt; 1 since p &gt; 1. Consequently,
</p>
<p>S2n�1. p/ &lt; 1C g C g2 C : : :C gn�1 D
1 � gn
1 � g
</p>
<p>&lt;
</p>
<p>1X
</p>
<p>iD0
gi D 1
</p>
<p>1 � g D
2p�1
</p>
<p>2p�1 � 1;
</p>
<p>where we use the geometric series, see Problem 3.2. Hence, S2n�1. p/ is bounded
for every n, while growing monotonically at the same time, which establishes
</p>
<p>convergence for p &gt; 1. Hence, the proof of (5.1) is complete.
</p>
<p>We want to add a final remark. While convergence is ensured for p &gt; 1, an
</p>
<p>explicit expression for the limit is by no means obvious, and indeed only known
</p>
<p>for selected values. For example, for p D 2 one has the famous result by Leonhard
Euler:
</p>
<p>lim
J!1
</p>
<p>SJ.2/ D
1X
</p>
<p>jD1
</p>
<p>1
</p>
<p>j2
D �
</p>
<p>2
</p>
<p>6
:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Problems and Solutions 121
</p>
<p>5.2 Since the limit of the ratio of interest is indeterminate, we apply L&rsquo;Hospital&rsquo;s
</p>
<p>rule:
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>jd�1
D lim
</p>
<p>j!1
j1�d
</p>
<p>g�j
D lim
</p>
<p>j!1
.1 � d/j�d
</p>
<p>g�j log.g/.�1/
</p>
<p>D d � 1
log.g/
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>jd
D 0 ; d � 0 :
</p>
<p>If �1 &lt; d &lt; 0, the expression g j=jd is still indeterminate. Application of
L&rsquo;Hospital&rsquo;s rule twice, however, yields:
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>jd
D lim
</p>
<p>j!1
j�d
</p>
<p>g�j
D lim
</p>
<p>j!1
�d j�d�1
</p>
<p>g�1 log.g/.�1/
</p>
<p>D d
log.g/
</p>
<p>lim
j!1
</p>
<p>g j
</p>
<p>j1Cd
D 0:
</p>
<p>This establishes the claim.
</p>
<p>5.3 Prior to solving the problem, we review some useful properties of the
</p>
<p>Gamma function that is often employed to simplify manipulations with binomial
</p>
<p>expressions, see e.g. Syds&aelig;ter, Str&oslash;m, and Berck (1999, p.52), and in much greater
</p>
<p>detail Gradshteyn and Ryzhik (2000, Sect. 8.31), or Rudin (1976, Ch. 8) containing
</p>
<p>proofs. For integer numbers, &#13; coincides with the factorial,
</p>
<p>&#13; .n C 1/ D n .n � 1/ � � � 2 D nŠ ;
</p>
<p>which implies a recursive relation holding in fact in general:
</p>
<p>&#13; .x C 1/ D x&#13; .x/ : (5.20)
</p>
<p>Hence, obviously &#13; .1/ D &#13; .2/ D 1, and a further value often encountered is
&#13; .0:5/ D p� . The recursive relation further yields the rate of divergence at the
origin,
</p>
<p>&#13; .x/ � x�1 ; x ! 0 ;
</p>
<p>which justifies the convention&#13; .0/=&#13; .0/ D 1. Finally, we want to approximate the
Gamma function for large arguments. Remember Stirling&rsquo;s formula for factorials,
</p>
<p>nŠ �
p
2� n
</p>
<p>�n
e
</p>
<p>�n
:</p>
<p/>
</div>
<div class="page"><p/>
<p>122 5 Long Memory and Fractional Integration
</p>
<p>It generalizes to
</p>
<p>&#13; .x/ �
r
2�
</p>
<p>x
</p>
<p>�x
e
</p>
<p>�x
</p>
<p>for x ! 1, which again has to be read as
</p>
<p>lim
x!1
</p>
<p>&#13; .x/
</p>
<p>,r
2�
</p>
<p>x
</p>
<p>�x
e
</p>
<p>�x
D 1 :
</p>
<p>Consequently, we have for finite x and y and large integer n that7
</p>
<p>&#13; .n C x/
&#13; .n C y/ � n
</p>
<p>x�y ; n ! 1 : (5.21)
</p>
<p>Now, we turn to establishing (5.4). Repeated application of (5.20) gives
</p>
<p>&#13; . j � d/
&#13; .�d/ D .j � d � 1/ . j � d � 2/ � � � .�d/ :
</p>
<p>By definition of the binomial coefficients we conclude from (5.3) with &#13; .jC1/ D jŠ
that
</p>
<p>�j D
&#13; .j � d/
</p>
<p>&#13; .j C 1/&#13; .�d/ ; j � 0
</p>
<p>� j
�d�1
</p>
<p>&#13; .�d/ ; j ! 1 ;
</p>
<p>where the approximation relies on (5.21). This is the required result.
</p>
<p>5.4 With Proposition 4.1 and (5.11) we compute
</p>
<p>&#13;.h/ D 2
�Z
</p>
<p>0
</p>
<p>f .�/ cos.�h/d�
</p>
<p>D 4
�d
</p>
<p>�
2�2
</p>
<p>�
2Z
</p>
<p>0
</p>
<p>sin�2d.x/ cos.2hx/dx;
</p>
<p>7Use .1C x=n/n ! ex.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.5 Problems and Solutions 123
</p>
<p>where we substituted �
2
D x. Both sin2.x/ and cos.2hx/ are even symmetric around
</p>
<p>�
2
</p>
<p>, such that
</p>
<p>�Z
</p>
<p>0
</p>
<p>sin�2d.x/ cos.2hx/dx D 2
</p>
<p>�
2Z
</p>
<p>0
</p>
<p>sin�2d.x/ cos.2hx/dx:
</p>
<p>Hence, the integration formula 3.631.8. from Gradshteyn and Ryzhik (2000) can be
</p>
<p>applied with � D 1 � 2d &gt; 0 as long as d &lt; 0:5 and a D 2h:
</p>
<p>&#13;.h/ D 4
�d�2
</p>
<p>�
</p>
<p>�Z
</p>
<p>0
</p>
<p>sin�2d.x/ cos.2hx/dx
</p>
<p>D 4
�d�2
</p>
<p>�
</p>
<p>� cos.h�/&#13; .2 � 2d/
2�2d.1 � 2d/&#13; .1 � d C h/&#13; .1� d � h/
</p>
<p>D �2 .�1/
h&#13; .2 � 2d/
</p>
<p>.1 � 2d/&#13; .1� d C h/&#13; .1 � d � h/
</p>
<p>D �2 .�1/
h&#13; .1 � 2d/
</p>
<p>&#13; .1 � d C h/&#13; .1 � d � h/ :
</p>
<p>Using (5.20) once more, one can show that
</p>
<p>&#13; .d C h/
&#13; .d/
</p>
<p>D .�1/h &#13; .1 � d/
&#13; .1 � d � h/ :
</p>
<p>Therefore, we finally have
</p>
<p>&#13;.h/ D �2 &#13; .1 � 2d/&#13; .d C h/
&#13; .d/&#13; .1 � d/&#13; .1 � d C h/ ;
</p>
<p>which is (5.19).
</p>
<p>5.5 With (5.19) we obtain
</p>
<p>&#13;.0I d/ D &#13; .1 � 2d/
.&#13; .1 � d//2 ;
</p>
<p>where we assumed �2 D 1 without loss of generality. Instead of the variance, we
will equivalently minimize the natural logarithm thereof. We determine as derivative
</p>
<p>@ log.&#13;.0I d//
@d
</p>
<p>D �2
�
&#13; 0.1 � 2d/
&#13; .1 � 2d/ �
</p>
<p>&#13; 0.1 � d/
&#13; .1 � d/
</p>
<p>�
</p>
<p>D �2 . .1 � 2d/ �  .1 � d// ;</p>
<p/>
</div>
<div class="page"><p/>
<p>124 5 Long Memory and Fractional Integration
</p>
<p>where the so-called psi function is defined as logarithmic derivative of the Gamma
</p>
<p>function,
</p>
<p> .x/ D @ log.&#13; .x//
@x
</p>
<p>:
</p>
<p>The psi function is strictly increasing for x &gt; 0, which can be shown in different
</p>
<p>ways, see e.g. Gradshteyn and Ryzhik (2000, 8.362.1). Consequently,
</p>
<p> .1 � d/ �  .1 � 2d/
</p>
<p>8
&lt;
:
</p>
<p>&gt; 0 ; 0 &lt; d &lt; 0:5
</p>
<p>D 0 ; d D 0
&lt; 0 ; d &lt; 0
</p>
<p>;
</p>
<p>which proves that log .&#13;.0I d//, and hence &#13;.0I d/, takes on its minimum for d D 0.
This solves the problem.
</p>
<p>5.6 The recursive relative for &#13;.h/ is obvious with (5.19) and (5.20) at hand.
</p>
<p>For h ! 1, the approximation in (5.21) yields
</p>
<p>&#13;.h/ � �2 &#13; .1 � 2d/
&#13; .d/&#13; .1 � d/ h
</p>
<p>2d�1 ;
</p>
<p>which defines the constant from Proposition 5.1 (b):
</p>
<p>&#13;d D
&#13; .1 � 2d/
</p>
<p>&#13; .d/&#13; .1 � d/ :
</p>
<p>The Gamma function is positive for positive arguments and negative on the interval
</p>
<p>.�1; 0/. Hence, the sign of &#13;d equals the sign of &#13; .d/ since d &lt; 0:5, which
completes the proof of Proposition 5.1 (b).
</p>
<p>5.7 In terms of autocorrelations the recursion from Proposition 5.1 (b) becomes
</p>
<p>�.hI d/ D f .hI d/ �.h � 1I d/ ; h � 1 ;
</p>
<p>where the factor f .hI d/ is positive for d &gt; 0,
</p>
<p>f .hI d/ D h � 1C d
h � d &gt; 0 with
</p>
<p>@f .hI d/
@d
</p>
<p>&gt; 0;
</p>
<p>such that �.hI d/ &gt; 0 since �.0I d/ D 1. Hence, we have
</p>
<p>@�.hI d/
@d
</p>
<p>D @f .hI d/
@d
</p>
<p>�.h � 1I d/C f .hI d/ @�.h � 1I d/
@d
</p>
<p>;</p>
<p/>
</div>
<div class="page"><p/>
<p>References 125
</p>
<p>which is positive by induction since
</p>
<p>@�.1I d/
@d
</p>
<p>D @f .1I d/
@d
</p>
<p>&gt; 0:
</p>
<p>Hence, �.hI d/ is growing with d as required.
5.8 The spectral density of the MA(1) component et D "t C b"t�1 is known from
Example 4.3:
</p>
<p>fe.0/ D .1C b/2�2=2�:
</p>
<p>We now express xt in terms of a fractional noise called yt D ��d"t:
</p>
<p>xt D ��dB.L/"t D B.L/��d"t
D .1C bL/ yt :
</p>
<p>Let &#13;x.h/ and &#13;y.h/ denote the autocovariances of fxtg and fytg, respectively. It holds
that
</p>
<p>&#13;x.h/ D E Œ.yt C byt�1/ .ytCh C bytCh�1/&#141;
D .1C b2/&#13;y.h/C b
</p>
<p>�
&#13;y.h � 1/C &#13;y.h C 1/
</p>
<p>�
:
</p>
<p>With the behavior of &#13;y.h/ from Proposition 5.1 it follows
</p>
<p>&#13;x.h/
</p>
<p>h2d�1
! .1C b2/ &#13;d�2 C 2 b &#13;d�2
</p>
<p>D &#13;d fe.0/ 2� ;
</p>
<p>as required.
</p>
<p>References
</p>
<p>Baillie, R. T. (1996). Long memory processes and fractional integration in econometrics. Journal
of Econometrics, 73, 5&ndash;59.
</p>
<p>Bondon, P., &amp; Palma, W. (2007). A class of antipersistent processes. Journal of Time Series
Analysis, 28, 261&ndash;273.
</p>
<p>Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
Springer.
</p>
<p>Demetrescu, M., Kuzin, V., &amp; Hassler, U. (2008). Long memory testing in the time domain.
Econometric Theory, 24, 176&ndash;215.
</p>
<p>Dickey, D. A., &amp; Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series
with a unit root. Journal of the American Statistical Association, 74, 427&ndash;431.
</p>
<p>Giraitis, L., Koul, H. L., &amp; Surgailis, D. (2012). Large sample inference for long memory processes.
London: Imperial College Press.</p>
<p/>
</div>
<div class="page"><p/>
<p>126 5 Long Memory and Fractional Integration
</p>
<p>Gradshteyn, I. S., &amp; Ryzhik, I. M. (2000). Table of integrals, series, and products (6th ed.).
London/San Diego: Academic Press.
</p>
<p>Granger, C. W. J. (1981). Some properties of time series data and their use in econometric model
specification. Journal of Econometrics, 16, 121&ndash;130.
</p>
<p>Granger, C. W. J., &amp; Joyeux, R. (1980). An Introduction to long-memory time series models and
fractional differencing. Journal of Time Series Analysis, 1, 15&ndash;29.
</p>
<p>Hassler, U. (2012). Impulse responses of antipersistent processes. Economics Letters, 116,
454&ndash;456.
</p>
<p>Hassler, U. (2014). Persistence under temporal aggregation and differencing. Economics Letters,
124, 318&ndash;322.
</p>
<p>Hassler, U., &amp; Kokoszka (2010). Impulse responses of fractionally integrated processes with long
memory. Econometric Theory, 26, 1855&ndash;1861.
</p>
<p>Hosking, J. R. M. (1981). Fractional differencing. Biometrika, 68, 165&ndash;176.
Maasoumi, E., &amp; McAleer, M. (2008). Realized volatility and long memory: An overview.
</p>
<p>Econometric Reviews, 27, 1&ndash;9.
Rudin, W. (1976). Principles of mathematical analyis (3rd ed.). New York: McGraw-Hill.
Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
</p>
<p>Berlin/New York: Springer.
Trench, W. F. (2013). Introduction to real analysis. Free Hyperlinked Edition 2.04 December 2013.
</p>
<p>Downloaded on 10th May 2014 from http://digitalcommons.trinity.edu/mono/7.</p>
<p/>
<div class="annotation"><a href="http://digitalcommons.trinity.edu/mono/7">http://digitalcommons.trinity.edu/mono/7</a></div>
</div>
<div class="page"><p/>
<p>6Processes with Autoregressive ConditionalHeteroskedasticity (ARCH)
</p>
<p>6.1 Summary
</p>
<p>In particular in the case of financial time series one often observes a highly
</p>
<p>fluctuating volatility (or variance) of a series: Agitated periods with extreme
</p>
<p>amplitudes alternate with rather quiet periods being characterized by moderate
</p>
<p>observations. After some short preliminary considerations concerning models with
</p>
<p>time-dependent heteroskedasticity, we will discuss the model of autoregressive
</p>
<p>conditional heteroskedasticity (ARCH), for which Robert F. Engle was awarded
</p>
<p>the Nobel prize in the year 2003. After a generalization (GARCH), there will
</p>
<p>be a discussion on extensions relevant for practice. Throughout this chapter,
</p>
<p>the innovations or shocks f"tg stand for a pure random process as defined in
Example 2.7.
</p>
<p>6.2 Time-Dependent Heteroskedasticity
</p>
<p>The heteroskedasticity allowed for here is modeled as time-dependent volatility by1
</p>
<p>xt D �t "t ; "t � iid.0; 1/ ; (6.1)
</p>
<p>1The following equation could be extended by a mean function, e.g. of a regression-type,
</p>
<p>xt D ˛C ˇzt C �t "t ;
</p>
<p>or
</p>
<p>xt D a1xt�1 C � � � C apxt�p C �t "t :
</p>
<p>We restrict our exposition and concentrate on modeling volatility exclusively, although in practice
time-dependent heteroskedasticity is often found with regression errors.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_6
</p>
<p>127</p>
<p/>
</div>
<div class="page"><p/>
<p>128 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>where f�tg is the volatility process being stochastically independent of f"tg by
assumption, and f"tg is a pure random process with unit variance. There exist two
routes to model the volatility process. The first one, often labeled as processes
</p>
<p>with stochastic volatility, assumes an unobserved, or latent, process fhtg behind the
volatility: �t D exp.ht=2/. This implies for the squared data
</p>
<p>x2t D eht"2t or log.x2t / D ht C log."2t / :
</p>
<p>For an early survey on stochastic volatility processes see Taylor (1994). A second
</p>
<p>strand in the literature assumes that �t depends on observed data, in particular
</p>
<p>on past observations xt�j. This class of models has been called autoregressive
conditional heteroskedasticity (ARCH). ARCH processes are widely spread and
</p>
<p>successful in practice and will be the focus of attention in the present chapter.
</p>
<p>Heteroskedasticity as a Function of the Past
</p>
<p>In this chapter the variance function is modeled by the observed past of the process
</p>
<p>itself:
</p>
<p>�2t D f .xt�1; xt�2; � � � / : (6.2)
</p>
<p>By plugging in xt�j from (6.1) one obtains:
</p>
<p>�2t D f .�t�1"t�1; �t�2"t�2; � � � / :
</p>
<p>We will show that the process from (6.1) is a martingale difference sequence.
</p>
<p>Remember the definition of the information set It�1 generated by the past of xt
up to xt�1. Then it holds that2
</p>
<p>E .xt j It�1/ D E .xt j xt�1; xt�2; � � � /
D E .�t"t j xt�1; xt�2; � � � /
D �tE ."t j xt�1; xt�2; � � � /
D �tE ."t/ ;
</p>
<p>as "t is independent of xt�j for j &gt; 0 by construction. With "t being zero on average,
it follows that
</p>
<p>E .xt j It�1/ D 0 ;
</p>
<p>2When conditioning on It�1, one often writes E .� j xt�1; xt�2; � � � / instead of E .� j It�1/.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.2 Time-Dependent Heteroskedasticity 129
</p>
<p>which proves (integrability of fxtg assumed) that fxtg is in fact a martingale
difference sequence, see below (2.11). The variance of the martingale difference
</p>
<p>is determined in the following way (as xt is zero on average,
</p>
<p>Var .xt/ D E
�
x2t
�
</p>
<p>D E
�
�2t "
</p>
<p>2
t
</p>
<p>�
</p>
<p>D E
�
�2t
�
</p>
<p>E
�
"2t
�
</p>
<p>D E
�
�2t
�
;
</p>
<p>as �2t from (6.2) and "t (with variance 1 from (6.1)) are stochastically independent.
</p>
<p>Hence, the following proposition is verified.
</p>
<p>Proposition 6.1 (Heteroskedastic Martingale Differences) Let fxtg be from (6.1)
and f�2t g from (6.2) with E.�2t / &lt;1 independent of f"tg. Then fxtg is a martingale
difference sequence with variance
</p>
<p>Var.xt/ D E.x2t / D E.�2t / :
</p>
<p>Let us remember Proposition 2.2. Due to the martingale difference property it
</p>
<p>holds that
</p>
<p>E.xt/ D 0 and &#13;.h/ D E.xtxtCh/ D 0 ; h &curren; 0 :
</p>
<p>Hence, the process is serially uncorrelated with expectation zero which would be
</p>
<p>supposed e.g. for returns. However, the process is generally not independent over
</p>
<p>time. The (weak) stationarity of fxtg depends on the possibly variable variance; if
the variance Var.xt/ is constant, then the entire process is stationary.
</p>
<p>Heuristics
</p>
<p>Now, the question is how the functional dependence in (6.2) should be specified
</p>
<p>and parameterized. Heteroskedasticity as an empirical phenomenon has been known
</p>
<p>to observers on financial markets for a long time. Before ARCH models were
</p>
<p>introduced, it had been measured by moving a window of width B through the data
</p>
<p>and averaging over the squares:
</p>
<p>s2t D
1
</p>
<p>B
</p>
<p>BX
</p>
<p>iD1
x2t�i :
</p>
<p>For every point in time t one averages over the past preceding B values in order to
</p>
<p>determine the variance in t. In doing so, we do not center xt�i around the arithmetic
mean as we think of returns with E.xt/ D 0 when applying the procedure, cf.</p>
<p/>
</div>
<div class="page"><p/>
<p>130 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>Proposition 6.1. With daily observations (with five trading days a week) one chooses
</p>
<p>e.g. B D 20 which approximately corresponds to a time window of a month. A first
improvement of the time-dependent volatility measurement is obtained by using a
</p>
<p>weighted average where the weights, gi � 0, are not negative:
</p>
<p>s2t D
BX
</p>
<p>iD1
gi x
</p>
<p>2
t�i with
</p>
<p>BX
</p>
<p>iD1
gi D 1: (6.3)
</p>
<p>Example 6.1 (Exponential Smoothing) For the weighting function gi one often uses
</p>
<p>an exponential decay:
</p>
<p>gi D
�i�1
</p>
<p>1C �C : : :C �B�1 with 0 &lt; � &lt; 1:
</p>
<p>Note that the denominator is just defined such that it holds that
PB
</p>
<p>iD1 gi D 1. With
growing B one furthermore obtains
</p>
<p>1C �C : : :C �B�1 D 1 � �
B
</p>
<p>1 � � !
1
</p>
<p>1 � �:
</p>
<p>Inserting the exponentially decaying weights in (6.3), we get the following result
</p>
<p>for B ! 1:
</p>
<p>s2t .�/ D .1 � �/
1X
</p>
<p>iD1
�i�1 x2t�i:
</p>
<p>Now it is an easy exercise to verify the following recursive relation:
</p>
<p>s2t .�/ D .1 � �/ x2t�1 C � s2t�1.�/: (6.4)
</p>
<p>We will call s2t .�/ the exponentially smoothed volatility or variance. In order to be
</p>
<p>able to calculate it for t D 2; : : : ; n, we need a starting value. Typically, s21.�/ D x21
is chosen which leads to s22.�/ D x21. �
</p>
<p>The ARCH and GARCH processes which are subsequently introduced are
</p>
<p>models leading to volatility specifications which generalize s2t and s
2
t .�/ from (6.3)
</p>
<p>and (6.4), respectively.
</p>
<p>6.3 ARCHModels
</p>
<p>So-called autoregressive conditional heteroskedasticity models can be traced back
</p>
<p>to Engle (1982). We consider the case of the order q and specify �2t from (6.2) as
</p>
<p>follows:
</p>
<p>�2t D ˛0 C ˛1 x2t�1 C : : :C ˛q x2t�q ; (6.5)</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 ARCH Models 131
</p>
<p>where it is assumed that
</p>
<p>˛0 &gt; 0 and ˛i � 0 ; i D 1; : : : ; q ; (6.6)
</p>
<p>in order to guarantee �2t &gt; 0. Note that this variance function corresponds to s
2
t
</p>
<p>from (6.3). For ˛1 D � � � D ˛q D 0, however, the case of homoskedasticity is
modeled.
</p>
<p>ConditionalMoments
</p>
<p>Given xt�1; : : : ; xt�q, one naturally obtains zero for the conditional expectation as
ARCH processes are martingale differences, see Proposition 6.1. For the conditional
</p>
<p>variance it holds that
</p>
<p>Var
�
xt j xt�1; : : : ; xt�q
</p>
<p>�
D E
</p>
<p>�
x2t j xt�1; : : : ; xt�q
</p>
<p>�
</p>
<p>D �2t E
�
"2t j xt�1; � � � ; xt�q
</p>
<p>�
</p>
<p>D �2t ;
</p>
<p>as "t is again independent of xt�j and has a unit variance. Hence, for the variance it
conditionally holds that:
</p>
<p>Var.xtjxt�1; : : : ; xt�q/ D ˛0 C ˛1 x2t�1 C : : :C ˛q x2t�q ;
</p>
<p>which explains the name of the models: The conditional variance is modeled
</p>
<p>autoregressively (where &ldquo;autoregressive&rdquo; means in this case: dependent on the past
</p>
<p>of the process). Thus, extreme amplitudes in the previous period are followed
</p>
<p>by high volatility in the present period resulting in so-called volatility clusters.
</p>
<p>If the assumption of normality of the innovations is added, then the conditional
</p>
<p>distribution of xt given the past is normal as well:
</p>
<p>xt j xt�1; : : : xt�q � N
�
0; ˛0 C ˛1 x2t�1 C : : :C ˛q x2t�q
</p>
<p>�
:
</p>
<p>But, although the original work by Engle (1982) assumed "t � iiN .0; 1/, the
assumption of normality is not crucial for ARCH effects.
</p>
<p>Stationarity
</p>
<p>By Proposition 6.1 it holds for the variance that
</p>
<p>Var.xt/ D E.�2t / D ˛0 C ˛1E.x2t�1/C � � � C ˛qE.x2t�q/ :
</p>
<p>If the process is stationary, Var.xt/ D Var.xt�j/, j D 1; : : : ; q, then its variance
results as
</p>
<p>Var.xt/ D
˛0
</p>
<p>1 � ˛1 � : : : � ˛q
:</p>
<p/>
</div>
<div class="page"><p/>
<p>132 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>For a positive variance expression, this requires necessarily (due to ˛0 &gt; 0):
</p>
<p>1 � ˛1 � : : : � ˛q &gt; 0 :
</p>
<p>In fact, this condition is sufficient for stationarity as well. In Problem 6.1 we
</p>
<p>therefore show the following result.
</p>
<p>Proposition 6.2 (Stationary ARCH)
</p>
<p>Let fxtg be from (6.1) and f�2t g from (6.5) with (6.6). The process is weakly
stationary if and only if it holds that
</p>
<p>qX
</p>
<p>jD1
˛j &lt; 1 :
</p>
<p>Correlation of the Squares
</p>
<p>We define et D x2t ��2t . Due to Proposition 6.1 the expected value is zero: E.et/ D 0.
Adding x2t to both sides of (6.5), one immediately obtains
</p>
<p>x2t D ˛0 C ˛1 x2t�1 C : : :C ˛q x2t�q C et :
</p>
<p>From this we learn that an ARCH(q) process implies an autoregressive structure
</p>
<p>for the squares fx2t g. The serial dependence of an ARCH process originates from
the squares of the process. Because of ˛i � 0, x2t and x2t�i are positively correlated
which again allows to capture volatility clusters.
</p>
<p>In Figs. 6.1 and 6.2 ARCH(1) time series of the length 500 are simulated. For
</p>
<p>this purpose pseudo-random numbers f"tg are generated as normally distributed and
˛0 D 1 is chosen. The effect of ˛1 is now quite obvious. The larger the value
of this parameter, the more obvious are the volatility clusters. Long periods with
</p>
<p>little movement are followed by shorter periods of vehement, extreme amplitudes
</p>
<p>which can be negative as well as positive. These volatility clusters become even
</p>
<p>more obvious in the respective lower panel of the figures, in which the squared
</p>
<p>observations fx2t g are depicted. Because of the positive autocorrelation of the
squares, small amplitudes tend again to be followed by small ones while extreme
</p>
<p>observations appear to follow each other.
</p>
<p>Skewness and Kurtosis
</p>
<p>In the first section of the chapter on basic concepts from probability theory we have
</p>
<p>defined the kurtosis by means of the fourth moment of a random variable and we
</p>
<p>have denoted the corresponding coefficient by &#13;2. For &#13;2 &gt; 3 the density function
</p>
<p>is more &ldquo;peaked&rdquo; than the one of the normal distribution: On the one hand the</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 ARCH Models 133
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
6
</p>
<p>&minus;
2
</p>
<p>2
4
</p>
<p>observations (alpha1 = 0.5)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
1
</p>
<p>0
2
</p>
<p>0
3
</p>
<p>0
</p>
<p>squared observations
</p>
<p>Fig. 6.1 ARCH(1) with ˛0 D 1 and ˛1 D 0:5
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
1
</p>
<p>0
0
</p>
<p>1
0
</p>
<p>2
0
</p>
<p>observations (alpha1 = 0.9)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>2
5
</p>
<p>0
</p>
<p>squared observations
</p>
<p>Fig. 6.2 ARCH(1) with ˛0 D 1 and ˛1 D 0:9
</p>
<p>values are more concentrated around the expected value, on the other hand there
</p>
<p>occur extreme observations in the tail of the distribution with higher probability</p>
<p/>
</div>
<div class="page"><p/>
<p>134 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>(&ldquo;fat-tailed and highly peaked&rdquo;). For stationary ARCH processes with Gaussian
</p>
<p>innovations ("t � iiN .0; 1/) it holds that the kurtosis exceeds 3 (provided it exists
at all):
</p>
<p>&#13;2 &gt; 3 :
</p>
<p>The corresponding derivation can be found in Problem 6.2. Due to this excess
</p>
<p>kurtosis, ARCH is generally incompatible with the assumption of an unconditional
</p>
<p>Gaussian distribution.
</p>
<p>We define the skewness coefficient &#13;1 similarly to the kurtosis by the third
</p>
<p>moment of a standardized random variable. The skewness coefficient of ARCH
</p>
<p>models depends on the symmetry of "t. If this innovation is symmetric, then it
</p>
<p>follows that E."3t / D 0. Hence, &#13;1 D 0 follows for the corresponding ARCH process
(due to independence of �t and "t):
</p>
<p>E.x3t / D E.�3t / � E."3t /
D E.�3t / � 0 D 0:
</p>
<p>Thereby it was only used that "t is symmetrically distributed.
</p>
<p>Example 6.2 (ARCH(1)) In particular for a stationary ARCH(1) process with
</p>
<p>˛21 &lt;
1
3
</p>
<p>and Gaussian innovations "t the kurtosis is finite and it results as (see
</p>
<p>Problem 6.3):
</p>
<p>&#13;2 D 3
1 � ˛21
1 � 3 ˛21
</p>
<p>&gt; 3 :
</p>
<p>For ˛21 � 13 there occur extreme observations with a high probability such that the
kurtosis is no longer constant. Consider a stationary ARCH(1) process (˛1 &lt; 1) with
</p>
<p>˛21 D 1=3. Under this condition one has for�4;t D E.x4t /with E.x2t�1/ D ˛0=.1�˛1/
assuming Gaussianity:
</p>
<p>�4;t D E."4t /E.�4t / D 3E
�
.˛0 C ˛1x2t�1/2
</p>
<p>�
</p>
<p>D 3
�
˛20 C 2
</p>
<p>˛20˛1
</p>
<p>1 � ˛1
C ˛21�4;t�1
</p>
<p>�
D c C 3 ˛21�4;t�1 D c C �4;t�1 ;
</p>
<p>where the constant c is appropriately defined. Continued substitution yields thus
</p>
<p>�4;t D c t C �4;0 :
</p>
<p>Hence, we observe that the kurtosis grows linearly over time if 3 ˛21 D 1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Generalizations 135
</p>
<p>6.4 Generalizations
</p>
<p>Some extensions of the ARCH model having originated from empirical features of
</p>
<p>financial data will be covered in the following.
</p>
<p>GARCH
</p>
<p>In practice, with many financial series it can be observed that the correlation of
</p>
<p>the squares reaches far into the past. Therefore, for an adequate modeling a large
</p>
<p>q is needed, i.e. a large number of parameters. A very economical parametrization,
</p>
<p>however, is allowed for by the GARCH model.
</p>
<p>Generalized ARCH processes of the order p and q were introduced by Bollerslev
</p>
<p>(1986) and are defined by their volatility function
</p>
<p>�2t D ˛0 C ˛1 x2t�1 C � � � C ˛q x2t�q C ˇ1 �2t�1 C � � � C ˇp �2t�p: (6.7)
</p>
<p>The result process is abbreviated as GARCH(p; q). In addition to the parameter
</p>
<p>restrictions from (6.6) it is required that
</p>
<p>ˇi � 0 ; i D 1; : : : ; p : (6.8)
</p>
<p>Jointly, these restrictions are clearly sufficient for �2t &gt; 0 but stricter than necessary.
</p>
<p>Substantially weaker assumptions were derived by Nelson and Cao (1992).
</p>
<p>We adopt the stationarity conditions for GARCH models from Bollerslev (1986,
</p>
<p>Theorem 1). The resulting variance will be determined in Problem 6.4. Thus, we
</p>
<p>obtain the following results.
</p>
<p>Proposition 6.3 (Stationary GARCH)
</p>
<p>Let fxtg be from (6.1) and f�2t g from (6.7) with (6.6) and (6.8). The process is
weakly stationary if and only if
</p>
<p>qX
</p>
<p>jD1
˛j C
</p>
<p>pX
</p>
<p>jD1
ˇj &lt; 1 :
</p>
<p>Then it holds for the variance that
</p>
<p>Var.xt/ D
˛0
</p>
<p>1 �
Pq
</p>
<p>jD1 ˛j �
Pp
</p>
<p>jD1 ˇj
:</p>
<p/>
</div>
<div class="page"><p/>
<p>136 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>It can be shown that the stationary GARCH process can be considered as an
</p>
<p>ARCH(1) process. Under the conditions from Proposition 6.3 it holds that (see
Problem 6.5):
</p>
<p>�2t D &#13;0 C
1X
</p>
<p>iD1
&#13;i x
</p>
<p>2
t�i with &#13;i � 0 and
</p>
<p>1X
</p>
<p>iD1
j&#13;ij &lt;1 : (6.9)
</p>
<p>Thus, the GARCH process allows for modeling an infinitely long dependence of the
</p>
<p>volatility on the past of the process itself with only p C q parameters although this
dependence decays with time (i.e. &#13;i ! 0 for i ! 1). The fact that GARCH can be
considered as ARCH(1) has the nice consequence that results for stationary ARCH
processes also hold for GARCH models. In particular, GARCH models are again
</p>
<p>special cases of processes with volatility (6.2) and therefore examples of martingale
</p>
<p>differences, i.e. Proposition 6.1 holds true. If we assume a Gaussian distribution of
</p>
<p>f"tg, it follows, just as for the ARCH(q) process of finite order, that the skewness is
zero and that the kurtosis exceeds the value 3.
</p>
<p>Example 6.3 (GARCH(1,1)) Consider the GARCH(1,1) case more explicitly. It
</p>
<p>is by far the most frequently used GARCH specification in practice. Continued
</p>
<p>substitution shows under the assumption of stationarity that .˛1 C ˇ1 &lt; 1/:
</p>
<p>�2t D
˛0
</p>
<p>1 � ˇ1
C ˛1
</p>
<p>1X
</p>
<p>iD1
ˇi�11 x
</p>
<p>2
t�i :
</p>
<p>Hence, we have an explicit ARCH(1) representation of GARCH(1,1). Assuming
that
</p>
<p>1 � .˛1 C ˇ1/2 � 2˛21 &gt; 0;
</p>
<p>the kurtosis is defined. In Problem 6.6 we show (with Gaussian distribution of f"t}):
</p>
<p>&#13;2 D 3
1 � .˛1 C ˇ1/2
</p>
<p>1 � .˛1 C ˇ1/2 � 2˛21
&gt; 3:
</p>
<p>Furthermore one shows by
</p>
<p>�2t D ˛0 C ˛1 x2t�1 C ˇ1 �2t�1
&rdquo;
</p>
<p>x2t D ˛0 C .˛1 C ˇ1/ x2t�1 � ˇ1
�
x2t�1 � �2t�1
</p>
<p>�
C x2t � �2t
</p>
<p>the equation
</p>
<p>x2t D ˛0 C .˛1 C ˇ1/ x2t�1 C et � ˇ1 et�1</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Generalizations 137
</p>
<p>with
</p>
<p>et D x2t � �2t ; E.et/ D 0 :
</p>
<p>The GARCH(1,1) process fxtg therefore corresponds to an ARMA(1,1) structure of
the squares fx2t g. �
</p>
<p>In Figs. 6.3 and 6.4 the influence of the sum of the parameters ˛1 C ˇ1 is
illustrated by means of simulated GARCH(1,1) observations. We therefore fix
</p>
<p>˛0 D 1 and ˛1 D 0:3 and vary ˇ1 in such a way that stationarity is ensured.
The larger ˇ1 (and therefore the sum of ˛1 C ˇ1), the more pronounced is the
change from quiet periods with little or, in absolute value, moderate amplitudes to
</p>
<p>excited periods in which extreme amplitudes follow each other. Again, this pattern
</p>
<p>of volatility becomes particularly apparent with the serially correlated squares in the
</p>
<p>lower panel, respectively.
</p>
<p>IGARCH
</p>
<p>Considering the volatility of GARCH(1,1),
</p>
<p>�2t D ˛0 C ˛1 x2t�1 C ˇ1 �2t�1;
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
4
</p>
<p>0
2
</p>
<p>4
</p>
<p>observations (alpha1 = 0.3, beta1 = 0.3)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
5
</p>
<p>1
5
</p>
<p>2
5
</p>
<p>squared observations
</p>
<p>Fig. 6.3 GARCH(1,1) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:3</p>
<p/>
</div>
<div class="page"><p/>
<p>138 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
1
</p>
<p>0
&minus;
</p>
<p>5
0
</p>
<p>5
</p>
<p>observations (alpha1 = 0.3, beta1 = 0.5)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
2
</p>
<p>0
6
</p>
<p>0
</p>
<p>squared observations
</p>
<p>Fig. 6.4 GARCH(1,1) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:5
</p>
<p>we are reminded of s2t .˛/ from (6.4). The difference being that ˛0 D 0, and it holds
that ˛1 C ˇ1 D 1 (i.e. ˛1 D 1 � � and ˇ1 D �, respectively). Models with such a
restriction violate the stationarity condition (˛1Cˇ1 &lt; 1). This can be shown when
forming the expected value of �2t with x
</p>
<p>2
t�1 D �2t�1 "2t�1:
</p>
<p>E.�2t / D ˛0 C ˛1 E.�2t�1/E."2t�1/C ˇ1E.�2t�1/
D ˛0 C .˛1 C ˇ1/E.�2t�1/:
</p>
<p>With ˛1 C ˇ1 D 1 one obtains
</p>
<p>E.�2t � �2t�1/ D ˛0 &gt; 0:
</p>
<p>In other words: The expectations for the increments of the volatility are positive for
</p>
<p>every point in time, modeling a volatility expectation which tends to infinity with t.
</p>
<p>This idea was generalized in literature. With
</p>
<p>qX
</p>
<p>jD1
˛j C
</p>
<p>pX
</p>
<p>jD1
ˇj D 1
</p>
<p>one talks about integrated GARCH processes (IGARCH) since Engle and Bollerslev
</p>
<p>(1986). This is a naming which becomes more understandable in the chapter on
</p>
<p>integrated processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Generalizations 139
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
4
</p>
<p>0
0
</p>
<p>2
0
</p>
<p>4
0
</p>
<p>observations (alpha1 = 0.3, beta1 = 0.7)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
5
</p>
<p>0
0
</p>
<p>1
5
</p>
<p>0
0
</p>
<p>squared observations
</p>
<p>Fig. 6.5 IGARCH(1,1) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:7
</p>
<p>In Fig. 6.5, an IGARCH(1,1) process (˛1 C ˇ1 D 1) was simulated according
to the scheme from Figs. 6.3 and 6.4. In comparison to the previous figures, in this
</p>
<p>case we find considerably more extreme volatility clusters which, however, are not
</p>
<p>exaggerated. The kind of depicted dynamics in Fig. 6.5 can be frequently observed
</p>
<p>in financial practice.
</p>
<p>GARCH-M
</p>
<p>We talk about &ldquo;GARCH in mean&rdquo;3 (GARCH-M) if the volatility term influences the
</p>
<p>(mean) level of the process. In order to explain this with regard to contents, we think
</p>
<p>of risk premia: For a high volatility of an investment (high-risk), a higher return is
</p>
<p>expected, on average. In equation form we write this down as follows:
</p>
<p>xt D � �t C ut ; (6.10)
</p>
<p>where futg is a GARCH process:
</p>
<p>ut D �t "t ; �2t D ˛0C˛1 u2t�1C� � �C˛q u2t�qCˇ1 �2t�1C: : :Cˇp �2t�p : (6.11)
</p>
<p>3Originally, the ARCH-M model was proposed by Engle, Lilien, and Robins (1987).</p>
<p/>
</div>
<div class="page"><p/>
<p>140 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
5
</p>
<p>0
5
</p>
<p>1
0
</p>
<p>observations (theta = 1)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
4
</p>
<p>0
8
</p>
<p>0
1
</p>
<p>2
0
</p>
<p>squared observations
</p>
<p>Fig. 6.6 GARCH(1,1)-M from (6.11) with ˛0 D 1, ˛1 D 0:3 and ˇ1 D 0:5
</p>
<p>Therefore, in this case the mean function�t is set to � �t. In some applications it has
</p>
<p>proved successful to model the risk premium as a multiple of the variance instead
</p>
<p>of modeling it by the standard deviation (� �t):
</p>
<p>xt D � �2t C ut :
</p>
<p>For both mean functions the GARCH-M process fxtg is no longer free from serial
correlation for � &gt; 0; it is no longer a martingale difference sequence.
</p>
<p>In Fig. 6.6 a GARCH-M series was generated as in (6.11). The volatility cluster
</p>
<p>can be well identified in the lower panel of the squares.The effect of � D 1 becomes
apparent in the upper panel: In the series of xt local, reversing trends can be spotted.
</p>
<p>Upward trends involve a high volatility, whereas quiet periods are marked by a
</p>
<p>decreasing or lower level.
</p>
<p>EGARCH
</p>
<p>We talk about exponential GARCH when the volatility is modeled as an exponential
</p>
<p>function of the past squares x2t�i. This suggestion originates from Nelson (1991) and
was made in order to capture the asymmetries in the volatility.4 It is observed that
</p>
<p>decreasing stock prices (negative returns) tend to involve higher volatilities than
</p>
<p>4We do not exactly present Nelson&rsquo;s model but a slightly modified implementation which is used
in the software package EViews.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Generalizations 141
</p>
<p>increasing ones. This so-called leverage effect is not captured by ordinary GARCH
</p>
<p>models.
</p>
<p>In EViews the variance for EGARCH is calculated as follows:
</p>
<p>log �2t D ! C
pX
</p>
<p>jD1
ˇj log �
</p>
<p>2
t�j C
</p>
<p>qX
</p>
<p>jD1
</p>
<p>�
˛jj"t�jj C &#13;j "t�j
</p>
<p>�
;
</p>
<p>where "t�j is defined as
</p>
<p>"t�j D
xt�j
�t�j
</p>
<p>:
</p>
<p>For &#13;j D 0 the sign is not an issue. However, for &#13;j &lt; 0 in the negative case
</p>
<p>˛jj"t�jj C &#13;j "t�j D .˛j � &#13;j/j"t�jj for "t�j &lt; 0
</p>
<p>has a stronger effect on log �2t than in the positive case
</p>
<p>˛jj"t�jj C &#13;j "t�j D .˛j C &#13;j/"t�j for "t�j &gt; 0:
</p>
<p>Note that for EGARCH the expression �2t is without parameter restrictions always
</p>
<p>positive by construction. Applying the exponential function, it results that
</p>
<p>�2t D exp
</p>
<p>2
4! C
</p>
<p>pX
</p>
<p>jD1
ˇj log �
</p>
<p>2
t�j C
</p>
<p>qX
</p>
<p>jD1
</p>
<p>�
˛jj"t�jj C &#13;j "t�j
</p>
<p>�
3
5 :
</p>
<p>Example 6.4 (EGARCH (1,1)) Again, as special case we treat the situation with
</p>
<p>p D q D 1,
</p>
<p>log �2t D ! C ˇ1 log �2t�1 C ˛1j"t�1j C &#13;1 "t�1 ;
</p>
<p>or after applying the exponential function:
</p>
<p>�2t D e!�
2ˇ1
t�1 � exp .˛1j"t�1j C &#13;1 "t�1/
</p>
<p>D e!�2ˇ1t�1 �
�
</p>
<p>exp .j"t�1j.˛1 � &#13;1// ; "t�1 � 0
exp ."t�1.˛1 C &#13;1// ; "t�1 � 0
</p>
<p>:
</p>
<p>In this case it is again shown that for &#13;1 &lt; 0 the leverage effect is modeled in
</p>
<p>such a way that negative observations have a larger volatility effect than positive
</p>
<p>observations of the same absolute value. �
</p>
<p>In Fig. 6.7 a realization of a simulated EGARCH(1,1) process is depicted. The
</p>
<p>&ldquo;leverage parameter&rdquo; is &#13;1 D �0:5. When the graphs of the squared and the original
observations are compared, it can be detected that the most extreme amplitudes are</p>
<p/>
</div>
<div class="page"><p/>
<p>142 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>0 100 200 300 400 500
</p>
<p>&minus;
1
</p>
<p>5
&minus;
</p>
<p>5
5
</p>
<p>observations (gamma = &minus;0.5)
</p>
<p>0 100 200 300 400 500
</p>
<p>0
1
</p>
<p>0
0
</p>
<p>2
0
</p>
<p>0
3
</p>
<p>0
0
</p>
<p>squared observations
</p>
<p>Fig. 6.7 EGARCH(1,1) with ! D 1, ˛1 D 0:3, ˇ1 D 0:5 and &#13;1 D �0:5
</p>
<p>in fact negative. Furthermore, it can be observed that periods with predominantly
</p>
<p>negative values are characterized by a high volatility.
</p>
<p>YAARCH
</p>
<p>The works of Engle (1982) and Bollerslev (1986) have set the stage for a downright
</p>
<p>ARCH industry. A large number of generalizations and extensions has been
</p>
<p>published and applied in practice. Most of these versions were published under more
</p>
<p>or less appealing acronyms. When Engle (2002) balanced the books after 20 years
</p>
<p>of ARCH, he added with some irony another acronym: YAARCH standing for Yet
</p>
<p>Another ARCH. There is no end in sight for this literature.
</p>
<p>6.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>6.1 Prove Proposition 6.2.
</p>
<p>Hint: According to Engle (1982, Theorem 2) the process is stationary if and only if
</p>
<p>it holds that
</p>
<p>˛.z/ D 0 ) jzj &gt; 1 ; (6.12)
</p>
<p>with ˛.z/ WD 1 � ˛1 z � : : : � ˛q zq.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Problems and Solutions 143
</p>
<p>6.2 Show that the kurtosis &#13;2 of an ARCH process exceeds the value 3. Assume a
</p>
<p>Gaussian distribution of f"tg and E.�4t / &lt;1.
</p>
<p>6.3 Calculate the kurtosis of a stationary ARCH(1) process as given in Example 6.2
</p>
<p>for the case that it exists. Assume a Gaussian distribution of f"tg.
</p>
<p>6.4 Assume fxtg to be a stationary GARCH process. Determine the variance
expression from Proposition 6.3.
</p>
<p>6.5 Assume fxtg to be a stationary GARCH process with (6.6) and (6.8). Determine
the ARCH(1) representation from (6.9).
</p>
<p>6.6 Calculate the kurtosis of a stationary GARCH(1,1) process as given in Exam-
</p>
<p>ple 6.3 for the case that it exists. Assume a Gaussian distribution of f"tg.
</p>
<p>Solutions
</p>
<p>6.1 We have to show the equivalence of (6.12) and the condition from Proposi-
</p>
<p>tion 6.2, given (6.6). This condition can also be written as ˛.1/ &gt; 0. Hence, we
</p>
<p>have to prove the equivalence:
</p>
<p>.6.12/ &rdquo; ˛.1/ &gt; 0:
</p>
<p>We proceed in two steps.
</p>
<p>&ldquo;)&rdquo;: Under the condition by Engle (1982) it holds that
</p>
<p>Var.xt/ D
˛0
</p>
<p>1 � ˛1 � : : : � ˛q
D ˛0
˛.1/
</p>
<p>� 0 :
</p>
<p>Due to ˛0 &gt; 0 it immediately follows that ˛.1/ � 0. The case ˛.1/ D 0, however,
is due to (6.12) excluded, such that ˛.1/ &gt; 0 can be concluded.
</p>
<p>&ldquo;(&rdquo;: For a root z of ˛.z/ it holds that:
</p>
<p>1 D
qX
</p>
<p>jD1
˛j z
</p>
<p>j :
</p>
<p>By the triangle inequality and applying (6.6) it follows that:
</p>
<p>1 �
qX
</p>
<p>jD1
</p>
<p>ˇ̌
˛j z
</p>
<p>j
ˇ̌
D
</p>
<p>qX
</p>
<p>jD1
˛j jz jj � max
</p>
<p>j
jz jj
</p>
<p>qX
</p>
<p>jD1
˛j
</p>
<p>D max
j
</p>
<p>jz jj .1 � ˛.1//
</p>
<p>&lt; max
j
</p>
<p>jz jj ;</p>
<p/>
</div>
<div class="page"><p/>
<p>144 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>where the assumption was used for the last inequality. Therefore, we have shown
</p>
<p>for a root z of ˛.z/ that it holds that maxj jzjj &gt; 1 and thus jzj &gt; 1.
Hence, the proof is completed.
</p>
<p>6.2 From Example 2.4 we adopt due to the Gaussian distribution
</p>
<p>E."4t / D E
 �
</p>
<p>"t � 0
1
</p>
<p>�4!
D 3 :
</p>
<p>Therefore, in a first step it follows due to the independence of �t and "t:
</p>
<p>E.x4t / D E
�
�4t "
</p>
<p>4
t
</p>
<p>�
D E
</p>
<p>�
�4t
�
</p>
<p>E
�
"4t
�
D 3E
</p>
<p>�
�4t
�
:
</p>
<p>Hence, because of E.xt/ D 0 and Proposition 6.1 the kurtosis of xt results as:
</p>
<p>&#13;2 D
E.x4t /
</p>
<p>.Var.xt//
2
D
</p>
<p>3E
�
�4t
�
</p>
<p>�
E
�
�2t
��2 :
</p>
<p>The usual variance decomposition, see Eq. (2.1),
</p>
<p>Var
�
�2t
�
D E
</p>
<p>�
�4t
�
�
�
E
�
�2t
��2 � 0 ;
</p>
<p>yields
</p>
<p>E
�
�4t
�
</p>
<p>�
E
�
�2t
��2 � 1 :
</p>
<p>Hence, the claim is verified: &#13;2 � 3.
6.3 As �t and "t are stochastically independent, it holds that
</p>
<p>E.xkt / D E.�kt /E."kt /;
</p>
<p>whereby the k-th central moment is given, as fxtg is a martingale difference sequence
with zero expectation. On the assumption of a standard normally distributed random
</p>
<p>process one obtains
</p>
<p>E."2t / D 1; E."3t / D 0 and E."4t / D 3:
</p>
<p>This implies for the ARCH(1) process that the skewness is zero due to E.x3t / D 0:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Problems and Solutions 145
</p>
<p>In order to determine the kurtosis, we first observe that the fourth moment is
</p>
<p>constant under the condition 3˛21 &lt; 1. To that end define �4;t and use �2 D
Var.xt/ D ˛01�˛1 :
</p>
<p>�4;t D E.x4t / D E."4t /E.�4t / D 3E
�
.˛0 C ˛1x2t�1/2
</p>
<p>�
</p>
<p>D 3
�
˛20 C 2
</p>
<p>˛20˛1
</p>
<p>1 � ˛1
C ˛21�4;t�1
</p>
<p>�
D c C 3 ˛21�4;t�1 ;
</p>
<p>where the constant c is defined appropriately. Infinite substitution yields
</p>
<p>�4;t D c
�
1C 3˛21 C .3˛21/2 C � � �
</p>
<p>�
D c
1 � 3˛21
</p>
<p>D �4 :
</p>
<p>With a constant �4 (and �2) one obtains
</p>
<p>�4 D E.x4t / D 3E.�4t / D 3E.˛20 C 2˛0 ˛1 x2t�1 C ˛21 x4t�1/
D 3Œ˛20 C 2˛0 ˛1 �2 C ˛21 �4&#141;
</p>
<p>or
</p>
<p>�4 D
3
</p>
<p>1 � 3 ˛21
</p>
<p>�
˛20 C
</p>
<p>2˛20 ˛1
</p>
<p>1 � ˛1
</p>
<p>�
</p>
<p>D 3
1 � 3 ˛21
</p>
<p>˛20.1C ˛1/
1 � ˛1
</p>
<p>:
</p>
<p>From this it follows that
</p>
<p>&#13;2 D
�4
</p>
<p>.Var.xt//2
D 3
1 � 3 ˛21
</p>
<p>.1 � ˛1/.1C ˛1/
</p>
<p>D 3 1 � ˛
2
1
</p>
<p>1� 3 ˛21
:
</p>
<p>Of course, these transformations were only possible for 1� 3˛21 &gt; 0. Hence, this is
the condition for a finite, constant kurtosis.
</p>
<p>6.4 We use the fact that �t from (6.7) is again independent of "t. This can be shown
</p>
<p>by substitution of �2t�j and x
2
t�i according to (6.1). Thus, as in Proposition 6.1, for
</p>
<p>stationarity and for arbitrary points in time, it holds that:
</p>
<p>Var.xt/ D E
�
�2t
�
D &#13;.0/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>146 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>Hence, by forming the expected value we obtain from (6.7):
</p>
<p>&#13;.0/ D ˛0 C ˛1 &#13;.0/C : : :C ˛q &#13;.0/C ˇ1 &#13;.0/C : : :C ˇp &#13;.0/ :
</p>
<p>Therefore, we can solve
</p>
<p>&#13;.0/ D ˛0
</p>
<p>1 �
qP
</p>
<p>jD1
˛j �
</p>
<p>pP
jD1
</p>
<p>ˇj
</p>
<p>;
</p>
<p>as claimed.
</p>
<p>6.5 We define the lag polynomial ˇ.L/ with
</p>
<p>ˇ.L/ D 1 � ˇ1 L � : : : � ˇp Lp :
</p>
<p>Hence, it holds that
</p>
<p>ˇ.L/ �2t D ˛0 C ˛1 x2t�1 C : : :C ˛q x2t�q :
</p>
<p>By assumption
</p>
<p>ˇj � 0 ; j D 1; : : : ; p ; and ˇ.1/ &gt; 0 :
</p>
<p>In Problem 6.1 we have shown that this is equivalent to
</p>
<p>ˇ.z/ D 0 ) jzj &gt; 1 :
</p>
<p>This is in turn the condition of invertibility known from Proposition 3.3 which
</p>
<p>guarantees a causal, absolutely summable series expansion with coefficients fcjg:
</p>
<p>1
</p>
<p>ˇ.L/
D
</p>
<p>1X
</p>
<p>jD0
cj L
</p>
<p>j ;
</p>
<p>1X
</p>
<p>jD0
jcjj &lt;1 :
</p>
<p>By comparison of coefficients one obtains from
</p>
<p>1 D
�
1 � ˇ1L � : : : � ˇpLp
</p>
<p>� 1X
</p>
<p>jD0
cjL
</p>
<p>j
</p>
<p>as usual
</p>
<p>c0 D 1
c1 D ˇ1 c0 D ˇ1 � 0</p>
<p/>
</div>
<div class="page"><p/>
<p>6.5 Problems and Solutions 147
</p>
<p>c2 D ˇ1 c1 C ˇ2 c0 D ˇ21 C ˇ2 � 0
:::
</p>
<p>cj D ˇ1 cj�1 C : : :C ˇp cj�p � 0 ; j � p :
</p>
<p>Thus, the inversion of ˇ.L/ yields:
</p>
<p>�2t D
˛0
</p>
<p>ˇ.1/
C
˛1 x
</p>
<p>2
t�1 C : : :C ˛q x2t�q
</p>
<p>ˇ.L/
</p>
<p>D &#13;0 C
1X
</p>
<p>iD1
&#13;i x
</p>
<p>2
t�i ;
</p>
<p>where &#13;i, i &gt; 0, results by convolution of
</p>
<p>˛1 L C : : :C ˛q Lq
ˇ.L/
</p>
<p>D
qX
</p>
<p>kD1
˛k L
</p>
<p>k
</p>
<p>1X
</p>
<p>jD1
cj L
</p>
<p>j:
</p>
<p>The non-negativity and summability of f˛kg and fcjg is conveyed to the series f&#13;ig.
Hence, the proof is complete.
</p>
<p>6.6 As for the ARCH(1) case it holds that
</p>
<p>�4 D E
�
x4t
�
D 3E
</p>
<p>�
�4t
�
:
</p>
<p>Applying E
�
x2t
�
D E
</p>
<p>�
�2t
�
D ˛0
</p>
<p>1�˛1�ˇ1 yields:
</p>
<p>E
�
�4t
�
D E
</p>
<p>��
˛0 C ˛1 x2t�1 C ˇ1 �2t�1
</p>
<p>�2�
</p>
<p>D E
�
˛20 C ˛21 x4t�1 C ˇ21 �4t�1
</p>
<p>�
</p>
<p>CE
�
2 ˛0 ˛1 x
</p>
<p>2
t�1 C 2 ˛0 ˇ1 �2t�1 C 2 ˛1 ˇ1 x2t�1 �2t�1
</p>
<p>�
</p>
<p>D ˛20 C 3 ˛21 E
�
�4t�1
</p>
<p>�
C ˇ21 E
</p>
<p>�
�4t�1
</p>
<p>�
</p>
<p>C 2 ˛
2
0 ˛1
</p>
<p>1 � ˛1 � ˇ1
C 2 ˛
</p>
<p>2
0 ˇ1
</p>
<p>1 � ˛1 � ˇ1
C 2 ˛1 ˇ1 E
</p>
<p>�
�4t�1
</p>
<p>�
E
�
"2t�1
</p>
<p>�
:
</p>
<p>As for the ARCH(1) case one has to show that E
�
�4t
�
</p>
<p>turns out to be constant under
</p>
<p>stationarity and the condition 1� 2 ˛21 � .˛1Cˇ1/2 &gt; 0. We omit this step here and
take E
</p>
<p>�
�4t
�
D E
</p>
<p>�
�4t�1
</p>
<p>�
for granted. It then holds that:
</p>
<p>E
�
�4t
�
D
</p>
<p>˛20 C
2 ˛20 .˛1Cˇ1/
1�˛1�ˇ1
</p>
<p>1 � 2 ˛21 � .˛1 C ˇ1/2
:</p>
<p/>
</div>
<div class="page"><p/>
<p>148 6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)
</p>
<p>From this it follows that
</p>
<p>&#13;2 D 3E
�
�4t
� 1
�
Var
</p>
<p>�
x2t
��2
</p>
<p>D 3 ˛
2
0 .1C ˛1 C ˇ1/
</p>
<p>.1� ˛1 � ˇ1/.1 � 2 ˛21 � .˛1 C ˇ1/2/
.1 � ˛1 � ˇ1/2
</p>
<p>˛20
</p>
<p>D 3 .1C .˛1 C ˇ1//.1� .˛1 C ˇ1//
1 � 2 ˛21 � .˛1 C ˇ1/2
</p>
<p>D 3 1 � .˛1 C ˇ1/
2
</p>
<p>1� 2 ˛21 � .˛1 C ˇ1/2
:
</p>
<p>This is in accordance with the claim.
</p>
<p>References
</p>
<p>Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econo-
metrics, 31, 307&ndash;327.
</p>
<p>Engle, R. F. (1982). Autoregressive conditional heteroskedasticity with estimates of the variance
of U.K. inflation. Econometrica, 50, 987&ndash;1008.
</p>
<p>Engle, R. F. (2002). New frontiers for ARCH models. Journal of Applied Econometrics, 17,
425&ndash;446.
</p>
<p>Engle, R. F., &amp; Bollerslev T. (1986). Modelling the persistence of conditional variances. Econo-
metric Reviews, 5, 1&ndash;50.
</p>
<p>Engle, R. F., Lilien, D. M., &amp; Robins, R. P. (1987). Estimating time-varying risk premia in the term
structure: the ARCH-M model. Econometrica, 55, 391&ndash;407.
</p>
<p>Nelson, D. B. (1991). Conditional heteroskedasticity in asset returns: A new approach. Economet-
rica, 59, 347&ndash;370.
</p>
<p>Nelson, D. B., &amp; Cao, Ch. Q. (1992). Inequality constraints in the univariate GARCH model.
Journal of Business &amp; Economic Statistics, 10, 229&ndash;235.
</p>
<p>Taylor, S.J. (1994). Modeling stochastic volatlity: A review and comparative study. Mathematical
Finance, 4, 183&ndash;204.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Stochastic Integrals</p>
<p/>
</div>
<div class="page"><p/>
<p>7Wiener Processes (WP)
</p>
<p>7.1 Summary
</p>
<p>The Wiener process (or the Brownian motion) is the starting point and the basis for
</p>
<p>all the following chapters.1 This is why we will consider this process more explicitly.
</p>
<p>It is a continuous-time process having as prominent a position in stochastic calculus
</p>
<p>as the Gaussian distribution in statistics. After introducing its defining properties
</p>
<p>intuitively, we will discuss important characteristics in the third section. Examples
</p>
<p>derived from the Wiener process will conclude the exposition.
</p>
<p>7.2 From RandomWalk toWiener Process
</p>
<p>We consider a nonstationary special case of the AR(1) process and thereby try to
</p>
<p>arrive at the Wiener process which is the most important continuous-time process in
</p>
<p>the fields of our applications.
</p>
<p>RandomWalks
</p>
<p>The cumulation of white noise is labeled random walk,
</p>
<p>xt D
tX
</p>
<p>jD1
"j ; t 2 f1; 2; : : : ; ng :
</p>
<p>1Norbert Wiener, 1894&ndash;1964, was a US-American mathematician. He succeeded in finding a
mathematically solid definition and discussion of the so-called Brownian motion named after
the nineteenth century British botanist Brown. With a microscope, Brown initially observed and
described erratic paths of molecules.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_7
</p>
<p>151</p>
<p/>
</div>
<div class="page"><p/>
<p>152 7 Wiener Processes (WP)
</p>
<p>Obviously, it holds that
</p>
<p>xt D xt�1 C "t ; x0 D 0 :
</p>
<p>In other words: The random walk results as an AR(1) process for the parameter
</p>
<p>value a D 1 and with the starting value zero2
</p>
<p>xt D a xt�1 C "t ; a D 1 ; x0 D 0 :
</p>
<p>As the process is nonstationary,
</p>
<p>E.xt/ D 0 ; Var.xt/ D �2 t ;
</p>
<p>it cannot have an infinitely long past, i.e. the index set is finite, T D f1; 2; : : : ; ng.
In a way, the random walk models the way home of a drunk who at a point in time
</p>
<p>t turns to the left or to the right by chance and uncorrelated with his previous path.
</p>
<p>Put more formally: The random walk is a martingale. We briefly want to convince
</p>
<p>ourselves of this fact. By substitution the AR(1) process yields
</p>
<p>xtCs D asxt C
s�1X
</p>
<p>jD0
a j"tCs�j :
</p>
<p>Therefore, for s &gt; 0 it holds that
</p>
<p>E.xtCsjIt/ D asxt C 0 ;
</p>
<p>where It again denotes the information set of the AR(1) process. Thus, the
</p>
<p>martingale condition (2.11) for AR(1) processes is fulfilled if and only if a D 1.
The second martingale condition, E.jxtj/ &lt; 1 ; is given as �2 &lt; 1 and hence
E.x2t / D t �2 &lt;1.
</p>
<p>Example 7.1 (Discrete-Valued Random Walk) Let the set of outcomes contain only
</p>
<p>two elements (e.g. coin toss: heads or tails),
</p>
<p>˝ D f!0 ; !1g ;
</p>
<p>with probabilities P .f!1g/ D 12 D P .f!0g/. Let f"tg be a white noise process
assigning the numerical values 1 and �1 to the events,
</p>
<p>".tI!1/ D 1 ; ".tI!0/ D �1 ; t D 1; 2; : : : ; n :
</p>
<p>2This special assumption for the starting value is made out of convenience; it is by no means crucial
for the behavior of a random walk.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 From Random Walk to Wiener Process 153
</p>
<p>For every point in time, this induces the probabilities
</p>
<p>P"."t D 1/ D P .f!1g/ D P"."t D �1/ D P .f!0g/ D
1
</p>
<p>2
:
</p>
<p>Then, for expectation and variance it is immediately obtained:
</p>
<p>E."t/ D 0 ; Var."t/ D 12
1
</p>
<p>2
C .�1/2 1
</p>
<p>2
D 1 :
</p>
<p>For t D 1; : : : ; n, the corresponding random walk xt D
Pt
</p>
<p>jD1 "j can only take on
the countably many values f�n;�n C 1; : : : ; n � 1; ng and is therefore also called
discrete-valued. �
</p>
<p>Example 7.2 (Continuous-Valued Random Walk) If f"tg is a Gaussian random
process,
</p>
<p>"t � N .0; �2/ ;
</p>
<p>then, obviously, the random walk based thereon is also Gaussian, where the variance
</p>
<p>grows linearly with time:
</p>
<p>xt D
tX
</p>
<p>jD1
"j � N .0; �2t/ :
</p>
<p>In this case, fxtg is a continuous random variable by assumption and hence this
random walk is also called continuous-valued. �
</p>
<p>Wiener Process
</p>
<p>At this point, the continuous-time Wiener process will not yet be defined rigorously,
</p>
<p>but we will approach it intuitively step by step. In order to do so, we choose the
</p>
<p>index set T D Œ0; 1&#141; with the equidistant, disjoint partition
</p>
<p>Œ0; 1/ D
n[
</p>
<p>iD1
</p>
<p>�
i � 1
</p>
<p>n
;
</p>
<p>i
</p>
<p>n
</p>
<p>�
:
</p>
<p>Now, the random walk is multiplied by the factor 1=
p
</p>
<p>n and expanded into a step
</p>
<p>function Xn.t/. Interval by interval, we define as continuous-time process:
</p>
<p>Xn.t/ D
1p
n
</p>
<p>i�1X
</p>
<p>jD1
"j for t 2
</p>
<p>�
i � 1
</p>
<p>n
;
</p>
<p>i
</p>
<p>n
</p>
<p>�
; i D 1; : : : ; n : (7.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>154 7 Wiener Processes (WP)
</p>
<p>In addition, we assume "t 2 f�1; 1g and set for t D 1
</p>
<p>Xn.1/ D
1p
n
</p>
<p>nX
</p>
<p>jD1
"j :
</p>
<p>For t D 0, i.e. i D 1 in (7.1), we follow the convention that a sum equals zero if
the upper summation limit is smaller than the lower one, which is why Xn.0/ D 0
begins in the origin. Apparently, Xn.t/ is a constant step function on an interval of
</p>
<p>the length 1=n, respectively; if Xn.t/ was only observed at the jump discontinuities,
</p>
<p>a time-discrete random walk would be obtained. Being dependent on the choice of
</p>
<p>n (i.e. the fineness of the partition), the process Xn.t/ is indexed accordingly.
</p>
<p>If "t is from Example 7.1, i.e. j"tj D 1, this means that each individual step of
the step function has the height 1=
</p>
<p>p
n in absolute value. Hence, Xn.t/ only takes on
</p>
<p>values from
</p>
<p>� �np
n
;
�n C 1p
</p>
<p>n
; : : : ;
</p>
<p>n � 1p
n
;
</p>
<p>np
n
</p>
<p>�
:
</p>
<p>Therefore, Xn.t/ is a continuous-time but discrete-valued process.
</p>
<p>Now, the starting point for the Wiener process is the step function Xn.t/ with "t
from Example 7.1. The number of the steps obviously depends on n which indicates
</p>
<p>the fineness of the partition of the unit interval. Simultaneously, the step height of
</p>
<p>the steps with n�0:5 becomes flatter, the finer it is partitioned. Note that, due to this
fact, the range becomes finer and finer and larger and larger as n grows. Hence,
</p>
<p>with n growing, Xn.t/ becomes &ldquo;more continuous&rdquo;, in the sense that the step heights
</p>
<p>n�0:5 turn out to be smaller; simultaneously, the jump discontinuities move together
more closely (the steps of the width 1=n get narrower) such that Xn.t/ can take on
</p>
<p>more and more possible values. In the limit .n ! 1/ a process named after Norbert
Wiener is obtained which we will always denote by W in the following:
</p>
<p>Xn.t/ ) W.t/ for n ! 1 ;
</p>
<p>where &ldquo;)&rdquo; denotes a mode of convergence which will be clarified in Chap. 14.
Intuitively speaking, it holds that for each of the uncountably many points in time t
</p>
<p>the function Xn.t/ converges in distribution to W.t/ just at this point. The transition
</p>
<p>of discrete-time and discrete-valued step functions from (7.1) to the Wiener process
</p>
<p>(for n growing) is illustrated in Fig. 7.1.3
</p>
<p>The Wiener process W.t/ as a limit of Xn.t/ is continuous-valued with range
</p>
<p>R D .�1; 1/ and of course it is continuous-time with t 2 Œ0; 1&#141;. Furthermore,
the Wiener process is a Gaussian process (normally distributed) which is not that
</p>
<p>surprising. As, due to the central limit theorem for n ! 1, it holds for the
</p>
<p>3In order not to violate the concept of functions, strictly speaking, the vertical lines would not be
allowed to occur in the graphs of the figure. We ignore this subtlety to enhance clarity.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.2 From Random Walk to Wiener Process 155
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>n=5
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
</p>
<p>.2
&minus;
</p>
<p>0
.8
</p>
<p>&minus;
0
</p>
<p>.4
0
</p>
<p>.0
</p>
<p>n=10
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.5
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>n=50
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>n=500
</p>
<p>Fig. 7.1 Step function from (7.1) on the interval [0,1]
</p>
<p>standardized sum of uncorrelated random variables f"jg (whose variance is one and
whose expectation is zero) that it tends to a standard normal distribution:
</p>
<p>Xn.1/ D
1p
n
</p>
<p>nX
</p>
<p>jD1
"j D
</p>
<p>Pn
jD1 "j � E
</p>
<p>�Pn
jD1 "j
</p>
<p>�
</p>
<p>r
Var
</p>
<p>�Pn
jD1 "j
</p>
<p>�
d! N .0; 1/ : (7.2)
</p>
<p>Here, &ldquo;
d!&rdquo; denotes the usual convergence in distribution; cf. Sect. 8.4. As Xn.1/
</p>
<p>tends to W.1/ at the same time, the Wiener process has to be a standard normally
</p>
<p>distributed random variable at t D 1. After giving a formal definition, we will again
bridge the gap from the Wiener process to Xn.t/.</p>
<p/>
</div>
<div class="page"><p/>
<p>156 7 Wiener Processes (WP)
</p>
<p>Formal Definition
</p>
<p>The Wiener process (WP) W.t/, t 2 Œ0;T&#141;, is defined by three assumptions. Put into
words, they read: It is a process with starting value zero and independent, normally
</p>
<p>distributed, stationary increments. These assumptions are to be concretized and
</p>
<p>specified. Hence, the Wiener process is defined by:
</p>
<p>(W1) The starting value is zero with probability one, P.W.0/ D 0/ D 1;
(W2) non-overlapping increments W.t1/ � W.t0/, : : :, W.tn/ � W.tn�1/, with 0 �
</p>
<p>t0 � t1 � : : : � tn, are independent for arbitrary n;
(W3) the increments follow a Gaussian distribution with the variance equalling the
</p>
<p>difference of the arguments, W.t/ � W.s/ � N .0; t � s/ with 0 � s &lt; t.
</p>
<p>Note that the variance of the increments does not depend on the point in time but
</p>
<p>only on the temporal difference. Furthermore, the covariance of non-overlapping
</p>
<p>increments is zero due to the independence and the joint distribution results as
</p>
<p>the product of the marginal distributions. Hence, the joint distribution of non-
</p>
<p>overlapping increments is multivariate normal. If all increments are measured over
</p>
<p>equidistant constant time intervals, ti�ti�1 D const, then the variances are identical.
Therefore, such a series of increments is (strictly) stationary.
</p>
<p>Although the WP is defined by its increments, they translate into properties of
</p>
<p>the level. Obviously, the first and the third property4 imply
</p>
<p>W.t/ � N .0; t/ ; (7.3)
</p>
<p>i.e. the Wiener process is clearly a stochastic function being normally distributed at
</p>
<p>every point in time with linearly growing variance t. More precisely, the WP is even
</p>
<p>a Gaussian process in the sense of the definition from Chap. 2. The autocovariances
</p>
<p>being necessary for the complete characterization of the multivariate normal
</p>
<p>distribution .W.t1/; : : : ; W.tn//
0, are determined as follows (see Problem 7.3):
</p>
<p>Cov.W.t/; W.s// D min.s; t/ : (7.4)
</p>
<p>The Wiener process, which here will always be denoted by W, is for us a special
</p>
<p>case of the more general Brownian motion.5 So to speak, it takes over the role of
</p>
<p>the standard normal distribution, and by multiplication with a constant a general
</p>
<p>Brownian motion is obtained as
</p>
<p>B.t/ D � W.t/ ; � &gt; 0 :
</p>
<p>4To be completely accurate, this needs to read: W.t/ � W.0/ � N .0; t/. As W.0/ is zero with
probability one, we set W.0/ equal to zero here and in the following; then, the corresponding
statements only hold with probability one.
5This convention does not hold beyond these pages. Many authors use the terms Wiener process
or Brownian motion interchangeably or they apply one of them exclusively.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Properties 157
</p>
<p>The assumptions (W1) to (W3) seem very natural if the WP is accepted as a limit
</p>
<p>of Xn.t/ from (7.1). For this process it holds by construction that
</p>
<p>&bull; Xn.t/ D 0 for t 2 Œ0; 1=n/,
&bull; e.g. the increments
</p>
<p>Xn
</p>
<p>�
k C 1
</p>
<p>n
</p>
<p>�
� Xn
</p>
<p>�
k
</p>
<p>n
</p>
<p>�
D "kC1p
</p>
<p>n
</p>
<p>and
</p>
<p>Xn
</p>
<p>�
k
</p>
<p>n
</p>
<p>�
� Xn .0/ D
</p>
<p>1p
n
</p>
<p>kX
</p>
<p>jD1
"j
</p>
<p>are uncorrelated (or even independent if f"tg is a pure random process),
&bull; Xn.1/ � Xn.0/ is approximately normally distributed due to (7.2).
</p>
<p>The three properties (W1) to (W3) just reflect the properties of the step function
</p>
<p>Xn.t/.
</p>
<p>7.3 Properties
</p>
<p>We have already come to know some properties of the WP, for example its
</p>
<p>autocovariance structure and the Gaussian distribution. For the understanding and
</p>
<p>handling of Wiener processes further properties are important.
</p>
<p>Pathwise Properties
</p>
<p>Loosely speaking, it holds that the Brownian motion is everywhere (i.e. for all t)
</p>
<p>continuous in terms of conventional calculus6; however, it is nowhere differentiable.
</p>
<p>These are pathwise properties, i.e. for a given !0, W.t/ D W.tI!0/ can be regarded
as a function which is continuous in t but which is nowhere differentiable. This
</p>
<p>is a matter of properties being mathematically rather deep but which can be made
</p>
<p>6Occasionally, the pathwise continuity is claimed to be the fourth defining property. This is to be
understood as follows. Billingsley (1986, Theorem 37.1) proves more or less the following: If one
has a WP W with (W1) to (W3) at hand, then a process W� can be constructed which is a WP in
the sense of (W1) to (W3), as well, which has the same distribution as W and which is pathwise
continuous. As W� and W are equal in distribution, they cannot be distinguished and therefore,
w.l.o.g. it can safely be assumed that one may work with the continuous W�.</p>
<p/>
</div>
<div class="page"><p/>
<p>158 7 Wiener Processes (WP)
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>time
</p>
<p>W
(t
</p>
<p>)
</p>
<p>Fig. 7.2 Simulated paths of the WP on the interval [0,1]
</p>
<p>heuristically plausible at least. Concerning this, let us consider
</p>
<p>W.t C h/ � W.t/ � N .0; h/ ; h &gt; 0 :
</p>
<p>For h ! 0 the given Gaussian distribution degenerates to zero suggesting
continuity: W.t C h/ � W.t/ � 0 for h � 0. Analogously, we observe a difference
quotient whose variance tends to infinity for h ! 0,
</p>
<p>W.t C h/� W.t/
h
</p>
<p>� N
�
0;
1
</p>
<p>h
</p>
<p>�
;
</p>
<p>which suggests that a usual derivative does not exist. Related to contents, this
</p>
<p>means that it is not possible to add a tangent line to W.t/ which would allow for
</p>
<p>approximating W.t C h/ for an ever so small h. Three simulated paths of the WP in
Fig. 7.2 illustrate these properties.
</p>
<p>Markov andMartingale Property
</p>
<p>In the previous section, we have learned that the random walk is a martingale. For
</p>
<p>the WP as a continuous-time counterpart, a corresponding result can be obtained</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 Properties 159
</p>
<p>(where It D � .W.r/; r � t/ contains all the information about the past up to t):
</p>
<p>E .jW.t/j/ &lt;1 ;
</p>
<p>E .W.t C s/ j It/ D W.t/ :
</p>
<p>The WP satisfies the Markov property (2.9) as well. In order to show this, we use the
</p>
<p>fact that the increment W.t C s/� W.t/ for s &gt; 0 is independent of the information
set It due to (W2). Hence, for W.t/ D v it holds that:
</p>
<p>P .W.t C s/ � w j It/ D P .W.t C s/ � W.t/ � w � v j It/
D P .W.t C s/ � W.t/ � w � v/ :
</p>
<p>At the same time it holds again due to independence that:
</p>
<p>P .W.t C s/ � w j W.t/ D v/ D P .W.t C s/ � W.t/ � w � v j W.t/ D v/
D P .W.t C s/ � W.t/ � w � v/ ;
</p>
<p>which just verifies the Markov property.
</p>
<p>Scale Invariance
</p>
<p>The Wiener process is a function being Gaussian for every point in time t with
</p>
<p>expectation zero and variance t. However, time can be measured in minutes, hours
</p>
<p>or other units. If the time scale is blown up or squeezed by the factor � &gt; 0, then it
</p>
<p>holds that
</p>
<p>W.� t/ � N .0; � t/ :
</p>
<p>The same distribution is obtained for the
p
�-fold of the Wiener process:
</p>
<p>p
� W.t/ � N .0; � t/ :
</p>
<p>That is why the Wiener process is called scale-invariant (or self-similar). Hence,
</p>
<p>W.� t/ and
p
� W.t/ are equal in distribution, which we formulate as
</p>
<p>p
� W.t/ � W.� t/ (7.5)
</p>
<p>as well. Such an equality in distribution is to be handled with care and by no means
</p>
<p>to be confused with ordinary equality. Naturally, it does not hold that, e.g. the double
</p>
<p>of W.t/ is equal to the value at the point in time 4t:
</p>
<p>p
� W.t/ &curren; W.� t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>160 7 Wiener Processes (WP)
</p>
<p>In other words: Scale invariance is a distributional and not a pathwise property.
</p>
<p>Up to this point, it has not been emphasized that the Wiener process is
</p>
<p>nonstationary. This can already be noted in (7.4) as for s &gt; 0 the covariance
</p>
<p>Cov.W.t/; W.t C s// D t does not result as dependent on the temporal distance
s, but as dependent on the point in time t itself. As we have said, the increments
</p>
<p>of the WP from (W2), however, are multivariate normal with expectations and
</p>
<p>covariances of zero and variances which are not affected by a shift on the time axis.
</p>
<p>The trending behavior of the nonstationary Wiener process will now be clarified by
</p>
<p>two propositions.
</p>
<p>Hitting Time
</p>
<p>Let Tb be the point in time at which the WP attains (or hits) a given value b &gt; 0 for
</p>
<p>the first time.7 By variable transformation it is shown that this random variable has
</p>
<p>the distribution function (see Eq. (7.14) in Problem 7.5)
</p>
<p>Fb.t/ WD P.Tb � t/ D
2p
2�
</p>
<p>Z 1
</p>
<p>b=
p
</p>
<p>t
</p>
<p>e�y
2=2dy:
</p>
<p>Thereby statement (a) from the following proposition is proved; statement (b) is
</p>
<p>obtained by means of the corresponding density function (see Problem 7.5).
</p>
<p>Proposition 7.1 (Hitting Time) For the hitting time Tb, where the WP hits b &gt; 0
</p>
<p>for the first time, it holds that:
</p>
<p>(a) P.Tb &gt; t/ ! 0 for t ! 1;
(b) E.Tb/ does not exist.
</p>
<p>The result Tb &gt; t is tantamount to the fact that W.s/ has not attained the value b
</p>
<p>up to t:
</p>
<p>P.Tb &gt; t/ D P
�
</p>
<p>max
0�s�t
</p>
<p>W.s/ &lt; b
</p>
<p>�
:
</p>
<p>Laxly formulated this proposition implies that, paradoxically, (a) sooner or later, the
</p>
<p>WP exceeds every value with certainty; (b) on average, this takes infinitely long:
</p>
<p>E.Tb/ D 1.
</p>
<p>7The random variable Tb is a so-called &ldquo;stopping time&rdquo;. This is a term from the theory of stochastic
processes which we will not elaborate on here.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Functions of Wiener Processes 161
</p>
<p>Zero Crossing
</p>
<p>Next, let p.t1; t2/ with 0 &lt; t1 &lt; t2 be the probability of the WP hitting the zero
</p>
<p>line at least once between t1 and t2 (even if not necessarily crossing it). We then talk
</p>
<p>about a zero crossing. The following proposition states how to calculate it. For a
</p>
<p>proof see e.g. Klebaner (2005, Theorem 3.25).
</p>
<p>Proposition 7.2 (Arcus Law)
</p>
<p>The probability of a zero crossing between t1 and t2, 0 &lt; t1 &lt; t2, equals
</p>
<p>p.t1; t2/ D
2
</p>
<p>�
arctan
</p>
<p>r
t2 � t1
</p>
<p>t1
;
</p>
<p>where arctan denotes the inverse of the tangent function tan D sin
cos
</p>
<p>.
</p>
<p>It is interesting to fathom out the limiting cases of Proposition 7.2. From the
</p>
<p>shape of the inverse function of the tangent function it results that
</p>
<p>lim
x!1
</p>
<p>arctan x D �
2
</p>
<p>and lim
x!0
</p>
<p>arctan x D 0 :
</p>
<p>Hence, substantially, for t2 ! 1 it follows that the probability of attaining the zero
line tends to one; for t2 ! t1, however, it naturally converges to zero.
</p>
<p>In the literature, an equivalent formulation of the Arcus Law is found:
</p>
<p>p.t1; t2/ D
2
</p>
<p>�
arccos
</p>
<p>r
t1
</p>
<p>t2
:
</p>
<p>The equivalence is based on the formula
</p>
<p>arctan x D arccos 1p
1C x2
</p>
<p>;
</p>
<p>see e.g. Gradshteyn and Ryzhik (2000, 1.624-8), where &ldquo;arccos&rdquo; stands for the
</p>
<p>inverse of the cosine function.
</p>
<p>7.4 Functions of Wiener Processes
</p>
<p>When applying stochastic calculus, one is often concerned with processes derived
</p>
<p>from the Brownian motion. In this section, some of these will be covered and
</p>
<p>illustrated graphically. We simulate processes on the interval Œ0; 1&#141;; for this purpose,
</p>
<p>the theoretically continuous processes are calculated at 1000 sampling points
</p>
<p>and plotted. The resulting graphs are based on pseudo-random variables. Details</p>
<p/>
</div>
<div class="page"><p/>
<p>162 7 Wiener Processes (WP)
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>W(t)
0.5*W(t)
</p>
<p>Fig. 7.3 WP and Brownian motion with � D 0:5
</p>
<p>regarding the simulation of stochastic processes are treated in Chap. 12 on stochastic
</p>
<p>differential equations.
</p>
<p>BrownianMotion B.t/
</p>
<p>In Fig. 7.3 a path of a WP and a Brownian motion based thereon with only half the
</p>
<p>standard deviation,
</p>
<p>W.t/ and B.t/ D 0:5W.t/ ;
</p>
<p>are depicted. Obviously, the one graph is just half of the other.
</p>
<p>BrownianMotion with Drift X.t/ D � t C � W.t/
</p>
<p>Here it holds that both the expectation and the variance grow linearly with t:
</p>
<p>X.t/ � N .�t; �2 t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Functions of Wiener Processes 163
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>W(t)
2t+W(t)
</p>
<p>Fig. 7.4 WP and Brownian motion with drift, where � D 1
</p>
<p>In Fig. 7.4 the WP from Fig. 7.3 and the process based thereon with drift are
</p>
<p>depicted. The drift parameter is � D 2 for � D 1, and the expectation function
2 t is also displayed.
</p>
<p>Brownian Bridge X.t/ D B.t/ � t B.1/
</p>
<p>This process is based on the Brownian motion, B.t/ D � W.t/, and fundamentally,
it is only defined for t 2 Œ0; 1&#141;. The name comes from the fact that the starting and
the final value are equal with probability one by construction: X.0/ D X.1/ D 0.
One can verify easily that (see Problem 7.6):
</p>
<p>Var.X.t// D t .1 � t/ �2 &lt; t �2 : (7.6)
</p>
<p>Hence, for t 2 .0; 1&#141; it holds that Var.X.t// &lt; Var.B.t//. This is intuitively clear:
With being forced back to zero, the Brownian bridge has to exhibit less variability
</p>
<p>than the Brownian motion. This is also shown in Fig. 7.5 for � D 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>164 7 Wiener Processes (WP)
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>W(t)
X(t)
</p>
<p>Fig. 7.5 WP and Brownian bridge (� D 1)
</p>
<p>ReflectedWiener Process X.t/ D jW.t/j
</p>
<p>Due to W.t/ � N .0; t/, for the distribution function it is elementary to obtain (see
Problem 7.7):
</p>
<p>P.X.t/ � x/ D 2p
2�t
</p>
<p>Z x
</p>
<p>�1
exp
</p>
<p>��y2
2t
</p>
<p>�
dy � 1 :
</p>
<p>Note that one integrates over twice the density of a Gaussian random variable with
</p>
<p>expectation zero. Therefore it immediately holds that
</p>
<p>P.X.t/ � x/ D 2p
2�t
</p>
<p>Z x
</p>
<p>0
</p>
<p>exp
</p>
<p>��y2
2t
</p>
<p>�
dy : (7.7)
</p>
<p>Expectation and variance of the reflected Wiener process can be determined from
</p>
<p>the corresponding density function. They read (see Problem 7.7):
</p>
<p>E.X.t// D
r
2 t
</p>
<p>�
; Var.X.t// D t
</p>
<p>�
1 � 2
</p>
<p>�
</p>
<p>�
&lt; t D Var.W.t// : (7.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Functions of Wiener Processes 165
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0 W(t)
</p>
<p>|W(t)|
E(|W(t)|)
</p>
<p>Fig. 7.6 WP and reflected WP along with expectation
</p>
<p>As the reflected Wiener process cannot become negative, it has a positive expected
</p>
<p>value growing with t. For the same reason its variance is smaller than the one of the
</p>
<p>unrestricted Wiener process, see Fig. 7.6.
</p>
<p>Geometric BrownianMotion X.t/ D e� tC� W.t/
</p>
<p>By definition, it holds in this case that the logarithm8 of the process is a Brownian
</p>
<p>motion with drift and therefore Gaussian,
</p>
<p>log X.t/ D � t C � W.t/ � N .�t; �2t/ :
</p>
<p>A random variable Y whose logarithm is Gaussian is called &ndash; as would seem natural
</p>
<p>&ndash; log-normal (logarithmically normally distributed). If it holds that
</p>
<p>log Y � N .�y; �2y /;
</p>
<p>8By &ldquo;log&rdquo; we denote the natural logarithm and not the common logarithm.</p>
<p/>
</div>
<div class="page"><p/>
<p>166 7 Wiener Processes (WP)
</p>
<p>then we know how the two first moments of Y look like, cf. e.g. Syds&aelig;ter, Str&oslash;m,
</p>
<p>and Berck (1999, p. 189) or Johnson, Kotz, and Balakrishnan (1994, Ch. 14):
</p>
<p>E.Y/ D e�yC�2y =2 ; Var.Y/ D e2�yC�2y
�
</p>
<p>e�
2
y � 1
</p>
<p>�
:
</p>
<p>Hence, by plugging in we obtain for the geometric Brownian motion
</p>
<p>E.X.t// D e.�C�2=2/ t and Var.X.t// D e.2�C�2/ t .e�2 t � 1/ : (7.9)
</p>
<p>While log X.t/ is Gaussian with a linear trend, �t, as expectation, X.t/ exhibits
</p>
<p>an exponentially growing expectation function. Particularly for � D 0 and � D 1
the results
</p>
<p>E.X.t// D et=2 and Var.X.t// D et .et � 1/ (7.10)
</p>
<p>are obtained. The on average exponential growth in the case of � &gt; ��2=2 is
illustrated in Fig. 7.7. In Fig. 7.8 we find graphs of the WP and a geometric Brownian
</p>
<p>motion with expectation one, namely with � D �0:5 and � D 1. Generally, for
� D ��2=2 an expectation function of one is obtained. Then, one also says that the
process does not exhibit a trend (or drift).
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>0
2
</p>
<p>4
6
</p>
<p>W(t)
X(t)
E(X(t))
</p>
<p>Fig. 7.7 Geometric Brownian motion with � D 1:5 and � D 1 along with expectation</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Functions of Wiener Processes 167
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>1
.0
</p>
<p>1
.5
</p>
<p>2
.0
</p>
<p>W(t)
X(t)
</p>
<p>Fig. 7.8 WP and geometric Brownian motion with � D �0:5 and � D 1
</p>
<p>In comparison to the expectation, the median of a geometric Brownian motion
</p>
<p>does not depend on � . Rather, it holds that (see Problem 7.8):
</p>
<p>P
�
e� tC� W.t/ � e� t
</p>
<p>�
D 0:5 ;
</p>
<p>such that the median results as e� t.
</p>
<p>Maximum of aWP X.t/ D max0�s�t W.s/
</p>
<p>At t, the maximum process is assigned the maximal value which the WP has taken
</p>
<p>on up to this point in time. Therefore, in periods of a decreasing Wiener process
</p>
<p>path, X.t/ is constant on the historic maximum until a new relative maximum is
</p>
<p>attained. However, this process has a distribution function that we have already come
</p>
<p>to know. By applying the distribution function of the stopping time which is given
</p>
<p>above Proposition 7.1, one shows (see Problem 7.9) that the maximum process and
</p>
<p>the reflected WP are equal in distribution:
</p>
<p>P.X.t/ � b/ D P.jW.t/j � b/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>168 7 Wiener Processes (WP)
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>W(t)
X(t)
E(X(t))
</p>
<p>Fig. 7.9 WP and maximum process along with expectation
</p>
<p>Therefore, expectation and variance of the maximum process of jW.t/j can naturally
be copied:
</p>
<p>E.X.t// D
r
2 t
</p>
<p>�
; Var.X.t// D t
</p>
<p>�
1 � 2
</p>
<p>�
</p>
<p>�
&lt; t D Var.W.t// : (7.11)
</p>
<p>The expected value is positive and grows with time as the WP again will replace a
</p>
<p>relative positive maximum by a new relative maximum. Due to the process being
</p>
<p>again and again constant over times, it is not surprising that its variance is smaller
</p>
<p>than the one of the underlying WP, cf. Fig. 7.9.
</p>
<p>IntegratedWiener Process X.t/ D
R t
0
</p>
<p>W.s/ ds
</p>
<p>As the Brownian motion is a continuous function, the Riemann integral can be
</p>
<p>defined pathwise. Hence, e.g. the following random variable is obtained:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>B.t/ dt D �
Z 1
</p>
<p>0
</p>
<p>W.t/ dt :</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 Functions of Wiener Processes 169
</p>
<p>Behind this random variable hides a normal distribution. The latter can be proved
</p>
<p>by using the definition of the Riemann integral or as a simple conclusion of the
</p>
<p>Proposition 8.3 below:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.t/ dt � N .0; 1=3/ : (7.12)
</p>
<p>Basically, by using the integral of a WP, a new stochastic process can also be
</p>
<p>generated by making the upper limit of integration time-dependent:
</p>
<p>X.t/ D
Z t
</p>
<p>0
</p>
<p>W.s/ ds :
</p>
<p>This idea forms the starting point for the subsequent chapter. In Fig. 7.10 the relation
</p>
<p>is shown between the WP and the integral X.t/ as the area beneath the graph.
</p>
<p>0.0 0.2 0.4 0.6 0.8 1.0
</p>
<p>&minus;
1
.0
</p>
<p>&minus;
0
.5
</p>
<p>0
.0
</p>
<p>0
.5
</p>
<p>W(t)
X(t)
</p>
<p>Fig. 7.10 WP and integrated WP</p>
<p/>
</div>
<div class="page"><p/>
<p>170 7 Wiener Processes (WP)
</p>
<p>7.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>7.1 Consider with � &gt; 0
</p>
<p>X.t/ D W.1/ � �W.1 � t/ for 0 � t � 1 :
</p>
<p>Determine the mean and variance of X.t/.
</p>
<p>7.2 Consider
</p>
<p>X.t/ D t W.t�1/ for t &gt; 0 :
</p>
<p>Determine the covariance of X.t/ and W.t/, Cov.X.t/;W.t//.
</p>
<p>7.3 Derive the autocovariance function of a WP, (7.4). Find a simple expression in
</p>
<p>t and s only for the autocorrelations
</p>
<p>�.s; t/ D Cov.W.t/;W.s//p
Var.W.t//Var.W.s//
</p>
<p>:
</p>
<p>7.4 Choose d 2 R such that Td�0:5 W.t/ and W.T t/ are equal in distribution.
</p>
<p>7.5 Prove Proposition 7.1 using the hints given in the text.
</p>
<p>7.6 Derive the autocovariance function of a Brownian bridge, and hence show (7.6).
</p>
<p>7.7 Determine the distribution function, (7.7), and the moments, (7.8), of a reflected
</p>
<p>Wiener process.
</p>
<p>7.8 Show that in the general case of a geometric Brownian motion, e� tC� W.t/, the
median is given by e� t.
</p>
<p>7.9 Show by means of the hints in the text that the maximum process of a WP and
</p>
<p>the corresponding reflected WP are equal in distribution:
</p>
<p>P
</p>
<p>�
max
0�s�t
</p>
<p>W.s/ � b
�
D P.jW.t/j � b/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Problems and Solutions 171
</p>
<p>Solutions
</p>
<p>7.1 As the Wiener process is on average zero at every point in time, this obviously
</p>
<p>holds for X.t/ as well. Therefore, the variance is calculated as follows:
</p>
<p>Var.X.t// D E.X2.t//
D EŒW2.1/ � 2�W.1/W.1 � t/C �2W2.1 � t/&#141;
D Var.W.1// � 2�Cov.W.1/;W.1 � t//C �2Var.W.1 � t//
D 1� 2� min.1; 1 � t/C �2.1 � t/
D 1� 2�.1 � t/C �2.1 � t/
D t C .1 � t/.1 � �/2 :
</p>
<p>7.2 Due to E.W.t// D E.X.t// D 0 one obtains:
</p>
<p>Cov.X.t/;W.t// D E.X.t/W.t//
D t E.W.t�1/W.t//
D t min.t�1; t/:
</p>
<p>Because of
</p>
<p>min.t�1; t/ D
�
</p>
<p>t ; 0 &lt; t � 1
t�1 ; t � 1
</p>
<p>it follows that
</p>
<p>Cov.X.t/;W.t// D
�
</p>
<p>t2 ; 0 &lt; t � 1
1 ; t � 1 :
</p>
<p>7.3 We simply apply the defining properties (W1), (W2) and (W3) or put
</p>
<p>differently (7.3). Due to (7.3) the WP has an expectation of zero such that
</p>
<p>Cov .W.t/;W.s// D E .W.t/W.s// :
</p>
<p>W.l.o.g. let s � t. By using (W1) and (W2) and after adding zero, we then write:
</p>
<p>E .W.t/W.s// D E .ŒW.s/C W.t/ � W.s/&#141; ŒW.s/ � W.0/&#141;/
D E
</p>
<p>�
ŒW.s/&#141;2
</p>
<p>�
C E .ŒW.t/ � W.s/&#141; ŒW.s/ � W.0/&#141;/
</p>
<p>D s C 0 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>172 7 Wiener Processes (WP)
</p>
<p>where the last equality uses Var.W.s// D s and the independence of non-
overlapping increments. As one could also assume t � s w.l.o.g., (7.4) is verified.
</p>
<p>With the autocovariance one obtains
</p>
<p>�.s; t/ D min.s; t/p
t s
</p>
<p>D min.s; t/p
max.s; t/ min.s; t/
</p>
<p>D
s
</p>
<p>min.s; t/
</p>
<p>max.s; t/
:
</p>
<p>7.4 This is a problem on self-similarity or scale invariance. Due to (7.3) it obviously
</p>
<p>holds that:
</p>
<p>Td�0:5W.t/ � N .0;T2d�1t/
</p>
<p>and
</p>
<p>W.Tt/ � N .0;Tt/ :
</p>
<p>Therefore, the corresponding variances are equal for d D 1. The corresponding
result is obtained from (7.5) as well:
</p>
<p>T0:5W.t/ � W.Tt/ :
</p>
<p>7.5 Proof of Proposition 7.1(a): Our proof consists of three steps. At first, we
</p>
<p>establish the equation
</p>
<p>P.Tb &lt; t/ D 2 P.W.t/ &gt; b/ : (7.13)
</p>
<p>Secondly, by using this we show:
</p>
<p>Fb.t/ WD P.Tb � t/ D
2p
2�
</p>
<p>Z 1
</p>
<p>b=
p
</p>
<p>t
</p>
<p>e�y
2=2 dy : (7.14)
</p>
<p>Note that in (7.14) the integrand amounts to the density function of the standard
</p>
<p>normal distribution. Hence, thirdly for t ! 1 the claim immediately follows
from (7.14) due to P.Tb &gt; t/ D 1 � P.Tb � t/:
</p>
<p>In order to accept (7.13), we remember (7.3). Accordingly, for Tb &lt; t it holds
</p>
<p>that
</p>
<p>W.t/ � W.Tb/ � N .0; t � Tb/ ;
</p>
<p>which is why from the symmetry of the Gaussian distribution with W.Tb/ D b it
follows for the conditional probability that:
</p>
<p>P .W.t/ &gt; b j Tb &lt; t/ D P .W.t/ � W.Tb/ &gt; 0 j Tb &lt; t/ D
1
</p>
<p>2
:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Problems and Solutions 173
</p>
<p>Hence, it results that:
</p>
<p>1
</p>
<p>2
D P.W.t/ &gt; b and Tb &lt; t/
</p>
<p>P.Tb &lt; t/
D P.W.t/ &gt; b/
</p>
<p>P.Tb &lt; t/
;
</p>
<p>where the last equality is caused by the fact that Tb &lt; t is implied by W.t/ &gt; b.
</p>
<p>Thus, we obtain Eq. (7.13) which will now be applied for deriving (7.14).
</p>
<p>Due to (7.3) it holds by definition that
</p>
<p>P.W.t/ &gt; b/ D
Z 1
</p>
<p>b
</p>
<p>1p
2�t
</p>
<p>e�x
2=2tdx :
</p>
<p>By variable transformation, y D xp
t
, it follows that
</p>
<p>P.W.t/ &gt; b/ D
Z 1
</p>
<p>b=
p
</p>
<p>t
</p>
<p>1p
2�
</p>
<p>e�y
2=2dy ;
</p>
<p>whereby (7.14) and hence claim (a) is proved due to P.Tb &lt; t/ D P.Tb � t/.
Proof of Proposition 7.1(b): With the density function fb.t/ D F0b.t/ the approach
</p>
<p>for the expected value reads as follows:
</p>
<p>E .Tb/ D
Z 1
</p>
<p>0
</p>
<p>t fb.t/ dt :
</p>
<p>Note that the distribution function derived in (7.14) has the following structure with
</p>
<p>the antiderivative H, H0 D h:
</p>
<p>Fb.t/ D
Z 1
</p>
<p>g.t/
</p>
<p>h.y/ dy D lim
c!1
</p>
<p>H.c/ � H.g.t// :
</p>
<p>Therefore, due to the chain rule it holds for the density that
</p>
<p>F0b.t/ D �h .g.t// g0.t/ D
b e�
</p>
<p>b2
</p>
<p>2t t�
3
2
</p>
<p>p
2�
</p>
<p>:
</p>
<p>Hence, the variable transformation results in t D b2u�2 with dt D �2 b2 u�3du:
</p>
<p>E .Tb/ D
Z 1
</p>
<p>0
</p>
<p>t F0b.t/ dt
</p>
<p>D bp
2�
</p>
<p>Z 1
</p>
<p>0
</p>
<p>e�
b2
</p>
<p>2t t�
1
2 dt D �2b
</p>
<p>2
</p>
<p>p
2�
</p>
<p>Z 0
</p>
<p>1
e�
</p>
<p>u2
</p>
<p>2 u�2 du
</p>
<p>D 2b
2
</p>
<p>p
2�
</p>
<p>Z 1
</p>
<p>0
</p>
<p>e�
u2
</p>
<p>2 u�2du</p>
<p/>
</div>
<div class="page"><p/>
<p>174 7 Wiener Processes (WP)
</p>
<p>� 2b
2
</p>
<p>p
2�
</p>
<p>Z 1
</p>
<p>0
</p>
<p>e�
u2
</p>
<p>2 u�2du
</p>
<p>� 2b
2
</p>
<p>p
2�
</p>
<p>e�
1
2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>u�2 du:
</p>
<p>However, this last integral written down symbolically does not exist since the
</p>
<p>antiderivative of u�2 is �u�1, and
Z 1
</p>
<p>"
</p>
<p>u�2 du D "�1 � 1
</p>
<p>diverges as "! 0. This completes the proof.
7.6 First, we determine that
</p>
<p>X.t/ D W.t/ � t W.1/ ; t 2 Œ0; 1&#141; ;
</p>
<p>has zero expectation:
</p>
<p>E.X.t// D E.W.t// � t E.W.1// D 0 � 0 :
</p>
<p>By multiplying out and application of (7.4) one can show that
</p>
<p>Cov.X.t/; X.s// D E.X.t/X.s//
D E
</p>
<p>�
W.t/W.s/ � tW.1/W.s/ � sW.1/W.t/C stW2.1/
</p>
<p>�
</p>
<p>D min.s; t/ � t min.s; 1/� s min.t; 1/C st
D min.s; t/ � st � st C st
D min.s; t/ � st :
</p>
<p>In particular, for s D t the variance formula (7.6) is obtained.
7.7 At first we determine the distribution function (7.7) for X.t/ D jW.t/j:
</p>
<p>Fx.x/ D P.X.t/ � x/ ; x � 0 ;
D P.W.t/ � x/ � P.W.t/ � �x/
D P.W.t/ � x/ � .1� P.W.t/ � x//
D 2 P.W.t/ � x/� 1 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 Problems and Solutions 175
</p>
<p>where the symmetry of the Gaussian distribution was used. With W.t/ � N .0; t/
we therefore have
</p>
<p>Fx.x/ D
2p
2�t
</p>
<p>Z x
</p>
<p>�1
e�
</p>
<p>y2
</p>
<p>2t dy � 1 ;
</p>
<p>or for the density
</p>
<p>fx.x/ D F0x.x/ D
2p
2�t
</p>
<p>e�
x2
</p>
<p>2t :
</p>
<p>In order to calculate expectation and variance, we determine the r-th moment in
</p>
<p>general:
</p>
<p>E.Xr.t// D
Z 1
</p>
<p>0
</p>
<p>xr fx.x/ dx D
2p
2�t
</p>
<p>Z 1
</p>
<p>0
</p>
<p>xre�
x2
</p>
<p>2t dx :
</p>
<p>By substitution, these moments can be reduced to the Gamma function, which was
</p>
<p>introduced in Problem 5.3, see also below Eq. (5.20). For a &gt; 0 and with a x2 D u
and du D 2ax dx, we obtain:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>xre�ax
2
</p>
<p>dx D
Z 1
</p>
<p>0
</p>
<p>�u
a
</p>
<p>� r�1
2
</p>
<p>e�u
du
</p>
<p>2a
</p>
<p>D 1
2
</p>
<p>a�
rC1
2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>u
rC1
2 �1e�udu
</p>
<p>D 1
2
</p>
<p>a�
rC1
2 &#13;
</p>
<p>�
r C 1
2
</p>
<p>�
:
</p>
<p>The Gamma function possesses a number of nice properties and special values,
</p>
<p>remember in particular e.g.
</p>
<p>&#13; .1/ D 1 ; &#13;
�
1
</p>
<p>2
</p>
<p>�
D
</p>
<p>p
� ; &#13; .n C 1/ D n&#13; .n/ :
</p>
<p>With a D 1
2t
</p>
<p>, for the moments it therefore follows that:
</p>
<p>E.X.t// D
r
2t
</p>
<p>�
; E.X2.t// D 2p
</p>
<p>2�t
</p>
<p>1
</p>
<p>2
.2t/
</p>
<p>3
2
1
</p>
<p>2
&#13;
</p>
<p>�
1
</p>
<p>2
</p>
<p>�
D t :
</p>
<p>The variance formula is obtained by the usual variance decomposition:
</p>
<p>Var.X.t// D E
�
X2.t/
</p>
<p>�
� .E.X.t///2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>176 7 Wiener Processes (WP)
</p>
<p>7.8 The random variable � W.t/ follows for fixed t a Gaussian distribution with
</p>
<p>expectation and median equal to zero. Hence, it follows that
</p>
<p>P.� W.t/ � 0/ D P
�
e� W.t/ � 1
</p>
<p>�
D 0:5 :
</p>
<p>By multiplying the inequality by e� t, we obtain
</p>
<p>P
�
e� t e� W.t/ � e� t
</p>
<p>�
D P
</p>
<p>�
e� tC� W.t/ � e� t
</p>
<p>�
D 0:5 :
</p>
<p>Therefore, the median of X.t/ D e� tC� W.t/ is determined independently of � as e� t
as claimed.
</p>
<p>7.9 As X.t/ D max0�s�t W.s/ is a continuous random variable for given t, it holds
that
</p>
<p>Fx.b/ WD P.X.t/ � b/ D P
�
</p>
<p>max
0�s�t
</p>
<p>W.s/ &lt; b
</p>
<p>�
:
</p>
<p>Remember the random variable Tb from Proposition 7.1 specifying the point in time
</p>
<p>at which W.t/ hits the value b for the first time. The event max0�s�t W.s/ &lt; b is
equivalent to the fact that the hitting time of b is larger than t. Therefore, when using
</p>
<p>the distribution function from Proposition 7.1, it holds that
</p>
<p>P.X.t/ � b/ D 1 � P.Tb � t/ D 1 �
2p
2�
</p>
<p>Z 1
</p>
<p>b=
p
</p>
<p>t
</p>
<p>exp
</p>
<p>�
� z
</p>
<p>2
</p>
<p>2
</p>
<p>�
dz :
</p>
<p>Naturally, the number 1 can be written as an integral over the density of the standard
</p>
<p>normal distribution:
</p>
<p>P.X.t/ � b/ D 2p
2�
</p>
<p>Z 1
</p>
<p>0
</p>
<p>exp
</p>
<p>�
� z
</p>
<p>2
</p>
<p>2
</p>
<p>�
dz � 2p
</p>
<p>2�
</p>
<p>Z 1
</p>
<p>b=
p
</p>
<p>t
</p>
<p>exp
</p>
<p>�
� z
</p>
<p>2
</p>
<p>2
</p>
<p>�
dz
</p>
<p>D 2p
2�
</p>
<p>Z b=pt
</p>
<p>0
</p>
<p>exp
</p>
<p>�
� z
</p>
<p>2
</p>
<p>2
</p>
<p>�
dz :
</p>
<p>By substitution,
</p>
<p>z D yp
t
</p>
<p>and dz D dyp
t
;
</p>
<p>and due to (7.7) the desired result is immediately obtained:
</p>
<p>P.X.t/ � b/ D 2p
2�t
</p>
<p>Z b
</p>
<p>0
</p>
<p>exp
</p>
<p>�
�y
</p>
<p>2
</p>
<p>2t
</p>
<p>�
dy
</p>
<p>D P .jW.t/j � b/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>References 177
</p>
<p>References
</p>
<p>Billingsley, P. (1986). Probability and measure (2nd ed.). New York: Wiley.
Gradshteyn, I. S., &amp; Ryzhik, I. M. (2000). Table of integrals, series, and products (6th ed.).
</p>
<p>London/San Diego: Academic Press.
Johnson, N. L., Kotz, S., &amp; Balakrishnan, N. (1994). Continuous univariate distributions, Volume
</p>
<p>1 (2nd ed.). New York: Wiley.
Klebaner, F. C. (2005). Introduction to stochastic calculus with applications (2nd ed.). London:
</p>
<p>Imperical College Press.
Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
</p>
<p>Berlin/New York: Springer.</p>
<p/>
</div>
<div class="page"><p/>
<p>8Riemann Integrals
</p>
<p>8.1 Summary
</p>
<p>In this chapter we deal with stochastic Riemann integrals, i.e. with ordinary
</p>
<p>Riemann integrals with a stochastic process as the integrand.1 Mathematically,
</p>
<p>these constructs are relatively unsophisticated, they can be defined pathwise for
</p>
<p>continuous functions as in conventional (deterministic) calculus. However, this
</p>
<p>pathwise definition will not be possible any longer for e.g. Ito integrals in the chapter
</p>
<p>after next. Hence, at this point we propose a way of defining integrals as a limit (in
</p>
<p>mean square) which will be useful later on. If the stochastic integrand is in particular
</p>
<p>a Wiener process, then the Riemann integral follows a Gaussian distribution with
</p>
<p>zero expectation and the familiar formula for the variance. A number of examples
</p>
<p>will facilitate the understanding of this chapter.
</p>
<p>8.2 Definition and Fubini&rsquo;s Theorem
</p>
<p>As one has done in deterministic calculus, we will define the Riemann integral by
</p>
<p>an adequate partition as the limit of a sum.
</p>
<p>Partition
</p>
<p>In order to define an integral of a function from 0 to t, we decompose the interval
</p>
<p>into n adjacent, non-overlapping subintervals which are allowed to intersect at the
</p>
<p>1Bernhard Riemann (1826&ndash;1866) studied with Gauss in G&ouml;ttingen where he himself became
a professor. Already before his day, integration had been used as a technique which reverses
differentiation by forming an antiderivative. However, Riemann explained for the first time under
which conditions a function possesses an antiderivative at all.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_8
</p>
<p>179</p>
<p/>
</div>
<div class="page"><p/>
<p>180 8 Riemann Integrals
</p>
<p>endpoints:
</p>
<p>Pn .Œ0; t&#141;/ W 0 D s0 &lt; s1 &lt; : : : &lt; sn D t : (8.1)
</p>
<p>In the following, we always assume that the partition Pn .Œ0; t&#141;/ becomes increas-
</p>
<p>ingly fine with n growing (&ldquo;adequate partition&rdquo;):
</p>
<p>max
1�i�n
</p>
<p>.si � si�1/ ! 0 for n ! 1 : (8.2)
</p>
<p>By s�i we denote an arbitrary point in the i-th interval,
</p>
<p>s�i 2 Œsi�1; si&#141; ; i D 1; : : : ; n :
</p>
<p>Occasionally, we will sum up the lengths of the subintervals. Obviously, it holds
</p>
<p>that
</p>
<p>nX
</p>
<p>iD1
.si � si�1/ D sn � s0 D t :
</p>
<p>In general, for a function ' one obtains:
</p>
<p>nX
</p>
<p>iD1
.'.si/� '.si�1// D '.t/ � '.0/ : (8.3)
</p>
<p>Sometimes, we will operate with the example of the equidistant partition. It is given
</p>
<p>by si D it=n:
</p>
<p>0 D s0 &lt; s1 D
t
</p>
<p>n
&lt; : : : &lt; sn�1 D
</p>
<p>n � 1
n
</p>
<p>t &lt; sn D t :
</p>
<p>Due to si � si�1 D 1=n the required refinement from (8.2) for n ! 1 is guaranteed.
</p>
<p>Definition and Existence
</p>
<p>Now, the product of a deterministic function f and a stochastic process X is to
</p>
<p>be integrated. To this end, the Riemann sum is defined by means of the notation
</p>
<p>introduced:
</p>
<p>Rn D
nX
</p>
<p>iD1
f .s�i /X.s
</p>
<p>�
i / .si � si�1/: (8.4)
</p>
<p>Here, we have a sum of rectangular areas, each with a width of .si � si�1/ and the
height f .s�i /X.s
</p>
<p>�
i /. With n growing, the area beneath f .s/X.s/ on Œ0; t&#141; is to be</p>
<p/>
</div>
<div class="page"><p/>
<p>8.2 Definition and Fubini&rsquo;s Theorem 181
</p>
<p>approximated all the better. If the limit of this sum for n ! 1 exists uniquely
and independently of the partition and of the choice of s�i , then it is defined as a
(stochastic) Riemann integral. In this case, the convergence occurs in mean square
</p>
<p>(&ldquo;
2!&rdquo;)2:
</p>
<p>Rn D
nX
</p>
<p>iD1
f .s�i /X.s
</p>
<p>�
i / .si � si�1/
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>f .s/X.s/ ds :
</p>
<p>One then says that the Riemann integral exists. For this existence, there is a
</p>
<p>sufficient and a necessary condition formulated in the following proposition. The
</p>
<p>proof is carried out with part (b) from Lemma 8.2 below, see Problem 8.1. Further
</p>
<p>elaborations on mean square convergence can be found at the end of the chapter.
</p>
<p>Proposition 8.1 (Existence of the Riemann Integral) The Riemann sum from
</p>
<p>Eq. (8.4) converges in mean square for n ! 1 under (8.2) if and only if the double
integral
</p>
<p>Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ f .r/E .X.s/X.r// drds
</p>
<p>exists.
</p>
<p>A sufficient condition for the existence of the Riemann integral is that the
</p>
<p>function f is continuous and that furthermore E .X.s/X.r// is continuous in both
</p>
<p>arguments. In order to find this, we define
</p>
<p>'.s/ WD f .s/
Z t
</p>
<p>0
</p>
<p>f .r/E .X.s/X.r// dr :
</p>
<p>Now, if the function E .X.s/X.r// is continuous in both arguments, this implies
</p>
<p>continuity of ' for a continuous f as the integral is a continuous functional, see e.g.
</p>
<p>Trench (2013, p. 462). Therefore, the ordinary Riemann integral of ' exists,
</p>
<p>Z t
</p>
<p>0
</p>
<p>'.s/ ds D
Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ f .r/E .X.s/X.r// drds :
</p>
<p>Hence, the Riemann sum from (8.4) converges due to Proposition 8.1.
</p>
<p>2A definition and discussion of this mode of convergence can be found in the fourth section of this
chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 8 Riemann Integrals
</p>
<p>Fubini&rsquo;s Theorem
</p>
<p>Frequently, we are interested in the average behavior, i.e. the expected value of
</p>
<p>Riemann integrals. The expected value, however, is defined as an integral itself such
</p>
<p>that one is confronted with double integrals. For calculating these, there is a simple
</p>
<p>rule which is finally based on the fact that the order of integration does not matter for
</p>
<p>double integrals over continuous functions. In deterministic calculus, this fact is also
</p>
<p>known as &ldquo;Fubini&rsquo;s theorem&rdquo; (see Footnote 6 in Sect. 2.3). Adapted to our problem
</p>
<p>of the expected value of a Riemann integral, the corresponding circumstances are
</p>
<p>given in the following proposition, also cf. Billingsley (1986, Theorem 18.3).
</p>
<p>Proposition 8.2 (Fubini&rsquo;s Theorem) If
R t
0 E.jX.s/j/ ds exists, it holds for a contin-
</p>
<p>uous X that:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>X.s/ ds
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>E.X.s// ds :
</p>
<p>The statement is easy to comprehend if one thinks of the integral as a finite
</p>
<p>Riemann sum. As is well known, in the discrete case, summation and expectation is
</p>
<p>interchangeable:
</p>
<p>E
</p>
<p> 
nX
</p>
<p>iD1
X.s�i / .si � si�1/
</p>
<p>!
D
</p>
<p>nX
</p>
<p>iD1
E.X.s�i // .si � si�1/ :
</p>
<p>Now, Fubini&rsquo;s theorem just guarantees a continuation of this interchangeability for
</p>
<p>n ! 1.
</p>
<p>Example 8.1 (Expected Value of the Integrated WP) Consider the special case of
</p>
<p>the integrated WP with X.s/ D W.s/ and f .s/ D 1. With the WP being continuous,
jW.t/j is a continuous process as well. In (7.8) we have determined the following
expression as the expected value:
</p>
<p>E.jW.t/j/ D
r
2t
</p>
<p>�
:
</p>
<p>Before applying Proposition 8.2, we check:
</p>
<p>Z t
</p>
<p>0
</p>
<p>E .jW.s/j/ ds D
Z t
</p>
<p>0
</p>
<p>r
2
</p>
<p>�
s
1
2 ds
</p>
<p>D
r
2
</p>
<p>�
</p>
<p>�
2
</p>
<p>3
s
3
2
</p>
<p>�t
</p>
<p>0
</p>
<p>D
r
2
</p>
<p>�
</p>
<p>2
</p>
<p>3
t
3
2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Riemann Integration of Wiener Processes 183
</p>
<p>As this quantity is finite, the requirements of Fubini&rsquo;s theorem are fulfilled. Hence,
</p>
<p>it follows that
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ ds
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>E.W.s// ds D 0 : �
</p>
<p>General Rules
</p>
<p>Note that our definition of the integral seems to be unnecessarily restrictive.
</p>
<p>However, the restriction on the interval Œ0; t&#141; is by no means crucial. The usual rules
</p>
<p>apply and are here symbolically described for an integrand g (without proof):
</p>
<p>Z b
</p>
<p>a
</p>
<p>g.x/ dx D
Z c
</p>
<p>a
</p>
<p>g.x/ dx C
Z b
</p>
<p>c
</p>
<p>g.x/ dx for a � c � b ;
</p>
<p>Z
.g1.x/C g2.x// dx D
</p>
<p>Z
g1.x/ dx C
</p>
<p>Z
g2.x/ dx ;
</p>
<p>Z
c g.x/ dx D c
</p>
<p>Z
g.x/ dx for c 2 R :
</p>
<p>8.3 Riemann Integration of Wiener Processes
</p>
<p>In this section, we concentrate on Riemann integrals where the stochastic part of the
</p>
<p>integrand is a WP: X.t/ D W.t/.
</p>
<p>Normal Distribution
</p>
<p>Frequently, Gaussian random variables are hidden behind Riemann integrals. In
</p>
<p>fact, it holds that all of the integrals discussed in this section follow Gaussian
</p>
<p>distributions with zero expectation. The variances can be determined according to
</p>
<p>the following proposition (for a proof see Problem 8.3).
</p>
<p>Proposition 8.3 (Normality of Riemann Integrals) Let f be a continuous deter-
</p>
<p>ministic function on Œ0; t&#141;. Then, it holds
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/W.s/ ds � N
�
0 ;
</p>
<p>Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/f .s/min.r; s/drds
</p>
<p>�
:
</p>
<p>The normality follows from the fact that the WP is a Gaussian process. Hence,
</p>
<p>the Riemann sum Rn from (8.4) follows a Gaussian distribution for finite n. As
</p>
<p>Rn converges in mean square, it follows from Lemma 8.1 (see below) that the</p>
<p/>
</div>
<div class="page"><p/>
<p>184 8 Riemann Integrals
</p>
<p>limit is Gaussian as well. Note that the finiteness of the variance expression from
</p>
<p>Proposition 8.3 is just sufficient and necessary for the existence of the Riemann
</p>
<p>integral (Proposition 8.1).
</p>
<p>Example 8.2 (Variance of the Integrated WP) Consider an integrated WP with
</p>
<p>f .s/ D 1 as in Example 8.1. We look for a closed expression for the variance ofR t
0 W.s/ds. Due to Proposition 8.3, the starting point is:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>min.r; s/ drds :
</p>
<p>Now, we employ a useful trick for many applications. The integral with respect to r
</p>
<p>is decomposed into the sum of two integrals with s as the integration limit such that
</p>
<p>the minimum function can be specified explicitly:
</p>
<p>Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>min.r; s/ drds D
Z t
</p>
<p>0
</p>
<p>�Z s
</p>
<p>0
</p>
<p>min.r; s/ dr C
Z t
</p>
<p>s
</p>
<p>min.r; s/ dr
</p>
<p>�
ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>�Z s
</p>
<p>0
</p>
<p>r dr C
Z t
</p>
<p>s
</p>
<p>s dr
</p>
<p>�
ds :
</p>
<p>Now, the integration of the power functions yields the requested variance:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>�Z s
</p>
<p>0
</p>
<p>r dr C
Z t
</p>
<p>s
</p>
<p>s dr
</p>
<p>�
ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>�
s2
</p>
<p>2
C s .t � s/
</p>
<p>�
ds
</p>
<p>D
�
</p>
<p>s2 t
</p>
<p>2
� s
</p>
<p>3
</p>
<p>6
</p>
<p>�t
</p>
<p>0
</p>
<p>D t
3
</p>
<p>3
: �
</p>
<p>Autocovariance Function
</p>
<p>With the time-dependent integration limit,
R t
0 f .s/W.s/ ds itself is a stochastic
</p>
<p>process. Therefore, it suggests itself to not only determine the variance as in
</p>
<p>Proposition 8.3, but the covariance function as well. The general result is given
</p>
<p>in the following proposition, which will be verified in Problem 8.7.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.3 Riemann Integration of Wiener Processes 185
</p>
<p>Proposition 8.4 (Autocovariance Function) For a continuous function f with
</p>
<p>integrable antiderivative F and Y.t/ D
R t
0
</p>
<p>f .s/W.s/ds it holds that:
</p>
<p>E.Y.t/ Y.t C h// D
Z t
</p>
<p>0
</p>
<p>f .s/
</p>
<p>�
s F.s/�
</p>
<p>Z s
</p>
<p>0
</p>
<p>F.r/dr C s.F.t C h/� F.s//
�
</p>
<p>ds ;
</p>
<p>where h � 0.
</p>
<p>Therefore, with h D 0 an alternative expression for the variance from Propo-
sition 8.3 is obtained. For concrete functions f , the formula can be simplified
</p>
<p>considerably. This is to be shown by the following example.
</p>
<p>Example 8.3 (Autocovariance of the Integrated WP) Once again, we examine the
</p>
<p>integrated WP with f .s/ D 1 and F.s/ D s as in Examples 8.1 and 8.2. Then,
plugging in yields:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>W.r/dr
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>�
s2 � 1
</p>
<p>2
s2 C s..t C h/ � s/
</p>
<p>�
ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>�
s.t C h/� 1
</p>
<p>2
s2
�
</p>
<p>ds
</p>
<p>D t
2.t C h/
2
</p>
<p>� t
3
</p>
<p>6
:
</p>
<p>Hence, for h D 0 the variance of the integrated Wiener process reads:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>�
D t
</p>
<p>3
</p>
<p>2
� t
</p>
<p>3
</p>
<p>6
D t
</p>
<p>3
</p>
<p>3
:
</p>
<p>Of course, we already know this from Example 8.2.�
</p>
<p>Examples
</p>
<p>For three special Gaussian integrals, which we will be confronted with over and
</p>
<p>over, the variances are to be calculated. We put the results in front.</p>
<p/>
</div>
<div class="page"><p/>
<p>186 8 Riemann Integrals
</p>
<p>Corollary 8.1 It holds that
</p>
<p>.a/
1R
0
</p>
<p>W.s/ ds � N .0; 1=3/ ;
</p>
<p>.b/ W.1/ �
1R
0
</p>
<p>W.s/ ds � N .0; 1=3/ ;
</p>
<p>.c/
1R
0
</p>
<p>.s � c/W.s/ ds � N .0; �2R/ ;
</p>
<p>where c 2 R and �2R D 8�25cC20c
2
</p>
<p>60
&gt; 0.
</p>
<p>The normality in (a) and (c) is clear due to Proposition 8.3. In (b) we have the sum
</p>
<p>of two Gaussian random variables which does not necessarily have to be Gaussian
</p>
<p>again unless a multivariate Gaussian distribution is present. Thus, the normality of
</p>
<p>(b) can only be proven in connection with Stieltjes integrals (see Problem 9.2).
</p>
<p>The result from (a) is a special case of Example 8.2 with t D 1. We show in
Problem 8.4 that the variance in (b) is just 1=3. The proof of (c) for c D 0 is
given in Problem 8.5; for an arbitrary c, the proof is basically similar, however, it
</p>
<p>gets computationally more involved. Note that the variance �2R cannot be zero or
</p>
<p>negative for any c (Problem 8.6).
</p>
<p>Again, there should be a word of warning concerning equality in distribution.
</p>
<p>From (b) it follows that:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ ds � W.1/ � N .0; 1=3/ :
</p>
<p>Therefore, the following random variables are equal in distribution,
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ ds � W.1/ �
Z 1
</p>
<p>0
</p>
<p>W.s/ ds ;
</p>
<p>although, pathwise it obviously holds that:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ ds � W.1/ &curren;
Z 1
</p>
<p>0
</p>
<p>W.s/ ds :
</p>
<p>8.4 Convergence in Mean Square
</p>
<p>Now, we hand in some basics which are not necessary for the understanding of
</p>
<p>Riemann integrals; however, they are helpful for some technical properties. In
</p>
<p>particular, for the elaboration on the Ito integral following below, the knowledge
</p>
<p>of convergence in mean square is advantageous for a complete understanding. For</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Convergence in Mean Square 187
</p>
<p>a brief introduction to the basics of asymptotic theory, P&ouml;tscher and Prucha (2001)
</p>
<p>can be recommended.
</p>
<p>Definition and Properties
</p>
<p>Let fXng, n 2 N, be a sequence of real random variables with
</p>
<p>E.X2n/ &lt;1 : (8.5)
</p>
<p>For a sequence fXng and a random variable X, we define the mean squared error
as distance or norm:
</p>
<p>MSE.Xn;X/ WD E
�
.Xn � X/2
</p>
<p>�
;
</p>
<p>One says, fXng converges in mean square to X for n tending to infinity if
</p>
<p>MSE.Xn;X/ ! 0 ; n ! 1 :
</p>
<p>Abbreviating, we write for this as well
</p>
<p>Xn
2! X :
</p>
<p>This limit is unique with probability one. Of course, it can be a random variable itself
</p>
<p>or a constant. In any case, due to (8.5) it holds that: E.X2/ &lt; 1. In fact, expected
value and variance of X can be determined from the moments of Xn. In particular, the
</p>
<p>limit of Gaussian random variables is again Gaussian. More precisely, the following
</p>
<p>lemma holds (see Problem 8.8 for a proof).
</p>
<p>Lemma 8.1 (Properties of the Limit in Mean Square) Let fXng with (8.5)
converge in mean square to X. Then it holds for n ! 1:
</p>
<p>(a) E.Xn/! E.X/;
(b) E.X2n/! E.X2/;
(c) if fXng is Gaussian, then X follows a Gaussian distribution as well.
</p>
<p>Naturally, the parameters of the Gaussian distribution X from (c) follow accord-
</p>
<p>ing to (a) and (b).
</p>
<p>Convergence to a Constant
</p>
<p>If the limit is a constant, then it is particularly easy to establish convergence in mean
</p>
<p>square. For this purpose, we consider the following derivation. By zero addition and</p>
<p/>
</div>
<div class="page"><p/>
<p>188 8 Riemann Integrals
</p>
<p>the binomial formula, the following expression is obtained:
</p>
<p>ŒXn � X&#141;2 D Œ.Xn � E.Xn// � .X � E.Xn//&#141;2
</p>
<p>D .Xn � E.Xn//2 � 2.Xn � E.Xn//.X � E.Xn//C .X � E.Xn//2:
</p>
<p>The expectation operator yields:
</p>
<p>MSE.Xn;X/ D Var.Xn/
�2E Œ.Xn � E.Xn//.X � E.Xn//&#141;C E
</p>
<p>�
.X � E.Xn//2
</p>
<p>�
:
</p>
<p>If X is a constant (a &ldquo;degenerate random variable&rdquo;), X D c, then the second term
becomes zero and the third term is the expected value of a constant. In other words,
</p>
<p>this yields:
</p>
<p>MSE.Xn; c/ D Var.Xn/C Œc � E.Xn/&#141;2 :
</p>
<p>Hence, fXng converges in mean square to a constant c if and only if it holds that
</p>
<p>Var.Xn/ ! 0 and E.Xn/ ! c ; n ! 1 :
</p>
<p>As is well known, this implies that fXng converges to c in probability as well (see
Lemma 8.3 below). Next, we cover criteria of convergence.
</p>
<p>Test of Convergence
</p>
<p>Now, we still need a convenient criterion in order to decide whether a series is
</p>
<p>convergent in mean square. In fact, we have two equivalent criteria. For the proof
</p>
<p>see Problem 8.9. The name goes back to the famous French mathematician Augustin
</p>
<p>Louis Cauchy (1789&ndash;1857).
</p>
<p>Lemma 8.2 (Cauchy Criterion) A series fXng with (8.5) converges in mean
square : : :
</p>
<p>(a) : : : if and only if it holds for arbitrary n and m that
</p>
<p>E
h
.Xm � Xn/2
</p>
<p>i
! 0 ; m; n ! 1I
</p>
<p>(b) : : : or put equivalently, if and only if it holds for arbitrary n and m that
</p>
<p>E .Xm Xn/ ! c &lt;1 ; m; n ! 1 ;
</p>
<p>where c 2 R is a constant.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Convergence in Mean Square 189
</p>
<p>Note that the convergence of the Cauchy criterion holds independently of how m
</p>
<p>and n tend to infinity. As well, the constant c results independently of the choice of
</p>
<p>m and n. As the criteria are sufficient and necessary, the proof of existence for mean
</p>
<p>square convergence can be supplied without determining the limit explicitly.
</p>
<p>Example 8.4 (Law of Large Numbers) Let f"tg be a white noise process, "t �
WN.0; �2/. Then, it can be shown that the arithmetic mean,
</p>
<p>Xn WD "n D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
"t ;
</p>
<p>converges in mean square without specifying the limit. It namely holds that
</p>
<p>E ."n "m/ D
1
</p>
<p>mn
E
</p>
<p>"
nX
</p>
<p>tD1
"t
</p>
<p>mX
</p>
<p>tD1
"t
</p>
<p>#
D 1
</p>
<p>mn
</p>
<p>min.n;m/X
</p>
<p>tD1
E
�
"2t
�
</p>
<p>D �2 min.n;m/
m n
</p>
<p>! 0
</p>
<p>for m ; n ! 1. Due to Lemma 8.2(b) we conclude that "n has a limit in mean
square.
</p>
<p>Let the limit of "n simply be called ". Naturally, it can be determined immediately.
</p>
<p>Due to
</p>
<p>E ."n/ D 0 and Var ."n/ D
�2
</p>
<p>n
</p>
<p>it follows from Lemma 8.1(a) and (b) for the limit that
</p>
<p>E ."/ D 0 and Var ."/ D 0 :
</p>
<p>Hence, the limit is equal to zero (with probability one). From this, it follows for
</p>
<p>xt D � C "t that the arithmetic mean of xt converges in mean square to the true
expected value, �. In the literature, this fact is also known as the &ldquo;law of large
</p>
<p>numbers&rdquo;. �
</p>
<p>Further Modes of Convergence
</p>
<p>Two weaker concepts of convergence can be defined via probability statements.
</p>
<p>First, we say fXng converges in probability to X if it holds for arbitrary " &gt; 0 that:
</p>
<p>lim
n!1
</p>
<p>P.jXn � Xj � "/ D 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>190 8 Riemann Integrals
</p>
<p>Symbolically, this is denoted as3
</p>
<p>Xn
p! X for n ! 1 :
</p>
<p>Secondly, one talks about convergence in distribution of fXng to X if it holds for
all points x 2 R at which the distribution function Fn.x/ of Xn is continuous, that
Fn.x/ tends to the distribution function F.x/ of X:
</p>
<p>lim
n!1
</p>
<p>Fn.x/ D lim
n!1
</p>
<p>P.Xn � x/ D P.X � x/ D F.x/ :
</p>
<p>The word &ldquo;distribution&rdquo; suggests the symbolic notation:
</p>
<p>Xn
d! X for n ! 1 :
</p>
<p>From Grimmett and Stirzaker (2001, p. 310) or P&ouml;tscher and Prucha (2001, Theorem
</p>
<p>5 and 9) we adopt the following results.
</p>
<p>Lemma 8.3 (Implications of Convergence) The following implications hold, n !
1.
</p>
<p>(a) Convergence in mean square implies convergence in probability:
</p>
<p>.Xn
2! X/ H) .Xn
</p>
<p>p! X/ :
</p>
<p>(b) Convergence in probability implies convergence in distribution:
</p>
<p>.Xn
p! X/ H) .Xn
</p>
<p>d! X/ :
</p>
<p>In general, the converse of Lemma 8.3(a) or (b) does not hold. However, if X D c
is a constant, then convergence in probability and convergence in distribution are
</p>
<p>equivalent, see Grimmett and Stirzaker (2001, p. 310) or P&ouml;tscher and Prucha (2001,
</p>
<p>Theorem 10).
</p>
<p>8.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>8.1 Prove Proposition 8.1.
</p>
<p>Hint: Use Lemma 8.2.
</p>
<p>3In particular in econometrics, one often writes alternatively plimXn D X as n ! 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Problems and Solutions 191
</p>
<p>8.2 Determine the expected value from Proposition 8.3.
</p>
<p>8.3 Determine the variance from Proposition 8.3.
</p>
<p>8.4 Calculate the variance from Corollary 8.1(b).
</p>
<p>8.5 Calculate the variance from Corollary 8.1(c) for the special case of c D 0.
</p>
<p>8.6 Show that the variance from Corollary 8.1(c) is positive.
</p>
<p>8.7 Prove Proposition 8.4.
</p>
<p>8.8 Prove Lemma 8.1.
</p>
<p>8.9 Prove Lemma 8.2.
</p>
<p>Solutions
</p>
<p>8.1 Analogously to the partition (8.1) and the Riemann sum Rn from (8.4), we define
</p>
<p>for arbitrary m with m ! 1:
</p>
<p>Pm .Œ0; t&#141;/ W 0 D r0 &lt; : : : &lt; rm D t ; max
1�j�m
</p>
<p>�
rj � rj�1
</p>
<p>�
! 0 ;
</p>
<p>Rm D
mX
</p>
<p>jD1
f .r�j /X.r
</p>
<p>�
j / .rj � rj�1/ ; r�j 2 Œrj�1; rj&#141; :
</p>
<p>In order to apply the existence criterion from Lemma 8.2(b), we formulate the
</p>
<p>product of the two Riemann sums as follows:
</p>
<p>Rn Rm D
nX
</p>
<p>iD1
</p>
<p>mX
</p>
<p>jD1
f .s�i /f .r
</p>
<p>�
j /X.s
</p>
<p>�
i /X.r
</p>
<p>�
j / .rj � rj�1/.si � si�1/ :
</p>
<p>Hence, the Riemann integral as limit of Rn exists if and only if E.Rn Rm/ converges.
</p>
<p>Further,
</p>
<p>E.Rn Rm/ D
nX
</p>
<p>iD1
</p>
<p>mX
</p>
<p>jD1
f .s�i /f .r
</p>
<p>�
j /E
</p>
<p>�
X.s�i /X.r
</p>
<p>�
j /
�
.rj � rj�1/.si � si�1/ ;
</p>
<p>converges if and only if the ordinary Riemann double integral
</p>
<p>Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ f .r/E .X.s/X.r// drds &lt;1</p>
<p/>
</div>
<div class="page"><p/>
<p>192 8 Riemann Integrals
</p>
<p>exists. The Cauchy criterion from Lemma 8.2 therefore amounts to a proof of
</p>
<p>Proposition 8.1.
</p>
<p>8.2 For the WP it holds that
</p>
<p>E.W.t// D 0 and E .W.s/W.r// D min.s; r/ :
</p>
<p>The minimum function is continuous in both the arguments. If f is a continuous
</p>
<p>function as well, then we know, with the considerations following Proposition 8.1,
</p>
<p>that the stochastic Riemann integral
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/W.s/ ds
</p>
<p>exists. In order to calculate the expected value, Fubini&rsquo;s theorem will be applied.
</p>
<p>For this purpose, we check that
</p>
<p>Z t
</p>
<p>0
</p>
<p>E .j f .s/W.s/j/ ds D
Z t
</p>
<p>0
</p>
<p>j f .s/j E .jW.s/j/ ds
</p>
<p>� max
0�s�t
</p>
<p>j f .s/j
Z t
</p>
<p>0
</p>
<p>E .jW.s/j/ ds
</p>
<p>is finite. The bound is based on the continuity and hence the finiteness of f . The
</p>
<p>integral
</p>
<p>Z t
</p>
<p>0
</p>
<p>E .jW.s/j/ ds
</p>
<p>was determined in Example 8.1 for t fixed to be finite. As the WP is continuous,
</p>
<p>Proposition 8.2 can be applied. According to this, it holds that:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/W.s/ ds
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>f .s/E.W.s// ds D 0 :
</p>
<p>8.3 Let us denote the Riemann integral by Y.t/:
</p>
<p>Y.t/ D
Z t
</p>
<p>0
</p>
<p>f .s/W.s/ds:
</p>
<p>We have already shown that E.Y.t// D 0. Hence, it follows that
</p>
<p>Var.Y.t// D E
�
Y2.t/
</p>
<p>�
</p>
<p>D E
�Z t
</p>
<p>0
</p>
<p>f .s/W.s/ds
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/W.r/dr
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Problems and Solutions 193
</p>
<p>D E
�Z t
</p>
<p>0
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .r/W.r/dr
</p>
<p>�
f .s/W.s/ds
</p>
<p>�
</p>
<p>D E
�Z t
</p>
<p>0
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .r/f .s/W.r/W.s/dr
</p>
<p>�
ds
</p>
<p>�
:
</p>
<p>By applying Fubini&rsquo;s theorem twice, we obtain:
</p>
<p>E
�
Y2.t/
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .r/f .s/W.r/W.s/dr
</p>
<p>�
ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>E.f .r/f .s/W.r/W.s//dr ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/f .s/E.W.r/W.s//dr ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/f .s/min.r; s/dr ds ;
</p>
<p>which is the requested result.
</p>
<p>8.4 Let us define
</p>
<p>Y.t/ D W.1/ �
Z 1
</p>
<p>0
</p>
<p>W.s/ ds
</p>
<p>with E.Y.t// D 0. Then, it holds that
</p>
<p>Var.Y.t// D E.Y2.t//
</p>
<p>D Var.W.1//C Var
�Z 1
</p>
<p>0
</p>
<p>W.s/ ds
</p>
<p>�
� 2E
</p>
<p>�
W.1/
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>�
</p>
<p>D 1C 1
3
� 2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>E.W.1/W.s//ds;
</p>
<p>where the variance from Corollary 8.1(a) and Fubini&rsquo;s theorem were used. On Œ0; 1&#141;
</p>
<p>it holds that:
</p>
<p>E.W.1/W.s// D min.1; s/ D s:
</p>
<p>Hence, the variance results as claimed:
</p>
<p>Var.Y.t// D 1C 1
3
� 2
</p>
<p>�
1
</p>
<p>2
s2
�1
</p>
<p>0
</p>
<p>D 1C 1
3
� 1 D 1
</p>
<p>3
:</p>
<p/>
</div>
<div class="page"><p/>
<p>194 8 Riemann Integrals
</p>
<p>8.5 For c D 0 the claim reads
</p>
<p>Var
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>s W.s/ ds
</p>
<p>�
D �2R D
</p>
<p>2
</p>
<p>15
:
</p>
<p>According to Proposition 8.3, the variance results as a double integral for f .s/ D s,
</p>
<p>�2R D
Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>rs min.r; s/ dr ds ;
</p>
<p>where the inner integral is appropriately decomposed to facilitate the calculation:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>Z 1
</p>
<p>0
</p>
<p>rs min.r; s/ dr ds D
Z 1
</p>
<p>0
</p>
<p>s
</p>
<p>�Z s
</p>
<p>0
</p>
<p>r min.r; s/ dr C
Z 1
</p>
<p>s
</p>
<p>r min.r; s/ dr
</p>
<p>�
ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>s
</p>
<p>�Z s
</p>
<p>0
</p>
<p>r2 dr C
Z 1
</p>
<p>s
</p>
<p>rs dr
</p>
<p>�
ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>s
</p>
<p>�
s3
</p>
<p>3
C s
2
.1� s2/
</p>
<p>�
ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>�
s4
</p>
<p>3
C s
</p>
<p>2
</p>
<p>2
� s
</p>
<p>4
</p>
<p>2
</p>
<p>�
ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>�
s2
</p>
<p>2
� s
</p>
<p>4
</p>
<p>6
</p>
<p>�
ds
</p>
<p>D
�
</p>
<p>s3
</p>
<p>6
� s
</p>
<p>5
</p>
<p>30
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
6
� 1
30
</p>
<p>D 4
30
:
</p>
<p>This corresponds to the claimed result.
</p>
<p>8.6 We consider the numerator of �2R,
</p>
<p>n.c/ D 8 � 25c C 20c2;
</p>
<p>and show that it does not have any real zeros. Setting n.c/ D 0 yields:
</p>
<p>c1;2 D
25˙
</p>
<p>p
252 � 4 � 20 � 8
2 � 20
</p>
<p>D 25˙
p
�15
</p>
<p>40
</p>
<p>D 25˙ i
p
15
</p>
<p>40
; i2 D �1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>8.5 Problems and Solutions 195
</p>
<p>Thus, no real zeros exist. Consequently, since n.0/ D 8 &gt; 0 the in c continuous
function n.c/ cannot be zero or negative; the same holds true for �2R , which proves
</p>
<p>the claim.
</p>
<p>Of course, an alternative proof consists in determining the real extrema of n.c/.
</p>
<p>There exists an absolute minimum and this turns out to be positive.
</p>
<p>8.7 If the Riemann integral is denoted by Y.t/, then it holds, as for the derivation of
</p>
<p>the variance, that:
</p>
<p>E.Y.t/ Y.t C h// D
Z t
</p>
<p>0
</p>
<p>f .s/
</p>
<p>�Z s
</p>
<p>0
</p>
<p>f .r/min.r; s/dr C
Z tCh
</p>
<p>s
</p>
<p>f .r/min.r; s/dr
</p>
<p>�
ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>f .s/
</p>
<p>�Z s
</p>
<p>0
</p>
<p>f .r/rdr C s
Z tCh
</p>
<p>s
</p>
<p>f .r/dr
</p>
<p>�
ds:
</p>
<p>Partial integration yields the following relation:
</p>
<p>Z s
</p>
<p>0
</p>
<p>f .r/rdr D F.s/s �
Z s
</p>
<p>0
</p>
<p>F.r/dr:
</p>
<p>By plugging in we obtain the claim.
</p>
<p>8.8 Proof of (a): By bounding the difference of the two expected values by means
</p>
<p>of the Cauchy-Schwarz inequality (2.5) one can immediately tell that this difference
</p>
<p>tends to zero in the case of convergence in mean square:
</p>
<p>jE.Xn/� E.X/j D jE.Xn � X/j
</p>
<p>�
p
</p>
<p>E Œ.Xn � X/2&#141; D
p
</p>
<p>MSE.Xn;X/
</p>
<p>! 0 ; n ! 1 :
</p>
<p>Proof of (b): The simple trick
</p>
<p>X2n � X2 D .Xn � X/2 C 2 .Xn � X/X
</p>
<p>yields upon expectation:
</p>
<p>E.X2n/ � E.X2/ D E
�
.Xn � X/2
</p>
<p>�
C 2E Œ.Xn � X/X&#141;
</p>
<p>� E
�
.Xn � X/2
</p>
<p>�
C 2 jE Œ.Xn � X/X&#141;j
</p>
<p>� E
�
.Xn � X/2
</p>
<p>�
C 2
</p>
<p>p
E Œ.Xn � X/2&#141;
</p>
<p>p
E.X2/
</p>
<p>D MSE.Xn;X/C 2
p
</p>
<p>MSE.Xn;X/
p
</p>
<p>E.X2/
</p>
<p>! 0 ; n ! 1 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>196 8 Riemann Integrals
</p>
<p>where again the Cauchy-Schwarz inequality (2.5) was used.
</p>
<p>Proof of (c): As is well known, convergence in mean square implies convergence
</p>
<p>in distribution, see Lemma 8.3. The latter is equivalent to the fact that the
</p>
<p>characteristic function �n.u/ of Xn tends to the characteristic function �.u/ of X.
4
</p>
<p>Now we show: If �n.u/ belongs to a Gaussian distribution, then this holds for �.u/
</p>
<p>as well. Hence, (a) and (b) in combination with the premise of a Gaussian sequence
</p>
<p>fXng imply:
</p>
<p>�n.u/ D exp
�
</p>
<p>i uE.Xn/ �
u2Var.Xn/
</p>
<p>2
</p>
<p>�
</p>
<p>! exp
�
</p>
<p>i uE.X/ � u
2Var.X/
</p>
<p>2
</p>
<p>�
D �.u/
</p>
<p>for n ! 1. Thus, the characteristic function of X as n ! 1 is that of a Gaussian
distribution as well.
</p>
<p>8.9 Proof of (a): Elementarily, it can be shown that the Cauchy criterion follows
</p>
<p>from convergence in mean square:
</p>
<p>E
�
.Xm � Xn/2
</p>
<p>�
D E
</p>
<p>h
..Xm � X/C .X � Xn//2
</p>
<p>i
</p>
<p>D E
�
.Xm � X/2
</p>
<p>�
C E
</p>
<p>�
.X � Xn/2
</p>
<p>�
</p>
<p>C 2E Œ.Xm � X/.X � Xn/&#141;
� E
</p>
<p>�
.Xm � X/2
</p>
<p>�
C E
</p>
<p>�
.X � Xn/2
</p>
<p>�
</p>
<p>C2
p
</p>
<p>E Œ.Xm � X/2&#141;
p
</p>
<p>E Œ.X � Xn/2&#141;
D MSE .Xm;X/C MSE .Xn;X/
</p>
<p>C2
p
</p>
<p>MSE .Xm;X/
p
</p>
<p>MSE .Xn;X/ ;
</p>
<p>where the bounding is again based on the Cauchy-Schwarz inequality (2.5). It is
</p>
<p>somewhat more involved that, inversely, the condition from (a) implies convergence
</p>
<p>in mean square as well. For the proof, we refer e.g. to the exposition on Hilbert
</p>
<p>spaces in Brockwell and Davis (1991, Ch. 2).
</p>
<p>4See e.g. sections 5.7 through 5.10 in Grimmett and Stirzaker (2001) for an introduction to the
theory and application of characteristic functions. In particular, it holds for the characteristic
function of a random variable with a Gaussian distribution, Y � N .�; �2/, that:
</p>
<p>�y.u/ D exp
�
</p>
<p>i u�� u
2�2
</p>
<p>2
</p>
<p>�
; i2 D �1 ; u 2 R :</p>
<p/>
</div>
<div class="page"><p/>
<p>References 197
</p>
<p>Proof of (b): If we take (a) for granted, the proof is simple. Due to
</p>
<p>E
�
.Xm � Xn/2
</p>
<p>�
D E.X2m/C E.X2n/ � 2E.Xm Xn/;
</p>
<p>one can immediately tell that the condition from (a) implies:
</p>
<p>E.Xm Xn/!
E.X2/C E.X2/
</p>
<p>2
D E.X2/ :
</p>
<p>Inversely, from the condition from (b) it naturally follows that
</p>
<p>E
�
.Xm � Xn/2
</p>
<p>�
D E.X2m/C E.X2n/� 2E.Xm Xn/
! c C c � 2 c D 0 :
</p>
<p>This completes the proof.
</p>
<p>References
</p>
<p>Billingsley, P. (1986). Probability and measure (2nd ed.). New York: Wiley.
Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
</p>
<p>Springer.
Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes (3rd ed.). Oxford:
</p>
<p>Oxford University Press.
P&ouml;tscher, B. M., &amp; Prucha, I. R. (2001). Basic elements of asymptotic theory. In B. H. Baltagi
</p>
<p>(Ed.), A companion to theoretical econometrics (pp. 201&ndash;229). Malden: Blackwell.
Trench, W. F. (2013). Introduction to real analysis. Free Hyperlinked Edition 2.04 December 2013.
</p>
<p>Downloaded on 10th May 2014 from http://digitalcommons.trinity.edu/mono/7.</p>
<p/>
<div class="annotation"><a href="http://digitalcommons.trinity.edu/mono/7">http://digitalcommons.trinity.edu/mono/7</a></div>
</div>
<div class="page"><p/>
<p>9Stieltjes Integrals
</p>
<p>9.1 Summary
</p>
<p>Below, we will encounter Riemann-Stieltjes integrals (or more briefly: Stieltjes
</p>
<p>integrals) as solutions of certain stochastic differential equations. They can be
</p>
<p>reduced to the sum of a Riemann integral and a multiple of the Wiener process.
</p>
<p>Stieltjes integrals are again Gaussian. As an example we consider the Ornstein-
</p>
<p>Uhlenbeck process which is defined by a Stieltjes integral and which will be dealt
</p>
<p>with in detail in the chapter on interest rate models.
</p>
<p>9.2 Definition and Partial Integration
</p>
<p>As a first step towards the Ito integral, we define Stieltjes1 integrals which can be
</p>
<p>reduced to Riemann integrals by integration by parts.
</p>
<p>Definition
</p>
<p>The Riemann-Stieltjes integral (or Stieltjes integral), as it is considered here,
</p>
<p>integrates over a deterministic function f .s/. Nevertheless, the Stieltjes integral is
</p>
<p>random as it is integrated with respect to the stochastic Wiener process W.s/. In
</p>
<p>order to understand what is meant by this, we recall the partition (8.1):
</p>
<p>Pn .Œ0; t&#141;/ W 0 D s0 &lt; s1 &lt; : : : &lt; sn D t ;
</p>
<p>1Thomas J. Stieltjes lived from 1856 to 1894. The Dutch mathematician generalized the concept
of integration by Riemann.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_9
</p>
<p>199</p>
<p/>
</div>
<div class="page"><p/>
<p>200 9 Stieltjes Integrals
</p>
<p>with s�i 2 Œsi�1; si&#141;. Hence, the Riemann-Stieltjes sum is defined as
</p>
<p>RSn D
nX
</p>
<p>iD1
f .s�i / .W.si/ � W.si�1// : (9.1)
</p>
<p>If an expression well-defined in mean square follows from this for n ! 1 under
(8.2), then we define it as a Stieltjes integral with the obvious notation
</p>
<p>RSn
2!
Z t
</p>
<p>0
</p>
<p>f .s/ dW.s/ :
</p>
<p>Note that dW.s/ does not stand for the derivative of the Wiener process as it does
</p>
<p>not exist. It is just a common symbolic notation.
</p>
<p>If f is continuously differentiable,2 then the existence of the Stieltjes integral is
</p>
<p>guaranteed, see Soong (1973, Theorem 4.5.2).
</p>
<p>Integration by Parts
</p>
<p>If f is continuously differentiable, then the Stieltjes integral can be expressed by a
</p>
<p>Riemann integral and the WP. This relation is also known as integration by parts. In
</p>
<p>Chap. 11 we will understand that it is a special case of Ito&rsquo;s lemma, which is why
</p>
<p>we do not have to concern ourselves with a proof of Proposition 9.1 at this point.
</p>
<p>Proposition 9.1 (Stieltjes Integral; Integration by Parts) For a continuously
</p>
<p>differentiable, deterministic function f we have that
</p>
<p>(a) the Stieltjes sum from (9.1) converges in mean square if it holds that max.si �
si�1/! 0,
</p>
<p>(b) and
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ dW.s/ D Œf .s/W.s/&#141;t0 �
Z t
</p>
<p>0
</p>
<p>W.s/ df .s/
</p>
<p>D f .t/W.t/ �
Z t
</p>
<p>0
</p>
<p>W.s/ f 0.s/ ds :
</p>
<p>where the last equality holds with probability one.3
</p>
<p>2We call a function continuously differentiable if it has a continuous first order derivative.
3Remember that we assumed P.W.0/ D 0/ D 1, which justifies the last statement. Whenever we
have equalities in a stochastic setting, they are typically understood to hold with probability one
for the rest of the book.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Definition and Partial Integration 201
</p>
<p>The result from (b) corresponds to the familiar rule of partial integration. As a
</p>
<p>refresher, we write this rule for two deterministic functions f and g:
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ g0.s/ ds D Œf .s/ g.s/&#141;t0 �
Z t
</p>
<p>0
</p>
<p>g.s/ f 0.s/ ds : (9.2)
</p>
<p>Hence, this is the integral form of the product rule of differentiation:
</p>
<p>dŒ f .s/ g.s/&#141;
</p>
<p>ds
D f 0.s/ g.s/C g0.s/ f .s/ ;
</p>
<p>or
</p>
<p>d Œ f .s/ g.s/&#141; D g.s/ df .s/C f .s/ dg.s/ ;
</p>
<p>or
</p>
<p>Œ f .s/ g.s/&#141;t0 D
Z t
</p>
<p>0
</p>
<p>g.s/ df .s/C
Z t
</p>
<p>0
</p>
<p>f .s/ dg.s/ :
</p>
<p>Therefore, one can make a mental note of Proposition 9.1 (b) by the well-known
</p>
<p>partial integration from (9.2).
</p>
<p>Example 9.1 (Corollary) As an application of Proposition 9.1 we consider
</p>
<p>Riemann-Stieltjes integrals for three particularly simple functions. We will
</p>
<p>encounter these relations repeatedly. The proof amounts to a simple exercise in
</p>
<p>substitution. It holds
</p>
<p>(a) for the identity function f .s/ D s:
Z t
</p>
<p>0
</p>
<p>s dW.s/ D t W.t/ �
Z t
</p>
<p>0
</p>
<p>W.s/ ds I
</p>
<p>(b) for f .s/ D 1 � s:
Z t
</p>
<p>0
</p>
<p>.1 � s/ dW.s/ D .1� t/W.t/C
Z t
</p>
<p>0
</p>
<p>W.s/ ds I
</p>
<p>(c) for the constant function f .s/ D 1:
Z t
</p>
<p>0
</p>
<p>dW.s/ D W.t/ :
</p>
<p>In (c) we again observe a formal analogy of the WP with the random walk. Just like
</p>
<p>the latter is defined as the sum over the past of a pure random process, see (1.8), the
</p>
<p>WP is the integral of its past independent increments. �</p>
<p/>
</div>
<div class="page"><p/>
<p>202 9 Stieltjes Integrals
</p>
<p>9.3 Gaussian Distribution and Autocovariances
</p>
<p>The reduction of Stieltjes integrals to Riemann integrals suggests that there are
</p>
<p>Gaussian processes hiding behind them. In fact, it holds that all Stieltjes integrals
</p>
<p>follow Gaussian distributions with expectation zero.
</p>
<p>Gaussian Distribution
</p>
<p>The Gaussian distribution itself is obvious: The Riemann-Stieltjes sum from (9.1)
</p>
<p>is, as the sum of multivariate Gaussian random variables, Gaussian as well. Then,
</p>
<p>this also holds for the limit of the sum due to Lemma 8.1. The expected value
</p>
<p>is zero due to Propositions 9.1(b) and 8.3. The variance results as a special case
</p>
<p>of the autocovariance given in Proposition 9.3. Hence, we obtain the following
</p>
<p>proposition.
</p>
<p>Proposition 9.2 (Normality of Stieltjes integrals) For a continuously differen-
</p>
<p>tiable, deterministic function f , it holds that
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .s/ dW.s/ � N
�
0 ;
</p>
<p>Z t
</p>
<p>0
</p>
<p>f 2.s/ ds
</p>
<p>�
:
</p>
<p>The variance of the Stieltjes integral is well motivated as follows. For the variance
</p>
<p>of the Riemann-Stieltjes sum,
</p>
<p>Var
</p>
<p> 
nX
</p>
<p>iD1
f .s�i / .W.si/ � W.si�1//
</p>
<p>!
;
</p>
<p>it follows for n ! 1, due to the independence of the increments of the WP, that:
</p>
<p>nX
</p>
<p>iD1
f 2.s�i /Var .W .si/ � W .si�1// D
</p>
<p>nX
</p>
<p>iD1
f 2.s�i / .si � si�1/
</p>
<p>!
Z t
</p>
<p>0
</p>
<p>f 2.s/ ds :
</p>
<p>The convergence takes place as f 2 is continuous and thus Riemann-integrable.
</p>
<p>Hence, for n ! 1 the expression from Proposition 9.2 is obtained.
Let us consider the integrals from Example 9.1 and calculate the variances for
</p>
<p>t D 1 (see Problem 9.1).</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Gaussian Distribution and Autocovariances 203
</p>
<p>Example 9.2 (Corollary) For the functions from Example 9.1 it holds:
</p>
<p>(a) for the identity function f .s/ D s:
</p>
<p>1Z
</p>
<p>0
</p>
<p>s dW.s/ � N .0; 1=3/ I
</p>
<p>(b) for f .s/ D 1 � s:
</p>
<p>1Z
</p>
<p>0
</p>
<p>.1 � s/ dW.s/ � N .0; 1=3/ I
</p>
<p>(c) for the constant function f .s/ D 1:
</p>
<p>W.t/ D
Z t
</p>
<p>0
</p>
<p>dW.s/ � N .0; t/ : �
</p>
<p>Autocovariance Function
</p>
<p>As a generalization of the variance, an expression for the covariance is to be found.
</p>
<p>Hence, let us define the process Y.t/ D
R t
0
</p>
<p>f .s/dW.s/. The autocovariance of Y.t/
</p>
<p>and Y.t C h/ with h � 0 can be well justified if one takes into account that
the increments dW.t/ of the WP are stochastically independent provided they do
</p>
<p>not overlap. Therefore, one should expect
R t
0
</p>
<p>f .s/dW.s/ and
R tCh
</p>
<p>t
f .r/dW.r/ to be
</p>
<p>uncorrelated:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>Z tCh
</p>
<p>t
</p>
<p>f .r/dW.r/
</p>
<p>�
D 0:
</p>
<p>If this is true, then, due to
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>f .r/dW.r/ D
Z t
</p>
<p>0
</p>
<p>f .r/dW.r/C
Z tCh
</p>
<p>t
</p>
<p>f .r/dW.r/
</p>
<p>the following result is obtained:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>f .r/dW.r/
</p>
<p>�
D E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/dW.r/
</p>
<p>�
</p>
<p>D Var
�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>204 9 Stieltjes Integrals
</p>
<p>Therefore, for an arbitrary h � 0 the autocovariance coincides with the variance in
t. In fact, this result can be verified more rigorously (see Problem 9.5).
</p>
<p>Proposition 9.3 (Autocovariance of Stieltjes Integrals) For a continuously dif-
</p>
<p>ferentiable, deterministic function f it holds that
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>f 2.s/ds
</p>
<p>with h � 0.
</p>
<p>Of course, for h D 0 the variance from Proposition 9.2 is obtained.
</p>
<p>Example 9.3 (Autocovariance of the WP) As an example, let us consider f .s/ D 1
with
</p>
<p>W.t/ D
Z t
</p>
<p>0
</p>
<p>dW.s/:
</p>
<p>Then, it follows for h � 0:
</p>
<p>E.W.t/W.t C h// D
Z t
</p>
<p>0
</p>
<p>ds D t D min.t; t C h/:
</p>
<p>Trivially, this just reproduces the autocovariance structure of the Wiener process
</p>
<p>already known from (7.4). �
</p>
<p>9.4 Standard Ornstein-Uhlenbeck Process
</p>
<p>The so-called Ornstein-Uhlenbeck process has been introduced in a publication by
</p>
<p>the physicists Ornstein and Uhlenbeck in 1930.
</p>
<p>Definition
</p>
<p>We define the Ornstein-Uhlenbeck process (OUP) with starting value Xc.0/ D 0
for an arbitrary real c as a Stieltjes integral,
</p>
<p>Xc.t/ WD ect
Z t
</p>
<p>0
</p>
<p>e�cs dW.s/; t � 0; Xc.0/ D 0 : (9.3)
</p>
<p>For c D 0 in (9.3) the Wiener process, X0.t/ D W.t/, is obtained. More precisely,
Xc.t/ from (9.3) is a standard OUP; a generalization will be offered in the chapter</p>
<p/>
</div>
<div class="page"><p/>
<p>9.4 Standard Ornstein-Uhlenbeck Process 205
</p>
<p>on interest rate dynamics. By definition, it holds that:
</p>
<p>Xc.t C 1/ D ect ec
�Z t
</p>
<p>0
</p>
<p>e�csdW.s/C
Z tC1
</p>
<p>t
</p>
<p>e�csdW.s/
</p>
<p>�
</p>
<p>D ecXc.t/C ec.tC1/
Z tC1
</p>
<p>t
</p>
<p>e�csdW.s/
</p>
<p>D ecXc.t/C ".t C 1/;
</p>
<p>where ".t C 1/ was defined implicitly. Note that the increments dW.s/ from t on in
".t C 1/ are independent of the increments up to t as they appear in Xc.t/. Hence,
the OUP is a continuous counterpart of the AR(1) process from Chap. 3 where the
</p>
<p>autoregressive parameter is denoted by ec. For c &lt; 0 this parameter is less than one,
</p>
<p>such that in this case we expect a stable adjustment or, in a way, a quasi-stationary
</p>
<p>behavior. This will be reflected by the behavior of the variance and the covariance
</p>
<p>function which are given, among others, in the following proposition.
</p>
<p>Properties
</p>
<p>The proof of Proposition 9.4 will be given in an exercise problem. It comprises an
</p>
<p>application of Propositions 9.1, 9.2 and 9.3.
</p>
<p>Proposition 9.4 (Ornstein-Uhlenbeck Process) It holds for the Ornstein-
</p>
<p>Uhlenbeck process from (9.3) that:
</p>
<p>.a/ Xc.t/ D W.t/C c ect
Z t
</p>
<p>0
</p>
<p>e�cs W.s/ ds ;
</p>
<p>.b/ Xc.t/ � N .0; .e2ct � 1/=2c/ ;
</p>
<p>.c/ E .Xc.t/Xc.t C h// D ech Var .Xc.t// ;
</p>
<p>where h � 0.
</p>
<p>Statement (a) establishes the usual relation between Stieltjes and Riemann integrals
</p>
<p>and, seen individually, it is not that thrilling. As for c D 0 the OUP coincides with
the WP, it is interesting to examine the variance from (b) for c ! 0. L&rsquo;Hospital&rsquo;s
rule yields:
</p>
<p>lim
c!0
</p>
<p>e2ct � 1
2c
</p>
<p>D lim
c!0
</p>
<p>2te2ct
</p>
<p>2
D t:</p>
<p/>
</div>
<div class="page"><p/>
<p>206 9 Stieltjes Integrals
</p>
<p>Hence, for c ! 0 the variance of the WP is embedded in (b). The covariance from
(c) allows for determining the autocorrelation:
</p>
<p>corr.Xc.t/;Xc.t C h// D
echVar.Xc.t//p
</p>
<p>Var.Xc.t//
p
</p>
<p>Var.Xc.t C h//
</p>
<p>D ech
p
</p>
<p>Var.Xc.t//p
Var.Xc.t C h//
</p>
<p>:
</p>
<p>Now, let us assume that c &lt; 0. Then it holds for t growing that:
</p>
<p>lim
t!1
</p>
<p>Var.Xc.t// D �
1
</p>
<p>2c
&gt; 0:
</p>
<p>Accordingly, it holds for the autocorrelation that:
</p>
<p>lim
t!1
</p>
<p>corr.Xc.t/;Xc.t C h// D ech; c &lt; 0:
</p>
<p>Thus, for c &lt; 0 we obtain the &ldquo;asymptotically stationary&rdquo; case with asymptotically
</p>
<p>constant variance and an autocorrelation being asymptotically dependent on the lag
</p>
<p>h only. Thereby, the autocorrelation results as the h-th power of the &ldquo;autoregressive
</p>
<p>parameter&rdquo; a D ec. With h growing, the autocovariance decays gradually. This finds
its counterpart in the discrete-time AR(1) process. Just as the random walk arises
</p>
<p>from the AR(1) process with the parameter value one, the WP with c D 0, i.e.
a D e0 D 1, is the corresponding special case of the OUP. Hence, we can definitely
consider the OUP as a continuous-time analog to the AR(1) process.
</p>
<p>Simulation
</p>
<p>The theoretical properties of the process for c &lt; 0 can be illustrated graphically.
</p>
<p>In Fig. 9.1 the simulated paths of two parameter constellations are shown. It can
</p>
<p>be observed that the process oscillates about the zero line where the variance or
</p>
<p>the deviation from zero for c D �0:1 is much larger4 than in the case c D �0:9.
This is clear against the background of (b) from Proposition 9.4 in which the first
</p>
<p>moment and the variance are given: The expected value is zero and the variance
</p>
<p>decreases with the absolute value of c increasing. The positive autocorrelation (cf.
</p>
<p>Proposition 9.4(c)) is obvious as well: Positive values tend to be followed by positive
</p>
<p>values and the inverse holds for negative observations. The closer to zero c is, the
</p>
<p>stronger the autocorrelation gets. That is why the graph for c D �0:1 is strongly
</p>
<p>4If the arithmetic mean of the 1000 observations of this time series is calculated, then by �0:72344
a notably negative number is obtained although the theoretical expected value is zero. Details on
the simulation of OUP paths are to follow in Sect. 13.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Problems and Solutions 207
</p>
<p>0 5 10 15 20
</p>
<p>&minus;
1
.5
</p>
<p>&minus;
0
.5
</p>
<p>0
.5
</p>
<p>c= &minus; 0.9
</p>
<p>0 5 10 15 20
</p>
<p>&minus;
2
.0
</p>
<p>&minus;
1
.0
</p>
<p>0
.0
</p>
<p>1
.0
</p>
<p>c= &minus; 0.1
</p>
<p>Fig. 9.1 Standard Ornstein-Uhlenbeck processes
</p>
<p>determined by the &ldquo;local&rdquo; trend and does not cross the zero line for longer time
</p>
<p>spans while for c D �0:9 the force which pulls the observations back to the zero
line is more effective such that the graph looks &ldquo;more stationary&rdquo; for c D �0:9.
</p>
<p>9.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>9.1 Calculate the variances from Example 9.2.
</p>
<p>9.2 Verify the Gaussian distribution from Corollary 8.1(b).
</p>
<p>9.3 Verify the following equality (with probability 1):
</p>
<p>Z t
</p>
<p>0
</p>
<p>s2 dW.s/ D t2 W.t/ � 2
Z t
</p>
<p>0
</p>
<p>s W.s/ ds :
</p>
<p>9.4 Determine the variance of the process X.t/ with
</p>
<p>X.t/ D
Z t
</p>
<p>0
</p>
<p>s2 dW.s/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>208 9 Stieltjes Integrals
</p>
<p>9.5 Prove Proposition 9.3.
</p>
<p>9.6 Prove (a) from Proposition 9.4.
</p>
<p>9.7 Show (b) from Proposition 9.4.
</p>
<p>9.8 Prove (c) from Proposition 9.4.
</p>
<p>Solutions
</p>
<p>9.1 From Proposition 9.2 it obviously follows for (a) that:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>s2ds D
�
</p>
<p>s3
</p>
<p>3
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
3
:
</p>
<p>Equally, one shows (b):
</p>
<p>Z 1
</p>
<p>0
</p>
<p>.1 � s/2ds D
�
� .1 � s/
</p>
<p>3
</p>
<p>3
</p>
<p>�1
</p>
<p>0
</p>
<p>D 1
3
:
</p>
<p>Finally, the result from (c) is known anyway.
</p>
<p>9.2 The result follows from the examples of this chapter. From Example 9.1(a) we
</p>
<p>obtain for t D 1:
</p>
<p>W.1/�
Z 1
</p>
<p>0
</p>
<p>W.s/ ds D
Z 1
</p>
<p>0
</p>
<p>s dW.s/ :
</p>
<p>Due to Example 9.2(a) the claim is verified.
</p>
<p>9.3 This is a straightforward application of Proposition 9.1. With f .s/ D s2 and
f 0.s/ D 2s the claim is established.
9.4 From Proposition 9.2 with f .s/ D s2 it follows for the variance
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>s2dW.s/
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>s4ds D
�
1
</p>
<p>5
s5
�t
</p>
<p>0
</p>
<p>D t
5
</p>
<p>5
:
</p>
<p>9.5 With Y.t/ D
R t
0
</p>
<p>f .s/dW.s/ we know from Proposition 9.1 that:
</p>
<p>Y.t/ D f .t/W.t/ �
Z t
</p>
<p>0
</p>
<p>f 0.s/W.s/ds:</p>
<p/>
</div>
<div class="page"><p/>
<p>9.5 Problems and Solutions 209
</p>
<p>Hence, the covariance results as
</p>
<p>E.Y.t/Y.t C h// D A � B � C C D
</p>
<p>where the expressions on the right-hand side are defined by multiplying Y.t/ and
</p>
<p>Y.t C h/. Now, we consider them one by one.
For A we obtain immediately:
</p>
<p>A D E Œ f .t/f .t C h/W.t/W.t C h/&#141;
D f .t/ f .t C h/min.t; t C h/
D f .t/ f .t C h/t:
</p>
<p>By Fubini&rsquo;s theorem it holds for B that:
</p>
<p>B D E
�
</p>
<p>f .t C h/
Z t
</p>
<p>0
</p>
<p>f 0.s/W.s/W.t C h/ds
�
</p>
<p>D f .t C h/
Z t
</p>
<p>0
</p>
<p>f 0.s/min.s; t C h/ds
</p>
<p>D f .t C h/
Z t
</p>
<p>0
</p>
<p>f 0.s/sds:
</p>
<p>Integration by parts in the following form,
</p>
<p>Z t
</p>
<p>0
</p>
<p>f .r/rdr D F.t/t �
Z t
</p>
<p>0
</p>
<p>F.r/dr with F0 D f ; (9.4)
</p>
<p>applied to f 0 yields:
</p>
<p>B D f .t C h/Œ f .t/t � F.t/C F.0/&#141;;
</p>
<p>where F.s/ denotes the antiderivative of f .s/. In the same way, we obtain
</p>
<p>C D E
�
</p>
<p>f .t/
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>f 0.s/W.s/W.t/ds
</p>
<p>�
</p>
<p>D f .t/
Z tCh
</p>
<p>0
</p>
<p>f 0.s/min.s; t/ds
</p>
<p>D f .t/
�Z t
</p>
<p>0
</p>
<p>f 0.s/sds C
Z tCh
</p>
<p>t
</p>
<p>f 0.s/tds
</p>
<p>�
</p>
<p>D f .t/ Œ f .t/t � F.t/C F.0/C t.f .t C h/ � f .t//&#141;
D f .t/ ŒF.0/� F.t/C t f .t C h/&#141; :</p>
<p/>
</div>
<div class="page"><p/>
<p>210 9 Stieltjes Integrals
</p>
<p>For the fourth expression Proposition 8.4 provides us with f 0 instead of f :
</p>
<p>D D E
�Z t
</p>
<p>0
</p>
<p>f 0.s/W.s/ds
Z tCh
</p>
<p>0
</p>
<p>f 0.r/W.r/dr
</p>
<p>�
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>f 0.s/ Œ f .s/s � .F.s/ � F.0//C s.f .t C h/� f .s//&#141; ds
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>f 0.s/dsF.0/ �
Z t
</p>
<p>0
</p>
<p>f 0.s/F.s/ds C f .t C h/
Z t
</p>
<p>0
</p>
<p>sf 0.s/ds:
</p>
<p>In addition to (9.4), we apply integration by parts in the form of
</p>
<p>Z t
</p>
<p>0
</p>
<p>f 0.s/F.s/ds D f .t/F.t/ � f .0/F.0/�
Z t
</p>
<p>0
</p>
<p>f 2.s/ds :
</p>
<p>Then it holds that:
</p>
<p>D D .f .t/ � f .0//F.0/� f .t/F.t/C f .0/F.0/C
Z t
</p>
<p>0
</p>
<p>f 2.s/ds
</p>
<p>Cf .t C h/ .f .t/t � F.t/C F.0//
</p>
<p>D
Z t
</p>
<p>0
</p>
<p>f 2.s/ds C .F.0/� F.t//.f .t/C f .t C h//C f .t/f .t C h/t:
</p>
<p>If we assemble the terms, then we obtain the autocovariance function in the desired
</p>
<p>form:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>f .s/dW.s/
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>f .r/dW.r/
</p>
<p>�
D A � B � C C D D
</p>
<p>Z t
</p>
<p>0
</p>
<p>f 2.s/ds:
</p>
<p>9.6 We use Proposition 9.1 with f .s/ D e�cs:
Z t
</p>
<p>0
</p>
<p>e�csdW.s/ D e�ctW.t/C c
Z t
</p>
<p>0
</p>
<p>e�csW.s/ ds:
</p>
<p>Multiplying by ect yields
</p>
<p>ect
Z t
</p>
<p>0
</p>
<p>e�csdW.s/ D W.t/C c ect
Z t
</p>
<p>0
</p>
<p>e�csW.s/ ds:
</p>
<p>On the left-hand side, we have the OUP Xc.t/ by definition which was to be verified.</p>
<p/>
</div>
<div class="page"><p/>
<p>Reference 211
</p>
<p>9.7 Due to Proposition 9.2 the OUP is Gaussian with expectation zero and variance
</p>
<p>Var.Xc.t// D e2ct
Z t
</p>
<p>0
</p>
<p>e�2csds
</p>
<p>D e2ct
�
</p>
<p>e�2cs
</p>
<p>�2c
</p>
<p>�t
</p>
<p>0
</p>
<p>D e2ct e
�2ct � 1
�2c
</p>
<p>D 1 � e
2ct
</p>
<p>�2c :
</p>
<p>This is equal to the claimed variance.
</p>
<p>9.8 As for the derivation of the variance, we use
</p>
<p>Z t
</p>
<p>0
</p>
<p>e�2csds D 1 � e
�2ct
</p>
<p>2c
:
</p>
<p>From Proposition 9.3 we know that this is also the expression for the autocovariance
</p>
<p>of the Stieltjes integrals .h � 0/:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>e�csdW.s/
Z tCh
</p>
<p>0
</p>
<p>e�csdW.s/
</p>
<p>�
D 1 � e
</p>
<p>�2ct
</p>
<p>2c
:
</p>
<p>Hence, we obtain for the OUP:
</p>
<p>E.Xc.t/Xc.t C h// D ectec.tCh/
1 � e�2ct
2c
</p>
<p>D ech e
2ct � 1
2c
</p>
<p>D ech Var.Xc.t//:
</p>
<p>Reference
</p>
<p>Soong, T. T. (1973). Random differential equations in science and engineering. New York:
Academic Press.</p>
<p/>
</div>
<div class="page"><p/>
<p>10Ito Integrals
</p>
<p>10.1 Summary
</p>
<p>Kiyoshi Ito (1915&ndash;2008) was awarded the inaugural Gauss Prize by the International
</p>
<p>Mathematical Union in 2006. Stochastic integration in the narrow sense can be
</p>
<p>traced back to his early work published in Japanese in the forties of the last
</p>
<p>century. We precede the general definition of the Ito integral with a special case.
</p>
<p>Concluding, we discuss the (quadratic) variation of a process without which a sound
</p>
<p>understanding of Ito&rsquo;s lemma will not be possible.
</p>
<p>10.2 A Special Case
</p>
<p>We start with a special case of Ito integration, so to speak the mother of all stochastic
</p>
<p>integrals. Thereby we will understand that, besides the Ito integral, infinitely many
</p>
<p>related integrals of a similar structure exist.
</p>
<p>Problems with the Definition
</p>
<p>The starting point is again a partition
</p>
<p>Pn .Œ0; t&#141;/ W 0 D s0 &lt; s1 &lt; : : : &lt; sn D t ;
</p>
<p>that gets finer for n growing since we continue to maintain (8.2). Given this
</p>
<p>decomposition of Œ0; t&#141;, we define analogously to the Riemann-Stieltjes sum for
</p>
<p>s�i 2 Œsi�1; si/:
</p>
<p>Sn.W/ D
nX
</p>
<p>iD1
W.s�i / .W.si/ � W.si�1// : (10.1)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_10
</p>
<p>213</p>
<p/>
</div>
<div class="page"><p/>
<p>214 10 Ito Integrals
</p>
<p>For n ! 1 we would like to denote the limit as
R t
0
</p>
<p>W.s/ dW.s/, which looks like a
</p>
<p>Stieltjes integral of a WP with respect to a WP. However, we will realize that:
</p>
<p>1. The limit of Sn.W/ is not unique, but depends on the choice of s
�
i ;
</p>
<p>2. the limit of Sn.W/ is not defined as a Stieltjes integral.
</p>
<p>As the Stieltjes integral has a unique limit independently of s�i , the second claim
follows from the first one. If one chooses in particular the lower endpoint of the
</p>
<p>interval as support, s�i D si�1, then this leads to the Ito integral. Hence, this special
case is called the Ito sum:
</p>
<p>In.W/ D
nX
</p>
<p>iD1
W.si�1/ .W.si/ � W.si�1// : (10.2)
</p>
<p>The following proposition specifies the dependence on s�i . The special case &#13; D 0
leading to the Ito integral will be proved in Problem 10.1; the general result is
</p>
<p>established e.g. in Tanaka (1996, eq. (2.40)). The convergence is again in mean
</p>
<p>square.
</p>
<p>Proposition 10.1 (Stochastic Integrals in Mean Square) Let s�i D .1� &#13;/ si�1C
&#13; si with 0 � &#13; &lt; 1. Then it holds for the sum from (10.1) with n ! 1 under (8.2):
</p>
<p>Sn.W/
2! 1
2
</p>
<p>�
W2.t/ � t
</p>
<p>�
C &#13; t :
</p>
<p>Before we discuss two special cases of Proposition 10.1, this striking result is to
</p>
<p>be somewhat better understood. We call it striking because it is counter-intuitive
</p>
<p>at first glance that the choice of s�i should matter with the intervals Œsi�1; si/ getting
narrower and narrower for n ! 1. To better understand this, we temporarily denote
the limit of Sn.W/ as S.&#13;/:
</p>
<p>Sn.W/
2! S.&#13;/ :
</p>
<p>Then one observes immediately:
</p>
<p>S.&#13;/ D S.0/C &#13; t :
</p>
<p>This means that the variance of all these stochastic integrals S.&#13;/ is identical, i.e.
</p>
<p>equal to the variance of S.0/. Hence, the choice of different support points s�i is only</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 A Special Case 215
</p>
<p>reflected in the expected value:
</p>
<p>E .S.&#13;// D 1
2
.E.W2.t// � t/C &#13; t
</p>
<p>D &#13; t :
</p>
<p>However, this expected value can be well understood as for finite sums it can be
</p>
<p>shown that (see Problem 10.4):
</p>
<p>E .Sn.W// D &#13; t :
</p>
<p>This simply follows from the fact that W.s�i / is not independent of W.si/� W.si�1/
for &#13; &gt; 0. Next, we turn towards the case &#13; D 0.
</p>
<p>Ito Integral
</p>
<p>For &#13; D 0, Sn.W/ from Proposition 10.1 merges into In.W/ from (10.2). The
proposition guarantees two different things: First, that the limit of In.W/ converges
</p>
<p>in mean square. We call this limit the Ito integral and write instead of S.0/ the
</p>
<p>following integral:
</p>
<p>In.W/
2!
Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ :
</p>
<p>Secondly, the proposition yields an expression for this Ito integral:
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ D 1
2
</p>
<p>W2.t/ � 1
2
</p>
<p>t : (10.3)
</p>
<p>By the way, (10.3) is just the &ldquo;stochastified chain rule&rdquo; for Wiener processes from
</p>
<p>(1.14).1 Note the analogy and the contrast to the deterministic case (with f .0/ D 0):
Z t
</p>
<p>0
</p>
<p>f .s/ df .s/ D 1
2
</p>
<p>f 2.t/ for f .0/ D 0 : (10.4)
</p>
<p>In Eq. (10.3) we find, so to speak, the archetype of Ito calculus, i.e. of stochastic
</p>
<p>calculus using Ito&rsquo;s lemma. The latter will be covered in the next chapter.
</p>
<p>1In particular for t D 1, (10.3) accomplishes the transition from (1.11) to (1.12) for the Dickey-
Fuller distribution.</p>
<p/>
</div>
<div class="page"><p/>
<p>216 10 Ito Integrals
</p>
<p>The moments of the Ito integral can be determined by (10.3). According to this
</p>
<p>equation it holds for the expected value:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/
</p>
<p>�
D 0 :
</p>
<p>The variance of the integral as well can be calculated elementarily2:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/
</p>
<p>�
D Var
</p>
<p>�
W2.t/
</p>
<p>2
� t
2
</p>
<p>�
</p>
<p>D E
"�
</p>
<p>W2.t/
</p>
<p>2
� t
2
</p>
<p>�2#
</p>
<p>D 1
4
</p>
<p>E
�
W4.t/ � 2 t W2.t/C t2
</p>
<p>�
</p>
<p>D 1
4
</p>
<p>�
3 t2 � 2 t2 C t2
</p>
<p>�
</p>
<p>D t
2
</p>
<p>2
;
</p>
<p>where the kurtosis of 3 for Gaussian random variables was used. Hence, we have
</p>
<p>the first two results of the following proposition
</p>
<p>Proposition 10.2 (Moments of
R t
0 W.s/ dW.s/) For I.t/ D
</p>
<p>R t
0 W.s/ dW.s/ it holds
</p>
<p>that
</p>
<p>E.I.t// D 0 and Var .I.t// D t
2
</p>
<p>2
;
</p>
<p>and
</p>
<p>E.I.t/ I.t C h// D t
2
</p>
<p>2
for h � 0 :
</p>
<p>2An alternative, interesting method uses the fact that the variance of a chi-squared distributed
random variable equals twice its degrees of freedom:
</p>
<p>Var
</p>
<p>�
W2.t/
</p>
<p>2
� t
2
</p>
<p>�
D 1
4
</p>
<p>Var.W2.t// D t
2
</p>
<p>4
Var
</p>
<p> �
W.t/p
</p>
<p>t
</p>
<p>�2!
D t
</p>
<p>2
</p>
<p>4
� 2 D t
</p>
<p>2
</p>
<p>2
;
</p>
<p>as it holds that W.t/=
p
</p>
<p>t � N .0; 1/ and therefore
�
</p>
<p>W.t/p
t
</p>
<p>�2
� �2.1/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 A Special Case 217
</p>
<p>As a third result it holds in Proposition 10.2, just as for the Stieltjes integral, that the
</p>
<p>autocovariance coincides with the variance, i.e.
</p>
<p>E.I.t/ I.t C h// D Var .I.t// ; h � 0:
</p>
<p>We will prove this in Problem 10.2.
</p>
<p>Stratonovich Integral
</p>
<p>For a reason that will soon become evident, sometimes a considered competitor of
</p>
<p>the Ito integral is the Stratonovich integral. It is defined as the limit of Sn.W/ from
</p>
<p>(10.1) with the midpoints of the intervals as s�i :
</p>
<p>s�i D
si�1 C si
</p>
<p>2
:
</p>
<p>This corresponds to the choice of &#13; D 0:5 in Proposition 10.1. Let the limit in mean
square be denoted as follows:
</p>
<p>nX
</p>
<p>iD1
W
</p>
<p>�
si�1 C si
</p>
<p>2
</p>
<p>�
.W.si/ � W.si�1//
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/ ;
</p>
<p>where &ldquo;@&rdquo; does not stand for the partial derivative but denotes the Stratonovich
</p>
<p>integral in contrast to the Ito integral. By the way, with &#13; D 0:5 Proposition 10.1
yields:
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/ D W
2.t/
</p>
<p>2
:
</p>
<p>Hence, the Stratonovich integral stands out due to the fact that the familiar
</p>
<p>integration rule known from ordinary calculus holds true. In differential notation
</p>
<p>this rule can be formulated symbolically as follows:
</p>
<p>@W2.t/
</p>
<p>2
D W.t/ @W.t/ :
</p>
<p>This just corresponds to the ordinary chain rule, cf. (10.4). Although the Ito and the
</p>
<p>Stratonovich integral are distinguished from each other only by the choice of s�i with
intervals getting shorter and shorter, they still have drastically different properties.
</p>
<p>Obviously, it holds for the expected value
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/
</p>
<p>�
D t
2
;</p>
<p/>
</div>
<div class="page"><p/>
<p>218 10 Ito Integrals
</p>
<p>while the Ito integral is zero on average. However, as aforementioned, the variances
</p>
<p>of
R
</p>
<p>W.s/dW.s/ and
R
</p>
<p>W.s/@W.s/ coincide, cf. Problem 10.3 as well.
</p>
<p>Example 10.1 (Alternative Stratonovich Sum) Sometimes the Stratonovich integral
</p>
<p>is defined as the limit of the following sum, see e.g. Klebaner (2005, eq. (5.65)):
</p>
<p>nX
</p>
<p>iD1
</p>
<p>W.si�1/C W.si/
2
</p>
<p>.W.si/� W.si�1// :
</p>
<p>The intuition behind this is, due to the continuity of the WP, that
</p>
<p>W.si�1/ � W
�
</p>
<p>si�1 C si
2
</p>
<p>�
� W.si/ :
</p>
<p>In fact, it can be shown more explicitly that the following difference becomes
</p>
<p>negligible in mean square:
</p>
<p>&#13; WD W
�
</p>
<p>si�1 C si
2
</p>
<p>�
� W.si�1/C W.si/
</p>
<p>2
</p>
<p>2! 0 :
</p>
<p>For this purpose we consider as the mean square deviation with s�i D .si�1 C si/=2:
</p>
<p>MSE.&#13;; 0/ D E
�
.&#13; � 0/2
</p>
<p>�
</p>
<p>D E
�
W2.s�i /� W.s�i / .W.si�1/C W.si//
</p>
<p>�
</p>
<p>CE
�
</p>
<p>W2.si�1/C 2W.si�1/W.si/C W2.si/
4
</p>
<p>�
:
</p>
<p>Due to si�1 &lt; s�i &lt; si the familiar variance and covariance formulas yield:
</p>
<p>MSE.&#13;; 0/ D s�i � si�1 � s�i C
si�1 C 2 si�1 C si
</p>
<p>4
</p>
<p>D si � si�1
4
</p>
<p>:
</p>
<p>As for n ! 1 the partition gets finer and finer, si � si�1 ! 0, the replacement of
W.s�i / by .W.si�1/C W.si// =2 is asymptotically well justified. �
</p>
<p>10.3 General Ito Integrals
</p>
<p>After covering general Ito integrals, we define so-called diffusions that we will be
</p>
<p>concerned with in the following chapters.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 General Ito Integrals 219
</p>
<p>Definition andMoments
</p>
<p>In order to define general Ito integrals, we consider for a stochastic process X as a
</p>
<p>generalization of the sum In.W/:
</p>
<p>In.X/ D
nX
</p>
<p>iD1
X.si�1/ .W.si/ � W.si�1// : (10.5)
</p>
<p>For the Ito integral two special things apply: First, the lower endpoint of the interval
</p>
<p>is chosen as s�i D si�1, i.e. X.si�1/; and secondly, we integrate with respect to
the WP, .W.si/ � W.si�1//. If X was integrated with respect to another stochastic
process, then one would obtain even more general stochastic integrals, which we
</p>
<p>are not interested in here.
</p>
<p>If X.t/ is a process with finite variance where the variance varies continuously in
</p>
<p>the course of time, and if X.t/ only depends on the past of the WP, W.s/ with s � t,
but not on its future, then the Ito sum converges uniquely and independently of the
</p>
<p>partition. The limit is called Ito integral and is denoted as follows:
</p>
<p>Z t
</p>
<p>0
</p>
<p>X.s/ dW.s/ :
</p>
<p>The assumptions about X.t/ are stronger than necessary, however, they guarantee
</p>
<p>the existence of the moments of an Ito integral, too. Similar assumptions can be
</p>
<p>found in Klebaner (2005, Theorem 4.3) or &Oslash;ksendal (2003, Corollary 3.1.7).
</p>
<p>Proposition 10.3 (General Ito Integral) Let X.s/ be a stochastic process on Œ0; t&#141;
</p>
<p>with two properties:
</p>
<p>(i) �2.s/ D E
�
X2.s/
</p>
<p>�
&lt;1 is a continuous function,
</p>
<p>(ii) X.s/ is independent of W.sj/� W.si/ with s � si &lt; sj.
</p>
<p>Then it holds that
</p>
<p>(a) the sum from (10.5) converges in mean square:
</p>
<p>nX
</p>
<p>iD1
X.si�1/ .W.si/ � W.si�1//
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>X.s/ dW.s/ I
</p>
<p>(b) the moments of the Ito integral are determined as:
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>X.s/ dW.s/
</p>
<p>�
D 0 ; Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>X.s/ dW.s/
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>E
�
X2.s/
</p>
<p>�
ds :</p>
<p/>
</div>
<div class="page"><p/>
<p>220 10 Ito Integrals
</p>
<p>Naturally, for X.s/ D W.s/ the extensively discussed example from the previous
section is obtained. In particular, in (b) the moments from Proposition 10.2 are
</p>
<p>reproduced, and it holds for the variance that:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/
</p>
<p>�
D
Z t
</p>
<p>0
</p>
<p>E.W2.s// ds D
Z t
</p>
<p>0
</p>
<p>s ds D t
2
</p>
<p>2
:
</p>
<p>Example 10.2 (Stieltjes Integral) Consider the special case where X.s/ is not
</p>
<p>stochastic but deterministic,
</p>
<p>X.s/ D f .s/;
</p>
<p>where f .s/ is continuous. Then, the conditions of existence are fulfilled which
</p>
<p>can easily be verified: The square, �2.s/ D f 2.s/, is continuous as well, and the
deterministic function is independent of W.s/. Hence, it holds that
</p>
<p>nX
</p>
<p>iD1
f .si�1/ .W.si/ � W.si�1//
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>f .s/ dW.s/ :
</p>
<p>In other words: For deterministic processes, X.s/ D f .s/, the Stieltjes and the Ito
integral coincide; the former is a special case of the latter. Due to E
</p>
<p>�
f 2.s/
</p>
<p>�
D f 2.s/
</p>
<p>the already familiar formulas for expectation and variance from Proposition 9.2 are
</p>
<p>embedded in the general Proposition 10.3. �
</p>
<p>Distribution and Further Properties
</p>
<p>As is well known, the special case of the Stieltjes integral is Gaussian. For the Ito
</p>
<p>integral this does not hold in general. This can clearly be seen in (10.3):
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ D W
2.t/ � t
2
</p>
<p>� �t
2
;
</p>
<p>i.e. the support of the distribution is bounded. Then again, the integral of a WP
</p>
<p>with respect to a thereof stochastically independent WP amounts to a Gaussian
</p>
<p>distribution. The following result is by Phillips and Park (1988).
</p>
<p>Proposition 10.4 (Ito Integral of an Independent WP) Let W.t/ and V.s/ be
</p>
<p>stochastically independent Wiener processes. Then it holds that
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>V2.s/ ds
</p>
<p>��0:5 Z 1
</p>
<p>0
</p>
<p>V.s/ dW.s/ � N .0; 1/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 General Ito Integrals 221
</p>
<p>By showing the conditional distribution of the left-hand side given V.t/ to just
</p>
<p>follow a N .0; 1/ distribution and therefore not to depend on this condition,
</p>
<p>one proves the result claimed in Proposition 10.4; see Phillips and Park (1988,
</p>
<p>Appendix) for details.
</p>
<p>Note that the Ito integral again defines a stochastic process in the bounds from 0
</p>
<p>to t whose properties could be discussed, which we will not do at this point. In the
</p>
<p>literature, however, it can be looked up that the Ito integral and the Wiener process
</p>
<p>share the continuity and the martingale properties. What is more, the integration
</p>
<p>rules outlined at the end of Sect. 8.2 hold true for Ito integrals as well, see e.g.
</p>
<p>Klebaner (2005, Thm. 4.3).
</p>
<p>Diffusions
</p>
<p>For economic modeling the Ito integral is an important ingredient. However, it gains
</p>
<p>its true importance only when combined with Riemann integrals. In the following
</p>
<p>chapters, the sum of both integrals constitutes so-called diffusions3 (diffusion
</p>
<p>processes). Hence, we now define processes X.t/ (with starting value X.0/) as
</p>
<p>follows:
</p>
<p>X.t/ D X.0/C
Z t
</p>
<p>0
</p>
<p>�.s/ ds C
Z t
</p>
<p>0
</p>
<p>�.s/ dW.s/ :
</p>
<p>Frequently, we will write this integral equation in differential form as follows:
</p>
<p>dX.t/ D �.t/ dt C �.t/ dW.t/ :
</p>
<p>The conditions set to �.s/ and �.s/ that guarantee the existence of such processes
</p>
<p>can be adopted from Propositions 8.1 and 10.3. In general, �.s/ and �.s/ are
</p>
<p>stochastic; particularly, they are allowed to be dependent on X.s/ itself. Therefore,
</p>
<p>we write �.s/ and �.s/ as abbreviations for functions which firstly explicitly depend
</p>
<p>on time and secondly depend on X simultaneously:
</p>
<p>�.s/ D � .s;X.s// ; �.s/ D � .s;X.s// :
</p>
<p>Processes � and � satisfying these conditions are used to define diffusions X.t/:
</p>
<p>dX.t/ D � .t;X.t// dt C � .t;X.t// dW.t/ ; t 2 Œ0;T&#141; : (10.6)
</p>
<p>3The name stems from molecular physics, where diffusions are used to model the change of
location of a molecule due to a deterministic component (drift) and an erratic (stochastic)
component. Physically, the influence of temperature on the motion hides behind the stochastics:
The higher the temperature of the matter in which the particles move, the more erratic is their
behavior.</p>
<p/>
</div>
<div class="page"><p/>
<p>222 10 Ito Integrals
</p>
<p>Recall that this differential equation actually means the following:
</p>
<p>X.t/ D X.0/C
Z t
</p>
<p>0
</p>
<p>� .s;X.s// ds C
Z t
</p>
<p>0
</p>
<p>� .s;X.s// dW.s/ :
</p>
<p>Example 10.3 (Brownian Motion with Drift) We consider the Brownian motion
</p>
<p>with drift and a starting value 0:
</p>
<p>X.t/ D �t C � W.t/
</p>
<p>D �
Z t
</p>
<p>0
</p>
<p>ds C �
Z t
</p>
<p>0
</p>
<p>dW.s/ :
</p>
<p>Therefore, the differential notation reads
</p>
<p>dX.t/ D � dt C � dW.t/ :
</p>
<p>Hence, this is a diffusion whose drift and volatility are constant:
</p>
<p>� .t;X.t// D � and � .t;X.t// D � : �
</p>
<p>10.4 (Quadratic) Variation
</p>
<p>From (10.3) we know that
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ D W
2.t/ � t
2
</p>
<p>:
</p>
<p>Now, we want to understand where the expression t comes from that is subtracted
</p>
<p>from W2.t/. It will be made clear that this is the so-called quadratic variation.
</p>
<p>(Absolute) Variation
</p>
<p>Again, the considerations are based on an adequate partition of the interval Œ0; t&#141;,
</p>
<p>Pn .Œ0; t&#141;/ W 0 D s0 &lt; s1 &lt; : : : &lt; sn D t :
</p>
<p>For a function g the variation over this partition is defined as4:
</p>
<p>Vn.g; t/ D
nX
</p>
<p>iD1
jg.si/ � g.si�1/j :
</p>
<p>4Sometimes we speak of absolute variation in order to avoid confusion with e.g. quadratic
variation.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 (Quadratic) Variation 223
</p>
<p>If the limit exists independently of the decomposition for n ! 1 under (8.2), then
one says that g is of finite variation and writes5:
</p>
<p>Vn.g; t/ ! V.g; t/ ; n ! 1 :
</p>
<p>The finite sum Vn.g; t/ measures for a certain partition the absolute increments of
</p>
<p>the function g on the interval Œ0; t&#141;. If the function evolves sufficiently smooth, then
</p>
<p>V.g; t/ takes on a finite value for .n ! 1/. For very jagged functions, however, it
may be that for an increasing refinement .n ! 1/ the increments of the graph of g
become larger and larger even for fixed t, such that g is not of finite variation.
</p>
<p>Example 10.4 (Monotonic Functions) For monotonic finite functions the variation
</p>
<p>can be calculated very easily and intuitively. In this case, V.g; t/ is simply the
</p>
<p>absolute value of the difference of the function at endpoints of the interval,
</p>
<p>jg.t/� g.0/j. First, let us assume that g grows monotonically on Œ0; t&#141;,
</p>
<p>g.si/ � g.si�1/ for si &gt; si�1 :
</p>
<p>Obviously, it then holds by (8.3) that
</p>
<p>Vn.g; t/ D
nX
</p>
<p>iD1
.g.si/� g.si�1// D g.t/ � g.0/ D V.g; t/ :
</p>
<p>For a monotonically decreasing function, it results quite analogously:
</p>
<p>Vn.g; t/ D
nX
</p>
<p>iD1
jg.si/ � g.si�1/j
</p>
<p>D �
nX
</p>
<p>iD1
.g.si/ � g.si�1//
</p>
<p>D g.0/� g.t/
D V.g; t/ :
</p>
<p>Monotonic functions are hence of finite variation. �
</p>
<p>Without the requirement of monotonicity, an intuitive sufficient condition exists
</p>
<p>for the function to be smooth enough to be of finite variation where this variation
</p>
<p>then has a familiar form as well.
</p>
<p>5If g is a deterministic function, then &ldquo;!&rdquo; means the usual convergence of analysis. If we allow
for g.t/ to be a stochastic process, then we mean the convergence in mean square: &ldquo;
</p>
<p>2!&rdquo;.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 10 Ito Integrals
</p>
<p>Proposition 10.5 (Variation of Continuously Differentiable Functions) Let g be
</p>
<p>a continuously differentiable function with derivative g0 on Œ0; t&#141;. Then g is of finite
variation and it holds that
</p>
<p>V.g; t/ D
Z t
</p>
<p>0
</p>
<p>jg0.s/j ds:
</p>
<p>The proof is given in Problem 10.6.
</p>
<p>Example 10.5 (Sine Wave) Let us consider a sine cycle of the frequency k on the
</p>
<p>interval Œ0; 2�&#141;:
</p>
<p>gk.s/ D sin.ks/; k D 1; 2; : : : :
</p>
<p>The derivative reads
</p>
<p>g0k.s/ D k cos.ks/:
</p>
<p>Accounting for the sign one obtains as the variation:
</p>
<p>V.g1; 2�/ D
Z 2�
</p>
<p>0
</p>
<p>jcos.s/j ds D 4
Z �=2
</p>
<p>0
</p>
<p>cos.s/ ds
</p>
<p>D 4
�
</p>
<p>sin
��
2
</p>
<p>�
� sin.0/
</p>
<p>�
</p>
<p>D 4 ;
</p>
<p>V.g2; 2�/ D
Z 2�
</p>
<p>0
</p>
<p>2 jcos.2s/j ds D 8
Z �=4
</p>
<p>0
</p>
<p>2 cos.2s/ ds
</p>
<p>D 8
�
</p>
<p>sin
��
2
</p>
<p>�
� sin.0/
</p>
<p>�
</p>
<p>D 8 ;
</p>
<p>V.gk; 2�/ D
Z 2�
</p>
<p>0
</p>
<p>k jcos.ks/j ds D 4k
Z �=2k
</p>
<p>0
</p>
<p>k cos.ks/ ds
</p>
<p>D 4k
�
</p>
<p>sin
��
2
</p>
<p>�
� sin.0/
</p>
<p>�
</p>
<p>D 4k :
</p>
<p>In Fig. 10.1 it can be observed, how the sum of (absolute) differences in amplitude
</p>
<p>grows with k growing. Accordingly, the absolute variation of gk.s/ D sin.ks/
multiplies with k. For k ! 1, g0k tends to infinity such that this derivative is not</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 (Quadratic) Variation 225
</p>
<p>0 1 2 3 4 5 6
</p>
<p>&minus;1
</p>
<p>&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>[0,2π]
</p>
<p>s
in
</p>
<p>(k
x
)
</p>
<p>sine wave
</p>
<p>0 1 2 3 4 5 6
</p>
<p>&minus;1
</p>
<p>&minus;0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>[0,2π]
</p>
<p>s
in
</p>
<p>(k
x
)
</p>
<p>k=1
</p>
<p>k=2
</p>
<p>k=1
</p>
<p>k=5
</p>
<p>Fig. 10.1 Sine cycles of different frequencies (Example 10.5)
</p>
<p>continuous anymore. Consequently, the absolute variation is not finite in the limiting
</p>
<p>case k ! 1. �
</p>
<p>Quadratic Variation
</p>
<p>In the same way as Vn.g; t/ a q-variation can be defined where we are only interested
</p>
<p>in the case q D 2, &ndash; the quadratic variation:
</p>
<p>Qn.g; t/ D
nX
</p>
<p>iD1
jg.si/ � g.si�1/j2 D
</p>
<p>nX
</p>
<p>iD1
.g.si/ � g.si�1//2 :
</p>
<p>As would seem natural, g is called of finite quadratic variation if it holds that
</p>
<p>Qn.g; t/ ! Q.g; t/ ; n ! 1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>226 10 Ito Integrals
</p>
<p>If g is a stochastic function, i.e. a stochastic process, then Q.g; t/ and V.g; t/ are
</p>
<p>defined as limits in mean square. Between the absolute variation V.g; t/ and the
</p>
<p>quadratic variation Q.g; t/ there are connections which we want to deal with now. If
</p>
<p>a continuous function is of finite variation, then it is of finite quadratic variation
</p>
<p>as well, where the latter is in fact zero. This is the statement of the following
</p>
<p>proposition. As it seems counterintuitive at first sight that Q, as the limit of a positive
</p>
<p>sum of squares Qn, can become zero, we start with an example.
</p>
<p>Example 10.6 (Identity Function) Let id be the identity function on Œ0; t&#141;:
</p>
<p>id.s/ D s:
</p>
<p>As the functions increases monotonically, it is of finite variation with
</p>
<p>V.id; t/ D id.t/ � id.0/ D t :
</p>
<p>For finite n it holds that:
</p>
<p>Qn.id; t/ D
nX
</p>
<p>iD1
.id.si/ � id.si�1//2
</p>
<p>D
nX
</p>
<p>iD1
.si � si�1/2
</p>
<p>&gt; 0 :
</p>
<p>Qn consists of n terms, where the lengths si � si�1 &gt; 0 are of the magnitude 1n . Due
to the squaring, the n terms are of the magnitude 1
</p>
<p>n2
. Hence, the sum converges to
</p>
<p>zero for n ! 1. This intuition can be formalized as follows:
</p>
<p>Qn.id; t/ D
nX
</p>
<p>iD1
.si � si�1/2
</p>
<p>� max
1�i�n
</p>
<p>.si � si�1/
nX
</p>
<p>iD1
.si � si�1/
</p>
<p>D max
1�i�n
</p>
<p>.si � si�1/Vn.id; t/
</p>
<p>D max
1�i�n
</p>
<p>.si � si�1/ t
</p>
<p>! 0;
</p>
<p>as max.si � si�1/! 0 for n ! 1. �</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 (Quadratic) Variation 227
</p>
<p>The next proposition gives a sufficient condition for the quadratic variation to
</p>
<p>vanish.
</p>
<p>Proposition 10.6 (Absolute and Quadratic Variation) Let g be a continuous
</p>
<p>function on Œ0; t&#141;. It then holds under (8.2) for n ! 1:
</p>
<p>Vn.g; t/ ! V.g; t/ &lt;1
</p>
<p>implies
</p>
<p>Qn.g; t/ ! 0 :
</p>
<p>If g is a stochastic process, then &ldquo;!&rdquo; is to be understood as convergence in mean
square.
</p>
<p>The proof is given in Problem 10.7. From the proposition it follows by contraposi-
</p>
<p>tion that: If we have a positive (finite) quadratic variation, then the process does not
</p>
<p>have a finite variation. Formally, we write: From
</p>
<p>Qn.g; t/! Q.g; t/ &lt;1 with Q.g; t/ &gt; 0
</p>
<p>it follows that there is no finite variation:
</p>
<p>Vn.g; t/! 1 :
</p>
<p>If a function g is so smooth that it has a continuous derivative, then Q.g; t/ D 0 by
Propositions 10.5 and 10.6; the other way round, values of Q.g; t/ &gt; 0 characterize
</p>
<p>how little smooth or jagged the function is.
</p>
<p>Wiener Processes
</p>
<p>As we know, the WP is nowhere differentiable, therefore it is everywhere so jagged
</p>
<p>that there is no valid tangent line approximation. Due to this extreme jaggedness the
</p>
<p>WP is of infinite variation as well, as we will show in a moment. More explicitly,
</p>
<p>we prove that the WP is of positive quadratic variation and does not have a finite
</p>
<p>absolute variation due to Proposition 10.6. We save the proof for an exercise
</p>
<p>(Problem 10.8).</p>
<p/>
</div>
<div class="page"><p/>
<p>228 10 Ito Integrals
</p>
<p>Proposition 10.7 (Quadratic Variation of the WP) For the Wiener process with
</p>
<p>n ! 1 it holds under (8.2):
</p>
<p>Qn.W; t/
2! t D Q.W; t/ :
</p>
<p>The expression Q.W; t/ D t characterizes the level of jaggedness or irregularity
of the Wiener process on the interval Œ0; t&#141;. This non-vanishing quadratic variation
</p>
<p>causes the problems and specifics of the Ito integral. Let us recapitulate: If the
</p>
<p>Wiener process was continuously differentiable, then it would be of finite variation
</p>
<p>due to Proposition 10.5 and it would have a vanishing quadratic variation due to
</p>
<p>Proposition 10.6. However, this is just not the case.
</p>
<p>Symbolic Notation
</p>
<p>In finance textbooks one frequently finds a notation for time that is strange at first
</p>
<p>sight:
</p>
<p>.dW.t//2 D dt : (10.7)
</p>
<p>How is this to be understood? Formal integration yields
</p>
<p>Z t
</p>
<p>0
</p>
<p>.dW.s//2 D t :
</p>
<p>As would seem natural, the &ldquo;integral&rdquo; on the left-hand side here stands for Q.W; t/:
</p>
<p>Qn.W; t/ D
nX
</p>
<p>iD1
.W.si/ � W.si�1//2
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>.dW.s//2 WD Q.W; t/ :
</p>
<p>Therefore, the integral equation and hence (10.7) is justified by Proposition 10.7:
</p>
<p>Q.W; t/ D t. We adopt the result into the following proposition. The expressions
</p>
<p>dW.t/ dt D 0 and .dt/2 D 0 (10.8)
</p>
<p>are to be understood similarly, namely in the sense of Proposition 10.8.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Problems and Solutions 229
</p>
<p>Proposition 10.8 (Symbolic Notation) It holds for n ! 1 under (8.2):
</p>
<p>.a/
</p>
<p>nX
</p>
<p>iD1
.W.si/ � W.si�1//2
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>.dW.s//2 D t ;
</p>
<p>.b/
</p>
<p>nX
</p>
<p>iD1
.W.si/ � W.si�1// .si � si�1/
</p>
<p>2!
Z t
</p>
<p>0
</p>
<p>dW.s/ ds D 0 ;
</p>
<p>.c/
</p>
<p>nX
</p>
<p>iD1
.si � si�1/2 !
</p>
<p>Z t
</p>
<p>0
</p>
<p>.ds/2 D 0 :
</p>
<p>In symbols, these facts are frequently formulated as in (10.7) and (10.8).
</p>
<p>Note that the expression in (c) in Proposition 10.8 is the quadratic variation of the
</p>
<p>identity function id.s/ D s:
</p>
<p>Qn.id; t/!
Z t
</p>
<p>0
</p>
<p>.ds/2 WD Q.id; t/ D 0 :
</p>
<p>Hence, the third claim is already established by Example 10.6. The expression from
</p>
<p>(b) in Proposition 10.8 is sometimes also called covariation (of W.s/ and id.s/ D s).
The claimed convergence to zero in mean square is shown in Problem 10.9.
</p>
<p>10.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>10.1 Prove Proposition 10.1 for &#13; D 0 (Ito integral).
Hint: Use Proposition 10.7.
</p>
<p>10.2 Prove the autocovariance from Proposition 10.2.
</p>
<p>10.3 Derive that the Ito integral from (10.3) and the corresponding Stratonovich
</p>
<p>integral have the same variance.
</p>
<p>10.4 Show for Sn.W/ from (10.1) with s
�
i from Proposition 10.1,
</p>
<p>s�i D .1 � &#13;/ si�1 C &#13; si ; 0 � &#13; &lt; 1 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>230 10 Ito Integrals
</p>
<p>that it holds:
</p>
<p>E .Sn.W// D &#13; t :
</p>
<p>10.5 Show: &#13;n
2! 0 with
</p>
<p>&#13;n D W ..1 � &#13;/ si�1 C &#13; si/ � Œ.1 � &#13;/W.si�1/C &#13; W.si/&#141;
</p>
<p>for &#13; 2 Œ0; 1&#141; for an adequate partition, i.e. for si � si�1 ! 0.
</p>
<p>10.6 Prove Proposition 10.5.
</p>
<p>10.7 Prove Proposition 10.6.
</p>
<p>10.8 Determine the quadratic variation of the Wiener process, i.e. verify Proposi-
</p>
<p>tion 10.7.
</p>
<p>10.9 Show (b) from Proposition 10.8.
</p>
<p>10.10 Determine the covariance of W.s/ and
R t
0
</p>
<p>W.r/ dW.r/ for s � t.
</p>
<p>Solutions
</p>
<p>10.1 In order to prove Proposition 10.1 for &#13; D 0, it has to be shown that In.W/
from (10.2) converges in mean square, namely to the expression given in (10.3). For
</p>
<p>this purpose, we write In.W/ as follows:
</p>
<p>In.W/ D
nX
</p>
<p>iD1
W.si�1/ .W.si/ � W.si�1//
</p>
<p>D 1
2
</p>
<p>"
2
</p>
<p>nX
</p>
<p>iD1
W.si/W.si�1/ � 2
</p>
<p>nX
</p>
<p>iD1
W2.si�1/
</p>
<p>#
</p>
<p>D 1
2
</p>
<p>"
nX
</p>
<p>iD1
</p>
<p>�
W2.si/ � W2.si�1/
</p>
<p>�
</p>
<p>�
nX
</p>
<p>iD1
</p>
<p>�
W2.si/� 2W.si/W.si�1/C W2.si�1/
</p>
<p>�
#
;</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Problems and Solutions 231
</p>
<p>where for the last equation we added zero,
�
W2.si/ � W2.si/
</p>
<p>�
. Hence, it furthermore
</p>
<p>follows by means of the quadratic variation:
</p>
<p>In.W/ D
1
</p>
<p>2
</p>
<p>"
W2.sn/ � W2.s0/�
</p>
<p>nX
</p>
<p>iD1
.W.si/ � W.si�1//2
</p>
<p>#
</p>
<p>D 1
2
</p>
<p>�
W2.t/ � W2.0/
</p>
<p>�
� 1
2
</p>
<p>Qn.W; t/ :
</p>
<p>The Wiener process is of finite quadratic variation and we know from Proposi-
</p>
<p>tion 10.7: Qn.W; t/
2! t. This verifies the claim (as it holds that W.0/ D 0 with
</p>
<p>probability 1).
</p>
<p>10.2 Based on (10.3) we consider the process
</p>
<p>I.t/ D
Z t
</p>
<p>0
</p>
<p>W.s/dW.s/ D W
2.t/ � t
2
</p>
<p>:
</p>
<p>Due to the vanishing expected value, it holds for the autocovariance that:
</p>
<p>E.I.t/ I.s// D 1
4
</p>
<p>E
�
W2.t/W2.s/ � tW2.s/ � sW2.t/C st
</p>
<p>�
</p>
<p>D 1
4
</p>
<p>�
E
�
W2.t/W2.s/
</p>
<p>�
� ts � st C st
</p>
<p>�
:
</p>
<p>By adding zero one obtains:
</p>
<p>EŒW2.t/W2.s/&#141; D E
h
.W.t/ � W.s/C W.s//2 W2.s/
</p>
<p>i
</p>
<p>D E
h
.W.t/ � W.s//2 W2.s/
</p>
<p>i
</p>
<p>C2E
�
.W.t/ � W.s//W3.s/
</p>
<p>�
C E
</p>
<p>�
W4.s/
</p>
<p>�
:
</p>
<p>If we assume w.l.o.g. that s � t, then due to the independence of non-overlapping
increments of W it holds that:
</p>
<p>E
h
.W.t/ � W.s//2 W2.s/
</p>
<p>i
D E
</p>
<p>h
.W.t/ � W.s//2
</p>
<p>i
E
�
W2.s/
</p>
<p>�
</p>
<p>D Var .W.t/ � W.s//Var.W.s//
D .t � s/s</p>
<p/>
</div>
<div class="page"><p/>
<p>232 10 Ito Integrals
</p>
<p>and
</p>
<p>E
�
.W.t/ � W.s//W3.s/
</p>
<p>�
D E.W.t/ � W.s//E.W3.s// D 0:
</p>
<p>As the kurtosis of a Gaussian random variable is 3, it follows that
</p>
<p>EŒW4.s/&#141; D 3s2:
</p>
<p>Therefore, these results jointly yield the claimed outcome:
</p>
<p>E.I.t/ I.s// D .t � s/s
4
</p>
<p>C 3
4
</p>
<p>s2 � st
4
</p>
<p>D s
2
</p>
<p>2
; s � t :
</p>
<p>10.3 We know about the aforementioned Ito integral from Proposition 10.2 that its
</p>
<p>variance is t2=2. Hence, it is to be shown that:
</p>
<p>Var
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/
</p>
<p>�
D t
</p>
<p>2
</p>
<p>2
:
</p>
<p>We use Proposition 10.1,
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/ D W
2.t/
</p>
<p>2
;
</p>
<p>from which it follows immediately that
</p>
<p>E
</p>
<p>�Z t
</p>
<p>0
</p>
<p>W.s/ @W.s/
</p>
<p>�
D t
2
:
</p>
<p>The usual variance decomposition, see (2.1), hence yields
</p>
<p>Var
</p>
<p>�
W2.t/
</p>
<p>2
</p>
<p>�
D 1
4
</p>
<p>�
E
�
W4.t/
</p>
<p>�
� t2
</p>
<p>�
:
</p>
<p>Due to a kurtosis of 3, the fourth moment of a N .0; t/-distribution just amounts to
</p>
<p>3 t2. Thus, one obtains
</p>
<p>Var
</p>
<p>�
W2.t/
</p>
<p>2
</p>
<p>�
D 1
4
</p>
<p>�
3 t2 � t2
</p>
<p>�
D t
</p>
<p>2
</p>
<p>2
;
</p>
<p>which proves the claim.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Problems and Solutions 233
</p>
<p>10.4 The expectation of the sum Sn.W/ is equal to the sum of the expectations.
</p>
<p>Therefore, we consider an individual expectation,
</p>
<p>E
�
W.s�i / .W.si/ � W.si�1/
</p>
<p>�
D E
</p>
<p>�
W.s�i /W.si/ � W.s�i /W.si�1/
</p>
<p>�
</p>
<p>D min.s�i ; si/ � min.s�i ; si�1/
D .1 � &#13;/ si�1 C &#13; si � si�1
D &#13; .si � si�1/;
</p>
<p>where simply the well-known covariance formula was used. Hence, summation
</p>
<p>yields as desired
</p>
<p>E .Sn.W// D &#13;
nX
</p>
<p>iD1
.si � si�1/
</p>
<p>D &#13;. sn � s0/
D &#13; .t � 0/ :
</p>
<p>10.5 Convergence in mean square implies that the mean squared error tends to zero.
</p>
<p>The MSE with the limit zero reads MSE.&#13;n; 0/ D E.&#13; 2n /. Therefore, it remains to
be shown that: E.&#13; 2n /! 0:
</p>
<p>For this purpose one considers with s�i D .1 � &#13;/si�1 C &#13; si:
</p>
<p>&#13; 2n D W2.s�i / � 2W.s�i / Œ.1 � &#13;/W.si�1/C &#13; W.si/&#141;
C .1 � &#13;/2 W2.si�1/C 2&#13; .1 � &#13;/W.si�1/W.si/
C &#13;2W2.si/:
</p>
<p>Forming expectation yields:
</p>
<p>E.&#13; 2n / D s�i � 2
�
.1� &#13;/si�1 C &#13; s�i
</p>
<p>�
C .1� &#13;/2si�1
</p>
<p>C2&#13;.1� &#13;/ si�1 C &#13;2 si
D .1 � &#13;/si�1 C &#13; si � 2.1� &#13;/ si�1 � 2&#13;.1� &#13;/si�1 � 2 &#13;2 si
</p>
<p>C.1 � &#13;/2 si�1 C 2 &#13;.1� &#13;/ si�1 C &#13;2 si
D si.&#13; � &#13;2/C si�1
</p>
<p>�
.1 � &#13;/2 � .1 � &#13;/
</p>
<p>�
</p>
<p>D si.&#13; � &#13;2/C si�1.&#13;2 � &#13;/
D .si � si�1/ &#13; .1 � &#13;/:
</p>
<p>Hence, for n ! 1 the required result is obtained as si � si�1 tends to zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 10 Ito Integrals
</p>
<p>10.6 For a given partition we write
</p>
<p>Vn.g; t/ D
nX
</p>
<p>iD1
jg.si/ � g.si�1/j D
</p>
<p>nX
</p>
<p>iD1
</p>
<p>ˇ̌
ˇ̌
Z si
</p>
<p>si�1
</p>
<p>g0.s/ ds
</p>
<p>ˇ̌
ˇ̌ :
</p>
<p>As the derivative is continuous, jg0.s/j is continuous as well and hence integrable.
According to the mean value theorem an s�i 2 Œsi�1; si&#141; exists with
</p>
<p>Z si
si�1
</p>
<p>g0.s/ ds D g0.s�i / .si � si�1/ :
</p>
<p>Thus it follows that
</p>
<p>Vn.g; t/ D
nX
</p>
<p>iD1
jg0.s�i /j .si � si�1/
</p>
<p>!
Z t
</p>
<p>0
</p>
<p>jg0.s/j ds :
</p>
<p>Quod erat demonstrandum.
</p>
<p>10.7 The claim is based on the bound
</p>
<p>Qn.g; t/ � max
1�i�n
</p>
<p>.jg.si/� g.si�1/j/
nX
</p>
<p>iD1
jg.si/ � g.si�1/j
</p>
<p>D max
1�i�n
</p>
<p>.j g.si/ � g.si�1/ j/ Vn.g; t/ :
</p>
<p>Due to continuity it holds that
</p>
<p>max
1�i�n
</p>
<p>.j g.si/ � g.si�1/ j/ ! 0 :
</p>
<p>Hence, the claim immediately follows from the bound.
</p>
<p>10.8 It is to be shown that the mean squared error,
</p>
<p>MSE .Qn.W; t/; t/ D E
�
.Qn.W; t/ � t/2
</p>
<p>�
;
</p>
<p>tends to zero. For this purpose we proceed in two steps. In the first one we show
</p>
<p>that the MSE coincides with the variance of Qn.W; t/. In the second step it will be
</p>
<p>shown that the variance converges to zero.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.5 Problems and Solutions 235
</p>
<p>(1) For the first step we only need to derive E.Qn.W; t// D t. With
</p>
<p>Qn.W; t/ D
nX
</p>
<p>iD1
.W.si/ � W.si�1//2
</p>
<p>the required expectation can be easily determined:
</p>
<p>E .Qn.W; t// D
nX
</p>
<p>iD1
Var .W.si/ � W.si�1//
</p>
<p>D
nX
</p>
<p>iD1
.si � si�1/ D sn � s0 D t � 0
</p>
<p>D t :
</p>
<p>(2) Due to the independence of the increments of the WP one has
</p>
<p>Var .Qn.W; t// D
nX
</p>
<p>iD1
Var
</p>
<p>�
.W.si/ � W.si�1//2
</p>
<p>�
:
</p>
<p>Due to W.si/ � W.si�1/ � N .0; si � si�1/ and with a kurtosis of 3 for Gaussian
random variables, it furthermore holds that:
</p>
<p>Var
�
.W.si/ � W.si�1//2
</p>
<p>�
D E
</p>
<p>�
.W.si/ � W.si�1//4
</p>
<p>�
�
�
E
�
.W.si/� W.si�1//2
</p>
<p>��2
</p>
<p>D 3 ŒVar.W.si/ � W.si�1//&#141;2 � .si � si�1/2
</p>
<p>D 2 .si � si�1/2 :
</p>
<p>Hence, plugging in yields
</p>
<p>Var.Qn.W; t// D 2
nX
</p>
<p>iD1
.si � si�1/2
</p>
<p>� 2 max
1�i�n
</p>
<p>.si � si�1/
nX
</p>
<p>iD1
.si � si�1/
</p>
<p>D 2 max
1�i�n
</p>
<p>.si � si�1/ .sn � s0/
</p>
<p>! 0 ; n ! 1 ;
</p>
<p>which completes the proof.</p>
<p/>
</div>
<div class="page"><p/>
<p>236 10 Ito Integrals
</p>
<p>10.9 Let us call the aforementioned covariation CVn:
</p>
<p>CVn D
nX
</p>
<p>iD1
.W.si/ � W.si�1// .si � si�1/ :
</p>
<p>The claim reads: MSE.CVn; 0/ ! 0. As it obviously holds that E.CVn/ D 0, we
obtain
</p>
<p>MSE.CVn; 0/ D Var.CVn/ :
</p>
<p>Hence, it remains to be shown that this variance tends to zero: Due to the
</p>
<p>independence of the increments of the WP, one determines
</p>
<p>Var.CVn/ D
nX
</p>
<p>iD1
Var .W.si/� W.si�1// .si � si�1/2 ;
</p>
<p>and hence
</p>
<p>Var.CVn/ D
nX
</p>
<p>iD1
.si � si�1/3
</p>
<p>� max
1�i�n
</p>
<p>.si � si�1/
nX
</p>
<p>iD1
.si � si�1/2
</p>
<p>D max
1�i�n
</p>
<p>.si � si�1/Qn.id; t/
</p>
<p>! 0 ;
</p>
<p>where Qn.id; t/ is the quadratic variation of the identity function, see Example 10.6.
</p>
<p>Hence, the claim is established.
</p>
<p>10.10 We want to obtain the expected value of Y.s; t/ with
</p>
<p>Y.s; t/ WD W.s/
Z t
</p>
<p>0
</p>
<p>W.r/dW.r/ ; s � t:
</p>
<p>Due to (10.3) it again holds that:
</p>
<p>E .Y.s; t// D E
�
</p>
<p>W.s/
W2.t/ � t
</p>
<p>2
</p>
<p>�
</p>
<p>D 1
2
</p>
<p>E
�
W.s/W2.t/
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>References 237
</p>
<p>Therefore, we study
</p>
<p>E
�
W.s/W2.t/
</p>
<p>�
D E
</p>
<p>�
W.s/.W.t/ � W.s/C W.s//2
</p>
<p>�
</p>
<p>D E
�
W.s/.W.t/ � W.s//2
</p>
<p>�
</p>
<p>C2E
�
W2.s/.W.t/ � W.s//
</p>
<p>�
C EŒW3.s/&#141;:
</p>
<p>Let us consider the last three terms one by one. Due to the independence of the
</p>
<p>increments, one obtains:
</p>
<p>E
�
W.s/.W.t/ � W.s//2
</p>
<p>�
D E.W.s//E
</p>
<p>�
.W.t/ � W.s//2
</p>
<p>�
D 0:
</p>
<p>Moreover, it is obvious that the second term is also zero. For the third term the
</p>
<p>symmetry of the Gaussian distribution yields E
�
W3.s/
</p>
<p>�
D 0. Summing up, we have
</p>
<p>shown that
</p>
<p>E
�
W.s/W2.t/
</p>
<p>�
D 0 ; s � t;
</p>
<p>and hence
</p>
<p>E
</p>
<p>�
W.s/
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.r/dW.r/
</p>
<p>�
D 0 ; s � t:
</p>
<p>References
</p>
<p>Klebaner, F. C. (2005). Introduction to stochastic calculus with applications (2nd ed.). London:
Imperical College Press.
</p>
<p>&Oslash;ksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.).
Berlin/New York: Springer.
</p>
<p>Phillips, P. C. B., &amp; Park, J. Y. (1988). Asymptotic equivalence of ordinary least squares and
generalized least squares in regressions with integrated regressors. Journal of the American
Statistical Association, 83, 111&ndash;115.
</p>
<p>Tanaka, K. (1996). Time series analysis: Nonstationary and noninvertible distribution theory.
New York: Wiley.</p>
<p/>
</div>
<div class="page"><p/>
<p>11Ito&rsquo;s Lemma
</p>
<p>11.1 Summary
</p>
<p>If a process is given as a stochastic Riemann and/or Ito integral, then one may
</p>
<p>wish to determine how a function of the process looks. This is achieved by Ito&rsquo;s
</p>
<p>lemma as an ingredient of stochastic calculus. In particular, stochastic integrals can
</p>
<p>be determined and stochastic differential equations can be solved with it; we will
</p>
<p>get to know stochastic variants of familiar rules of differentiation (chain and product
</p>
<p>rule). For this purpose we approach Ito&rsquo;s lemma step by step by first discussing it
</p>
<p>for Wiener processes, then by generalizing it for diffusion processes and finally by
</p>
<p>considering some extensions.
</p>
<p>11.2 The Univariate Case
</p>
<p>The WP itself is a special case of a diffusion as defined in (10.6). With
</p>
<p>� .t;W.t// D 0 and � .t;W.t// D 1
</p>
<p>Eq. (10.6) becomes (with probability one)
</p>
<p>W.t/ D W.0/C
Z t
</p>
<p>0
</p>
<p>dW.s/ D
Z t
</p>
<p>0
</p>
<p>dW.s/ :
</p>
<p>Thus, we consider this special case first.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_11
</p>
<p>239</p>
<p/>
</div>
<div class="page"><p/>
<p>240 11 Ito&rsquo;s Lemma
</p>
<p>ForWiener Processes
</p>
<p>As a revision, let us recall (10.3), which can be written equivalently as
</p>
<p>2
</p>
<p>Z t
</p>
<p>0
</p>
<p>W.s/ dW.s/ D W2.t/ � t :
</p>
<p>If g.W/ D W2 is defined with derivatives g0.W/ D 2W and g00.W/ D 2, then this
equation can also be formulated as follows:
</p>
<p>Z t
</p>
<p>0
</p>
<p>g0 .W.s// dW.s/ D g .W.t// � t
</p>
<p>D g .W.t// � 1
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00 .W.s// ds :
</p>
<p>Now, this is just the form of Ito&rsquo;s lemma for functions g of a Wiener process. It is a
</p>
<p>corollary of the more general case (Proposition 11.1) which will be covered in the
</p>
<p>following. Throughout, we will assume that g has a continuous second derivative
</p>
<p>(&ldquo;twice continuously differentiable&rdquo;).
</p>
<p>Corollary 11.1 (Ito&rsquo;s Lemma for WP) Let g W R ! R be twice continuously
differentiable. Then it holds that
</p>
<p>dg .W.t// D g0 .W.t// dW.t/C 1
2
</p>
<p>g00 .W.t// dt :
</p>
<p>In integral form this corollary to Ito&rsquo;s lemma is to be read as follows:
</p>
<p>g .W.t// D g .W.0//C
Z t
</p>
<p>0
</p>
<p>g0 .W.s// dW.s/C 1
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00 .W.s// ds :
</p>
<p>Strictly speaking, this integral equation is the statement of the corollary, which
</p>
<p>is abbreviated by the differential notation. However, in doing so it must not be
</p>
<p>forgotten that the WP is not differentiable. Sometimes one also writes even more
</p>
<p>briefly:
</p>
<p>dg .W/ D g0.W/ dW C 1
2
</p>
<p>g00.W/ dt :
</p>
<p>Example 11.1 (Powers of the WP) For g.W/ D 1
2
</p>
<p>W2 this special case of Ito&rsquo;s
</p>
<p>lemma just proves (10.3). In general, one obtains for m � 2 from Corollary 11.1
with g.W/ D Wm
</p>
<p>m
:
</p>
<p>d
</p>
<p>�
Wm.t/
</p>
<p>m
</p>
<p>�
D Wm�1.t/ dW.t/C m � 1
</p>
<p>2
Wm�2.t/ dt ;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The Univariate Case 241
</p>
<p>or in integral notation
</p>
<p>Wm.t/ D m
Z t
</p>
<p>0
</p>
<p>Wm�1.s/ dW.s/C m.m � 1/
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>Wm�2.s/ ds : �
</p>
<p>Explanation and Proof
</p>
<p>Corollary 11.1 can be considered as a stochastic chain rule and can loosely be
</p>
<p>formulated as follows: the derivative of g.W.t// results as the product of the
</p>
<p>outer derivative (g0.W/) and the inner derivative (dW), plus an Ito-specific extra
term consisting of the second derivative of g times 1
</p>
<p>2
. Where this term comes
</p>
<p>from (second order Taylor series expansion) and why no further terms occur
</p>
<p>(higher order derivatives), we want to clarify now. For this purpose we prove
</p>
<p>Corollary 11.1 (almost completely) although it is, as mentioned above, a corollary
</p>
<p>to Proposition 11.1.
</p>
<p>With sn D t and s0 D 0 it holds due to (8.3) that:
</p>
<p>g.W.t// D g.W.0//C
nX
</p>
<p>iD1
.g.W.si// � g.W.si�1/// :
</p>
<p>Now, on the right-hand side a second order Taylor expansion of g.W.si// about
</p>
<p>W.si�1/ yields
</p>
<p>g.W.si// D g.W.si�1//C g0.W.si�1// .W.si/� W.si�1//
</p>
<p>C g
00.�i/
</p>
<p>2
.W.si/ � W.si�1//2 ;
</p>
<p>with �i between W.si�1/ and W.si/:
</p>
<p>j �i � W.si�1/j 2 .0; j W.si/ � W.si�1/j/ :
</p>
<p>By substitution of g.W.si// � g.W.si�1//, g.W.t// � g.W.0// can be expressed by
two sums:
</p>
<p>g.W.t// � g.W.0// D ˙1 C˙2
</p>
<p>with
</p>
<p>˙1 D
nX
</p>
<p>iD1
g0.W.si�1// .W.si/ � W.si�1// ;
</p>
<p>˙2 D
1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
g00.�i/ .W.si/� W.si�1//2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>242 11 Ito&rsquo;s Lemma
</p>
<p>Now, ˙1 just coincides with the Ito sum from (10.5) such that it holds due to
</p>
<p>Proposition 10.3 that:
</p>
<p>˙1
2!
Z t
</p>
<p>0
</p>
<p>g0.W.s// dW.s/ :
</p>
<p>Furthermore, we know from the section on quadratic variation (Proposition 10.8)
</p>
<p>.dW.s//2 D ds :
</p>
<p>As the quadratic variation of the WP is not negligible (Proposition 10.7), this
</p>
<p>suggests the following approximation:
</p>
<p>˙2 �
1
</p>
<p>2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00.W.s// .dW.s//2
</p>
<p>D 1
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00.W.s// ds :
</p>
<p>A corresponding convergence in mean square can actually be established, which
</p>
<p>we will dispense with at this point. Hence, except for this technical detail,
</p>
<p>Corollary 11.1 is verified.
</p>
<p>Additionally, we want to consider why higher order derivatives do not matter for
</p>
<p>Ito&rsquo;s lemma. For a third order Taylor expansion e.g. it follows
</p>
<p>g.W.si// � g.W.si�1// D g0.W.si�1//.W.si/ � W.si�1//
</p>
<p>C g
00.W.si�1//
</p>
<p>2
.W.si/ � W.si�1//2
</p>
<p>C g
000.�i/
</p>
<p>6
.W.si/� W.si�1//3 :
</p>
<p>Thus, due to the summation, the term
</p>
<p>˙3 D
nX
</p>
<p>iD1
g000.�i/ .W.si/� W.si�1//3
</p>
<p>occurs. However, it is negligible:
</p>
<p>j˙3j �
nX
</p>
<p>iD1
</p>
<p>ˇ̌
g000.�i/
</p>
<p>ˇ̌
jW.si/ � W.si�1/j .W.si/ � W.si�1//2
</p>
<p>� max
1�i�n
</p>
<p>˚ˇ̌
g000.�i/
</p>
<p>ˇ̌
jW.si/� W.si�1/j
</p>
<p>�
� Qn.W; t/
</p>
<p>2! 0 � t D 0 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 The Univariate Case 243
</p>
<p>as the quadratic variation of the WP tends to t and as it furthermore holds that
</p>
<p>MSE ŒW.si/� W.si�1/; 0&#141; D Var .W.si/ � W.si�1// D si � si�1 ! 0 :
</p>
<p>For Diffusions
</p>
<p>Now, we turn to Ito&rsquo;s lemma for diffusions. In this section, we consider the
</p>
<p>univariate case of only one diffusion that depends on one WP only. The following
</p>
<p>variant of Ito&rsquo;s lemma is again a kind of stochastic chain rule and the idea for the
</p>
<p>proof is again based on a second order Taylor expansion.
</p>
<p>Proposition 11.1 (Ito&rsquo;s Lemma with One Dependent Variable) Let g W R ! R
be twice continuously differentiable and X.t/ a diffusion on Œ0;T&#141; with (10.6), or
</p>
<p>briefly:
</p>
<p>dX.t/ D �.t/ dt C �.t/ dW.t/ :
</p>
<p>Then it holds that
</p>
<p>dg .X.t// D g0 .X.t// dX.t/C 1
2
</p>
<p>g00 .X.t// �2.t/ dt :
</p>
<p>If X.t/ D W.t/ is a Wiener process, i.e. �.t/ D 0 and �.t/ D 1, then Corollary 11.1
is obtained as a special case.
</p>
<p>The statement in Proposition 11.1 is given somewhat succinctly It can be
</p>
<p>condensed even more by suppressing the dependence on time:
</p>
<p>dg .X/ D g0 .X/ dX C 1
2
</p>
<p>g00 .X/ �2 dt :
</p>
<p>However, it needs to be clear that by substituting dX.t/ one obtains for the
</p>
<p>differential dg .X.t// the following lengthy expression:
</p>
<p>�
g0 .X.t// �.t;X.t//C 1
</p>
<p>2
g00 .X.t// �2.t;X.t//
</p>
<p>�
dt C g0 .X.t// �.t;X.t// dW.t/ :
</p>
<p>The corresponding statement in integral notation naturally looks yet more extensive.
</p>
<p>Example 11.2 (Differential of the Exponential Function) Let a diffusion X.t/ be
</p>
<p>given,
</p>
<p>dX.t/ D �.t/ dt C �.t/ dW.t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>244 11 Ito&rsquo;s Lemma
</p>
<p>Then, how does the differential of eX.t/ read? This example is particularly easy to
</p>
<p>calculate as it holds for g.x/ D ex that:
</p>
<p>g00.x/ D g0.x/ D g.x/ D ex:
</p>
<p>Hence Ito&rsquo;s lemma yields:
</p>
<p>deX.t/ D eX.t/dX.t/C e
X.t/
</p>
<p>2
�2.t/dt
</p>
<p>D eX.t/
�
�.t/C �
</p>
<p>2.t/
</p>
<p>2
</p>
<p>�
dt C eX.t/ �.t/dW.t/:
</p>
<p>If X.t/ is deterministic, i.e. �.t/ D 0, then it results
</p>
<p>deX.t/
</p>
<p>dt
D eX.t/ dX.t/
</p>
<p>dt
;
</p>
<p>which just corresponds to the traditional chain rule (outer derivative times inner
</p>
<p>derivative). �
</p>
<p>On the Proof
</p>
<p>Just as for the proof of Corollary 11.1, one obtains with �i, where
</p>
<p>j�i � X.si�1/j 2 .0; jX.si/� X.si�1/j/ ;
</p>
<p>from the Taylor expansion:
</p>
<p>g.X.t// � g.X.0// D ˙1 C˙2 ;
</p>
<p>˙1 D
nX
</p>
<p>iD1
g0 .X.si�1// .X.si/ � X.si�1//
</p>
<p>˙2 D
1
</p>
<p>2
</p>
<p>nX
</p>
<p>iD1
g00.�i/ .X.si/ � X.si�1//2 :
</p>
<p>The first sum is approximated as desired:
</p>
<p>˙1 �
Z t
</p>
<p>0
</p>
<p>g0.X.s// dX.s/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Bivariate Diffusions with One WP 245
</p>
<p>The second sum is approximated by
</p>
<p>˙2 �
1
</p>
<p>2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00.X.s// .dX.s//2 :
</p>
<p>By multiplying out the square of the differential of the Ito process,
</p>
<p>.dX.s//2 D �2.s/.ds/2 C 2�.s/ �.s/ dW.s/ ds C �2.s/ .dW.s//2 ;
</p>
<p>one shows due to (cf. Proposition 10.8),
</p>
<p>.ds/2 D 0 ; dW.s/ ds D 0 ; .dW.s//2 D ds;
</p>
<p>for the second sum:
</p>
<p>˙2 �
1
</p>
<p>2
</p>
<p>Z t
</p>
<p>0
</p>
<p>g00.X.s// �2.s/ ds :
</p>
<p>This verifies Proposition 11.1 at least heuristically.
</p>
<p>11.3 Bivariate Diffusions with OneWP
</p>
<p>A generalization of Proposition 11.1, which is sometimes needed, is presented by
</p>
<p>the following variant of Ito&rsquo;s lemma. The function g be dependent on two diffusions
</p>
<p>X1 and X2, where both are driven by the very same Wiener process. Occasionally, we
</p>
<p>will call this case (referring to the literature on interest rate models) the one-factor
</p>
<p>case as it is the identical factor W.t/ driving both diffusions.
</p>
<p>One-Factor Case
</p>
<p>Let g be a function in two arguments, whose partial derivatives are denoted by
</p>
<p>@g.X1;X2/
</p>
<p>@Xi
and
</p>
<p>@2g.X1;X2/
</p>
<p>@Xi @Xj
:
</p>
<p>Then, the following proposition is a special case of Proposition 11.3.
</p>
<p>Proposition 11.2 (Ito&rsquo;s Lemma with Two Dependent Variables) Let gWR�R !
R be twice continuously differentiable with respect to both arguments, and let Xi.t/
</p>
<p>be diffusions on Œ0;T&#141; with the same WP:
</p>
<p>dXi.t/ D �i.t/ dt C �i.t/ dW.t/ ; i D 1; 2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>246 11 Ito&rsquo;s Lemma
</p>
<p>Then it holds that
</p>
<p>dg .X1.t/; X2.t// D
@g .X1.t/; X2.t//
</p>
<p>@X1
dX1.t/C
</p>
<p>@g .X1.t/; X2.t//
</p>
<p>@X2
dX2.t/
</p>
<p>C 1
2
</p>
<p>�
@2g .X1.t/; X2.t//
</p>
<p>@X21
�21 .t/C
</p>
<p>@2g .X1.t/; X2.t//
</p>
<p>@X22
�22 .t/
</p>
<p>�
dt
</p>
<p>C @
2g .X1.t/; X2.t//
</p>
<p>@X1 @X2
�1.t/ �2.t/ dt :
</p>
<p>Note that substitution of dXi.t/ in Proposition 11.2 leads again to an integral
</p>
<p>equation for the process g .X1.t/; X2.t// including Riemann and Ito integrals.
</p>
<p>Frequently, the time dependence of the processes will be suppressed in order to
</p>
<p>obtain a more economical formulation of Proposition 11.2:
</p>
<p>dg .X1; X2/ D
@g .X1; X2/
</p>
<p>@X1
dX1 C
</p>
<p>@g .X1; X2/
</p>
<p>@X2
dX2
</p>
<p>C1
2
</p>
<p>�
@2g .X1; X2/
</p>
<p>@X21
�21 C
</p>
<p>@2g .X1; X2/
</p>
<p>@X22
�22
</p>
<p>�
dt
</p>
<p>C@
2g .X1; X2/
</p>
<p>@X1 @X2
�1 �2 dt :
</p>
<p>By this notation one recognizes, that again a second order Taylor expansion hides
</p>
<p>behind Proposition 11.2, but now of the two-dimensional function g,
</p>
<p>dg .X1; X2/ D
@g .X1; X2/
</p>
<p>@X1
dX1 C
</p>
<p>@g .X1; X2/
</p>
<p>@X2
dX2
</p>
<p>C1
2
</p>
<p>�
@2g .X1; X2/
</p>
<p>@X21
.dX1/
</p>
<p>2 C @
2g .X1; X2/
</p>
<p>@X22
.dX2/
</p>
<p>2
</p>
<p>�
</p>
<p>C1
2
</p>
<p>�
@2g .X1; X2/
</p>
<p>@X1 @X2
dX1dX2 C
</p>
<p>@2g .X1; X2/
</p>
<p>@X2 @X1
dX2dX1
</p>
<p>�
;
</p>
<p>because the mixed second derivatives coincide due to the continuity assumed. With
</p>
<p>(10.7) and (10.8) it can easily be shown that (we again suppress the arguments)
</p>
<p>.dXi/
2 D �2i .dt/2 C 2�i �i dtdW C �2i .dW/2 D 0C 0C �2i dt ;
</p>
<p>and for the covariance expression as well
</p>
<p>dX1 dX2 D �1 �2 dt : (11.1)</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Bivariate Diffusions with One WP 247
</p>
<p>Example 11.3 (One-Factor Product Rule) Proposition 11.2 provides us with a
</p>
<p>stochastic product rule for X1.t/X2.t/:
</p>
<p>d.X1.t/X2.t// D X2.t/ dX1.t/C X1.t/ dX2.t/C �1.t/ �2.t/ dt : (11.2)
</p>
<p>Under �1.t/ D 0 or �2.t/ D 0 (no stochastics), the well-known product rule is just
reproduced. The derivation of (11.2) follows for g.x1; x2/ D x1 x2 with
</p>
<p>@g
</p>
<p>@x1
D x2;
</p>
<p>@2 g
</p>
<p>@ x21
D 0;
</p>
<p>@g
</p>
<p>@x2
D x1;
</p>
<p>@2 g
</p>
<p>@ x22
D 0
</p>
<p>and
</p>
<p>@2g
</p>
<p>@x1 @x2
D @
</p>
<p>2g
</p>
<p>@x2 @x1
D 1:
</p>
<p>Hence, we obtain an abbreviated form:
</p>
<p>d .X1 X2/ D
@g .X1; X2/
</p>
<p>@X1
dX1 C
</p>
<p>@g .X1; X2/
</p>
<p>@X2
dX2 C �1 �2 dt ;
</p>
<p>where the second derivatives were plugged in. If one substitutes the first derivatives,
</p>
<p>then one obtains the result from (11.2). �
</p>
<p>Time as a Dependent Variable
</p>
<p>Frequently it is of interest to consider another special case of Proposition 11.2.
</p>
<p>Again, g is a function in two arguments; however, the first one is time t, and the
</p>
<p>second one is a diffusion X.t/:
</p>
<p>g W Œ0;T&#141; � R ! R
.t ;X/ 7! g.t;X/ :
</p>
<p>We consciously suppress the fact that the diffusion is time-dependent as well. Since,
</p>
<p>when we talk about the derivative of g with respect to time, then we refer strictly
</p>
<p>formally to the partial derivative with respect to the first argument. Sometimes this
</p>
<p>is confusing for beginners. For example for
</p>
<p>g.t;X.t// D g.t;X/ D t X.t/</p>
<p/>
</div>
<div class="page"><p/>
<p>248 11 Ito&rsquo;s Lemma
</p>
<p>the derivative with respect to t refers to:
</p>
<p>@g .t; X.t//
</p>
<p>@t
D X.t/ :
</p>
<p>Hence, for the partial derivatives we purposely do not consider that X itself is a
</p>
<p>function of t.
</p>
<p>With X1.t/ D t and X.t/ D X2.t/ from Proposition 11.2 we obtain for �1.t/ D 1
and �1.t/ D 0 the following circumstance.
</p>
<p>Corollary 11.2 (Ito&rsquo;s Lemma with Time as a Dependent Variable) Let g W
Œ0;T&#141; �R ! R be twice continuously differentiable with respect to both arguments
and let X.t/ be a diffusion on Œ0;T&#141; with (10.6), or briefly
</p>
<p>dX.t/ D �.t/ dt C �.t/ dW.t/ :
</p>
<p>Then it holds that
</p>
<p>dg .t; X.t// D @g .t; X.t//
@t
</p>
<p>dt C @g .t; X.t//
@X
</p>
<p>dX.t/C 1
2
</p>
<p>@2g .t; X.t//
</p>
<p>@X2
�2.t/ dt :
</p>
<p>Again, suppressing time-dependence, this can be condensed to
</p>
<p>dg .t; X/ D @g .t; X/
@t
</p>
<p>dt C @g .t; X/
@X
</p>
<p>dX C 1
2
</p>
<p>@2g .t; X/
</p>
<p>@X2
�2 dt :
</p>
<p>Example 11.4 (OUP as a Diffusion) As an application we can just prove that the
</p>
<p>standard Ornstein-Uhlenbeck process Xc.t/ from (9.3) is a diffusion with
</p>
<p>� .t;Xc.t// D c Xc.t/ and � .t;Xc.t// D 1 :
</p>
<p>For this purpose we define as an auxiliary quantity the process
</p>
<p>X.t/ D
Z t
</p>
<p>0
</p>
<p>e�csdW.s/ ;
</p>
<p>or
</p>
<p>dX.t/ D e�ctdW.t/ :
</p>
<p>With this variable we define the function g such,
</p>
<p>g.t;X/ D ectX ;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Bivariate Diffusions with One WP 249
</p>
<p>that it holds for the OUP:
</p>
<p>Xc.t/ D g.t;X.t// D ectX.t/:
</p>
<p>With the derivatives
</p>
<p>@g.t;X/
</p>
<p>@t
D cectX; @g.t;X/
</p>
<p>@X
D ect; @
</p>
<p>2g.t;X/
</p>
<p>@X2
D 0
</p>
<p>it follows from Corollary 11.2:
</p>
<p>dXc.t/ D cectX.t/dt C ectdX.t/C 0
D cXc.t/dt C dW.t/;
</p>
<p>where dX.t/ was substituted. �
</p>
<p>Further examples for practicing Corollary 11.2 can be found in the problem section.
</p>
<p>K-Variate Diffusions
</p>
<p>Concerning the contents, there is no reason why Proposition 11.2 should be written
</p>
<p>just with two processes. Let us consider as a generalization the case where g depends
</p>
<p>on K diffusions, all of them given by the identical WP:
</p>
<p>gW RK ! R ; i.e. g D g .X1; : : : ;XK/ 2 R :
</p>
<p>Then it holds with dXk.t/, k D 1; : : : ;K, due to a second order Taylor expansion
that:
</p>
<p>dg .X1; : : : ;XK/ D
KX
</p>
<p>kD1
</p>
<p>@g
</p>
<p>@Xk
dXk C
</p>
<p>1
</p>
<p>2
</p>
<p>KX
</p>
<p>kD1
</p>
<p>KX
</p>
<p>jD1
</p>
<p>@2g
</p>
<p>@Xk @Xj
dXk dXj :
</p>
<p>As in the bivariate case, one obtains dXk dXj D �k �j dt, cf. (11.1). Sometimes, as in
Corollary 11.2, time as a further variable is allowed for,
</p>
<p>gW Œ0;T&#141; � RK ! R ; i.e. g D g .t;X1; : : : ;XK/ 2 R ;
</p>
<p>and
</p>
<p>dg .t;X1; : : : ;XK/ D
@g
</p>
<p>@t
dt C
</p>
<p>KX
</p>
<p>kD1
</p>
<p>@g
</p>
<p>@Xk
dXk C
</p>
<p>1
</p>
<p>2
</p>
<p>KX
</p>
<p>kD1
</p>
<p>KX
</p>
<p>jD1
</p>
<p>@2g
</p>
<p>@Xk @Xj
dXk dXj :</p>
<p/>
</div>
<div class="page"><p/>
<p>250 11 Ito&rsquo;s Lemma
</p>
<p>11.4 Generalization for Independent WP
</p>
<p>We keep to the multivariate generalization, however, allowing for several, stochas-
</p>
<p>tically independent Wiener processes behind the diffusions.
</p>
<p>The General Case
</p>
<p>Now, W1.t/; : : : ;Wd.t/ denote stochastically independent standard Wiener pro-
</p>
<p>cesses. We allow for d factors driving each of the K diffusions. According to this, let
</p>
<p>X.t/ be a K-dimensional diffusion1 X0.t/ D .X1.t/; : : : ;XK.t//, defined by d factors
Wj.t/, j D 1; : : : ; d:
</p>
<p>dXk.t/ D �k.t/dt C
dX
</p>
<p>jD1
�kj.t/dWj.t/; k D 1; : : : ;K:
</p>
<p>In order to have a diffusion, it holds for �k and �kj that they may only depend on
</p>
<p>X.t/ and t:
</p>
<p>�k.t/ D �k.t;X.t//; k D 1; : : :K;
</p>
<p>�kj.t/ D �kj.t;X.t//; k D 1; : : :K; j D 1; : : : d:
</p>
<p>For a function g, which maps X.t/ to the real numbers, Ito&rsquo;s lemma reads as follows,
</p>
<p>cf. &Oslash;ksendal (2003, Theorem 4.2.1).
</p>
<p>Proposition 11.3 (Ito&rsquo;s Lemma (Independent WP)) Let g W RK ! R be twice
continuously differentiable with respect to all the arguments, and let Xk.t/ be
</p>
<p>diffusions on Œ0;T&#141; depending on d independent Wiener processes:
</p>
<p>dXk.t/ D �k.t/dt C
dX
</p>
<p>jD1
�kj.t/dWj.t/; k D 1; : : : ;K:
</p>
<p>Then it holds for X0.t/ D .X1.t/; : : : ;XK.t// that
</p>
<p>dg.X.t// D
KX
</p>
<p>kD1
</p>
<p>@g.X.t//
</p>
<p>@Xk
dXk.t/C
</p>
<p>1
</p>
<p>2
</p>
<p>KX
</p>
<p>iD1
</p>
<p>KX
</p>
<p>kD1
</p>
<p>@2g.X.t//
</p>
<p>@Xi@Xk
dXi.t/dXk.t/
</p>
<p>1For vectors and matrices, the superscript denotes transposition and not differentiation. Further,
the dimension d of the multivariate Wiener process should not be confused with the differential
operator denoted by the same symbol.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Generalization for Independent WP 251
</p>
<p>with
</p>
<p>dXi.t/dXk.t/ D
dX
</p>
<p>jD1
�ij.t/�kj.t/dt: (11.3)
</p>
<p>Heuristically, (11.3) can be well justified. For this purpose we consider vectors of
</p>
<p>length d:
</p>
<p>� k.t/ D
</p>
<p>0
B@
�k1.t/
:::
</p>
<p>�kd.t/
</p>
<p>1
CA ; k D 1; : : : ;K ; and W.t/ D
</p>
<p>0
B@
</p>
<p>W1.t/
:::
</p>
<p>Wd.t/
</p>
<p>1
CA ;
</p>
<p>such that
</p>
<p>dXk.t/ D �k.t/dt C � 0k.t/dW.t/ :
</p>
<p>Neglecting the dependence on time, it follows
</p>
<p>dXi dXk D �i �k.dt/2 C �i � 0kdW.t/dt C �k � 0idW.t/dt
C� 0idW.t/� 0kdW.t/
</p>
<p>D � 0idW.t/dW0.t/� k
</p>
<p>due to (see Proposition 10.8)
</p>
<p>.dt/2 D 0 and dWj.t/dt D 0
</p>
<p>and
</p>
<p>� 0kdW.t/ D .� 0kdW.t//0 D dW0.t/� k:
</p>
<p>Let us consider the matrix
</p>
<p>dW.t/ dW0.t/ D
</p>
<p>0
BBB@
</p>
<p>.dW1.t//
2 dW1.t/dW2.t/ : : : dW1.t/dWd.t/
</p>
<p>dW2.t/dW1.t/ .dW2.t//
2 : : : dW2.t/dWd.t/
</p>
<p>:::
:::
</p>
<p>: : :
:::
</p>
<p>dWd.t/dW1.t/ dWd.t/dW2.t/ : : : .dWd.t//
2
</p>
<p>1
CCCA :
</p>
<p>As is well known, it holds due to (10.7) that:
</p>
<p>.dWj.t//
2 D dt:</p>
<p/>
</div>
<div class="page"><p/>
<p>252 11 Ito&rsquo;s Lemma
</p>
<p>Furthermore, it can be shown for stochastically independent Wiener processes that
</p>
<p>dWi.t/dWk.t/ D 0; i &curren; k:
</p>
<p>Overall, we hence obtain
</p>
<p>dW.t/dW0.t/ D Iddt;
</p>
<p>with the d-dimensional identity matrix Id. All in all it follows
</p>
<p>dXi.t/dXk.t/ D � 0i.t/ Iddt � k.t/
D � 0i.t/� k.t/ dt ;
</p>
<p>which is given in (11.3).
</p>
<p>The 2-Factor Case
</p>
<p>Let us consider the case K D d D 2. Then, Proposition 11.3 becomes more clearly
</p>
<p>dg.X.t// D
2X
</p>
<p>kD1
</p>
<p>@g.X.t//
</p>
<p>@Xk
dXk.t/C
</p>
<p>1
</p>
<p>2
</p>
<p>2X
</p>
<p>iD1
</p>
<p>2X
</p>
<p>kD1
</p>
<p>@2g.X.t//
</p>
<p>@Xi@Xk
dXi.t/dXk.t/
</p>
<p>with
</p>
<p>dX1dX1 D .�211 C �212/dt ;
</p>
<p>dX2dX2 D .�221 C �222/dt ;
</p>
<p>and
</p>
<p>dX1dX2 D .�11�21 C �12�22/dt:
</p>
<p>Two interesting special cases result:
</p>
<p>1. �12 D �22 D 0 (one-factor model),
2. �12 D �21 D 0 (independent diffusions).
</p>
<p>The first case naturally corresponds to the one from the previous section: Both
</p>
<p>diffusions only depend on the same WP. The second case is the opposite extreme
</p>
<p>where both diffusions depend only on one or other of the two stochastically
</p>
<p>independent processes:
</p>
<p>dXk.t/ D �k.t/dt C �kk.t/dWk.t/; k D 1; 2:</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Generalization for Independent WP 253
</p>
<p>We want to discuss both borderline cases based on two examples.
</p>
<p>Example 11.5 (2-Factor Product Rule) Proposition 11.3 with K D d D 2 yields
with the derivatives from Example 11.3 as product rule:
</p>
<p>d.X1 X2/ D X2 dX1 C X1 dX2 C dX1 dX2 : (11.4)
</p>
<p>In the borderline case of only one factor, naturally the result from Eq. (11.2) is
</p>
<p>reproduced. In the second borderline case of stochastically independent diffusions,
</p>
<p>however, it holds, as in the deterministic case, that
</p>
<p>d.X1 X2/ D X2 dX1 C X1 dX2 :
</p>
<p>Without restrictions (11.4) reads as follows:
</p>
<p>d.X1 X2/ D X2 dX1 C X1 dX2 C .�11�21 C �12�22/dt :
</p>
<p>The example illustrates that a proper application of Ito&rsquo;s lemma needs to account
</p>
<p>for the number of factors underlying the diffusions. �
</p>
<p>Example 11.6 (2-Factor Quotient Rule) For X2.t/ &curren; 0 and
</p>
<p>g.X1;X2/ D
X1
</p>
<p>X2
</p>
<p>we obtain:
</p>
<p>@g
</p>
<p>@X1
D X�12 ;
</p>
<p>@g
</p>
<p>@X2
D �X1 X�22 ;
</p>
<p>@2g
</p>
<p>@X21
D 0; @
</p>
<p>2g
</p>
<p>@X22
D 2X1 X�32 ;
</p>
<p>@2g
</p>
<p>@X1@X2
D �X�22 :
</p>
<p>Hence Proposition 11.3 yields with K D d D 2 suppressing the arguments:
</p>
<p>d
</p>
<p>�
X1
</p>
<p>X2
</p>
<p>�
D X2 dX1 � X1 dX2
</p>
<p>X22
C X1 X
</p>
<p>�1
2 .�
</p>
<p>2
21 C �222/ � .�11�21 C �12�22/
</p>
<p>X22
dt :
</p>
<p>(11.5)
</p>
<p>If X2 is a deterministic function .�21 D �22 D 0/, then the conventional quotient
rule is reproduced. �</p>
<p/>
</div>
<div class="page"><p/>
<p>254 11 Ito&rsquo;s Lemma
</p>
<p>11.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>11.1 Prove part (a) from Example 9.1.
</p>
<p>Hint: Choose g .t;W/ D t W in Corollary 11.2 or in (11.2).
</p>
<p>11.2 Prove part (b) from Example 9.1.
</p>
<p>Hint: Choose g .t;W/ D .1 � t/W in Corollary 11.2 or in (11.2).
</p>
<p>11.3 Prove part (b) from Proposition 9.1 (integration by parts).
</p>
<p>Hint: Choose g .t;W/ D f .t/W in Corollary 11.2 or in (11.2).
</p>
<p>11.4 Prove statement (a) from Proposition 9.4 with Ito&rsquo;s lemma.
</p>
<p>Hint: Choose g .t;W/ D e�ctW.
</p>
<p>11.5 Prove for the OUP from Proposition 9.4:
</p>
<p>Z t
</p>
<p>0
</p>
<p>Xc.s/ dW.s/ D
1
</p>
<p>2
</p>
<p>�
X2c .t/ � t
</p>
<p>�
� c
</p>
<p>Z t
</p>
<p>0
</p>
<p>X2c .s/ ds
</p>
<p>Note that for c D 0 (WP) this reproduces (10.3).
Hint: Choose g.Xc.t// D X2c .t/ in Ito&rsquo;s lemma.
</p>
<p>11.6 Determine the differential of W.t/=eW.t/ according to the one-factor product
</p>
<p>rule (11.2).
</p>
<p>11.7 Determine the differential of W.t/=eW.t/ directly from Corollary 11.1.
</p>
<p>Solutions
</p>
<p>11.1 For the proof we use Corollary 11.2 with
</p>
<p>g.t;W/ D tW:
</p>
<p>The derivatives needed read:
</p>
<p>@g.t;W/
</p>
<p>@t
D W; @g.t;W/
</p>
<p>@W
D t; @
</p>
<p>2g.t;W/
</p>
<p>@W2
D 0:
</p>
<p>Hence, one determines with Corollary 11.2:
</p>
<p>d.tW.t// D W.t/dt C tdW.t/C 0
2
:</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Problems and Solutions 255
</p>
<p>Due to g.0;W.0// D 0, we obtain as an integral equation
</p>
<p>tW.t/ D
Z t
</p>
<p>0
</p>
<p>W.s/ds C
Z t
</p>
<p>0
</p>
<p>sdW.s/;
</p>
<p>which was to be shown.
</p>
<p>11.2 As in Problem 11.1 we consider
</p>
<p>g.t;W/ D .1� t/W
</p>
<p>with
</p>
<p>@g.t;W/
</p>
<p>@t
D �W; @g.t;W/
</p>
<p>@W
D .1 � t/; @
</p>
<p>2g.t;W/
</p>
<p>@W2
D 0:
</p>
<p>Therefore, substitution into Corollary 11.2 yields
</p>
<p>d..1 � t/W.t// D �W.t/dt C .1� t/dW.t/C 0
2
:
</p>
<p>As W.0/ D 0 with probability one, it follows that
</p>
<p>.1 � t/W.t/ D �
Z t
</p>
<p>0
</p>
<p>W.s/ds C
Z t
</p>
<p>0
</p>
<p>.1 � s/dW.s/;
</p>
<p>which was to be shown.
</p>
<p>11.3 As an adequate function g we choose
</p>
<p>g.t;W/ D f .t/W ;
</p>
<p>where f .t/ is deterministic. Then, Corollary 11.2 is used with
</p>
<p>@g.t;W/
</p>
<p>@t
D f 0.t/W; @g.t;W/
</p>
<p>@W
D f .t/; @
</p>
<p>2g.t;W/
</p>
<p>@W2
D 0:
</p>
<p>This yields for the differential:
</p>
<p>dg.t;W.t// D f 0.t/W.t/dt C f .t/dW.t/C 0
2
:
</p>
<p>In integral notation this reads
</p>
<p>g.t;W.t// D g.0;W.0//C
Z t
</p>
<p>0
</p>
<p>f 0.s/W.s/ds C
Z t
</p>
<p>0
</p>
<p>f .s/dW.s/:</p>
<p/>
</div>
<div class="page"><p/>
<p>256 11 Ito&rsquo;s Lemma
</p>
<p>As W.0/ D 0 with probability one, we hence obtain the desired result:
</p>
<p>f .t/W.t/ D
Z t
</p>
<p>0
</p>
<p>f 0.s/W.s/ds C
Z t
</p>
<p>0
</p>
<p>f .s/dW.s/:
</p>
<p>11.4 If one chooses g.t;W/ D e�ctW with
</p>
<p>@g.t;W/
</p>
<p>@t
D �ce�ctW; @g.t;W/
</p>
<p>@W
D e�ct; @
</p>
<p>2g.t;W/
</p>
<p>@W2
D 0;
</p>
<p>then Corollary 11.2 allows for the following calculation:
</p>
<p>d.e�ctW.t// D �ce�ctW.t/dt C e�ctdW.t/;
</p>
<p>i.e.
</p>
<p>e�ctW.t/ D W.0/� c
Z t
</p>
<p>0
</p>
<p>e�csW.s/ds C
Z t
</p>
<p>0
</p>
<p>e�csdW.s/
</p>
<p>or
</p>
<p>W.t/ D �cect
Z t
</p>
<p>0
</p>
<p>e�csW.s/ds C ect
Z t
</p>
<p>0
</p>
<p>e�csdW.s/
</p>
<p>D �cect
Z t
</p>
<p>0
</p>
<p>e�csW.s/ds C Xc.t/:
</p>
<p>Rearranging terms completes the proof.
</p>
<p>11.5 With the function g.Xc/ D X2c and its derivatives,
</p>
<p>g0.Xc/ D 2Xc; g00.Xc/ D 2;
</p>
<p>Proposition 11.1 can be applied. We know that Xc.t/ is a diffusion with (see
</p>
<p>Example 11.4):
</p>
<p>dXc.t/ D cXc.t/dt C dW.t/:
</p>
<p>Plugging in into Proposition 11.1 shows:
</p>
<p>dX2c .t/ D 2Xc.t/dXc.t/C
2
</p>
<p>2
dt
</p>
<p>D .2cX2c .t/C 1/dt C 2Xc.t/dW.t/:</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Problems and Solutions 257
</p>
<p>With starting value Xc.0/ D 0 this translates into the following integral equation:
</p>
<p>X2c .t/ D
Z t
</p>
<p>0
</p>
<p>.2cX2c .s/C 1/ds C 2
Z t
</p>
<p>0
</p>
<p>Xc.s/dW.s/:
</p>
<p>This is equivalent to
</p>
<p>Z t
</p>
<p>0
</p>
<p>Xc.s/dW.s/ D
1
</p>
<p>2
</p>
<p>�
X2c .t/ �
</p>
<p>Z t
</p>
<p>0
</p>
<p>ds
</p>
<p>�
� c
</p>
<p>Z t
</p>
<p>0
</p>
<p>X2c .s/ds;
</p>
<p>which amounts to the claim.
</p>
<p>11.6 We define
</p>
<p>X1.t/ D W.t/; X2.t/ D e�W.t/;
</p>
<p>and we are interested in the differential of the product. In order to apply the one-
</p>
<p>factor product rule, we need the differentials of the factors. For X1 it obviously holds
</p>
<p>that: dX1 D dW. For e�W.t/ Example 11.2 yields
</p>
<p>dX2 D de�W D �e�WdW C
e�W
</p>
<p>2
dt :
</p>
<p>Hence, we have
</p>
<p>�1.t/ D 1; �2.t/ D �e�W.t/:
</p>
<p>Plugging in into the product rule (11.2) yields:
</p>
<p>d
�
We�W
</p>
<p>�
D e�WdX1 C WdX2 � e�Wdt
</p>
<p>D e�WdW C W
�
�e�WdW C e
</p>
<p>�W
</p>
<p>2
dt
</p>
<p>�
� e�Wdt
</p>
<p>D e�W
�
</p>
<p>W
</p>
<p>2
� 1
</p>
<p>�
dt C e�W.1 � W/dW:
</p>
<p>11.7 As g.W/ D W
eW
</p>
<p>is a simple function of W, Corollary 11.1 yields a direct
</p>
<p>approach to the differential. For this purpose, we only need the derivatives (quotient
</p>
<p>rule):
</p>
<p>g0.W/ D e
W � W eW
</p>
<p>e2W
D 1 � W
</p>
<p>eW
;
</p>
<p>g00.W/ D �e
W � .1 � W/ eW
</p>
<p>e2W
D W � 2
</p>
<p>eW
:</p>
<p/>
</div>
<div class="page"><p/>
<p>258 11 Ito&rsquo;s Lemma
</p>
<p>Thus, it follows from Ito&rsquo;s lemma that
</p>
<p>d
</p>
<p>�
W
</p>
<p>eW
</p>
<p>�
D g0.W/ dW C 1
</p>
<p>2
g00.W/ dt
</p>
<p>D 1 � W
eW
</p>
<p>dW C W � 2
2eW
</p>
<p>dt
</p>
<p>D e�W
�
</p>
<p>W
</p>
<p>2
� 1
</p>
<p>�
dt C e�W.1 � W/ dW:
</p>
<p>Of course, this result coincides with the one from the previous problem.
</p>
<p>Reference
</p>
<p>&Oslash;ksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.).
Berlin/New York: Springer.</p>
<p/>
</div>
<div class="page"><p/>
<p>Part III
</p>
<p>Applications</p>
<p/>
</div>
<div class="page"><p/>
<p>12Stochastic Differential Equations (SDE)
</p>
<p>12.1 Summary
</p>
<p>In the following section we discuss the most general stochastic differential equation
</p>
<p>considered here, whose solution is a diffusion. Then, linear differential equations
</p>
<p>(with variable coefficients) will be studied extensively. Here we obtain analytical
</p>
<p>solutions by Ito&rsquo;s lemma. We discuss special cases that are widespread in the
</p>
<p>literature on finance. In the fourth section we turn to numerical solutions allowing
</p>
<p>to simulate processes. The sample paths displayed in the figures of Chap. 13 are
</p>
<p>constructed that way.
</p>
<p>12.2 Definition and Existence
</p>
<p>After a definition and a discussion of conditions for existence, we will consider the
</p>
<p>deterministic case as well. Deterministic differential equations are embedded into
</p>
<p>the stochastic ones as special cases.
</p>
<p>Diffusions
</p>
<p>We defined the solution of
</p>
<p>dX.t/ D �.t/ dt C �.t/ dW.t/
</p>
<p>as a diffusion process, where �.t/ and �.t/ are allowed to depend on t and on X.t/
</p>
<p>itself. As the most general case of this chapter we consider diffusions as in (10.6):
</p>
<p>dX.t/ D �.t;X.t// dt C �.t;X.t// dW.t/ ; t 2 Œ0;T&#141; : (12.1)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_12
</p>
<p>261</p>
<p/>
</div>
<div class="page"><p/>
<p>262 12 Stochastic Differential Equations (SDE)
</p>
<p>The solutions1 of such differential equations can also be written in integral form:
</p>
<p>X.t/ D X.0/C
Z t
</p>
<p>0
</p>
<p>�.s;X.s// ds C
Z t
</p>
<p>0
</p>
<p>�.s;X.s// dW.s/ ; t 2 Œ0;T&#141; : (12.2)
</p>
<p>Under what conditions is such a definition possible? In other words: Which
</p>
<p>requirements have to be met by the functions�.t; x/ and �.t; x/, such that a solution
</p>
<p>of (12.1) exists at all &ndash; and uniquely so? This mathematical aspect is not to be overly
</p>
<p>deepened at this point, however, neither is it to be completely neglected. We consider
</p>
<p>stronger but simpler sufficient conditions than necessary. For a profound discussion
</p>
<p>see e.g. &Oslash;ksendal (2003). The first assumption requires that � and � are smooth
</p>
<p>enough in the argument x:2
</p>
<p>(E1) The partial derivatives of �.t; x/ and �.t; x/ with respect to x exist and are
</p>
<p>continuous in x.
</p>
<p>Secondly, we maintain a linear restriction of the growth of the diffusion process:
</p>
<p>(E2) There exist constants K1 and K2 with
</p>
<p>j�.t; x/j C j�.t; x/j � K1 C K2jxj :
</p>
<p>And finally we need a well defined starting value, which may be stochastic:
</p>
<p>(E3) X.0/ is independent of W.t/ with E.X2.0// &lt;1.
</p>
<p>Under these assumptions &Oslash;ksendal (2003, Theorem 5.2.1) proves the following
</p>
<p>proposition.
</p>
<p>Proposition 12.1 (Existence of a Unique Solution) Under the assumptions (E1)
</p>
<p>to (E3), Eq. (12.1) has a unique solution X.t/ of the form (12.2) with continuous
</p>
<p>paths and E.X2.t// &lt;1.
</p>
<p>The assumption (E3) can always be met by assuming a fixed starting value. The
</p>
<p>second assumption is necessary for the existence of a (finite) solution while (E1)
</p>
<p>1Strictly speaking, this is a so-called &ldquo;strong solution&rdquo; in contrast to a &ldquo;weak solution&rdquo;. For a weak
solution the behavior of X.t/ is only characterized in distribution. We will not concern ourselves
with weak solutions.
2Normally, one demands that they satisfy a Lipschitz condition. A function f is called Lipschitz
continuous if it holds for all x and y that there exists a constant K with
</p>
<p>j f .x/� f .y/j � K jx � yj :
</p>
<p>We can conceal this condition by requiring the stronger sufficient continuous differentiability.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Definition and Existence 263
</p>
<p>guarantees the uniqueness of this solution. This is to be illustrated by means of two
</p>
<p>deterministic examples.
</p>
<p>Example 12.1 (Violation of the Assumptions) We examine two examples known
</p>
<p>from the literature on deterministic differential equations where �.t; x/ D 0.
Similar cases can be found e.g. in &Oslash;ksendal (2003). In the first example we set
</p>
<p>�.t;X.t// D X2=3.t/:
</p>
<p>dX.t/ D X2=3.t/ dt ; X.0/ D 0 ; t � 0 :
</p>
<p>We define for an arbitrary a &gt; 0 infinitely many solutions:
</p>
<p>Xa.t/ D
(
</p>
<p>0 ; t � a
.t�a/3
27
</p>
<p>; t &gt; a :
</p>
<p>By differentiating one can observe that any Xa.t/ indeed satisfies the given equation.
</p>
<p>The reason for the ambiguity of the solutions lies in the violation of (E1) as the
</p>
<p>partial derivative,
</p>
<p>@�.t; x/
</p>
<p>@x
D 2
3
</p>
<p>x�1=3 ;
</p>
<p>does not exist at x D 0.
The second example reads for �.t;X.t// D X2.t/:
</p>
<p>dX.t/ D X2.t/ dt ; X.0/ D 1 ; t 2 Œ0; 1/ :
</p>
<p>Again, by elementary means one proves that the solution reads
</p>
<p>X.t/ D .1 � t/�1 ; 0 � t &lt; 1 ;
</p>
<p>and hence tends to 1 for t ! 1. The reason for this lies in a violation of (E2): The
quadratic function �.t; x/ D x2 cannot be linearly bounded. �
</p>
<p>Linear Coefficients
</p>
<p>In order to be able to state analytical solutions, we frequently restrict generality and
</p>
<p>consider linear differential equations:
</p>
<p>dX.t/ D .c1.t/X.t/C c2.t// dt C .�1.t/X.t/C �2.t// dW.t/ ; t � 0 ; (12.3)
</p>
<p>where the variable coefficients ci.t/ and �i.t/, i D 1; 2, are continuous deterministic
functions of time. Here, X.t/ enters � and � just linearly. Obviously, the partial</p>
<p/>
</div>
<div class="page"><p/>
<p>264 12 Stochastic Differential Equations (SDE)
</p>
<p>derivatives from (E1) are constant (in x) and thus continuous. In addition, one
</p>
<p>obtains a linear bound:
</p>
<p>j�.t; x/j C j�.t; x/j � jc1.t/j jxj C jc2.t/j C j�1.t/j jxj C j�2.t/j
D .jc1.t/j C j�1.t/j/ jxj C .jc2.t/j C j�2.t/j/
� K2 jxj C K1 :
</p>
<p>As ci.t/ and �i.t/ are continuous in t and hence bounded for finite t, positive
</p>
<p>constants K1 and K2 can be specified such that the inequality above holds true.
</p>
<p>Therefore, (E2) is satisfied. Therefore, a unique solution exists for linear stochastic
</p>
<p>differential equations. What is more: Ito&rsquo;s lemma will allow as well for the
</p>
<p>specification of an explicit form of this analytical solution from which one can
</p>
<p>determine first and second moments as functions of time. The next section is
</p>
<p>reserved for studying equation (12.3). Before, we consider the borderline case of
</p>
<p>a deterministic linear differential equation.
</p>
<p>Deterministic Case
</p>
<p>By setting �1.t/ D �2.t/ D 0 in (12.3), we obtain a deterministic linear differential
equation (in small letters to distinguish from the stochastic case),
</p>
<p>dx.t/ D .c1.t/ x.t/C c2.t// dt ; t � 0 ; (12.4)
</p>
<p>or as well
</p>
<p>x0.t/ D c1.t/ x.t/C c2.t/ :
</p>
<p>Frequently, one speaks of first order differential equations, as only the first derivative
</p>
<p>is involved. As is well known, the solution reads (see Problem 12.1)
</p>
<p>x.t/ D z.t/
�
</p>
<p>x.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds
</p>
<p>�
(12.5)
</p>
<p>with
</p>
<p>z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>c1.s/ ds
</p>
<p>�
: (12.6)
</p>
<p>For c2.t/ D 0 one obtains from (12.4) the related homogeneous differential
equation (with starting value 1),
</p>
<p>dz.t/ D c1.t/ z.t/ dt ; z.0/ D 1 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Linear Stochastic Differential Equations 265
</p>
<p>which just has z.t/ from (12.6) as a solution. The following example presents the
</p>
<p>special case of constant coefficients.
</p>
<p>Example 12.2 (Constant Coefficients) In the case of constant coefficients,
</p>
<p>c1.t/ D c1 D const ; c2.t/ D c2 D const ;
</p>
<p>the solution from (12.5) simplifies, see Problem 12.1:
</p>
<p>x.t/ D ec1t
�
</p>
<p>x.0/C c2
c1
</p>
<p>�
1 � e�c1t
</p>
<p>��
</p>
<p>D ec1t
�
</p>
<p>x.0/C c2
c1
</p>
<p>�
� c2
</p>
<p>c1
:
</p>
<p>Hence, for negative values of c1 it holds that the equation is stable in the sense that
</p>
<p>the solution tends towards a fixed value:
</p>
<p>x.t/
t!1�! �c2
</p>
<p>c1
DW � ; c1 &lt; 0 :
</p>
<p>Basically, one can already observe this from the equation itself:
</p>
<p>dx.t/ D .c1 x.t/C c2/ dt
D c1 .x.t/ � �/ dt :
</p>
<p>Namely, if x.t/ lies above the limit �, then the expression in brackets is positive and
</p>
<p>hence the change is negative such that x.t/ adjusts towards the limit �. Conversely,
</p>
<p>x.t/ &lt; � causes a positive derivative such that x.t/ grows and moves towards the
</p>
<p>limit. All in all, for c1 &lt; 0 a convergence to � is modeled. �
</p>
<p>In the following, we will see that the solution of the deterministic linear equation
</p>
<p>is embedded into the stochastic one for �1.t/ D �2.t/ D 0.
</p>
<p>12.3 Linear Stochastic Differential Equations
</p>
<p>For the solution of the equation (12.3) we expect a similar structure as in the
</p>
<p>deterministic case, (12.5), i.e. a homogeneous solution as a multiplicative factor
</p>
<p>has to be expected. Hence, we start with the solution of a homogeneous stochastic
</p>
<p>equation.</p>
<p/>
</div>
<div class="page"><p/>
<p>266 12 Stochastic Differential Equations (SDE)
</p>
<p>Homogeneous Solution
</p>
<p>For c2.t/ D �2.t/ D 0 one obtains from (12.3) the corresponding homogeneous
linear equation. In doing so, we rename X and choose 1 as the starting value3:
</p>
<p>dZ.t/ D c1.t/ Z.t/ dt C �1.t/Z.t/ dW.t/ ; Z.0/ D 1: (12.7)
</p>
<p>Now, Ito&rsquo;s lemma (Proposition 11.1) is applied to g.Z.t// D log.Z.t//. Thus, we
obtain as the solution of (12.7),
</p>
<p>Z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>�
c1.s/ �
</p>
<p>1
</p>
<p>2
�21 .s/
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/ dW.s/
</p>
<p>�
; (12.8)
</p>
<p>see Problem 12.2. Hence, for �1.t/ D 0 the deterministic solution from (12.6)
is reproduced. The solution with an arbitrary starting value different from zero
</p>
<p>therefore reads
</p>
<p>X.t/ D X.0/ exp
�Z t
</p>
<p>0
</p>
<p>�
c1.s/ �
</p>
<p>1
</p>
<p>2
�21 .s/
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/ dW.s/
</p>
<p>�
:
</p>
<p>General Solution
</p>
<p>Let us return to the solution of equation (12.3). Now, analogously to the determin-
</p>
<p>istic case (12.5), let us define Z.t/ from (12.8) as a homogeneous solution. At the
</p>
<p>end of the section we will establish the following proposition whilst applying two
</p>
<p>versions of Ito&rsquo;s lemma. Two interesting, alternative proofs will be given in exercise
</p>
<p>problems.
</p>
<p>Proposition 12.2 (Solution of Linear SDE with Variable Coefficients) The solu-
</p>
<p>tion of (12.3) with in t continuous deterministic coefficients is
</p>
<p>X.t/ D Z.t/
�
</p>
<p>X.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/ � �1.s/�2.s/
Z.s/
</p>
<p>ds C
Z t
</p>
<p>0
</p>
<p>�2.s/
</p>
<p>Z.s/
dW.s/
</p>
<p>�
(12.9)
</p>
<p>with the homogeneous solution
</p>
<p>Z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>�
c1.s/�
</p>
<p>1
</p>
<p>2
�21 .s/
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/ dW.s/
</p>
<p>�
:
</p>
<p>3The renaming justifies the assumption regarding the starting value. Consider
</p>
<p>dX.t/ D c1.t/X.t/ dt C �1.t/X.t/ dW.t/ ; X.0/ &curren; 0 ;
</p>
<p>with a starting value different from zero, then by division one can normalize Z.t/ D X.t/=X.0/.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Linear Stochastic Differential Equations 267
</p>
<p>For �1.t/ D �2.t/ D 0 we again obtain the known result of a deterministic
differential equation, cf. (12.5).
</p>
<p>Expected Value and Variance
</p>
<p>The process defined by (12.3) reads in integral notation
</p>
<p>X.t/ D X.0/C
Z t
</p>
<p>0
</p>
<p>.c1.s/X.s/C c2.s// ds C
Z t
</p>
<p>0
</p>
<p>.�1.s/X.s/C �2.s// dW.s/ :
</p>
<p>Let us define the expectation function as
</p>
<p>�1.t/ WD E.X.t// ;
</p>
<p>then it holds due to Propositions 8.2 (Fubini) and 10.3 that:
</p>
<p>�1.t/ D E .X.0//C
Z t
</p>
<p>0
</p>
<p>.c1.s/E .X.s//C c2.s// ds C 0
</p>
<p>D �1.0/C
Z t
</p>
<p>0
</p>
<p>.c1.s/ �1.s/C c2.s// ds :
</p>
<p>This corresponds exactly with the deterministic equation (12.4). Hence, the solution
</p>
<p>is known from (12.5) and one obtains the form given in Proposition 12.3. The
</p>
<p>derivation of an expression for the second moment is somewhat more complex,
</p>
<p>�2.t/ WD E.X2.t// ;
</p>
<p>see Problem 12.3.
</p>
<p>Proposition 12.3 (Moments of the Solution of a Linear SDE) Under the
</p>
<p>assumptions of Proposition 12.2 it holds that
</p>
<p>�1.t/ D z.t/
�
�1.0/C
</p>
<p>Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds
</p>
<p>�
; z.t/ D exp
</p>
<p>�Z t
</p>
<p>0
</p>
<p>c1.s/ds
</p>
<p>�
(12.10)
</p>
<p>and
</p>
<p>�2.t/ D �.t/
�
�2.0/C
</p>
<p>Z t
</p>
<p>0
</p>
<p>&#13;2.s/
</p>
<p>�.s/
ds
</p>
<p>�
; �.t/ D exp
</p>
<p>�Z t
</p>
<p>0
</p>
<p>&#13;1.s/ds
</p>
<p>�
; (12.11)
</p>
<p>where
</p>
<p>&#13;1.t/ D 2 c1.t/C �21 .t/ ; &#13;2.t/ D 2 Œc2.t/C �1.t/ �2.t/&#141; �1.t/C �22 .t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>268 12 Stochastic Differential Equations (SDE)
</p>
<p>Example 12.3 (Homogeneous Linear SDE (Constant Coefficients)) Since the
</p>
<p>works by Black and Scholes (1973) and Merton (1973) one assumes for the stock
</p>
<p>price X.t/ the model of a homogeneous linear SDE with constant coefficients (and
</p>
<p>starting value X.0/, cf. (1.3)):
</p>
<p>dX.t/ D c1 X.t/ dt C �1 X.t/ dW.t/ :
</p>
<p>The solution resulting from (12.9) or rather from Proposition 12.2 is a geometric
</p>
<p>Brownian motion,
</p>
<p>X.t/ D X.0/ exp
��
</p>
<p>c1 �
1
</p>
<p>2
�21
</p>
<p>�
t C �1 W.t/
</p>
<p>�
:
</p>
<p>This process has already been discussed in Chap. 7. With the generally derived
</p>
<p>formulas we can now recheck the moment functions from (7.9). Proposition 12.3
</p>
<p>yields (see Problem 12.4)
</p>
<p>�1.t/ D �1.0/ exp .c1 t/ ;
</p>
<p>�2.t/ D �2.0/ exp
˚�
2 c1 C �21
</p>
<p>�
t
�
:
</p>
<p>Now, assume a fixed starting value X.0/. Then, it holds that
</p>
<p>�1.0/ D X.0/ and �2.0/ D X2.0/ ;
</p>
<p>and hence
</p>
<p>Var.X.t// D �2.t/ � �21.t/
D X2.0/ exp .2 c1 t/
</p>
<p>�
exp
</p>
<p>�
�21 t
</p>
<p>�
� 1
</p>
<p>�
:
</p>
<p>With X.0/ D 1, � D c1 � 12�21 and � D �1 this corresponds to the notation from
Chap. 7. The moments from (7.9) are indeed reproduced. �
</p>
<p>Inhomogeneous Linear SDE with Additive Noise
</p>
<p>For c2.t/ &curren; 0 the linear SDE is inhomogeneous. However, at the same time the
increments of the Wiener process (&ldquo;noise&rdquo;) enter into (12.3) additively, i.e. �1.t/ D
0:
</p>
<p>dX.t/ D .c1.t/X.t/C c2.t// dt C �2.t/ dW.t/ : (12.12)</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Linear Stochastic Differential Equations 269
</p>
<p>The solution results from (12.9) in Proposition 12.2 as
</p>
<p>X.t/ D z.t/
�
</p>
<p>X.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�2.s/
</p>
<p>z.s/
dW.s/
</p>
<p>�
; (12.13)
</p>
<p>where z.t/ is a deterministic function:
</p>
<p>z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>c1.s/ ds
</p>
<p>�
:
</p>
<p>Note that X.t/, as a Stieltjes integral, is a Gaussian process due to Proposition 9.2. Its
</p>
<p>moments result correspondingly (for a fixed starting value X.0/). We collect these
</p>
<p>results in a corollary.
</p>
<p>Corollary 12.1 (Additive Noise) The solution of (12.12) with in t continuous
</p>
<p>deterministic coefficients is given by (12.13). The starting value X.0/ be determin-
</p>
<p>istic. Then, the process is Gaussian with:
</p>
<p>�1.t/ D z.t/
�
</p>
<p>X.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds
</p>
<p>�
; z.t/ D exp
</p>
<p>�Z t
</p>
<p>0
</p>
<p>c1.s/ ds
</p>
<p>�
; (12.14)
</p>
<p>Var.X.t// D z2.t/
Z t
</p>
<p>0
</p>
<p>�
�2.s/
</p>
<p>z.s/
</p>
<p>�2
ds : (12.15)
</p>
<p>We illustrate the corollary with the following example.
</p>
<p>Example 12.4 (Convergence to Zero) As a concrete example, let us consider the
</p>
<p>process given by the following equation with starting value 0:
</p>
<p>dX.t/ D �X.t/ dt C dW.t/p
1C t
</p>
<p>; t � 0 ; X.0/ D 0 :
</p>
<p>This equation is a special case of additive noise as it holds that �1.t/ D 0. The
remaining coefficient restrictions read:
</p>
<p>c1.t/ D �1; c2.t/ D 0; �2.t/ D
1p
1C t
</p>
<p>:
</p>
<p>What behavior is to be expected intuitively for X.t/? The volatility term, �2.t/, tends
</p>
<p>to zero with t growing; does this also hold true for the variance of the process? And
</p>
<p>c1.t/ D �1 implies that positive values influence the change negatively and vice
versa; does the process hence fluctuate around the expectation of zero? In fact, we
</p>
<p>can show that the process with vanishing variance varies around zero and therefore</p>
<p/>
</div>
<div class="page"><p/>
<p>270 12 Stochastic Differential Equations (SDE)
</p>
<p>converges to zero.4 For this we need the first two moments. These can be obtained
</p>
<p>from (12.14) and (12.15):
</p>
<p>E.X.t// D 0 ;
</p>
<p>Var.X.t// D e�2t
Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>1C sds:
</p>
<p>What can be learned from this about the variance for t increasing? In Problem 12.7
</p>
<p>we show
</p>
<p>Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>1C s ds �
e2t
</p>
<p>1C t � 1 :
</p>
<p>Then, this proves Var.X.t// ! 0 for t ! 1. Hence, it is obvious that X.t/ indeed
tends to zero in mean square. �
</p>
<p>Proof of Proposition 12.2
</p>
<p>With the homogeneous solution
</p>
<p>Z.t/ WD exp
�Z t
</p>
<p>0
</p>
<p>�
c1.s/ �
</p>
<p>1
</p>
<p>2
�21 .s/
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/ dW.s/
</p>
<p>�
;
</p>
<p>of
</p>
<p>dZ.t/ D c1.t/ Z.t/ dt C �1 .t/Z.t/ dW.t/
</p>
<p>we define the two auxiliary quantities
</p>
<p>X1.t/ WD Z�1.t/ ; X2.t/ WD X.t/ :
</p>
<p>Note that X.t/ is the process defined by (12.3) such that the differential of X2.t/ is
</p>
<p>shown in (12.3). The proof suggested here uses the product rule for d .X1.t/X2.t//.
</p>
<p>However, for a valid application the derivation of dX1.t/ is necessary as well.
</p>
<p>As a first step we use Ito&rsquo;s lemma in the form of Proposition 11.1 in order to
</p>
<p>determine the differential for X1.t/ with
</p>
<p>g.Z/ D Z�1 ; g0.Z/ D �Z�2 ; g00.Z/ D 2 Z�3 :
</p>
<p>4For this purpose we do not need an explicit expression for the process which, however, can be
easily obtained from (12.13) with X.0/ D 0:
</p>
<p>X.t/ D e�t
Z t
</p>
<p>0
</p>
<p>esp
1C s
</p>
<p>dW.s/:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.3 Linear Stochastic Differential Equations 271
</p>
<p>The differential becomes
</p>
<p>dX1.t/ D g0.Z.t// dZ.t/C
1
</p>
<p>2
g00.Z.t// �21 .t/ Z
</p>
<p>2.t/ dt
</p>
<p>D �c1.t/ Z.t/ dt C �1 .t/Z.t/ dW.t/
Z2.t/
</p>
<p>C 2
2
</p>
<p>�21 .t/ Z
2.t/
</p>
<p>Z3.t/
dt
</p>
<p>D �
2
1 .t/ � c1.t/
</p>
<p>Z.t/
dt � �1.t/
</p>
<p>Z.t/
dW.t/
</p>
<p>D
�
�21 .t/ � c1.t/
</p>
<p>�
X1.t/ dt � �1.t/X1.t/ dW.t/ :
</p>
<p>In a second step, we can now apply the stochastic product rule (see Eq. (11.2)) as an
</p>
<p>implication of Proposition 11.2 to the auxiliary quantities5:
</p>
<p>d .X1.t/X2.t// D X1.t/ dX2.t/C X2.t/ dX1.t/
� .�1.t/X2.t/C �2.t// �1.t/X1.t/ dt :
</p>
<p>If the differentials dX1.t/ and dX2.t/ are plugged in, then some terms cancel each
</p>
<p>other such that it just remains:
</p>
<p>d .X1.t/X2.t// D X1.t/ .c2.t/ dt C �2.t/ dW.t// � �1.t/ �2.t/X1.t/ dt
</p>
<p>D c2.t/ � �1.t/�2.t/
Z.t/
</p>
<p>dt C �2.t/
Z.t/
</p>
<p>dW.t/ :
</p>
<p>Due to
</p>
<p>X1.t/X2.t/ D
X.t/
</p>
<p>Z.t/
;
</p>
<p>it follows by integrating in a third step:
</p>
<p>X.t/
</p>
<p>Z.t/
D X.0/
</p>
<p>Z.0/
C
Z t
</p>
<p>0
</p>
<p>c2.s/ � �1.s/�2.s/
Z.s/
</p>
<p>ds C
Z t
</p>
<p>0
</p>
<p>�2.s/
</p>
<p>Z.s/
dW.s/ :
</p>
<p>As Z.0/ D 1, we have established (12.9) and hence completed the proof. Two
alternative proofs, which are again based on Ito&rsquo;s lemma (or implications thereof),
</p>
<p>are covered as exercise problems.
</p>
<p>5There is the risk of confusing the symbols �i; i D 1; 2, from Eq. (12.3) with the ones from
Eq. (11.1). Note that the volatility of X1 (i.e. &ldquo;�1&rdquo;) is given by ��1 X1 while the volatility term
&ldquo;�2&rdquo; of X2 just reads �1 X2 C �2!</p>
<p/>
</div>
<div class="page"><p/>
<p>272 12 Stochastic Differential Equations (SDE)
</p>
<p>12.4 Numerical Solutions
</p>
<p>Even if an analytical expression for the solution of a SDE is known, numerical
</p>
<p>solutions in the sense of simulated approximations to paths of a process are of
</p>
<p>interest. Such a simulation of a solution is, on the one hand, desired for reasons
</p>
<p>of a graphic illustration; on the other hand, in practice a whole family of numerical
</p>
<p>solutions is simulated in order to obtain a whole scenario of possible trajectories.
</p>
<p>Euler Approximation
</p>
<p>The interval Œ0;T&#141; from (12.1) is divided w.l.o.g. in n equidistant intervals of the
</p>
<p>length T
n
</p>
<p>. The corresponding partition reads:
</p>
<p>0 D t0 &lt; t1 D
T
</p>
<p>n
&lt; : : : &lt; ti D
</p>
<p>iT
</p>
<p>n
&lt; : : : &lt; tn D T :
</p>
<p>The theoretical solution from (12.2) of an arbitrary diffusion is now considered on
</p>
<p>the subinterval Œti�1; ti&#141;, i D 1; : : : ; n:
</p>
<p>X .ti/ D X .ti�1/C
Z ti
</p>
<p>ti�1
</p>
<p>�.s;X.s// ds C
Z ti
</p>
<p>ti�1
</p>
<p>�.s;X.s// dW.s/ :
</p>
<p>This allows for the following approximation6 as it is discussed e.g. in Mikosch
</p>
<p>(1998):
</p>
<p>X .ti/ � X .ti�1/
</p>
<p>C
Z ti
</p>
<p>ti�1
</p>
<p>� .ti�1;X .ti�1// ds C
Z ti
</p>
<p>ti�1
</p>
<p>� .ti�1;X .ti�1// dW.s/ ;
</p>
<p>which can also be written as:
</p>
<p>X .ti/ � X .ti�1/
</p>
<p>C � .ti�1;X .ti�1//
T
</p>
<p>n
C � .ti�1;X .ti�1// .W .ti/ � W .ti�1// :
</p>
<p>For this purpose
</p>
<p>Z ti
ti�1
</p>
<p>ds D ti � ti�1 D
T
</p>
<p>n
and
</p>
<p>Z ti
ti�1
</p>
<p>dW.s/ D W .ti/� W .ti�1/
</p>
<p>6In the literature, one speaks of an Euler approximation. An improvement is known under the
keyword Milstein approximation. In order to explain what is meant by &ldquo;improve&rdquo; in this case, one
would have to become more involved in numerics.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Problems and Solutions 273
</p>
<p>was used. Hence, we have a recursive scheme. Given X0 D X.0/ one calculates for
i D 1:
</p>
<p>X1 D X0 C �.0;X0/
T
</p>
<p>n
C �.0;X0/
</p>
<p>�
W
</p>
<p>�
T
</p>
<p>n
</p>
<p>�
� W.0/
</p>
<p>�
;
</p>
<p>and in general, for i D 1; : : : ; n:
</p>
<p>Xi D Xi�1 C � .ti�1;Xi�1/
T
</p>
<p>n
C � .ti�1;Xi�1/ .W .ti/ � W .ti�1// : (12.16)
</p>
<p>Thus we obtain n observations Xi (i.e. n C 1 observations including the starting
value), with which a path of the continuous-time process X.t/ on Œ0;T&#141; is simulated.
</p>
<p>However, this simulation requires Gaussian pseudo random numbers in (12.16),
</p>
<p>W .ti/� W .ti�1/ D W
�
</p>
<p>iT
</p>
<p>n
</p>
<p>�
� W
</p>
<p>�
.i � 1/ T
</p>
<p>n
</p>
<p>�
� iiN
</p>
<p>�
0;
</p>
<p>T
</p>
<p>n
</p>
<p>�
:
</p>
<p>For this purpose, a series of stochastically independentN
�
0; T
</p>
<p>n
</p>
<p>�
-distributed random
</p>
<p>variables "i need to be simulated instead of W.ti/ � W.ti�1/, in order to obtain a
numerical solution Xi, i D 1; : : : ; n for the diffusion X.t/ from (12.2) according
to (12.16). Naturally, with n growing the approximation of a numerical solution
</p>
<p>improves.
</p>
<p>12.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>12.1 Show that the function given in (12.5) solves the deterministic differential
</p>
<p>equation (12.4). How does it look like in the case of constant coefficients?
</p>
<p>12.2 Show that Z.t/ from (12.8) solves the homogeneous SDE (12.7) with Z.0/ D
1.
</p>
<p>Hint: See the text.
</p>
<p>12.3 Prove (12.11) from Proposition 12.3.
</p>
<p>Hint: Determine for g .X.t// D X2.t/ an expression with Ito&rsquo;s lemma.
</p>
<p>12.4 Derive the expectation and the variance of the geometric Brownian motion
</p>
<p>with Proposition 12.3,
</p>
<p>X.t/ D X.0/ exp
��
</p>
<p>c1 �
1
</p>
<p>2
�21
</p>
<p>�
t C �1 W.t/
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>274 12 Stochastic Differential Equations (SDE)
</p>
<p>12.5 Determine the process X.t/ for which it holds that:
</p>
<p>dX.t/ D X.t/ dW.t/ ; X.0/ D 1 :
</p>
<p>Hint: Proposition 12.2.
</p>
<p>12.6 Find the solution of
</p>
<p>dX.t/ D �X.t/
1C t dt C
</p>
<p>dW.t/
</p>
<p>1C t ; t � 0 ;
</p>
<p>for X.0/ D 0. Show that it tends to zero in mean square.
Hint: Proposition 12.2.
</p>
<p>12.7 Show for the Example 12.4:
</p>
<p>Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>1C s ds �
e2t
</p>
<p>1C t � 1 :
</p>
<p>12.8 Determine the solution of
</p>
<p>dX.t/ D � X.t/
1 � t dt C dW.t/ ; 0 � t &lt; 1 ;
</p>
<p>with X.0/ D 0. Show that Var.X.t// D .1� t/ t and hence that X.t/ tends to zero in
mean square for t ! 1. (This reminds us of the Brownian bridge, see (7.6). In fact,
the above SDE defines a Brownian bridge, cf. Grimmett &amp; Stirzaker, 2001, p. 535.)
</p>
<p>12.9 Prove Proposition 12.2 by directly applying Proposition 11.2.
</p>
<p>Hint: Choose g.X;Z/ D X=Z.
</p>
<p>12.10 Prove Proposition 12.2 with the quotient rule from (11.5).
</p>
<p>Hint: First derive the quotient rule for the one-factor case (d D 1) as a special case
of (11.5).
</p>
<p>Solutions
</p>
<p>12.1 The solution from (12.5) reads
</p>
<p>x.t/ D z.t/
�
</p>
<p>x.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds
</p>
<p>�</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Problems and Solutions 275
</p>
<p>with
</p>
<p>z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>c1.s/ds
</p>
<p>�
:
</p>
<p>Let us define the square bracket as b.t/:
</p>
<p>b.t/ D
�
</p>
<p>x.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>z.s/
ds
</p>
<p>�
D x.t/
</p>
<p>z.t/
</p>
<p>and
</p>
<p>b0.t/ D c2.t/
z.t/
</p>
<p>:
</p>
<p>The derivative of z.t/ is
</p>
<p>z0.t/ D c1.t/ exp
�Z t
</p>
<p>0
</p>
<p>c1.s/ds
</p>
<p>�
D c1.t/z.t/:
</p>
<p>Hence, the product rule yields:
</p>
<p>x0.t/ D z0.t/ b.t/C z.t/ b0.t/
</p>
<p>D c1.t/ z.t/
x.t/
</p>
<p>z.t/
C z.t/ c2.t/
</p>
<p>z.t/
</p>
<p>D c1.t/ x.t/C c2.t/ ;
</p>
<p>which just corresponds to the claim.
</p>
<p>In the case of constant coefficients, x.t/ from (12.5) with z.t/ D ec1t becomes:
</p>
<p>x.t/ D ec1tŒx.0/C c2
Z t
</p>
<p>0
</p>
<p>e�c1sds&#141;
</p>
<p>D ec1tŒx.0/ � c2
c1
.e�c1t � 1/&#141;
</p>
<p>D ec1t
�
</p>
<p>x.0/C c2
c1
</p>
<p>�
� c2
</p>
<p>c1
:
</p>
<p>12.2 For Z.t/ from (12.7) it holds that
</p>
<p>dZ.t/ D �.t;Z.t//dt C �.t;Z.t//dW.t/
</p>
<p>with
</p>
<p>�.t;Z.t// D c1.t/Z.t/; �.t;Z.t// D �1.t/Z.t/:</p>
<p/>
</div>
<div class="page"><p/>
<p>276 12 Stochastic Differential Equations (SDE)
</p>
<p>Therefore, Proposition 11.1 yields:
</p>
<p>dg.Z.t// D g0.Z.t//dZ.t/C 1
2
</p>
<p>g00.Z.t//�2.t;Z.t//dt:
</p>
<p>With
</p>
<p>g.x/ D log.x/; g0.x/ D 1
x
; g00.x/ D � 1
</p>
<p>x2
;
</p>
<p>we hence obtain
</p>
<p>d log.Z.t// D �.t;Z.t//dt C �.t;Z.t//dW.t/
Z.t/
</p>
<p>� 1
2
</p>
<p>�2.t;Z.t//
</p>
<p>Z2.t/
dt
</p>
<p>D c1.t/dt C �1.t/dW.t/ �
�21 .t/
</p>
<p>2
dt :
</p>
<p>Integration yields
</p>
<p>log.Z.t// D log.Z.0//C
Z t
</p>
<p>0
</p>
<p>�
c1.s/�
</p>
<p>�21 .s/
</p>
<p>2
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/dW.s/:
</p>
<p>Because of Z.0/ D 1, the exponential function yields as desired:
</p>
<p>Z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>�
c1.s/ �
</p>
<p>�21 .s/
</p>
<p>2
</p>
<p>�
ds C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�1.s/dW.s/
</p>
<p>�
:
</p>
<p>12.3 Proposition 11.1 with
</p>
<p>g.x/ D x2; g0.x/ D 2x; g00.x/ D 2
</p>
<p>is applied to X2 where the differential dX.t/ is given by Eq. (12.3). This leads to
</p>
<p>dX2.t/ D 2X.t/dX.t/C .�1.t/X.t/C �2.t//2dt
D Œ2X.t/ .c1.t/X.t/C c2.t//C .�1.t/X.t/C �2.t//2&#141;dt
</p>
<p>C2X.t/.�1.t/X.t/C �2.t//dW.t/:
</p>
<p>As an integral equation this reads as follows:
</p>
<p>X2.t/ D X2.0/C
Z t
</p>
<p>0
</p>
<p>h
2X.s/ .c1.s/X.s/C c2.s//C .�1.s/X.s/C �2.s//2
</p>
<p>i
ds
</p>
<p>C2
Z t
</p>
<p>0
</p>
<p>X.s/.�1.s/X.s/C �2.s//dW.s/:</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Problems and Solutions 277
</p>
<p>The expectation of the second integral is zero due to Proposition 10.3. The
</p>
<p>expectation of the first integral results due to Fubini&rsquo;s theorem as
</p>
<p>Z t
</p>
<p>0
</p>
<p>E
�
2c1.s/X
</p>
<p>2.s/C 2c2.s/X.s/C �21 .s/X2.s/C 2�1.s/�2.s/X.s/C �22 .s/
�
</p>
<p>ds:
</p>
<p>With the definition of �1.s/ and �2.s/ it thus follows that
</p>
<p>�2.t/ D �2.0/C
Z t
</p>
<p>0
</p>
<p>��
2 c1.s/C �21 .s/
</p>
<p>�
�2.s/
</p>
<p>�
ds
</p>
<p>C 2
Z t
</p>
<p>0
</p>
<p>�
.c2.s/C �1.s/ �2.s// �1.s/C �22 .s/
</p>
<p>�
ds :
</p>
<p>In differential notation this equation reads
</p>
<p>d�2.t/ D .&#13;1.t/ �2.t/C &#13;2.t// dt ;
</p>
<p>where the functions &#13;1.t/ and &#13;2.t/ in the proposition following (12.11) were
</p>
<p>adequately defined. Therefore, the second moment results as the solution of a
</p>
<p>deterministic differential equation of the form (12.4). Its solution can be found
</p>
<p>in (12.5). Hence, the proposition is verified.
</p>
<p>12.4 The geometric Brownian motion solves the homogeneous linear equation with
</p>
<p>constant coefficients:
</p>
<p>c2.t/ D �2.t/ D 0; c1.t/ D c1 D const.; �1.t/ D �1 D const.
</p>
<p>For a stochastic starting value it holds that:
</p>
<p>�1.0/ D E.X.0//; �2.0/ D E.X2.0//:
</p>
<p>By plugging in, Proposition 12.3 yields
</p>
<p>�1.t/ D ec1tŒ�1.0/C 0&#141; D �1.0/ exp.c1t/ :
</p>
<p>With the definitions from Proposition 12.3 one determines
</p>
<p>&#13;1.t/ D 2 c1 C �21 DW &#13;1 and &#13;2.t/ D 0 :
</p>
<p>Hence, substitution yields
</p>
<p>�2.t/ D e&#13;1tŒ�2.0/C 0&#141; D �2.0/ exp.&#13;1t/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>278 12 Stochastic Differential Equations (SDE)
</p>
<p>Thus, the variance is calculated as
</p>
<p>Var.X.t// D �2.t/ � �21.t/
D �2.0/ expf.2c1 C �21 /tg � �21.0/ expf2c1tg:
</p>
<p>12.5 The equation at hand is linear with
</p>
<p>c1.t/ D c2.t/ D 0; �2.t/ D 0:
</p>
<p>Furthermore, it holds that
</p>
<p>�1.t/ D 1:
</p>
<p>Therefore, the solution deduced from Proposition 12.2 reads:
</p>
<p>X.t/ D Z.t/ŒX.0/C 0&#141;
</p>
<p>with
</p>
<p>Z.t/ D exp
n
� t
2
C W.t/
</p>
<p>o
:
</p>
<p>In particular for X.0/ D 1 (analogously to e0 D 1) it hence holds that:
</p>
<p>X.t/ D exp
n
W.t/ � t
</p>
<p>2
</p>
<p>o
:
</p>
<p>Due to the analogy to det D et dt with e0 D 1 this process X.t/ is sometimes called
&ldquo;Ito exponential&rdquo;. It is noteworthy that the Ito exponential is not given by expfW.t/g.
</p>
<p>12.6 The equation is linear, see Eq. (12.3), and corresponds to the special case of
</p>
<p>additive noise, cf. (12.12), i.e. �1.t/ D 0. The remaining coefficients read:
</p>
<p>c1.t/ D �
1
</p>
<p>1C t ; c2.t/ D 0; �2.t/ D
1
</p>
<p>1C t :
</p>
<p>Hence, the expression for the solution from (12.13) yields with X.0/ D 0:
</p>
<p>X.t/ D z.t/
Z t
</p>
<p>0
</p>
<p>1
</p>
<p>.1C s/z.s/dW.s/;
</p>
<p>where
</p>
<p>z.t/ D exp
�
�
Z t
</p>
<p>0
</p>
<p>.1C s/�1ds
�
</p>
<p>D expf�Œlog.1C s/&#141;t0g</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Problems and Solutions 279
</p>
<p>D expf� log.1C t/C 0g
</p>
<p>D exp
�
</p>
<p>log
</p>
<p>�
1
</p>
<p>1C t
</p>
<p>��
D 1
1C t :
</p>
<p>Since �2.t/=z.t/ D 1, the solution simplifies radically:
</p>
<p>X.t/ D 1
1C t
</p>
<p>Z t
</p>
<p>0
</p>
<p>dW.s/ D W.t/
1C t :
</p>
<p>For this solution it obviously holds that:
</p>
<p>E.X.t// D 0
</p>
<p>Var.X.t// D t
.1C t/2 ! 0; t ! 1:
</p>
<p>Thus, for t ! 1 we have established
</p>
<p>MSE .X.t/; 0/ D EŒ.X.t/ � 0/2&#141;! 0
</p>
<p>which just corresponds to the required convergence in mean square.
</p>
<p>12.7 In order to prove the inequality claimed, we define the function
</p>
<p>g.s/ D 1
2
</p>
<p>e2s
</p>
<p>1C s
</p>
<p>with the derivative (quotient rule)
</p>
<p>g0.s/ D e
2s
</p>
<p>1C s �
1
</p>
<p>2
</p>
<p>e2s
</p>
<p>.1C s/2 :
</p>
<p>Let us call the integral of interest I,
</p>
<p>I D
Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>1C s ds:
</p>
<p>Then it follows
</p>
<p>I D
Z t
</p>
<p>0
</p>
<p>g0.s/ds C 1
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>.1C s/2 ds
</p>
<p>D g.t/� g.0/C 1
2
</p>
<p>Z t
</p>
<p>0
</p>
<p>e2s
</p>
<p>.1C s/2 ds
</p>
<p>� g.t/� g.0/C 1
2
</p>
<p>I;</p>
<p/>
</div>
<div class="page"><p/>
<p>280 12 Stochastic Differential Equations (SDE)
</p>
<p>where the bound follows from .1C s/ � .1C s/2. By rearranging terms it results
that
</p>
<p>I � 2.g.t/ � g.0//:
</p>
<p>With the definition of g it follows
</p>
<p>I � e
2t
</p>
<p>.1C t/ � 1 ;
</p>
<p>which was to be shown.
</p>
<p>12.8 This is again an inhomogeneous linear equation with additive noise:
</p>
<p>c1.t/ D �
1
</p>
<p>1 � t ; c2.t/ D 0; �1.t/ D 0; �2.t/ D 1:
</p>
<p>With X.0/ D 0, X.t/ from (12.13) turns out to be:
</p>
<p>X.t/ D z.t/
Z t
</p>
<p>0
</p>
<p>.z.s//�1 dW.s/
</p>
<p>with
</p>
<p>z.t/ D exp
�
�
Z t
</p>
<p>0
</p>
<p>1
</p>
<p>1 � sds
�
</p>
<p>D exp
˚
Œlog.1 � s/&#141;t0
</p>
<p>�
</p>
<p>D 1 � t;
</p>
<p>i.e.
</p>
<p>X.t/ D .1 � t/
Z t
</p>
<p>0
</p>
<p>1
</p>
<p>1 � s dW.s/:
</p>
<p>Due to c2.t/ D X.0/ D 0, (12.14) yields:
</p>
<p>E.X.t// D 0:
</p>
<p>Due to (12.15), the variance is:
</p>
<p>Var.X.t// D .1 � t/2
Z t
</p>
<p>0
</p>
<p>1
</p>
<p>.1 � s/2 ds
</p>
<p>D .1 � t/2
�
</p>
<p>1
</p>
<p>.1 � s/
</p>
<p>�t
</p>
<p>0</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 Problems and Solutions 281
</p>
<p>D .1 � t/2
�
</p>
<p>1
</p>
<p>1 � t � 1
�
D
</p>
<p>D .1 � t/2 t
1 � t D .1 � t/t:
</p>
<p>For t ! 1 the variance shrinks to zero such that it holds that X.t/ tends to 0 in mean
square:
</p>
<p>MSE.X.t/; 0/ D Var.X.t// D EŒ.X.t/ � 0/2&#141;! 0:
</p>
<p>12.9 The key problem with this exercise is not to confuse the different meanings
</p>
<p>of �i.t/; i D 1; 2 , in Proposition 11.2 and Eq. (12.3). Hence, firstly we adapt
Proposition 11.2 for the processes X.t/ from (12.3) and Z.t/ from (12.7):
</p>
<p>dX.t/ D �x.t/ dt C �x.t/ dW.t/ ;
</p>
<p>�x.t/ D c1.t/X.t/C c2.t/ ; �x.t/ D �1.t/X.t/C �2.t/ ;
</p>
<p>dZ.t/ D �z.t/ dt C �z.t/ dW.t/ ;
</p>
<p>�z.t/ D c1.t/ Z.t/ ; �z.t/ D �1.t/ Z.t/ :
</p>
<p>Following the hint, we consider
</p>
<p>g.X;Z/ D X
Z
</p>
<p>D X Z�1
</p>
<p>with
</p>
<p>@g
</p>
<p>@X
D Z�1 ; @
</p>
<p>2g
</p>
<p>@X2
D 0
</p>
<p>@g
</p>
<p>@Z
D �XZ�2 ; @
</p>
<p>2g
</p>
<p>@Z2
D 2XZ�3 ; @
</p>
<p>2g
</p>
<p>@X @Z
D �Z�2 :
</p>
<p>Hence, Ito&rsquo;s lemma (Proposition 11.2) yields:
</p>
<p>d
</p>
<p>�
X
</p>
<p>Z
</p>
<p>�
D Z�1 dX � XZ�2 dZ C 1
</p>
<p>2
</p>
<p>�
0C 2XZ�3 �2z
</p>
<p>�
dt � Z�2�x �z dt
</p>
<p>D Z�1 .c1X C c2/ dt C Z�1 .�1X C �2/ dW � XZ�2c1Z dt
�XZ�2�1Z dW C XZ�3�21Z2 dt � Z�2.�1X C �2/ �1Z dt
</p>
<p>D
�
Z�1c2 � Z�1�1�2
</p>
<p>�
dt C Z�1�2 dW:</p>
<p/>
</div>
<div class="page"><p/>
<p>282 12 Stochastic Differential Equations (SDE)
</p>
<p>Integration yields:
</p>
<p>X.t/
</p>
<p>Z.t/
D X.0/
</p>
<p>Z.0/
C
Z t
</p>
<p>0
</p>
<p>c2.s/ � �1.s/�2.s/
Z.s/
</p>
<p>ds C
Z t
</p>
<p>0
</p>
<p>�2.s/
</p>
<p>Z.s/
dW.s/ :
</p>
<p>If this equation is multiplied by Z.t/, then, due to Z.0/ D 1, one obtains the desired
result.
</p>
<p>12.10 We apply (11.5) with
</p>
<p>X1 D X and X2 D Z ;
</p>
<p>where X and Z are driven by the same Wiener process, say W1 D W. Then the
one-factor quotient rule is obtained by the following restrictions:
</p>
<p>�11 D �x and �12 D 0 ;
</p>
<p>�21 D �z and �22 D 0 :
</p>
<p>For this purpose, �x and �z were defined in the previous problem. Then, the one-
</p>
<p>factor quotient rule yields:
</p>
<p>d
</p>
<p>�
X
</p>
<p>Z
</p>
<p>�
D ZdX � XdZ
</p>
<p>Z2
C
</p>
<p>XZ�1 �2z � �x�z
Z2
</p>
<p>dt
</p>
<p>D Z.c1X C c2/ dt C Z.�1X C �2/ dW � Xc1Z dt � X�1Z dW
Z2
</p>
<p>C XZ
�1�21Z
</p>
<p>2 � .�1X C �2/�1Z
Z2
</p>
<p>dt
</p>
<p>D Z.c2 � �1�2/
Z2
</p>
<p>dt C Z�2
Z2
</p>
<p>dW
</p>
<p>D .c2 � �1�2/
Z
</p>
<p>dt C �2
Z
</p>
<p>dW :
</p>
<p>As before, we obtain the desired result by integration and multiplication by Z.t/.
</p>
<p>References
</p>
<p>Black, F., &amp; Scholes, M. (1973). The pricing of options and corporate liabilities. The Journal of
Political Economy, 81, 637&ndash;654.
</p>
<p>Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes (3rd ed.). Oxford:
Oxford University Press.
</p>
<p>Merton, R. C. (1973). Theory of rational option pricing. The Bell Journal of Economics and
Management Science, 4, 141&ndash;183.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 283
</p>
<p>Mikosch, Th. (1998). Elementary stochastic calculus with finance in view. Singapore: World
Scientific Publishing.
</p>
<p>&Oslash;ksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.).
Berlin/New York: Springer.</p>
<p/>
</div>
<div class="page"><p/>
<p>13Interest Rate Models
</p>
<p>13.1 Summary
</p>
<p>The results from the previous chapter will be applied to stochastic differential
</p>
<p>equations that were suggested in the literature for modeling interest rate dynamics.
</p>
<p>However, we do not model yield curves with various maturities, but consider the
</p>
<p>model for one interest rate only driven by one Wiener process (one-factor model).
</p>
<p>The next section starts with the general Ornstein-Uhlenbeck process which has the
</p>
<p>drawback of allowing for negative values. Subsequently, we discuss linear models
</p>
<p>for which negativity is ruled out. Finally, a class of nonlinear models will be
</p>
<p>considered.
</p>
<p>13.2 Ornstein-Uhlenbeck Process (OUP)
</p>
<p>We have already encountered the standard OUP in the chapter on Stieltjes integrals.
</p>
<p>Now, we discuss the general case, which has served as an interest rate model in the
</p>
<p>literature on finance.
</p>
<p>Vasicek
</p>
<p>We now assume constant coefficients for the inhomogeneous linear SDE with
</p>
<p>additive noise in (12.12):
</p>
<p>c1.t/ D c1 D const; c2.t/ D c2 D const; �2.t/ D �2 D const; �1.t/ D 0 :
</p>
<p>This defines the general Ornstein-Uhlenbeck process,
</p>
<p>dX.t/ D .c1 X.t/C c2/ dt C �2 dW.t/ : (13.1)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_13
</p>
<p>285</p>
<p/>
</div>
<div class="page"><p/>
<p>286 13 Interest Rate Models
</p>
<p>According to Corollary 12.1 the solution is
</p>
<p>X.t/ D ec1t
�
</p>
<p>X.0/� c2
c1
</p>
<p>�
e�c1t � 1
</p>
<p>�
C
Z t
</p>
<p>0
</p>
<p>�2 e
�c1s dW.s/
</p>
<p>�
: (13.2)
</p>
<p>In particular for c2 D 0 and �2 D 1 we obtain the standard OUP with X.0/ D 0
from (9.3) in Sect. 9.4. The following equation sheds additional light on the OUP:
</p>
<p>dX.t/ D c1 .X.t/ � �/ dt C �2 dW.t/ ; with � WD �
c2
</p>
<p>c1
: (13.3)
</p>
<p>In this manner, Vasicek (1977) modeled the interest rate dynamics, cf. (1.7). Due
</p>
<p>to (13.2), the solution of this interest rate equation reads:
</p>
<p>X.t/ D ec1t
�
</p>
<p>X.0/C �
�
e�c1t � 1
</p>
<p>�
C
Z t
</p>
<p>0
</p>
<p>�2 e
�c1s dW.s/
</p>
<p>�
:
</p>
<p>Setting the starting value to �, X.0/ D �, then one obtains the form immediately
corresponding to the standard OUP (9.3),
</p>
<p>X.t/ D �C ec1t
Z t
</p>
<p>0
</p>
<p>�2 e
�c1s dW.s/ ;
</p>
<p>with expectation �. From (12.14) and (12.15) we obtain for an arbitrary fixed
</p>
<p>starting value X.0/:
</p>
<p>�1.t/ D E .X.t// D ec1t X.0/C �
�
1 � ec1t
</p>
<p>�
; (13.4)
</p>
<p>Var.X.t// D �
2
2
</p>
<p>�2 c1
�
1 � e2 c1t
</p>
<p>�
: (13.5)
</p>
<p>Particularly the mean value function �1.t/ results as a convex combination of the
</p>
<p>long-term mean value� and the starting value X.0/. For c1 &lt; 0, these moments tend
</p>
<p>to a fixed value and the process can be understood as asymptotically stationary:
</p>
<p>�1.t/ ! � for c1 &lt; 0 ;
</p>
<p>Var.X.t// ! �
2
2
</p>
<p>�2 c1
for c1 &lt; 0 ;
</p>
<p>where the limits are taken as t ! 1. Processes with this property are also called
&ldquo;mean-reverting&rdquo;. The adjustment parameter c1 &lt; 0 measures the &ldquo;speed of mean-
</p>
<p>reversion&rdquo;, i.e. the strength of adjustment: The smaller (more negative) c1, the more
</p>
<p>strongly dX.t/ reacts as a function of the deviation of X.t/ from �. If X.t/ &gt; �, then
</p>
<p>c1 &lt; 0 causes c1.X.t/ � �/ to have a negative impact on X.t/ such that the process</p>
<p/>
</div>
<div class="page"><p/>
<p>13.2 Ornstein-Uhlenbeck Process (OUP) 287
</p>
<p>tends to decrease and therefore approaches�; conversely, it holds that for X.t/ &lt; �
</p>
<p>the process experiences a positive impulse. Furthermore, one observes the distinct
</p>
<p>influence of the parameter c1 on the (asymptotic) variance: The smaller the negative
</p>
<p>c1, the smaller is the asymptotic expression; however, for a negative c1 near zero
</p>
<p>the variance becomes large and the OUP loses the property of &ldquo;mean reversion&rdquo; for
</p>
<p>c1 D 0.
Determining the autocovariance function for c1 &lt; 0 is also useful. We denote it
</p>
<p>by &#13;.t; t C h/ at lag h:
</p>
<p>&#13;.t; t C h/ D E Œ.X.t/ � �1.t//.X.t C h/ � �1.t C h//&#141; ; h � 0 ;
</p>
<p>D E
�
</p>
<p>ec1t�2
</p>
<p>Z t
</p>
<p>0
</p>
<p>e�c1s dW.s/ ec1.tCh/�2
</p>
<p>Z tCh
</p>
<p>0
</p>
<p>e�c1s dW.s/
</p>
<p>�
</p>
<p>D �22 E ŒXc1.t/Xc1.t C h/&#141; ;
</p>
<p>where Xc1.t/ is the standard OUP with zero expectation. From Proposition 9.4 we
</p>
<p>hence adopt
</p>
<p>&#13;.t; t C h/ D �22 ec1h
e2c1t � 1
2c1
</p>
<p>! ��
2
2 e
</p>
<p>c1h
</p>
<p>2c1
; t ! 1 :
</p>
<p>Thus, for a large t there results an autocovariance function only depending on the
</p>
<p>temporal distance h. All in all, this is why the OUP with c1 &lt; 0 can be labeled as
</p>
<p>asymptotically (t ! 1) weakly stationary.
</p>
<p>Simulations
</p>
<p>In the following, processes with T D 20 and n D 1000 are simulated. For reasons
of graphical comparability, the same WP is always assumed, i.e. the 1000 random
</p>
<p>variables filtered by the recursion (12.16) are always the same ones.
</p>
<p>First, we examine the impact of the adjustment parameter c1 on the behavior of
</p>
<p>the OUP. In Fig. 13.1, �2 D 0:01. As we want to think about interest rates when
looking at this figure, expectation and starting value are chosen to be � D 5 .%/.
Here, it is obvious that the solid line deviates less strongly from the expected value
</p>
<p>for c1 D �0:9 and is thus &ldquo;more stationary&rdquo; than the dashed graph for c1 D �0:1.
This is evident as the smaller (i.e. the more negative) c1, the smaller is the variance
</p>
<p>�22 =.�2c1/ for t growing.
In the second figure, we have an OUP with the same parameter set-up for
</p>
<p>c1 D �0:9, however, with a starting value different from � D 5, X.0/ D 5:1.
Furthermore, the expected value function �1.t/ is given and one can observe how
</p>
<p>rather rapidly it approaches the value � D 5 (Fig. 13.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>288 13 Interest Rate Models
</p>
<p>0 5 10 15 20
</p>
<p>4
.9
</p>
<p>7
4
</p>
<p>.9
8
</p>
<p>4
.9
</p>
<p>9
5
</p>
<p>.0
0
</p>
<p>5
.0
</p>
<p>1
5
</p>
<p>.0
2
</p>
<p>5
.0
</p>
<p>3
5
</p>
<p>.0
4
</p>
<p>X1(t)
X2(t)
</p>
<p>Fig. 13.1 OUP for c1 D �0:9 (X1) and c1 D �0:1 (X2) (X.0/ D � D 5, �2 D 0:01)
</p>
<p>Despite the convenient property of mean reversion, the OUP is only partly
</p>
<p>suitable for interest rate modeling: Note that the process takes on negative values
</p>
<p>with a positive probability. This is due to the fact that the OUP, as a Stieltjes integral,
</p>
<p>is Gaussian:
</p>
<p>X.t/ � N .�1.t/;Var.X.t/// :
</p>
<p>Subsequent to Vasicek (1977), interest rate models without this drawback have been
</p>
<p>discussed.
</p>
<p>13.3 Positive Linear Interest RateModels
</p>
<p>For now we stay in the class of linear SDEs, however, we restrict the discussion to
</p>
<p>the case in which positivity (more precisely: nonnegativity) is guaranteed.
</p>
<p>Sufficient Condition
</p>
<p>A sufficient condition for a positive evolution of the solution of a linear SDE is easy
</p>
<p>to be specified. For this purpose we naturally consider the general solution from</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Positive Linear Interest Rate Models 289
</p>
<p>0 5 10 15 20
</p>
<p>4
.9
</p>
<p>8
5
.0
</p>
<p>0
5
.0
</p>
<p>2
5
.0
</p>
<p>4
5
.0
</p>
<p>6
5
.0
</p>
<p>8
5
.1
</p>
<p>0
</p>
<p>X(t)
E[X(t)]
</p>
<p>Fig. 13.2 OUP for c1 D �0:9 and Starting Value X.0/ D 5:1 including Expected Value Function
(� D 5, �2 D 0:01)
</p>
<p>Proposition 12.2. Note that Z.t/, as an exponential function, is always positive. With
</p>
<p>the restriction �2.t/ D 0 the following diffusion is obtained:
</p>
<p>X.t/ D Z.t/
�
</p>
<p>X.0/C
Z t
</p>
<p>0
</p>
<p>c2.s/
</p>
<p>Z.s/
ds
</p>
<p>�
: (13.6)
</p>
<p>With a positive starting value and c2.t/ � 0, a positive evolution of X.t/ is ensured.
The models in this section are of the form (13.6).
</p>
<p>Dothan
</p>
<p>Let us consider a special case more extensively. Dothan (1978) suggested for the
</p>
<p>interest rate dynamics a special case of the geometric Brownian motion:
</p>
<p>dX.t/ D �1 X.t/ dW.t/ ; X.0/ &gt; 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>290 13 Interest Rate Models
</p>
<p>With c2.t/ D �2.t/ D 0 it holds in this case that
</p>
<p>X.t/ D X.0/ exp
��
</p>
<p>�1
2
�21
</p>
<p>�
t C �1 W.t/
</p>
<p>�
;
</p>
<p>and hence, the interest rate X.t/ can in fact not get negative. Not X.t/ follows a
</p>
<p>Gaussian distribution but log.X.t// does. Furthermore, in Example 12.3 we have
</p>
<p>determined the moments (for a fixed starting value):
</p>
<p>�1.t/ D X.0/ and Var.X.t// D X2.0/
�
exp.�21 t/ � 1
</p>
<p>�
:
</p>
<p>Thus, the variance of the process increases exponentially which is why the model
</p>
<p>may not be satisfactory for interest rates.
</p>
<p>Brennan-Schwartz
</p>
<p>Brennan and Schwartz (1980) suggested another attractive variant. It consists of a
</p>
<p>combination of Vasicek (1977) and Dothan (1978); we choose the drift component
</p>
<p>just as for the Ornstein-Uhlenbeck process and the volatility just as for the geometric
</p>
<p>Brownian motion:
</p>
<p>dX.t/ D c1 .X.t/ � �/ dt C �1 X.t/ dW.t/ ; X.0/ D � &gt; 0 ; (13.7)
</p>
<p>where, for simplicity, the starting value is set equal to �. For c1 &lt; 0 it holds that
</p>
<p>c2 D �c1� &gt; 0 such that we have indeed a positive interest rate dynamics. For this
model one can show (see Problem 13.4) that the expected value results just as for
</p>
<p>Dothan (1978),
</p>
<p>�1.t/ D � D X.0/ ;
</p>
<p>while it holds for the variance:
</p>
<p>Var.X.t// D �
2 �21
</p>
<p>2 c1 C �21
</p>
<p>�
exp
</p>
<p>�
.2 c1 C �21 / t
</p>
<p>�
� 1
</p>
<p>�
:
</p>
<p>If c1 &lt; ��21 =2 (i.e. 2 c1 C �21 &lt; 0), then it holds that the variance tends to a fixed
positive value (t ! 1):
</p>
<p>Var.X.t// ! � �
2 �21
</p>
<p>2 c1 C �21
for c1 &lt; �
</p>
<p>�21
2
:
</p>
<p>If the volatility parameter �1 is relatively small compared to the absolute value of
</p>
<p>the negative adjustment parameter c1, then the model (13.7) provides a process with
</p>
<p>a fixed expected value and an asymptotically constant variance. Again, one speaks</p>
<p/>
</div>
<div class="page"><p/>
<p>13.3 Positive Linear Interest Rate Models 291
</p>
<p>of &ldquo;mean reversion&rdquo;. Interestingly, the variance is not only influenced by �1 and
</p>
<p>c1 in an obvious manner: The greater �1, the greater Var.X.t//, and the greater c1
in absolute value, the more strongly or the faster the adjustment happens (and the
</p>
<p>smaller is the variance). The parameter � &gt; 0 from the drift function as well has
</p>
<p>a positive effect on the variance. Intuitively, this is obvious: The smaller � (i.e.
</p>
<p>the closer to zero), the lesser X.t/ can spread as the process does not get negative;
</p>
<p>conversely, it holds that the scope for the variance between the zero line and �
</p>
<p>increases with � growing.
</p>
<p>Simulations
</p>
<p>Again, processes with T D 20 and T D 1000 were simulated. For reasons of
graphical comparability, the same WP as in the previous section is assumed, i.e.
</p>
<p>the 1000 random variables being filtered by the recursion (12.16) are identical.
</p>
<p>In Fig. 13.3 it is obvious how the variance of the geometric Brownian motion
</p>
<p>suggested by Dothan (1978) increases with the parameter �1. Although the expected
</p>
<p>value is constant and equal to the starting value, long periods are possible and
</p>
<p>probable in which the process does not cross the expected value. A more plausible
</p>
<p>interest rate dynamics can be observed in Fig. 13.4 for two values of the adjustment
</p>
<p>0 5 10 15 20
</p>
<p>4
.9
</p>
<p>5
.0
</p>
<p>5
.1
</p>
<p>5
.2
</p>
<p>5
.3
</p>
<p>5
.4
</p>
<p>5
.5
</p>
<p>X1(t)
X2(t)
</p>
<p>Fig. 13.3 Dothan for �1 D 0:01 (X1) and �1 D 0:02 (X2) (X.0/ D � D 5)</p>
<p/>
</div>
<div class="page"><p/>
<p>292 13 Interest Rate Models
</p>
<p>0 5 10 15 20
</p>
<p>4
.8
</p>
<p>5
4
</p>
<p>.9
0
</p>
<p>4
.9
</p>
<p>5
5
</p>
<p>.0
0
</p>
<p>5
.0
</p>
<p>5
5
</p>
<p>.1
0
</p>
<p>5
.1
</p>
<p>5
5
</p>
<p>.2
0 X1(t)
</p>
<p>X2(t)
</p>
<p>Fig. 13.4 Brennan-Schwartz for c1 D �0:9 (X1) and c1 D �0:1 (X2) (X.0/ D � D 5, �1 D
0:01)
</p>
<p>parameter c1. The values are chosen small enough (relative to �1), such that the
</p>
<p>variance remains bounded and converges to a fixed value. It is obvious: The greater
</p>
<p>c1 in absolute value, the smaller the variance.
</p>
<p>13.4 Nonlinear Models
</p>
<p>Chan, Karolyi, Longstaff, and Sanders (1992) [in short: CKLS] considered the
</p>
<p>following class of nonlinear equations for modeling short-term interest rates which
</p>
<p>is covered in this section:
</p>
<p>dX.t/ D c1 .X.t/ � �/ dt C � X&#13; .t/ dW.t/ ; � &gt; 0 ; 0 � &#13; � 1 : (13.8)
</p>
<p>Thus, the modeling of the drift component always corresponds to the one by Vasicek
</p>
<p>(1977). The OUP from (13.1) just results for &#13; D 0 while &#13; D 1 leads to the
just discussed process from (13.7). Noninteger values of &#13; in between provide a
</p>
<p>nonlinear interest rate dynamics. The process from (13.8) is sometimes also called</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Nonlinear Models 293
</p>
<p>model with constant elasticity as it holds for the elasticity with the derivative of the
</p>
<p>volatility � X&#13; .t/ with respect to X that:
</p>
<p>d .� X&#13; /
</p>
<p>d X
</p>
<p>X
</p>
<p>� X&#13;
D &#13; :
</p>
<p>In order to show the interpretation of &#13; as an elasticity, we consider a discretiza-
</p>
<p>tion of the CKLS process as for the computer simulation. For this purpose, we define
</p>
<p>for discrete steps of the length 1, t D 1; 2; : : : ;T:
</p>
<p>xt WD X.t/ ; "t WD �W.t/ D W.t/ � W.t � 1/ :
</p>
<p>The discrete-time version of (13.8) hence reads
</p>
<p>�xt D c1 .xt�1 � �/C �x&#13;t�1"t ; "t � iiN .0; 1/ ;
</p>
<p>or
</p>
<p>xt D xt�1 C c1 .xt�1 � �/C �x&#13;t�1"t :
</p>
<p>For the conditional variance it holds:
</p>
<p>Var.xtjxt�1/ D �2x2&#13;t�1 :
</p>
<p>Correspondingly, it holds for the conditional standard deviation that e.g. a doubling
</p>
<p>of xt�1 leads to a multiplication by the factor 2&#13; :
</p>
<p>p
Var .xtjQxt�1/ D � Qx&#13;t�1 for Qxt�1 D 2 xt�1 ;
</p>
<p>D � 2&#13;x&#13;t�1
D 2&#13;
</p>
<p>p
Var .xtjxt�1/ :
</p>
<p>Two simulated paths of the CKLS model are depicted in Fig. 13.5. They only
</p>
<p>differ in the elasticity &#13; . It is not surprising that the deviations from � get greater
</p>
<p>with a greater &#13; .
</p>
<p>Cox, Ingersoll &amp; Ross [CIR]
</p>
<p>A particularly prominent representative of (13.8) is obtained for &#13; D 0:5. This
model is often used following Cox, Ingersoll, and Ross (1985):
</p>
<p>dX.t/ D c1 .X.t/� �/ dt C �
p
</p>
<p>X.t/ dW.t/ ; � &gt; 0 ; c1 &lt; 0 : (13.9)</p>
<p/>
</div>
<div class="page"><p/>
<p>294 13 Interest Rate Models
</p>
<p>0 5 10 15 20
</p>
<p>4
.9
</p>
<p>4
4
</p>
<p>.9
6
</p>
<p>4
.9
</p>
<p>8
5
</p>
<p>.0
0
</p>
<p>5
.0
</p>
<p>2
5
</p>
<p>.0
4
</p>
<p>5
.0
</p>
<p>6
</p>
<p>X1(t)
X2(t)
</p>
<p>Fig. 13.5 CKLS with &#13; D 0:25 (X1) and &#13; D 0:75 (X2) for c1 D �0:9 (X.0/ D � D 5,
� D 0:01)
</p>
<p>The conditional standard deviation is modeled as a square root which is why one
</p>
<p>also speaks of (13.9) as a &ldquo;square root process&rdquo;. Consequently, the conditional
</p>
<p>variance of the increments is proportional to the level of the process.
</p>
<p>For this nonlinear SDE it can be formally shown, which is also intuitive: If X.t/
</p>
<p>(starting from a positive starting value X.0/ &gt; 0) takes on the value zero, then the
</p>
<p>variance is zero as well, but the change dX.t/ gets a positive impulse of the strength
</p>
<p>�c1 � such that the process is reflected on the zero line for � &gt; 0. Insofar the
square root process overcomes the deficiency of the OUP as an interest rate model.
</p>
<p>However, an analytical representation of the solution of (13.9) is not known.
</p>
<p>Already for the ordinary square root process from (13.9) with �.t; x/ D �px
the condition of existence (E1) from Proposition 12.1 is not fulfilled anymore as the
</p>
<p>derivative at zero does not exist. Fortunately, there are weaker conditions ensuring
</p>
<p>the existence of a solution of (13.9) &ndash; however, they do not guarantee the finiteness
</p>
<p>of the first two moments anymore. In order to show that finite moments exist up
</p>
<p>to the second order, we would need more fundamental arguments. Instead, we start
</p>
<p>with calculating the moments (finiteness assumed).
</p>
<p>For reasons of simplicity, assume a fixed starting value equal to � in the
</p>
<p>following: X.0/ D �. Then we obtain (see Problem 13.5), as for the OUP, on
average
</p>
<p>�1.t/ D E.X.t// D ec1t X.0/C �
�
1 � ec1t
</p>
<p>�
D � :</p>
<p/>
</div>
<div class="page"><p/>
<p>13.4 Nonlinear Models 295
</p>
<p>Under the assumption on the starting value X.0/ D �, for the second moment we
obtain (cf. Problem 13.6)
</p>
<p>�2.t/ D �2 �
�2 �
</p>
<p>2 c1
.1 � e2 c1 t/ ;
</p>
<p>from which it immediately follows for the variance
</p>
<p>Var.X.t// D �
2 �
</p>
<p>�2 c1
.1 � e2 c1 t/ ! �
</p>
<p>2 �
</p>
<p>�2 c1
; t ! 1 :
</p>
<p>The asymptotic variance for t ! 1 hence coincides with the one of the OUP if
� D 1; for � &lt; 1 it turns out to be smaller (as the process is reflected on the zero
line and therefore varies in a narrow band) while it is obviously greater for � &gt; 1.
</p>
<p>The border case � D 0 makes sense as well: Here, the asymptotic variance is zero
as, sooner or later, the process is absorbed by the zero line.
</p>
<p>For Fig. 13.6 an OUP with c1 D �0:9 and �2 D 0:01 was simulated but
the expected value of 5 % is now written as 0.05. In the example it becomes
</p>
<p>clear that the OUP can definitely become negative. In comparison, we observe a
</p>
<p>numerical solution of the corresponding square root process from (13.9) with the
</p>
<p>0 5 10 15 20
</p>
<p>0
.0
</p>
<p>0
0
</p>
<p>.0
5
</p>
<p>0
.1
</p>
<p>0
</p>
<p>OUP
CIR
</p>
<p>Fig. 13.6 OUP and CIR for c1 D �0:9 (X.0/ D � D 0:05, � D �2 D 0:01)</p>
<p/>
</div>
<div class="page"><p/>
<p>296 13 Interest Rate Models
</p>
<p>same volatility parameter and the same drift component. The picture confirms the
</p>
<p>theoretical considerations: The process exhibits a smaller variance and does not get
</p>
<p>negative.
</p>
<p>Further Models and Parameter Estimation
</p>
<p>Marsh and Rosenfeld (1983) mention the variant with � D 0 as a borderline case
of (13.8). Cox, Ingersoll, and Ross (1980) consider a version with &#13; &gt; 1 for a special
</p>
<p>investigation:
</p>
<p>dX.t/ D � X3=2.t/ dW.t/ :
</p>
<p>Finally, some models are applied which leave the framework of CKLS from (13.8)
</p>
<p>entirely, e.g. Constantinides and Ingersoll (1984) with
</p>
<p>dX.t/ D c X2.t/ dt C � X3=2.t/ dW.t/ ;
</p>
<p>where both drift and volatility are nonlinear.
</p>
<p>Given the copious possibilities for specifying a diffusion process, it is not
</p>
<p>surprising that it has been tried to, first, estimate unknown parameters and second
</p>
<p>to statistically discriminate between the different model classes. Beside the work by
</p>
<p>CKLS, the papers by Broze, Scaillet, and Zako&iuml;an (1995) and Tse (1995) should be
</p>
<p>mentioned. As a first introduction to the topic of estimation of diffusion parameters,
</p>
<p>the corresponding chapter by Gourieroux and Jasiak (2001) is recommended.
</p>
<p>13.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>13.1 Derive the solution (13.2) of Eq. (13.1).
</p>
<p>13.2 Derive the moments, (13.4) and (13.5), of the OUP (a fixed starting value X.0/
</p>
<p>assumed).
</p>
<p>13.3 Discuss Eq. (13.1) for c1 D 0 as a special case of the OUP (a proposal by
Merton, 1973). For this purpose, consider the solution, the expected value and the
</p>
<p>variance for c1 ! 0, if necessary with L&rsquo;Hospital&rsquo;s rule. (You should be familiar
with the results. By which name do you know the process as well?)
</p>
<p>13.4 Consider now, as a combination of the interest models by Vasicek (1977) and
</p>
<p>Dothan (1978), the process from (13.7) by Brennan and Schwartz (1980),
</p>
<p>dX.t/ D c1 .X.t/ � �/ dt C �1 X.t/ dW.t/ ; � D X.0/ &gt; 0 ;</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Problems and Solutions 297
</p>
<p>particularly with the starting value X.0/ D �. Determine expectation and variance.
How do these behave for t ! 1 if it holds that 2 c1 &lt; ��21 ?
</p>
<p>13.5 Consider the square root process (13.9) by Cox et al. (1985). Under the
</p>
<p>assumption X.0/ D � for the starting value, derive an expression for the expected
value.
</p>
<p>13.6 Again, consider the square root process (13.9) by Cox et al. (1985). Under the
</p>
<p>assumption X.0/ D � for the starting value, derive an expression for the variance.
</p>
<p>Solutions
</p>
<p>13.1 Equation (13.1) is a special case of (12.12) with constant coefficients. Hence,
</p>
<p>the solution results from (12.13) with
</p>
<p>z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>c1ds
</p>
<p>�
D ec1t:
</p>
<p>From this it follows
</p>
<p>tZ
</p>
<p>0
</p>
<p>c2
</p>
<p>z.s/
ds D c2
</p>
<p>tZ
</p>
<p>0
</p>
<p>e�c1s ds
</p>
<p>D �c2
c1
</p>
<p>e�c1s
ˇ̌
ˇ̌
t
</p>
<p>0
</p>
<p>D �c2
c1
.e�c1t � 1/:
</p>
<p>Thus, from (12.13) we obtain the desired result:
</p>
<p>X.t/ D ec1t
�
</p>
<p>X.0/� c2
c1
</p>
<p>�
e�c1t � 1
</p>
<p>�
C
Z t
</p>
<p>0
</p>
<p>�2 e
�c1s dW.s/
</p>
<p>�
:
</p>
<p>13.2 The expected value function is determined from (12.14):
</p>
<p>�1.t/ D ec1t
�
</p>
<p>X.0/C
Z t
</p>
<p>0
</p>
<p>c2 e
�c1s ds
</p>
<p>�
</p>
<p>D ec1t
�
</p>
<p>X.0/� c2
c1
.e�c1t � 1/
</p>
<p>�
:
</p>
<p>With � D � c2
c1
</p>
<p>the formula from (13.4) results.</p>
<p/>
</div>
<div class="page"><p/>
<p>298 13 Interest Rate Models
</p>
<p>The variance expression follows from (12.15):
</p>
<p>Var.X.t// D
�
ec1t
�2
Z t
</p>
<p>0
</p>
<p>� �2
ec1s
</p>
<p>�2
ds
</p>
<p>D e2c1t �22
Z t
</p>
<p>0
</p>
<p>e�2c1s ds
</p>
<p>D e2c1t �22
��1
2c1
</p>
<p>�
e�2c1s
</p>
<p>ˇ̌
ˇ̌
t
</p>
<p>0
</p>
<p>D �e2c1t �
2
2
</p>
<p>2c1
</p>
<p>�
e�2c1t � 1
</p>
<p>�
:
</p>
<p>This expression coincides with (13.5).
</p>
<p>13.3 L&rsquo;Hospital&rsquo;s rule provides for c1 ! 0:
</p>
<p>lim
c1!0
</p>
<p>e�c1t � 1
c1
</p>
<p>D lim
c1!0
</p>
<p>�t e�c1t
1
</p>
<p>D �t:
</p>
<p>Hence, X.t/ from (13.2) merges for c1 ! 0 into
</p>
<p>X.t/ D
�
</p>
<p>X.0/C c2 t C
Z t
</p>
<p>0
</p>
<p>�2 dW.s/
</p>
<p>�
</p>
<p>D X.0/C c2 t C �2 W.t/:
</p>
<p>Equation (13.4) is not suitable for determining the expected value as � D �c2
c1
</p>
<p>is not
</p>
<p>defined for c1 ! 0. Instead, one directly obtains (for X(0) fixed):
</p>
<p>�1.t/ D E.X.t// D X.0/C c2 t C 0:
</p>
<p>This linear growth is distinctive for the Brownian motion with drift, cf. Chap. 7.
</p>
<p>In order to determine the variance from (13.5), it is again argued with
</p>
<p>L&rsquo;Hospital&rsquo;s rule:
</p>
<p>lim
c1!0
</p>
<p>1 � e2c1t
2 c1
</p>
<p>D lim
c1!0
</p>
<p>�2t e2c1t
2
</p>
<p>D �t:
</p>
<p>Therefore it holds that
</p>
<p>lim
c1!0
</p>
<p>Var.X.t// D �22 t D Var.�2 W.t//:
</p>
<p>This is the familiar variance of a Brownian motion with drift. Indeed, a Brownian
</p>
<p>motion with drift is the same as the process resulting from the OUP for c1 D 0 or
c1 ! 0.</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Problems and Solutions 299
</p>
<p>13.4 This equation is linear and does not belong to the category &ldquo;additive noise&rdquo;. It
</p>
<p>is rather a special case of (12.3) with
</p>
<p>c1.t/ D c1; c2.t/ D ��c1; �1.t/ D �1; �2.t/ D 0:
</p>
<p>With
</p>
<p>z.t/ D ec1t
</p>
<p>due to Proposition 12.3 the expected value is given by
</p>
<p>E.X.t// D ec1t
�
</p>
<p>E.X.0//� �c1
Z t
</p>
<p>0
</p>
<p>e�c1sds
</p>
<p>�
</p>
<p>D ec1tŒE.X.0//C �e�c1t � �&#141;
D �C ec1tŒE.X.0//� �&#141;:
</p>
<p>Hence, for c1 &lt; 0 it holds:
</p>
<p>E.X.t//! �; t ! 1:
</p>
<p>For the second moment, we determine from (12.11):
</p>
<p>�2.t/ D expf.2c1 C �21 /tg
�
�2.0/ �
</p>
<p>Z t
</p>
<p>0
</p>
<p>2c1��1.s/
</p>
<p>expf.2c1 C �21 /sg
ds
</p>
<p>�
:
</p>
<p>In particular for X.0/ D �, this simplifies, due to E.X.t// D �, to (with �2.0/ D
�2):
</p>
<p>�2.t/ D expf.2c1 C �21 /tg
�
�2 � 2c1�2
</p>
<p>Z t
</p>
<p>0
</p>
<p>expf�.2c1 C �21 /sgds
�
</p>
<p>D expf.2c1 C �21 /tg
"
�2 C 2c1�2
</p>
<p>�
expf�.2c1 C �21 /sg
</p>
<p>2c1 C �21
</p>
<p>�t
</p>
<p>0
</p>
<p>#
</p>
<p>D expf.2c1 C �21 /tg
�
�2 C 2c1�
</p>
<p>2
</p>
<p>2c1 C �21
.expf�.2c1 C �21 /tg � 1/
</p>
<p>�
</p>
<p>D 2c1�
2
</p>
<p>2c1 C �21
C �
</p>
<p>2
1�
</p>
<p>2 expf.2c1 C �21 /tg
2c1 C �21
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>300 13 Interest Rate Models
</p>
<p>Thus, the variance for X.0/ D � reads:
</p>
<p>Var.X.t// D �2.t/ � �2
</p>
<p>D �
2
1�
</p>
<p>2 expf.2c1 C �21 /tg
2c1 C �21
</p>
<p>C 2c1�
2
</p>
<p>2c1 C �21
� .2c1 C �
</p>
<p>2
1 /�
</p>
<p>2
</p>
<p>2c1 C �21
</p>
<p>D �
2
1�
</p>
<p>2 expf.2c1 C �21 /tg
2c1 C �21
</p>
<p>� �
2
1
</p>
<p>2c1 C �21
�2
</p>
<p>D �
2
1 �
</p>
<p>2
</p>
<p>2c1 C �21
</p>
<p>�
expf.2c1 C �21 / tg � 1
</p>
<p>�
:
</p>
<p>If 2c1 &lt; ��21 , then it hence holds
</p>
<p>Var.X.t//! ��
2
1
</p>
<p>2c1 C �21
�2 &gt; 0 ;
</p>
<p>as t ! 1.
13.5 In order to determine the expected value function, we write Eq. (13.9) in
</p>
<p>integral form:
</p>
<p>X.t/ D X.0/C
Z t
</p>
<p>0
</p>
<p>c1.X.s/� �/ds C �
Z t
</p>
<p>0
</p>
<p>p
X.s/dW.s/:
</p>
<p>By assumption, �1.0/ D E.X.0// D E.�/ D �: Thus, Propositions 8.2 and 10.3(b)
yield:
</p>
<p>�1.t/ D �C
Z t
</p>
<p>0
</p>
<p>c1.�1.s/ � �/ds C 0
</p>
<p>or rather
</p>
<p>d�1.t/ D c1.�1.t/ � �/dt:
</p>
<p>Due to (12.5), the solution of this deterministic differential equation reads:
</p>
<p>�1.t/ D z.t/
�
�1.0/C
</p>
<p>Z t
</p>
<p>0
</p>
<p>.�c1�/
z.s/
</p>
<p>ds
</p>
<p>�
</p>
<p>D ec1t
�
� � �c1
</p>
<p>Z t
</p>
<p>0
</p>
<p>e�c1sds
</p>
<p>�
</p>
<p>D ec1t
�
�C �e�c1sjt0
</p>
<p>�
</p>
<p>D �ec1t
�
1C e�c1t � 1
</p>
<p>�
</p>
<p>D �:</p>
<p/>
</div>
<div class="page"><p/>
<p>13.5 Problems and Solutions 301
</p>
<p>13.6 In order to determine the function of the second moment analogously to the
</p>
<p>expected value in Problem 13.5, we search for an integral equation for X2.t/. This
</p>
<p>is provided by Ito&rsquo;s lemma for g.X/ D X2 with g0.X/ D 2X and g00.X/ D 2:
</p>
<p>dX2.t/ D 2X.t/dX.t/C 1
2
2�2.t/dt
</p>
<p>D 2X.t/dX.t/C �2X.t/dt;
</p>
<p>where �.t/ D �
p
</p>
<p>X.t/ from (13.9) was substituted. Plugging in the definition of
</p>
<p>dX.t/ further provides:
</p>
<p>dX2.t/ D
�
2c1X
</p>
<p>2.t/ � 2�c1X.t/C �2X.t/
�
</p>
<p>dt C 2�X.t/
p
</p>
<p>X.t/dW.t/;
</p>
<p>or rather
</p>
<p>X2.t/ D X2.0/C
Z t
</p>
<p>0
</p>
<p>�
2c1X
</p>
<p>2.s/C .�2 � 2�c1/X.s/
�
</p>
<p>ds C 2�
Z t
</p>
<p>0
</p>
<p>X.s/
p
</p>
<p>X.s/dW.s/:
</p>
<p>With X.0/ D � forming expectation yields from Propositions 8.2 and 10.3(b):
</p>
<p>�2.t/ D �2 C
Z t
</p>
<p>0
</p>
<p>�
2c1�2.s/C .�2 � 2�c1/�
</p>
<p>�
ds C 0;
</p>
<p>as �1.t/ D � is constant. Thus, for �2.t/ a deterministic differential equation
results:
</p>
<p>d�2.t/ D .2c1�2.t/C �2� � 2�2c1/dt:
</p>
<p>With
</p>
<p>z.t/ D exp
�Z t
</p>
<p>0
</p>
<p>2c1ds
</p>
<p>�
D e2c1t
</p>
<p>the solution reads
</p>
<p>�2.t/ D e2c1t
�
�2 C
</p>
<p>Z t
</p>
<p>0
</p>
<p>�2� � 2�2c1
e2c1s
</p>
<p>ds
</p>
<p>�
</p>
<p>D e2c1t
"
�2 � .�
</p>
<p>2� � 2�2c1/
2c1
</p>
<p>e�2c1s
ˇ̌
ˇ̌
t
</p>
<p>0
</p>
<p>#
</p>
<p>D e
2c1t
</p>
<p>2c1
</p>
<p>�
2c1�
</p>
<p>2 � .�2� � 2�2c1/.e�2c1t � 1/
�</p>
<p/>
</div>
<div class="page"><p/>
<p>302 13 Interest Rate Models
</p>
<p>D e
2c1t
</p>
<p>2c1
</p>
<p>�
�2�� .�2� � 2�2c1/e�2c1t
</p>
<p>�
</p>
<p>D �2 � �
2�
</p>
<p>2c1
C �
</p>
<p>2�
</p>
<p>2c1
e2c1t:
</p>
<p>Thus, in a last step the required variance is calculated as
</p>
<p>Var.X.t// D �2.t/ � �21.t/
</p>
<p>D �
2�
</p>
<p>2c1
.e2c1t � 1/:
</p>
<p>References
</p>
<p>Brennan, M. J, &amp; Schwartz, E. S. (1980). Analyzing convertible bonds. The Journal of Financial
and Quantitative Analysis, 15, 907&ndash;929.
</p>
<p>Broze, L., Scaillet, O., &amp; Zako&iuml;an, J.-M. (1995). Testing for continuous-time models of the short-
term interest rate. Journal of Empirical Finance, 2, 199&ndash;223.
</p>
<p>Chan, K. C., Karolyi, G. A., Longstaff, F. A., &amp; Sanders, A. B. (1992). An empirical comparision
of alternative models of the short-term interest rate. The Journal of Finance, XLVII, 1209&ndash;1227.
</p>
<p>Constantinides, G. M., &amp; Ingersoll, J. E., Jr. (1984). Optimal bond trading with personal taxes.
Journal of Financial Economics, 13, 299&ndash;335.
</p>
<p>Cox, J. C., Ingersoll, J. E., Jr., &amp; Ross S. A. (1980). An analysis of variable rate loan contracts. The
Journal of Finance, 35, 389&ndash;403.
</p>
<p>Cox, J. C., Ingersoll, J. E., Jr., &amp; Ross S. A. (1985). A theory of the term structure of interest rates.
Econometrica, 53, 385&ndash;407.
</p>
<p>Dothan, L. U. (1978). On the term structure of interest rates. Journal of Financial Economics, 6,
59&ndash;69.
</p>
<p>Gourieroux, Chr., &amp; Jasiak, J. (2001). Financial econometrics: Problems, models, and methods.
Princeton: Princeton University Press.
</p>
<p>Marsh, T. A., &amp; Rosenfeld, E. R. (1983). Stochastic processes for interest rates and equilibrium
bond prices. The Journal of Finance, XXXVIII, 635&ndash;646.
</p>
<p>Merton, R. C. (1973). Theory of rational option pricing. The Bell Journal of Economics and
Management Science, 4, 141&ndash;183.
</p>
<p>Tse, Y. K. (1995). Some international evidence on the stochastic behavior of interest rates. Journal
of International Money and Finance, 14, 721&ndash;738.
</p>
<p>Vasicek, O. (1977). An equilibrium characterization of the term structure. Journal of Financial
Economics, 5, 177&ndash;188.</p>
<p/>
</div>
<div class="page"><p/>
<p>14Asymptotics of Integrated Processes
</p>
<p>14.1 Summary
</p>
<p>This chapter aims at providing the basics in order to understand the asymptotic
</p>
<p>distributions of modern time series econometrics. In the first section, we treat the
</p>
<p>mathematical problems of a functional limit theory as solved and get to know the
</p>
<p>basic ingredients of a functional limit theory. Then, we proceed somewhat more
</p>
<p>abstractly by presenting the mathematical hurdles to be overcome in order to arrive
</p>
<p>at a functional limit theory. Finally, we consider multivariate generalizations.
</p>
<p>14.2 Limiting Distributions of Integrated Processes
</p>
<p>Under classical assumptions it holds that the arithmetic mean of a sample converges
</p>
<p>to the expected value of the sample variables for the growing sample size.1 However,
</p>
<p>if the sample is generated by a random walk, then this does not hold any longer.
</p>
<p>Hence, limits and limiting distributions for so-called integrated processes will now
</p>
<p>be discussed.
</p>
<p>Long-Run Variance
</p>
<p>In order to technically formulate the concept of an integrated process, we need
</p>
<p>the so-called long-run variance. However, this involves an old acquaintance from
</p>
<p>Chap. 4.
</p>
<p>1For a review on the convergence of random sequences, we recommend P&ouml;tscher and Prucha
(2001).
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_14
</p>
<p>303</p>
<p/>
</div>
<div class="page"><p/>
<p>304 14 Asymptotics of Integrated Processes
</p>
<p>Let fetg denote a stationary discrete-time process with zero expectation and the
auotcovariances
</p>
<p>&#13;e.h/ D Cov.et; etCh/ D E.et etCh/ ; E.et/ D 0 :
</p>
<p>As long-run variance we define
</p>
<p>!2e D &#13;e.0/C 2
1X
</p>
<p>hD1
&#13;e.h/ &lt;1 : (14.1)
</p>
<p>Here, we rule out the case of a fractionally integrated process with long memory,
</p>
<p>I.d/with d &gt; 0, as introduced in Chap. 5, since we require the autocovariances to be
</p>
<p>summable: !2e &lt; 1. In the case of a pure random process or white noise, et D "t,
variance and long-run variance naturally coincide:
</p>
<p>!2" D �2 D &#13;".0/ if &#13;".h/ D 0 ; h &curren; 0 :
</p>
<p>In general, it holds that the long-run variance is a multiple of the spectrum at the
</p>
<p>frequency zero, see (4.3):
</p>
<p>!2e D 2� fe.0/ :
</p>
<p>Now, we further assume an MA(1) process for fetg, see (3.2):
</p>
<p>et D
1X
</p>
<p>jD0
cj "t�j ; c0 D 1 ; t D 1; : : : ; n ; (14.2)
</p>
<p>with absolutely summable coefficients:
</p>
<p>1X
</p>
<p>jD0
jcjj &lt;1 : (14.3)
</p>
<p>In order to determine the long-run variance, we establish an alternative expression
</p>
<p>for MA(1) processes in Problem 14.1:
</p>
<p>!2e D �2
0
@
</p>
<p>1X
</p>
<p>jD0
cj
</p>
<p>1
A
2
</p>
<p>: (14.4)
</p>
<p>Due to 2� fe.0/ D !2e , one can directly read this relation from Eq. (4.5), too.
</p>
<p>Example 14.1 (Long-run Variance of MA(1)) We consider a moving average pro-
</p>
<p>cess of order 1,
</p>
<p>et D "t C b "t�1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Limiting Distributions of Integrated Processes 305
</p>
<p>From Example 4.3 we adopt the spectrum,
</p>
<p>f .�/ D
�
1C b2 C 2b cos .�/
</p>
<p>�
�2=2� :
</p>
<p>Thus, at the origin the long-run variance results,
</p>
<p>!2 D 2� f .0/ D .1C b/2 �2:
</p>
<p>At the minimum, this expression takes on the value zero which happens for b D �1:
</p>
<p>et D "t � "t�1 D �"t :
</p>
<p>In this case fetg is &ldquo;overdifferenced&rdquo;. What is meant by this, will be explained in the
following. �
</p>
<p>Integrated Processes
</p>
<p>We revisit Example 14.1 and consider the differences of a stationary MA(1)
process fetg:
</p>
<p>�et D et � et�1 D
1X
</p>
<p>jD0
cj "t�j �
</p>
<p>1X
</p>
<p>jD0
cj "t�j�1
</p>
<p>D c0 "t C
1X
</p>
<p>jD1
</p>
<p>�
cj � cj�1
</p>
<p>�
"t�j :
</p>
<p>Therefore, f�etg is also a stationary process where the coefficients are now called
fdjg:
</p>
<p>�et D
1X
</p>
<p>jD0
dj "t�j ; d0 D c0 D 1 ; dj D cj � cj�1 :
</p>
<p>By definition, it hence holds
</p>
<p>1X
</p>
<p>jD0
dj D c0 C .c1 � c0/C .c2 � c1/C � � � D 0 :
</p>
<p>Thus, we obtain for the long-run variance:
</p>
<p>!2�e D �2
0
@
</p>
<p>1X
</p>
<p>jD0
dj
</p>
<p>1
A
2
</p>
<p>D 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>306 14 Asymptotics of Integrated Processes
</p>
<p>The process f�etg is overdifferenced: It is differenced more often than necessary
for stationarity as fetg itself is already stationary.2 Overdifferencing is reflected in
the long-run variance being zero.
</p>
<p>If the stationary, absolutely summable process fetg has a positive long-run
variance, then we call it integrated of order zero (in symbols: et � I.0/):
</p>
<p>et � I.0/ &rdquo; 0 &lt; !2e &lt;1 :
</p>
<p>Verbally, this means: We have to difference zero times in order to attain stationarity,
</p>
<p>and it is not differenced once more than needed. Technically, in terms of Chap. 5,
</p>
<p>this means that the process is fractionally integrated of order d D 0.
Finally, the process fxtg with
</p>
<p>xt D
tX
</p>
<p>jD1
ej ; et � I.0/ ; t D 1; : : : ; n ;
</p>
<p>is called integrated of order one, I(1), as it is defined as sum (&ldquo;integral&rdquo;) of an I(0)
</p>
<p>process. The random walk from (1.8) e.g. is integrated of order one and obviously
</p>
<p>nonstationary. It holds for I(1) random walks that differencing once,
</p>
<p>�xt D et ;
</p>
<p>is required by definition to obtain stationarity. Hence, I(1) processes are sometimes
</p>
<p>called difference-stationary. Also, I(1) processes are often labelled as unit root
</p>
<p>processes, or are said to have an autoregressive unit root. We briefly want to
</p>
<p>elaborate on this terminology. Assume that fetg is a stationary autoregressive process
of order p,
</p>
<p>Ae.L/et D "t with Ae.L/ D 1 � a1L � � � � � apLp :
</p>
<p>Consequently, the I(1) process is autoregressive of order p C 1 since
</p>
<p>�xt D
"t
</p>
<p>Ae.L/
or Ax.L/xt D "t ;
</p>
<p>where
</p>
<p>Ax.L/ D Ae.L/ .1 � L/
D 1 � .a1 � 1/L � .a2 � a1/L2 � � � � � .ap � ap�1/Lp C apLpC1 :
</p>
<p>2In econometrics, this overdifferencing is also described by the fact that f�etg is integrated of order
�1, �et � I(�1): fetg is differenced one more time although the process is already stationary.</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Limiting Distributions of Integrated Processes 307
</p>
<p>Hence, Ax.z/ has a unit root, meaning that Ax.z/ D 0 has a solution on the unit
circle, namely the real root z D 1: Ax.1/ D 0.
</p>
<p>The Functional Central Limit Theorem (FCLT)
</p>
<p>Now, we make statements on the distribution of the stochastic step function (the
</p>
<p>partial sum process)
</p>
<p>Xn.s/ D
n�0:5
</p>
<p>!e
</p>
<p>bs ncX
</p>
<p>jD1
ej ; s 2 Œ0; 1&#141; : (14.5)
</p>
<p>Here, byc denotes the integer part of a real number y. In order to be able to divide by
!e, the process fetg has to be I(0). In (7.1), we have already considered a precursor
of this step function as it holds:
</p>
<p>Xn.s/ D
(
</p>
<p>1
</p>
<p>!e
p
</p>
<p>n
</p>
<p>Pi�1
jD1 ej ; s 2
</p>
<p>�
i�1
</p>
<p>n
; i
</p>
<p>n
</p>
<p>�
; i D 1; 2; : : : ; n
</p>
<p>1
</p>
<p>!e
p
</p>
<p>n
</p>
<p>Pn
jD1 ej ; s D 1:
</p>
<p>For a graphical illustration, recall Fig. 7.1. The following proposition for MA(1)
processes from (14.2) holds under some additional assumptions.3
</p>
<p>Proposition 14.1 (FCLT) Let fetg from (14.2) be integrated of order zero and
satisfy some additional assumptions. Then it holds for Xn.s/ from (14.5) that,
</p>
<p>Xn.s/ D
n�0:5
</p>
<p>!e
</p>
<p>bs ncX
</p>
<p>jD1
ej ) W.s/ ; s 2 Œ0; 1&#141; ; n ! 1 ;
</p>
<p>where !2e &gt; 0 is from (14.1).
</p>
<p>Note that the FCLT, so to speak, consists of infinitely many central limit theorems.
</p>
<p>For a fixed s it namely holds that
</p>
<p>Xn.s/ D
n�0:5
</p>
<p>!e
</p>
<p>bs ncX
</p>
<p>jD1
ej
</p>
<p>d! W.s/ � N .0; s/ :
</p>
<p>3Phillips and Solo (1992) assume that the innovations f"tg form an iid sequence and thatP1
jD0 jjcjj &lt; 1, which is more restrictive than (14.3). Phillips (1987) or Phillips and Perron
</p>
<p>(1988) do without the iid assumption, but require more technical restrictions. For a discussion of
further sets of assumptions ensuring Proposition 14.1 see also Davidson (1994).</p>
<p/>
</div>
<div class="page"><p/>
<p>308 14 Asymptotics of Integrated Processes
</p>
<p>However, as this holds for each s 2 Œ0; 1&#141;, we have quasi uncountably many central
limit theorems collected in Proposition 14.1. The mathematically precise collection
</p>
<p>is the so-called weak convergence in function spaces which is symbolized by &ldquo;)&rdquo;.
For the following, an intuitive notion thereof suffices, somewhat more rigorous
</p>
<p>remarks will be given in the next section.
</p>
<p>As the I(1) process fxtg was defined as the sum of the past of fetg, the
circumstance from the proposition can also be expressed as follows: It holds for
</p>
<p>xt D
Pt
</p>
<p>jD1 ej that
</p>
<p>n�0:5
</p>
<p>!e
xbs nc ) W.s/ ; s 2 Œ0; 1&#141; :
</p>
<p>The first FCLT was proved by Donsker for pure random processes (Donsker, 1951).
</p>
<p>Particularly for an iid sequence et D "t, one hence speaks of Donsker&rsquo;s theorem.
Frequently, a FCTL also operates under the name &ldquo;invariance principle&rdquo; as it is
</p>
<p>invariant with respect to the distribution of fetg.
</p>
<p>First Implications
</p>
<p>The following proposition assembles some implications being of immediate rele-
</p>
<p>vance in application. As an exercise, we encourage the reader to come up with the
</p>
<p>proof in order to understand why which powers of the sample size n appear in the
</p>
<p>normalization of the sums; see Problems 14.3 through 14.5. Note that the weak
</p>
<p>convergence, &ldquo;)&rdquo;, will be discussed in the next section, while &ldquo; d!&rdquo; stands for the
usual convergence in distribution.
</p>
<p>Proposition 14.2 (Some Limiting Distributions) Let xt D xt�1 C et with x0 D 0,
t D 1; : : : ; n; i.e.
</p>
<p>xt D
tX
</p>
<p>jD1
ej ;
</p>
<p>where fetg is I(0) as in Proposition 14.1. Then it holds for n ! 1 W
</p>
<p>.a/ n�
3
2
</p>
<p>nP
tD1
</p>
<p>xt�1
d! !e
</p>
<p>1R
0
</p>
<p>W.s/ ds ;
</p>
<p>.b/ n�
3
2
</p>
<p>nP
tD1
</p>
<p>tet
d! !e
</p>
<p>1R
0
</p>
<p>s dW.s/ ;
</p>
<p>.c/ n�
5
2
</p>
<p>nP
tD1
</p>
<p>t xt�1
d! !e
</p>
<p>1R
0
</p>
<p>s W.s/ ds ;
</p>
<p>.d/ n�0:5
Œs n&#141;P
tD1
.et � e/ ) !e .W.s/ � s W.1// ; e D 1n
</p>
<p>nP
tD1
</p>
<p>et ;</p>
<p/>
</div>
<div class="page"><p/>
<p>14.2 Limiting Distributions of Integrated Processes 309
</p>
<p>.e/ n�2
nP
</p>
<p>tD1
x2t�1
</p>
<p>d! !2e
1R
0
</p>
<p>W2.s/ ds ;
</p>
<p>. f / n�1
nP
</p>
<p>tD1
xt�1 et
</p>
<p>d! !
2
e
</p>
<p>2
</p>
<p>�
W2.1/� &#13;e.0/
</p>
<p>!2e
</p>
<p>�
</p>
<p>D !2e
</p>
<p>(
1R
0
</p>
<p>W.s/ dW.s/C !
2
e�&#13;e.0/
2!2e
</p>
<p>)
;
</p>
<p>with &#13;e.0/ D Var.et/ and !2e from (14.1).
</p>
<p>Two remarks on the functional form of the statements shall be given.
</p>
<p>Remark 1 Note the elegant and evocative functional analogy of the sums on the
</p>
<p>left-hand side, respectively, and the integrals on the right-hand side in (a)&ndash;(c) and
</p>
<p>(e): Here, the sums are substituted by integrals, the Wiener process corresponds
</p>
<p>to the I(1) process fxtg, and the increments of the WP dW correspond to the I(0)
increments�xt D et.
</p>
<p>Remark 2 If et D "t is white noise, then Ito&rsquo;s lemma in form of (10.3) also yields
an accordance of the functional form of sample variables and limiting distributions
</p>
<p>in (f):
</p>
<p>n�1
nX
</p>
<p>tD1
xt�1 "t
</p>
<p>d! !
2
"
</p>
<p>2
</p>
<p>�
W2.1/ � 1
</p>
<p>�
D !2"
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ dW.s/ :
</p>
<p>As well, the limiting process appearing in (d) is intuitively well justified. It is a
</p>
<p>Brownian bridge with W.1/� 1W.1/ D 0 which just reflects
</p>
<p>nX
</p>
<p>tD1
.et � e/ D 0
</p>
<p>for s D 1.
</p>
<p>Example 14.2 (Demeaned WP) From Proposition 14.2 (a) results due to x0 D 0:
</p>
<p>n�0:5 x D n� 32
nX
</p>
<p>tD1
xt D n�
</p>
<p>3
2
</p>
<p> 
nX
</p>
<p>tD1
xt�1 C xn
</p>
<p>!
D n� 32
</p>
<p>0
@
</p>
<p>nX
</p>
<p>tD1
xt�1 C
</p>
<p>nX
</p>
<p>jD1
ej
</p>
<p>1
A
</p>
<p>d! !e
Z 1
</p>
<p>0
</p>
<p>W.s/ ds C 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>310 14 Asymptotics of Integrated Processes
</p>
<p>Hence, it holds for fxtg after demeaning the following FCTL:
</p>
<p>xbs nc � x
!e
</p>
<p>p
n
</p>
<p>) W.s/ �
Z 1
</p>
<p>0
</p>
<p>W.r/ dr ;
</p>
<p>where
</p>
<p>W.s/ WD W.s/ �
Z 1
</p>
<p>0
</p>
<p>W.r/ dr
</p>
<p>is also called a demeaned Wiener process. �
</p>
<p>In the example it was argued that it is negligible for the asymptotics whether we
</p>
<p>sum over xt�1 or xt. This holds in Proposition 14.2 (a), (c), (e) but not in (f), where
on the right-hand side the sign in front of &#13;e.0/ changes. The following corollary
</p>
<p>summarizes the corresponding results, cf. as well Problem 14.2.
</p>
<p>Corollary 14.1 (Some Limiting Distributions) Let xt D xt�1 C et with x0 D 0,
t D 1; : : : ; n; i.e.
</p>
<p>xt D
tX
</p>
<p>jD1
ej ;
</p>
<p>where fetg is I(0) as in Proposition 14.1. Then it holds for n ! 1 W
</p>
<p>n�
3
2
</p>
<p>nP
tD1
</p>
<p>xt
d! !e
</p>
<p>1R
0
</p>
<p>W.s/ ds ;
</p>
<p>n�
5
2
</p>
<p>nP
tD1
</p>
<p>t xt
d! !e
</p>
<p>1R
0
</p>
<p>s W.s/ ds ;
</p>
<p>n�2
nP
</p>
<p>tD1
x2t
</p>
<p>d! !2e
1R
0
</p>
<p>W2.s/ ds ;
</p>
<p>n�1
nP
</p>
<p>tD1
xt et
</p>
<p>d! !
2
e
</p>
<p>2
</p>
<p>�
W2.1/C &#13;e.0/
</p>
<p>!2e
</p>
<p>�
</p>
<p>D !2e
</p>
<p>(
1R
0
</p>
<p>W.s/ dW.s/C !
2
e C&#13;e.0/
2 !2e
</p>
<p>)
;
</p>
<p>with &#13;e.0/ D Var.et/ and !2e from (14.1).
</p>
<p>14.3 Weak Convergence of Functions
</p>
<p>In this subsection we want to briefly occupy ourselves with the mathematical
</p>
<p>concepts hiding behind Proposition 14.1. More rigorous expositions addressing an
</p>
<p>econometric audience can be found in Davidson (1994) or White (2001), see also
</p>
<p>the classical mathematical reference by Billingsley (1968).</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Weak Convergence of Functions 311
</p>
<p>Metric Function Spaces
</p>
<p>Recall the stochastic step function that has led us to the WP, see (7.1),
</p>
<p>Xn.t/ D
(
</p>
<p>1
�
p
</p>
<p>n
</p>
<p>Pi�1
jD1 "j ; t 2
</p>
<p>�
i�1
</p>
<p>n
; i
</p>
<p>n
</p>
<p>�
; i D 1; 2; : : : ; n
</p>
<p>1
</p>
<p>�
p
</p>
<p>n
</p>
<p>Pn
jD1 "j ; t D 1 ;
</p>
<p>which can also be written more compactly as
</p>
<p>Xn.t/ D
n�0:5
</p>
<p>�
</p>
<p>bt ncX
</p>
<p>jD1
"j ; t 2 Œ0; 1&#141; :
</p>
<p>Furthermore, we define QXn.t/ as the function that coincides with Xn.t/ at the lower
endpoint of the interval. However, it is not constant on the intervals, but varies
</p>
<p>linearly:
</p>
<p>QXn.t/ D
n�0:5
</p>
<p>�
</p>
<p>bntcX
</p>
<p>jD1
"j C .nt � bntc/
</p>
<p>"bntcC1
�
p
</p>
<p>n
; t 2 Œ0; 1&#141; :
</p>
<p>By construction, QXn.t/ is a continuous function on Œ0; 1&#141;, for which we also
abbreviate
</p>
<p>QXn 2 C Œ0; 1&#141; :
</p>
<p>In contrast, Xn.t/ is only right-continuous and exhibits (removable) discontinuities
</p>
<p>of the first type (i.e. jump discontinuities). It belongs to the set of so-called cadlag4
</p>
<p>functions that is denoted by D Œ0; 1&#141; due to the discontinuities:
</p>
<p>Xn 2 D Œ0; 1&#141; :
</p>
<p>Obviously, the set of continuous functions is a subset of the cadlag functions, i.e.
</p>
<p>C Œ0; 1&#141; � D Œ0; 1&#141;. Now, we want Xn.t/ as well as QXn.t/ to converge to a WP W.t/.
For this purpose we need a distance measure in function spaces, a metric d. A
</p>
<p>precise mathematical definition follows.
</p>
<p>Metric space: Let M be an arbitrary set and d a metric, i.e. a mapping,
</p>
<p>d W M � M ! RC0 ;
</p>
<p>4This French acronym (sometimes also &ldquo;c&agrave;dl&agrave;g&rdquo;) stands for &ldquo;continue &agrave; droite, (avec une) limite &agrave;
gauche&rdquo;: right-continuous and bounded on the left.</p>
<p/>
</div>
<div class="page"><p/>
<p>312 14 Asymptotics of Integrated Processes
</p>
<p>which assigns to x and y from M a non-negative number such that the following
</p>
<p>three conditions are satisfied:
</p>
<p>d.x; y/ D 0 &rdquo; x D y ;
d.x; y/ D d.y; x/ (symmetry) ;
</p>
<p>d.x; y/ � d.x; z/C d.z; y/ (triangle inequality) :
</p>
<p>Then, M endowed with d is called a metric space, .M; d/.
</p>
<p>Example 14.3 (Supremum Metric) Particularly C Œ0; 1&#141; or D Œ0; 1&#141; are readily
</p>
<p>endowed with the supremum metric:
</p>
<p>ds.f ; g/ WD sup
0�t�1
</p>
<p>j f .t/ � g.t/j ; f ; g 2 D Œ0; 1&#141; :
</p>
<p>In Problem 14.6 it is shown that the above-mentioned three defining properties are
</p>
<p>indeed fulfilled. �
</p>
<p>However, as Xn.t/ and W.t/ are stochastic functions, a convergence of fXng to
W cannot simply be based on ds.Xn;W/. The convergence of fXng to W has to be
formulated rather as a statement on probabilities or expected values. In order to
</p>
<p>specify this, we need the concept of continuous functionals.
</p>
<p>Continuous Functionals
</p>
<p>Let the mapping h assign a real number to the function f 2 D Œ0; 1&#141;,
</p>
<p>h W D Œ0; 1&#141; ! R :
</p>
<p>As the argument of h is a function, one often speaks of a functional.
</p>
<p>Now, let the set of cadlag functions be equipped with a metric d, i.e. let
</p>
<p>.D Œ0; 1&#141; ; d/ be a metric space. Then the functional h with h: D Œ0; 1&#141;! R is called
continuous with respect to d if it holds for all f ; g 2 D Œ0; 1&#141; that
</p>
<p>jh.f / � h.g/j ! 0
</p>
<p>for
</p>
<p>d.f ; g/ ! 0 :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Weak Convergence of Functions 313
</p>
<p>An alternative definition of continuity reads: h is called continuous with respect to
</p>
<p>d if there exists a ı &gt; 0 with
</p>
<p>jh.f / � h.g/j &lt; " for d.f ; g/ &lt; ı :
</p>
<p>for each " &gt; 0.
</p>
<p>Strictly speaking, continuity is a &ldquo;pointwise&rdquo; property; however, if a functional
</p>
<p>is continuous for every considered function, then one generally speaks of continuity
</p>
<p>of the functional. The integral over a function is a typical example for a continuous
</p>
<p>functional.
</p>
<p>Example 14.4 (Three Functionals) Frequently, we encounter the following func-
</p>
<p>tionals in econometrics:
</p>
<p>h1.f / D
Z 1
</p>
<p>0
</p>
<p>f .t/ dt ;
</p>
<p>h2.f / D
Z 1
</p>
<p>0
</p>
<p>f 2.t/ dt ;
</p>
<p>h3.f / D
1
</p>
<p>R 1
0
</p>
<p>f 2.t/ dt
:
</p>
<p>It can be shown that they are continuous on D Œ0; 1&#141; with respect to the supremum
</p>
<p>metric (cf. Problem 14.7). �
</p>
<p>Weak Convergence
</p>
<p>We consider a set of stochastic elements, let them be random variables or stochastic
</p>
<p>functions. Let M be a set of stochastic elements and d a metric. We define somewhat
</p>
<p>loosely, see Billingsley (1968, Thm. 2.1): A sequence Sn 2 M, n 2 N, converges
weakly to S 2 M for n ! 1 if
</p>
<p>lim
n!1
</p>
<p>E.h.Sn// D E.h.S//
</p>
<p>for all real-valued mappings h that are bounded and uniformly continuous with
</p>
<p>respect to d. Symbolically, we write
</p>
<p>Sn ) S :
</p>
<p>This definition in terms of expected values is not very illustrative as it is hard to
</p>
<p>imagine all mappings which are bounded and continuous. In order to translate weak
</p>
<p>convergence into a probability statement, we consider the indicator function Ia for</p>
<p/>
</div>
<div class="page"><p/>
<p>314 14 Asymptotics of Integrated Processes
</p>
<p>an arbitrary real a and x 2 R:
</p>
<p>Ia.x/ WD I.�1;a&#141;.x/ D
�
1 ; x � a
0 ; x &gt; a
</p>
<p>:
</p>
<p>By linearization on Œa; a C "&#141; for an arbitrarily small " &gt; 0, the indicator function
can be continuously approximated by
</p>
<p>QIa.x/ WD
</p>
<p>8
&lt;
:
</p>
<p>1 ; x � a
1 � x�a
</p>
<p>"
; a � x � a C "
</p>
<p>0 ; x � a C "
:
</p>
<p>The approximation can become arbitrarily close to Ia for small ". Let us now choose
</p>
<p>M D D Œ0; 1&#141; : Then it holds for the stochastic cadlag processes Xn.t/ and X.t/ that
</p>
<p>P.Xn.t/ � a/ D E ŒIa.Xn.t//&#141; � E
� QIa.Xn.t//
</p>
<p>�
;
</p>
<p>P.X.t/ � a/ D E ŒIa.X.t//&#141; � E
� QIa.X.t//
</p>
<p>�
:
</p>
<p>Hence, it holds for the continuous bounded functional h D QIa for an arbitrary a 2 R
in case of weak convergence of fXn.t/g to X.t/, i.e. for E
</p>
<p>� QIa.Xn.t//
�
! E
</p>
<p>�QIa.X.t//
�
,
</p>
<p>that
</p>
<p>P.Xn.t/ � a/ � P.X.t/ � a/ :
</p>
<p>Hence, we have the following illustration of fXn.t/g converging weakly to X.t/: For
every point in time t it holds that the sequence of distribution functions, P.Xn.t/ �
a/, tends to the distribution function of X.t/.
</p>
<p>If M particularly denotes the set of real random variables and if Xn ) X holds,
then the same argument shows for the distribution function that:
</p>
<p>Fn.a/ WD P.Xn � a/ � P.X � a/ D F.a/ :
</p>
<p>With the definition from the end of Chap. 8, weak convergence of random variables
</p>
<p>hence implies their convergence in distribution, Xn
d! X. The converse holds as
</p>
<p>well: For random variables fXng and X, weak convergence is synonymous with
convergence in distribution.
</p>
<p>ContinuousMapping Theorem
</p>
<p>A further ingredient of the proof of statements as in Proposition 14.2 is presented
</p>
<p>by the continuous mapping theorem (actually: about mappings which are discon-
</p>
<p>tinuous only on &ldquo;infinitesimal sets&rdquo;); see Billingsley (1968, Thm. 5.1), Davidson</p>
<p/>
</div>
<div class="page"><p/>
<p>14.3 Weak Convergence of Functions 315
</p>
<p>(1994), and White (2001). We consider two versions of the proposition which are
</p>
<p>both special cases of a more general formulation.
</p>
<p>Proposition 14.3 (Continuous Mapping Theorem (CMT)) For continuous map-
</p>
<p>pings of convergent series it holds:
</p>
<p>(a) Let fXng be a sequence of real random variables and h; h: R ! R; a continuous
function. From Xn
</p>
<p>d! X for n ! 1 it follows
</p>
<p>h.Xn/
d! h.X/ :
</p>
<p>(b) Let fXn.s/g and X.s/ belong to D Œ0; 1&#141; and be h; h: D Œ0; 1&#141;! R; a continuous
functional. From Xn.s/) X.s/ for n ! 1 it follows
</p>
<p>h.Xn.s//
d! h.X.s// :
</p>
<p>Verbally, the continuous mapping theorem means that mapping and limits can be
</p>
<p>interchanged without altering the result: It does not matter whether h is applied first
</p>
<p>and then n is let to infinity, or whether n ! 1 first is followed by the mapping. At
first sight, this may seem trivial which it is definitely not, see Example 14.5.
</p>
<p>Remember that for X D c D const convergence in distribution is equivalent
to convergence in probability, see Sect. 8.4. Hence, the CMT holds as well for
</p>
<p>convergence in probability to a constant: From
</p>
<p>Xn
p! c
</p>
<p>for n ! 1 it follows that h.Xn/ tends in probability to the corresponding constant:
</p>
<p>h.Xn/
p! h.c/ :
</p>
<p>In the literature, this fact is also known as Slutsky&rsquo;s theorem. For this, we consider
</p>
<p>an example.
</p>
<p>Example 14.5 (Consistency of Moment Estimators) Let fytg with yt D �C "t be a
white noise process with expected value �. For the arithmetic mean of a sample of
</p>
<p>the size n we know from Example 8.4 (law of large numbers):
</p>
<p>yn D
1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
yt
</p>
<p>p! � ;
</p>
<p>i.e. the empirical mean is a consistent estimator for the theoretical mean. Frequently,
</p>
<p>however, one is interested in a parameter which is a function of �:
</p>
<p>� D h.�/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>316 14 Asymptotics of Integrated Processes
</p>
<p>An estimator for � constructed according to the method of moments is simply based
</p>
<p>on the substitution of the unknown expected value by its consistent estimator:
</p>
<p>O�n D h.yn/ :
</p>
<p>Slutsky&rsquo;s theorem as a special case of (a) from Proposition 14.3 then guarantees the
</p>
<p>consistency of the moment estimator, provided h is continuous:
</p>
<p>O�n
p! h.�/ D � :
</p>
<p>Such an interchangeability of some operation and a mapping is by no means trivial.
</p>
<p>It does e.g. not hold for the expectation in general: For non-linear functions h one
</p>
<p>has:
</p>
<p>E
�
O�n
�
D E .h.yn// &curren; h .E.yn// D h .�/ :
</p>
<p>If e.g. fytg is exponentially distributed with the parameter �, i.e.
</p>
<p>P.yt � y/ D 1 � e�� y ; � &gt; 0 ; y � 0 ;
</p>
<p>then it holds that
</p>
<p>� D 1
�
; and � D h.�/ D 1
</p>
<p>�
:
</p>
<p>The function h is continuous in � &gt; 0, which is why the moment estimator for � is
</p>
<p>consistent:
</p>
<p>O�n D
1
</p>
<p>yn
</p>
<p>p! 1
�
</p>
<p>D � :
</p>
<p>However, one can show that it holds for an iid sample that
</p>
<p>E. O�n/ D
n
</p>
<p>n � 1 � &curren;
1
</p>
<p>E.yn/
D � ;
</p>
<p>which is why the estimator is not unbiased for � in finite samples. �
</p>
<p>In order to justify the limit theory from the first section (i.e. in order to prove
</p>
<p>something like Proposition 14.1), mathematicians have followed two paths. First,
</p>
<p>the treatment of QXn.t/ 2 CŒ0; 1&#141; with the ordinary supremum metric. For the proof
of econometric propositions as e.g. Proposition 14.2 this has the disadvantage that
</p>
<p>the impractical &ldquo;continuity appendage&rdquo;,
</p>
<p>QXn.t/ � Xn.t/ D .nt � bntc/
"bntcC1
�
p
</p>
<p>n
;</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Multivariate Limit Theory 317
</p>
<p>needs to be dragged along, cf. e.g. Tanaka (1996). Second, the treatment of the more
</p>
<p>compact cadlag functions Xn.t/ which, however, requires a more complicated metric
</p>
<p>(Skorohod metric) and additional considerations. These mathematical difficulties
</p>
<p>are indeed solved and do not have to bother us, see e.g. Billingsley (1968) or
</p>
<p>Davidson (1994). Hence, we always work with fXn.t/g in this book.
</p>
<p>14.4 Multivariate Limit Theory
</p>
<p>The multivariate limit theory is based, among others, on a vector variant of
</p>
<p>Proposition 14.1 and yields generalizations of Proposition 14.2 or Corollary 14.1.
</p>
<p>For the sake of simplicity, we narrow the exposition down to the bivariate case. The
</p>
<p>following elements of a functional limit theory are kept sufficiently general to cover
</p>
<p>the case of cointegration as well as the case of no cointegration in the following two
</p>
<p>chapters. Here, we deviate from the convention of Sect. 11.4 and do not use bold
</p>
<p>letters to denote vectors or matrices.
</p>
<p>Integrated Vectors
</p>
<p>The transposition of the vector zt is denoted by z
0
t. Let z
</p>
<p>0
t D .z1;t; z2;t/ be a bivariate
</p>
<p>I(1) vector with starting value zero (we assume this for convenience), this means
</p>
<p>both components are I(1). Then it holds for the differences by definition,
</p>
<p>�zt DW wt D
�
</p>
<p>w1;t
</p>
<p>w2;t
</p>
<p>�
;
</p>
<p>that they are stationary with expectation zero, more precisely: Integrated of order
</p>
<p>zero. In generalization of the univariate autocovariance function we define
</p>
<p>&#13;w.h/ D E
�
wtw
</p>
<p>0
tCh
�
D
�
</p>
<p>E.w1;tw1;tCh/ E.w1;tw2;tCh/
E .w2;tw1;tCh/ E.w2;tw2;tCh/
</p>
<p>�
:
</p>
<p>Note that these matrices are not symmetric in h. Rather it holds that
</p>
<p>&#13;w.�h/ D &#13; 0w.h/ :
</p>
<p>The long-run variance matrix is defined as a generalization of (14.1),
</p>
<p>˝w D
1X
</p>
<p>hD�1
&#13;w.h/ D
</p>
<p>�
!21 !12
!12 !
</p>
<p>2
2
</p>
<p>�
; !2i &gt; 0 ; i D 1; 2 : (14.6)
</p>
<p>This matrix is symmetric (˝ D ˝ 0) and positive semi-definite (˝ � 0) by
construction; sometimes we omit the subscript and write˝ instead of˝w. Note that</p>
<p/>
</div>
<div class="page"><p/>
<p>318 14 Asymptotics of Integrated Processes
</p>
<p>˝ cannot be equal to the zero matrix as fwtg is not &ldquo;overdifferenced&rdquo; but I(0). Nev-
ertheless, the matrix does not have to be invertible. In the following chapter we will
</p>
<p>learn that the presence of so-called cointegration depends on the rank of the matrix.
</p>
<p>Now, let W.t/ denote a vector of the length 2, namely the bivariate standard
</p>
<p>Wiener process. Its components are stochastically independent such that this vector
</p>
<p>is bivariate Gaussian with the identity matrix I2:
</p>
<p>W.t/ D
�
</p>
<p>W1.t/
</p>
<p>W2.t/
</p>
<p>�
� N2.0; t I2/ :
</p>
<p>The corresponding Brownian motion is defined as a vector as follows:
</p>
<p>B.t/ D
�
</p>
<p>B1.t/
</p>
<p>B2.t/
</p>
<p>�
D ˝0:5W.t/;
</p>
<p>with
</p>
<p>B.t/ � N2.0; t˝/;
</p>
<p>˝ D ˝0:5.˝0:5/0 :
</p>
<p>For the existence and construction of a matrix˝0:5 with the given properties, which
</p>
<p>is to some extent a &ldquo;square root of a matrix&rdquo;, we refer to the literature, e.g. Dhrymes
</p>
<p>(2000, Def. 2.35). However, concrete numerical examples are provided here.
</p>
<p>Example 14.6 (˝ D ˝0:5.˝0:5/0) First consider a matrix of rank 1,
</p>
<p>˝1 D
�
1 1
</p>
<p>1 1
</p>
<p>�
:
</p>
<p>Now, let us define
</p>
<p>P1 D
1p
2
</p>
<p>�
1 1
</p>
<p>1 1
</p>
<p>�
D P01 with P1P1 D ˝1 :
</p>
<p>Multiplied by itself, P1 just yields the starting matrix ˝1. In the second example of
</p>
<p>a diagonal matrix with full rank,
</p>
<p>˝2 D
�
�21 0
</p>
<p>0 �22
</p>
<p>�
;
</p>
<p>the construction of the square root becomes even more obvious:
</p>
<p>P2 D
�
�1 0
</p>
<p>0 �2
</p>
<p>�
D P02 with P2P2 D ˝2 :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.4 Multivariate Limit Theory 319
</p>
<p>As˝2 is a diagonal matrix, P2 D ˝0:52 is just generated by taking the square root of
the diagonal. Let us consider a third example with full rank:
</p>
<p>˝3 D
�
2 1
</p>
<p>1 2
</p>
<p>�
:
</p>
<p>Here it is not obvious which form˝0:53 may have. However, one can check that
</p>
<p>P3 D
1
</p>
<p>2
</p>
<p>�p
3C 1
</p>
<p>p
3 � 1p
</p>
<p>3 � 1
p
3C 1
</p>
<p>�
D P03 with P3P3 D ˝3 :
</p>
<p>In the case where ˝ has full rank it is actually easy to come up with one specific
</p>
<p>factorization. Under full rank,˝ has a strictly positive determinant such that
</p>
<p>t11 D
s
!21 �
</p>
<p>!212
!22
</p>
<p>&gt; 0 :
</p>
<p>One may hence define the following triangular matrix factorizing˝ from (14.6),
</p>
<p>T D
 
</p>
<p>t11
!12
!2
</p>
<p>0 !2
</p>
<p>!
with T T 0 D
</p>
<p>�
!21 !12
</p>
<p>!12 !
2
2
</p>
<p>�
; (14.7)
</p>
<p>which is sometimes called Cholesky decomposition of ˝ . For ˝3 one obtains that
</p>
<p>way
</p>
<p>T3 D
1p
2
</p>
<p>�p
3 1
</p>
<p>0 2
</p>
<p>�
:
</p>
<p>We hence reinforce the postulate that˝0:5 of a matrix˝ is not unique. In particular,
</p>
<p>T3 is not symmetric, while P3 is. Still, it holds T3 T
0
3 D P3P3 D ˝3. �
</p>
<p>Functional Limit Theory
</p>
<p>Phillips (1986) and Phillips and Durlauf (1986) introduced under appropriate
</p>
<p>assumptions multivariate generalizations of Proposition 14.1 into econometrics:
</p>
<p>n�0:5zbsnc D n�0:5
bsncX
</p>
<p>tD1
wt ) ˝0:5w W.s/ D
</p>
<p>�
B1.t/
</p>
<p>B2.t/
</p>
<p>�
: (14.8)</p>
<p/>
</div>
<div class="page"><p/>
<p>320 14 Asymptotics of Integrated Processes
</p>
<p>For the individual components this means:
</p>
<p>n�0:5z1;bsnc ) B1.s/ and n�0:5z2;bsnc ) B2.s/ ;
</p>
<p>where the Brownian motions are generally not independent of each other. Indepen-
</p>
<p>dence is only present if ˝w is diagonal (!12 D 0) as it then holds:
�
</p>
<p>B1.t/
</p>
<p>B2.t/
</p>
<p>�
D
�
!1W1.t/
</p>
<p>!2W2.t/
</p>
<p>�
:
</p>
<p>Under adequate technical conditions, which need not to be specified here, the
</p>
<p>following proposition holds, cf. as well Johansen (1995, Theorem B.13).
</p>
<p>Proposition 14.4 (I(1) Asymptotics) Let fztg be a 2-dimensional integrated pro-
cess and �zt D zt � zt�1 D wt with E.wt/ D .0; 0/0 and ˝w from (14.6). Then it
holds under some additional assumptions that
</p>
<p>.a/ n�1:5
nX
</p>
<p>tD1
zt
</p>
<p>d! ˝0:5w
Z 1
</p>
<p>0
</p>
<p>W.s/ds;
</p>
<p>.b/ n�2
nX
</p>
<p>tD1
ztz
</p>
<p>0
t
</p>
<p>d! ˝0:5w
Z 1
</p>
<p>0
</p>
<p>W.s/W 0.s/ds .˝0:5w /
0
</p>
<p>.c/ n�1
nX
</p>
<p>tD1
ztw
</p>
<p>0
t
</p>
<p>d! ˝0:5w
Z 1
</p>
<p>0
</p>
<p>W.s/dW 0.s/ .˝0:5w /
0 C
</p>
<p>1X
</p>
<p>hD0
&#13;w.h/
</p>
<p>as n ! 1.
</p>
<p>Naturally, these results can be expressed in terms of B D ˝0:5w W as well:
</p>
<p>˝0:5w
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ds D
Z 1
</p>
<p>0
</p>
<p>B.s/ds;
</p>
<p>˝0:5w
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/W 0.s/ds .˝0:5w /
0 D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>B.s/B0.s/ds ;
</p>
<p>˝0:5w
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/dW 0.s/ .˝0:5w /
0 D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>B.s/dB0.s/ :
</p>
<p>The limit from Proposition 14.4 (a) is to be read as a vector of Riemann integrals,
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ ds D
</p>
<p>0
BBB@
</p>
<p>1R
0
</p>
<p>W1.s/ds
</p>
<p>1R
0
</p>
<p>W2.s/ds
</p>
<p>1
CCCA :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Problems and Solutions 321
</p>
<p>In (b), we have a square matrix:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/W 0.s/ ds D
</p>
<p>0
BBB@
</p>
<p>1R
0
</p>
<p>W21 .s/ds
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p>1R
0
</p>
<p>W2.s/W1.s/ds
1R
0
</p>
<p>W22 .s/ds
</p>
<p>1
CCCA :
</p>
<p>Concluding, both these outcomes are results from (14.8) and from a multivariate
</p>
<p>version of the continuous mapping theorem, cf. Proposition 14.3. The third result
</p>
<p>from Proposition 14.4, the matrix of Ito integrals, corresponds to result (f) from
</p>
<p>Proposition 14.2, also cf. Corollary 14.1:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/dW 0.s/ D
</p>
<p>0
BBB@
</p>
<p>1R
0
</p>
<p>W1.s/dW1.s/
1R
0
</p>
<p>W1.s/dW2.s/
</p>
<p>1R
0
</p>
<p>W2.s/dW1.s/
1R
0
</p>
<p>W2.s/dW2.s/
</p>
<p>1
CCCA :
</p>
<p>In the multivariate setting, such a convergence cannot be elementarily derived any
</p>
<p>longer. For a proof see e.g. Phillips (1988) or Hansen (1992a).
</p>
<p>14.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>14.1 Derive with elementary means the expression (14.4) for the long-run variance
</p>
<p>of the MA(1) process fetg from (14.2).
</p>
<p>14.2 Derive the limiting distribution of n�1
Pn
</p>
<p>tD1 xtet from Corollary 14.1.
</p>
<p>14.3 Prove Proposition 14.2(a), (c), (d) and (e). When doing this, you may assume
</p>
<p>the functionals to be continuous with respect to an appropriate metric.
</p>
<p>14.4 Prove Proposition 14.2(b).
</p>
<p>14.5 Prove Proposition 14.2(f).
</p>
<p>14.6 Check that ds.f ; g/ with
</p>
<p>ds.f ; g/ D sup
0�t�1
</p>
<p>j f .t/ � g.t/j ; f ; g 2 D Œ0; 1&#141; ;
</p>
<p>is a metric (supremum metric).</p>
<p/>
</div>
<div class="page"><p/>
<p>322 14 Asymptotics of Integrated Processes
</p>
<p>14.7 Show that the integral functionals h1, h2 and h3 from Example 14.4 are
</p>
<p>continuous on D Œ0; 1&#141; with respect to the supremum metric.
</p>
<p>Solutions
</p>
<p>14.1 First we adopt the autocovariance function from Proposition 3.2:
</p>
<p>&#13;e.h/ D �2
1X
</p>
<p>jD0
cj cjCh:
</p>
<p>In (14.4), the long-run variance is formulated as follows:
</p>
<p>!2e D �2
0
@
</p>
<p>1X
</p>
<p>jD0
cj
</p>
<p>1
A
2
</p>
<p>:
</p>
<p>As the coefficients fcjg are absolutely summable, the infinite sums can be multiplied
out:
</p>
<p>!2e
�2
</p>
<p>D
</p>
<p>0
@
</p>
<p>1X
</p>
<p>jD0
cj
</p>
<p>1
A
0
@
</p>
<p>1X
</p>
<p>jD0
cj
</p>
<p>1
A
</p>
<p>D c0 c0 C c0 c1 C c0 c2 C : : :
Cc1 c0 C c1 c1 C c1 c2 C : : :
Cc2 c0 C c2 c1 C c2 c2 C : : :
C : : :
</p>
<p>D
1X
</p>
<p>jD0
c2j C 2
</p>
<p>1X
</p>
<p>jD0
cj cjC1 C 2
</p>
<p>1X
</p>
<p>jD0
cj cjC2 C : : :
</p>
<p>D 1
�2
.&#13;e.0/C 2&#13;e.1/C 2&#13;e.2/C : : :/ :
</p>
<p>Hence, the equivalence of the representations of the long-run variance from (14.1)
</p>
<p>and (14.4) is derived.
</p>
<p>14.2 Due to xt D xt�1 C et we write
</p>
<p>n�1
nX
</p>
<p>tD1
xtet D n�1
</p>
<p>nX
</p>
<p>tD1
xt�1et C n�1
</p>
<p>nX
</p>
<p>tD1
e2t :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Problems and Solutions 323
</p>
<p>The limiting behavior of the first sum on the right-hand side is known from
</p>
<p>Proposition 14.2, and the second sum on the right-hand side tends to Var.et/ D
&#13;e.0/. Thus, elementary transformations yield
</p>
<p>n�1
nX
</p>
<p>tD1
xtet
</p>
<p>d! !
2
e
</p>
<p>2
</p>
<p>�
W2.1/ � &#13;e.0/
</p>
<p>!2e
</p>
<p>�
C &#13;e.0/
</p>
<p>D !
2
e
</p>
<p>2
</p>
<p>�
W2.1/C &#13;e.0/
</p>
<p>!2e
</p>
<p>�
</p>
<p>D !2e
�
</p>
<p>W2.1/ � 1
2
</p>
<p>C !
2
e C &#13;e.0/
2!2e
</p>
<p>�
:
</p>
<p>The application of Ito&rsquo;s lemma completes the proof.
</p>
<p>14.3
</p>
<p>(a) The interval Œ0; 1/ is split up into n subintervals of the same length,
</p>
<p>Œ0; 1/ D
n[
</p>
<p>tD1
</p>
<p>�
t � 1
</p>
<p>n
;
</p>
<p>t
</p>
<p>n
</p>
<p>�
:
</p>
<p>On each of these subintervals, we define the step function Xn.s/ as the
</p>
<p>appropriately normalized I(1) process,
</p>
<p>Xn.s/ D
1p
n!e
</p>
<p>t�1X
</p>
<p>jD1
ej D
</p>
<p>xt�1p
n!e
</p>
<p>; s 2
�
</p>
<p>t � 1
n
;
</p>
<p>t
</p>
<p>n
</p>
<p>�
;
</p>
<p>and on the endpoint, it holds for s D 1:
</p>
<p>Xn.1/ D
1p
n!e
</p>
<p>nX
</p>
<p>jD1
ej D
</p>
<p>xnp
n!e
</p>
<p>:
</p>
<p>Due to Proposition 14.1 we have (n ! 1)
</p>
<p>Xn.s/ ) W.s/:
</p>
<p>Furthermore, we take a trick into account which allows for expressing a sum
</p>
<p>over xt�1 as an integral:
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>xt�1 ds D xt�1 s
ˇ̌
ˇ
</p>
<p>t
n
t�1
</p>
<p>n
</p>
<p>D xt�1
�
</p>
<p>t
</p>
<p>n
� t � 1
</p>
<p>n
</p>
<p>�
D xt�1
</p>
<p>n
:</p>
<p/>
</div>
<div class="page"><p/>
<p>324 14 Asymptotics of Integrated Processes
</p>
<p>Equipped with this, one explicitly obtains
</p>
<p>n�
3
2
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
xt�1 D
</p>
<p>n�
1
2
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
</p>
<p>xt�1
n
</p>
<p>D n
� 12
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>xt�1 ds
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>xt�1p
n!e
</p>
<p>ds
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>Xn.s/ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>Xn.s/ ds:
</p>
<p>As we may assume that the functional
</p>
<p>h.g/ D
Z 1
</p>
<p>0
</p>
<p>g.s/ds
</p>
<p>is continuous with respect to an appropriate metric, Proposition 14.3 yields for
</p>
<p>n ! 1:
Z 1
</p>
<p>0
</p>
<p>Xn.s/ ds
d!
Z 1
</p>
<p>0
</p>
<p>W.s/ ds :
</p>
<p>Hence, the proof is complete.
</p>
<p>(c) Just as for the proof of (a) we define the stochastic step function
</p>
<p>Xn.s/ D
1p
n!e
</p>
<p>t�1X
</p>
<p>jD1
ej D
</p>
<p>xt�1p
n!e
</p>
<p>; s 2
�
</p>
<p>t � 1
n
;
</p>
<p>t
</p>
<p>n
</p>
<p>�
;
</p>
<p>and in addition the analogously constructed deterministic step function
</p>
<p>Tn.s/ D
t
</p>
<p>n
D bsnc C 1
</p>
<p>n
; s 2
</p>
<p>�
t � 1
</p>
<p>n
;
</p>
<p>t
</p>
<p>n
</p>
<p>�
;
</p>
<p>t D 1; : : : ; n; and Tn.1/ D 1. Then it holds that
</p>
<p>n�
5
2
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
txt�1 D
</p>
<p>n�
3
2
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
t
</p>
<p>xt�1
n
</p>
<p>D n
� 32
</p>
<p>!e
</p>
<p>nX
</p>
<p>tD1
t
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>xt�1 ds</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Problems and Solutions 325
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>t
</p>
<p>n
</p>
<p>xt�1p
n!e
</p>
<p>ds
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>Tn.s/Xn.s/ ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>Tn.s/Xn.s/ ds:
</p>
<p>For n ! 1 it holds
</p>
<p>Tn.s/Xn.s/ ) sW.s/;
</p>
<p>and hence, due to the continuity of the integral function, as claimed
</p>
<p>Z 1
</p>
<p>0
</p>
<p>Tn.s/Xn.s/ ds
d!
Z 1
</p>
<p>0
</p>
<p>s W.s/ ds :
</p>
<p>(d) Again, with the definition of Xn.s/ it is shown:
</p>
<p>n�
1
2
</p>
<p>!e
</p>
<p>bsncX
</p>
<p>tD1
.et � Ne/ D
</p>
<p>n�
1
2
</p>
<p>!e
</p>
<p>�
xbsnc � bsncNe
</p>
<p>�
</p>
<p>D Xn.s/ �
bsnc
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
</p>
<p>etp
n!e
</p>
<p>D Xn.s/ �
bsnc
</p>
<p>n
Xn.1/
</p>
<p>) W.s/ � s W.1/:
</p>
<p>(e) The proof is entirely analogous to (a),
</p>
<p>n�2
</p>
<p>!2e
</p>
<p>nX
</p>
<p>tD1
x2t�1 D
</p>
<p>1
</p>
<p>.
p
</p>
<p>n!e/2
</p>
<p>nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>x2t�1 ds
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>X2n.s/ ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>X2n.s/ ds
</p>
<p>d!
Z 1
</p>
<p>0
</p>
<p>W2.s/ ds:</p>
<p/>
</div>
<div class="page"><p/>
<p>326 14 Asymptotics of Integrated Processes
</p>
<p>14.4 The result can be shown in three steps.
</p>
<p>(i) By definition it holds:
</p>
<p>n�1
nX
</p>
<p>tD1
xt�1 D n�1f0C e1 C .e1 C e2/C : : :C .e1 C e2 C : : :C en�1/g
</p>
<p>D n�1f.n � 1/e1 C .n � 2/e2 C : : :C en�1g
</p>
<p>D n�1
nX
</p>
<p>tD1
.n � t/et
</p>
<p>D
nX
</p>
<p>tD1
et � n�1
</p>
<p>nX
</p>
<p>tD1
tet:
</p>
<p>(ii) Thus, the sum of interest is reduced to known quantities:
</p>
<p>n�1
nX
</p>
<p>tD1
tet D
</p>
<p>nX
</p>
<p>tD1
et � n�1
</p>
<p>nX
</p>
<p>tD1
xt�1
</p>
<p>D xn � n�1
nX
</p>
<p>tD1
xt�1 ;
</p>
<p>or
</p>
<p>n�
3
2
</p>
<p>nX
</p>
<p>tD1
tet D
</p>
<p>xnp
n
� n� 32
</p>
<p>nX
</p>
<p>tD1
xt�1
</p>
<p>d! !e W.1/ � !e
Z 1
</p>
<p>0
</p>
<p>W.s/ ds;
</p>
<p>where Proposition 14.1 for s D 1 and Proposition 14.2 (a) were applied.
(iii) From Example 9.1 (a) with t D 1 we know:
</p>
<p>W.1/�
Z 1
</p>
<p>0
</p>
<p>W.s/ds D
Z 1
</p>
<p>0
</p>
<p>sdW.s/:
</p>
<p>Hence, the claim is proved.
</p>
<p>14.5 The first result is based on the binomial formula applied to xt D xt�1 C et:
</p>
<p>x2t D x2t�1 C 2xt�1 et C e2t :</p>
<p/>
</div>
<div class="page"><p/>
<p>14.5 Problems and Solutions 327
</p>
<p>Solving this for the mixed term, we obtain:
</p>
<p>n�1
nX
</p>
<p>tD1
xt�1 et D
</p>
<p>n�1
</p>
<p>2
</p>
<p>nX
</p>
<p>tD1
</p>
<p>�
x2t � x2t�1 � e2t
</p>
<p>�
</p>
<p>D n
�1
</p>
<p>2
</p>
<p> 
x2n � x20 �
</p>
<p>nX
</p>
<p>tD1
e2t
</p>
<p>!
</p>
<p>D 1
2
</p>
<p> �
xnp
</p>
<p>n
</p>
<p>�2
� 0 � 1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
e2t
</p>
<p>!
</p>
<p>d! 1
2
</p>
<p>�
!2e W
</p>
<p>2.1/ � &#13;e.0/
�
</p>
<p>D !
2
e
</p>
<p>2
</p>
<p>�
W2.1/ � &#13;e.0/
</p>
<p>!2e
</p>
<p>�
:
</p>
<p>Thus, we come to the second claim. Obviously, it holds that
</p>
<p>!2e
2
</p>
<p>�
W2.1/� &#13;e.0/
</p>
<p>!2e
</p>
<p>�
D !
</p>
<p>2
e
</p>
<p>2
</p>
<p>�
W2.1/ � 1C !
</p>
<p>2
e � &#13;e.0/
!2e
</p>
<p>�
:
</p>
<p>The special case (10.3) of Ito&rsquo;s lemma then establishes the equality claimed.
</p>
<p>14.6 In order to have a metric, three conditions from the text need to be fulfilled.
</p>
<p>(i) The condition f D g means
</p>
<p>f .t/ D g.t/; t 2 Œ0; 1&#141;;
</p>
<p>which is equivalent to
</p>
<p>jf .t/ � g.t/j D 0; t 2 Œ0; 1&#141;:
</p>
<p>From this it immediately follows ds.f ; g/ D 0: Conversely,
</p>
<p>sup
0�t�1
</p>
<p>jf .t/ � g.t/j D 0
</p>
<p>immediately implies jf .t/ � g.t/j D 0: Therefore, two functions f and g in are
indeed equal if and only if they have zero distance according to the supremum
</p>
<p>metric.
</p>
<p>(ii) The symmetry condition is obviously given as it holds for the absolute value:
</p>
<p>jf .t/ � g.t/j D jg.t/� f .t/j:</p>
<p/>
</div>
<div class="page"><p/>
<p>328 14 Asymptotics of Integrated Processes
</p>
<p>(iii) Finally, the triangle inequality can be established by adding zero:
</p>
<p>ds.f ; g/ D sup
t
</p>
<p>jf .t/ � h.t/C h.t/� g.t/j
</p>
<p>� sup
t
</p>
<p>fjf .t/ � h.t/j C jh.t/� g.t/jg
</p>
<p>� sup
t
</p>
<p>jf .t/ � h.t/j C sup
t
</p>
<p>jh.t/ � g.t/j
</p>
<p>D ds.f ; h/C ds.h; g/:
</p>
<p>The second, rather plausible inequality follows from the properties of the
</p>
<p>supremum metric, see e.g. Syds&aelig;ter, Str&oslash;m, and Berck (1999, p.77).
</p>
<p>14.7 We consider three functionals hi.f /; i D 1; 2; 3; which assign a real number
to the function f .t/.
</p>
<p>(i) The first functional is just the integral from zero to one. Here it holds:
</p>
<p>jh1.f / � h1.g/j D
ˇ̌
ˇ̌
Z 1
</p>
<p>0
</p>
<p>f .t/dt �
Z 1
</p>
<p>0
</p>
<p>g.t/dt
</p>
<p>ˇ̌
ˇ̌
</p>
<p>�
Z 1
</p>
<p>0
</p>
<p>jf .t/ � g.t/jdt
</p>
<p>�
Z 1
</p>
<p>0
</p>
<p>sup
0�s�1
</p>
<p>jf .s/ � g.s/jdt
</p>
<p>D
 
</p>
<p>sup
0�s�1
</p>
<p>jf .s/ � g.s/j
! Z 1
</p>
<p>0
</p>
<p>dt
</p>
<p>D sup
0�s�1
</p>
<p>jf .s/ � g.s/j
</p>
<p>D ds.f ; g/:
</p>
<p>Hence, the smaller the deviation of f and g, the nearer are h1.f / and h1.g/. This
</p>
<p>exactly corresponds to the definition of continuity.
</p>
<p>(ii) The second functional is the integral over a quadratic function. Here we obtain
</p>
<p>with the binomial formula and the triangle inequality:
</p>
<p>jh2.f / � h2.g/j D
ˇ̌
ˇ̌
Z 1
</p>
<p>0
</p>
<p>f 2.t/dt �
Z 1
</p>
<p>0
</p>
<p>g2.t/dt
</p>
<p>ˇ̌
ˇ̌
</p>
<p>D
ˇ̌
ˇ̌
Z 1
</p>
<p>0
</p>
<p>.g.t/ � f .t//2dt � 2
Z 1
</p>
<p>0
</p>
<p>g.t/.g.t/ � f .t//dt
ˇ̌
ˇ̌</p>
<p/>
</div>
<div class="page"><p/>
<p>References 329
</p>
<p>�
Z 1
</p>
<p>0
</p>
<p>jg.t/ � f .t/j2dt C 2
Z 1
</p>
<p>0
</p>
<p>jg.t/jjg.t/ � f .t/jdt
</p>
<p>�
 
</p>
<p>sup
0�t�1
</p>
<p>jg.t/ � f .t/j
!2
</p>
<p>C 2 sup
0�t�1
</p>
<p>jg.t/ � f .t/j
Z 1
</p>
<p>0
</p>
<p>jg.t/jdt:
</p>
<p>For the last inequality, it was approximated by the supremum as in (i) and then
</p>
<p>integrated from 0 to 1. Hence, it holds by definition
</p>
<p>jh2.f / � h2.g/j � .ds.g; f //2 C 2ds.g; f /
Z 1
</p>
<p>0
</p>
<p>jg.t/jdt:
</p>
<p>As g.t/ belongs to DŒ0; 1&#141; and is thus absolutely integrable, h2.f / tends to h2.g/
</p>
<p>if the distance between f and g gets smaller, which amounts to continuity.
</p>
<p>(iii) The third functional is 1
h2
</p>
<p>. Hence, we reduce the continuity of h3 to the one of
</p>
<p>h2:
</p>
<p>jh3.f / � h3.g/j D
ˇ̌
ˇ̌
ˇ
</p>
<p>1
R 1
0
</p>
<p>f 2.t/dt
� 1R 1
</p>
<p>0
g2.t/dt
</p>
<p>ˇ̌
ˇ̌
ˇ
</p>
<p>D
j
R 1
0 g
</p>
<p>2.t/dt �
R 1
0 f
</p>
<p>2.t/dtj
R 1
0 f
</p>
<p>2.t/dt
R 1
0 g
</p>
<p>2.t/dt
</p>
<p>D jh2.g/� h2.f /jR 1
0 f
</p>
<p>2.t/dt
R 1
0 g
</p>
<p>2.t/dt
:
</p>
<p>Hence, from the (quadratic) integrability of f and g and the continuity of h2
follows, as required, the continuity of h3.
</p>
<p>References
</p>
<p>Billingsley, P. (1968). Convergence of probability measures. New York: Wiley.
Davidson, J. (1994). Stochastic limit theory: An introduction for econometricians.
</p>
<p>Oxford/New York: Oxford University Press.
Dhrymes, Ph. J. (2000). Mathematics for econometrics (3rd ed.). New York: Springer.
Donsker, M. D. (1951). An invariance principle for certain probability limit theorems. Memoirs of
</p>
<p>the American Mathematical Society, 6, 1&ndash;12.
Hansen, B. E. (1992a). Convergence to stochastic integrals for dependent heterogeneous processes.
</p>
<p>Econometric Theory, 8, 489&ndash;500.
Johansen, S. (1995). Likelihood-based inference in cointegrated vector autoregressive models.
</p>
<p>Oxford/New York: Oxford University Press.
Phillips, P. C. B. (1986). Understanding spurious regressions in econometrics. Journal of Econo-
</p>
<p>metrics, 33, 311&ndash;340.
Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica, 55, 277&ndash;301.
Phillips, P. C. B. (1988). Weak convergence of sample covariance matrices to stochastic integrals
</p>
<p>via martingale approximations. Econometric Theory, 4, 528&ndash;533.</p>
<p/>
</div>
<div class="page"><p/>
<p>330 14 Asymptotics of Integrated Processes
</p>
<p>Phillips, P. C. B., &amp; Durlauf, S. N. (1986). Multiple time series regression with integrated
processes. Review of Economic Studies, LIII, 473&ndash;495.
</p>
<p>Phillips, P. C. B, &amp; Perron, P. (1988). Testing for a unit root in time series regression. Biometrika,
75, 335&ndash;346.
</p>
<p>Phillips, P. C. B., &amp; Solo, V. (1992). Asymptotics for linear processes. The Annals of Statistics, 20,
971&ndash;1001.
</p>
<p>P&ouml;tscher, B. M., &amp; Prucha, I. R. (2001). Basic elements of asymptotic theory. In B. H. Baltagi
(Ed.), A companion to theoretical econometrics (pp. 201&ndash;229). Malden: Blackwell.
</p>
<p>Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
Berlin/New York: Springer.
</p>
<p>Tanaka, K. (1996). Time series analysis: Nonstationary and noninvertible distribution theory.
New York: Wiley.
</p>
<p>White, H. (2001). Asymptotic theory for econometricians (2nd ed.). London/San Diego: Academic
Press.</p>
<p/>
</div>
<div class="page"><p/>
<p>15Trends, Integration Tests and NonsenseRegressions
</p>
<p>15.1 Summary
</p>
<p>Now we consider some applications of the propositions from the previous chapter.
</p>
<p>In particular, fetg and fxtg are integrated of order 0 and integrated of order
1, respectively, cf. the definitions above Proposition 14.2. It turns out that the
</p>
<p>regression of a time series on a linear trend leads to asymptotically Gaussian
</p>
<p>estimators. However, test statistics constructed to distinguish between integration
</p>
<p>of order 1 and 0 are not Gaussian. Finally, we cover the problem of nonsense
</p>
<p>regressions, which occur particularly in the case of independent integrated variables.
</p>
<p>15.2 Trend Regressions
</p>
<p>Let fytg be trending in the sense that the expectation follows a linear time trend,
E.yt/ D ˇt. The slope parameter is estimated following the least squares (LS)
method. The residuals, crest D yt � Ǒt, are then the detrended series. Estimation
relies on a sample of size n.
</p>
<p>Detrending
</p>
<p>The time series fytg is regressed on a linear time trend according to the least squares
method. For the sake of simplicity, we neglect a constant intercept that would have
</p>
<p>to be included in practice. The LS estimator Ǒ of the regression
</p>
<p>yt D Ǒ t Ccrest ; t D 1; : : : ; n ; (15.1)
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_15
</p>
<p>331</p>
<p/>
</div>
<div class="page"><p/>
<p>332 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>with the empirical residuals fcrestg is
</p>
<p>Ǒ D
Pn
</p>
<p>tD1 t ytPn
tD1 t
</p>
<p>2
:
</p>
<p>For the denominator the following formula holds
</p>
<p>nX
</p>
<p>tD1
t2 D n .n C 1/ .2n C 1/
</p>
<p>6
D n
</p>
<p>3
</p>
<p>3
C n
</p>
<p>2
</p>
<p>2
C n
6
; (15.2)
</p>
<p>such that one obtains asymptotically (n ! 1)
</p>
<p>1
</p>
<p>n3
</p>
<p>nX
</p>
<p>tD1
t2 ! 1
</p>
<p>3
:
</p>
<p>This limit is a special case of a more general result dealt with in Problem 15.1.
</p>
<p>In this chapter we consider two models with a linear time trend in the mean. First,
</p>
<p>if the deviations from the linear trend are stationary, i.e. if the true model reads
</p>
<p>yt D ˇ t C et ; t D 1; : : : ; n ; (15.3)
</p>
<p>then we say that fytg is trend stationary. More precisely, fetg satisfies the assump-
tions of an I(0) process discussed in the previous chapter with long-run variance !2e
defined in (14.1). Second, the stochastic component may be I(1). Then one says that
</p>
<p>fytg is integrated with drift (ˇ &curren; 0): �yt D ˇ C et. By integrating (summing up)
with a starting value of zero, this translates into
</p>
<p>yt D ˇ t C xt ; t D 1; : : : ; n ; xt D
tX
</p>
<p>jD1
ej: (15.4)
</p>
<p>An example will illustrate the difference between these two trend models.
</p>
<p>Example 15.1 (Linear Time Trend) In Fig. 15.1 we see two time series, following
</p>
<p>the slope 0:1 on average, t D 1; 2; : : : ; 250. The upper graph shows a trend
stationary series,
</p>
<p>y
.0/
t D 0:1t C "t;
</p>
<p>where "t � iiN .0; 1/. The lower diagram shows with identical f"tg
</p>
<p>y
.1/
t D 0:1t C
</p>
<p>tX
</p>
<p>jD1
"j ;</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Trend Regressions 333
</p>
<p>0 50 100 150 200 250
</p>
<p>0
5
</p>
<p>1
5
</p>
<p>2
5
</p>
<p>trend stationary with slope 0.1
</p>
<p>0 50 100 150 200 250
</p>
<p>0
5
</p>
<p>1
5
</p>
<p>2
5
</p>
<p>integrated with drift (slope 0.1)
</p>
<p>Fig. 15.1 Linear Time Trend
</p>
<p>i.e. y
.1/
t is I.1/ with drift:
</p>
<p>�y
.1/
t D 0:1C "t:
</p>
<p>The deviations from the linear time trend are in the lower case I.1/ and are hence
</p>
<p>much stronger than in the upper case. �
</p>
<p>Trend Stationarity
</p>
<p>The following proposition contains the properties of LS detrending under trend
</p>
<p>stationarity.
</p>
<p>Proposition 15.1 (Trend Stationary) Let fytg from (15.3) be trend stationary, and
let O!e denote a consistent estimator for !e. It then holds for Ǒ from (15.1) that
</p>
<p>n1:5
Ǒ � ˇ
O!e
</p>
<p>d! N .0; 3/ (15.5)
</p>
<p>as n ! 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>334 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>A proof is provided in Problem 15.2 relying on Proposition 14.2 (b). In practice, a
</p>
<p>consistent estimator O!e for !e will be built from LS residuals, crest D yt � Ǒ t, see
below for details.
</p>
<p>Notice the fast convergence of the estimator Ǒ to its true value (with rate n1:5). In
real applications we would typically calculate a trend regression with intercept,
</p>
<p>yt D N̨ C Ň t C rest ; t D 1; : : : ; n :
</p>
<p>Hassler (2000) showed that limiting normality and the fast convergence rate of the
</p>
<p>LS estimator with intercept, Ň, pertains, although the variance is affected:
</p>
<p>n1:5
Ň � ˇ
N!e
</p>
<p>d! N .0; 12/ I
</p>
<p>again, this requires that N!e constructed from the residuals is consistent.
</p>
<p>I(1) with Drift
</p>
<p>Now we assume fytg to be integrated of order 1, possibly with drift. Note, however,
that the following proposition does not require ˇ &curren; 0, as we can learn from the
proof given in Problem 15.3.
</p>
<p>Proposition 15.2 (I(1) with Drift) Let fytg from (15.4) be I(1), possibly with drift:
�yt D ˇ C et. Let O!e denote a consistent estimator for !e. It then holds for Ǒ
from (15.1) that
</p>
<p>n0:5
Ǒ � ˇ
O!e
</p>
<p>d! N
�
0;
6
</p>
<p>5
</p>
<p>�
(15.6)
</p>
<p>as n ! 1.
</p>
<p>Consistent estimation of the long-term variance will rely on the differences of the
</p>
<p>residuals (f�crestg). Here, the LS estimator obviously converges much more slowly,
namely with the more usual rate n0:5 instead of n1:5 as in the trend stationary case.
</p>
<p>This does not come as a surprise given the impression from Fig. 15.1: Since the
</p>
<p>trend stationary series follows the straight line more closely, the estimation of the
</p>
<p>slope is more precise than in the I(1) case with drift.
</p>
<p>A final word on Proposition 15.2: The same variance as in (15.6) is obtained in
</p>
<p>the case of a regression with intercept, see Durlauf and Phillips (1988).</p>
<p/>
</div>
<div class="page"><p/>
<p>15.2 Trend Regressions 335
</p>
<p>Consistent Estimation of the Long-Run Variance
</p>
<p>As we have just seen, a consistent estimation of !2e is frequently needed in
</p>
<p>practice in order to apply the functional limit theory. For this purpose, fetg is
to be approximated by residuals or differences thereof: In the trend stationary
</p>
<p>case we have crest D Oet, while for I(1) processes it holds crest D Oxt, such that
�crest D Oet. The intuition behind an estimator O!2e is readily available from (14.1),
!2e D &#13;e.0/ C 2
</p>
<p>P1
hD1 &#13;e.h/, although three modifications are required. First, the
</p>
<p>theoretical autocovariances need to be replaced by the sample analogues,
</p>
<p>O&#13;e.h/ D
1
</p>
<p>n
</p>
<p>n�hX
</p>
<p>tD1
Oet OetCh :
</p>
<p>Second, the infinite sum has to be cut off as sample autocovariances can be
</p>
<p>computed only up to the lag n � 1. Third, in order to really have a consistent
(and positive) estimator, a weight function wB.�/ depending on a tuning parameter
B is needed (whose required properties will not be discussed at this point, but see
</p>
<p>Example 15.2). In the statistics literature, wB.�/ is often called a kernel. Put together,
we obtain as sample counterpart to (14.1):
</p>
<p>O!2e D O&#13;e.0/C 2
n�1X
</p>
<p>hD1
wB.h/ O&#13;e.h/ : (15.7)
</p>
<p>For most kernels wB.�/, the parameter B (the so-called bandwidth) takes over the
role of a truncation, i.e. the weights are zero for arguments greater than B:
</p>
<p>O!2e D O&#13;e.0/C 2
BX
</p>
<p>hD1
wB.h/ O&#13;e.h/ :
</p>
<p>In fact, the choice of B is decisive for the quality of an estimation of the long-
</p>
<p>run variance. On the one hand, B needs to tend to infinity with the sample size,
</p>
<p>on the other it needs to diverge more slowly than n. Further issues on bandwidth
</p>
<p>selection and choice of kernels have been pioneered by Andrews (1991); see also
</p>
<p>the exposition in Hamilton (1994, Sect. 10.5).
</p>
<p>Example 15.2 (Bartlett Weights) According to Maurice S. Bartlett (English statis-
</p>
<p>tician, 1910&ndash;2002), a very simple weight function has a triangular form:
</p>
<p>wB.h/ D
(
1 � h
</p>
<p>BC1 ; h D 1; 2; : : : ;B C 1
0 ; else
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>336 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>Hence, the sequence of weights reads for h D 1; 2; : : :B C 1:
</p>
<p>B
</p>
<p>B C 1 ;
B � 1
B C 1 ; : : : ;
</p>
<p>B C 1� h
B C 1 ; : : : ;
</p>
<p>1
</p>
<p>B C 1 ; 0 :
</p>
<p>Plugging this in (15.7), the sum is indeed truncated at B:
</p>
<p>O!2e D O&#13;e.0/C 2
BX
</p>
<p>hD1
</p>
<p>B C 1� h
B C 1 O&#13;e.h/ :
</p>
<p>Although the simple Bartlett weights are by no means optimal, they are widespread
</p>
<p>in econometrics up to the present time, often also named after Newey and West
</p>
<p>(1987) who popularized them in their paper. �
</p>
<p>15.3 Integration Tests
</p>
<p>We consider one test for the null hypothesis that there is integration of order 1 and
</p>
<p>one for the null hypothesis that there is integration of order 0.
</p>
<p>Dickey-Fuller [DF] Test for Nonstationarity
</p>
<p>The oldest and the most frequently applied test on the null hypothesis of integration
</p>
<p>of order 1 stems from Dickey and Fuller (1979).1 In the simplest case (without
</p>
<p>deterministics) the regression model reads
</p>
<p>xt D a xt�1 C et ; t D 1; : : : ; n ;
</p>
<p>with the null hypothesis
</p>
<p>H0 W a D 1 .i.e. xt is integrated of order 1/;
</p>
<p>against the alternative of stationarity, H1: jaj &lt; 1. For the LS estimator Oa from
</p>
<p>xt D Oa xt�1 C Oet ; t D 1; : : : ; n ; (15.8)
</p>
<p>1Many more procedures have been developed over the last decades, notably the test by Elliott et al.
(1996) with certain optimality properties.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Integration Tests 337
</p>
<p>we obtain under H0 the limiting distribution of the normalized LS estimator n .Oa�1/.
Often, one does not work with n .Oa � 1/, but the associated t-statistic:
</p>
<p>ta D
Oa � 1
</p>
<p>sa
with s2a D
</p>
<p>s2Pn
tD1 x
</p>
<p>2
t�1
</p>
<p>D n
�1 Pn
</p>
<p>tD1 Oe2tPn
tD1 x
</p>
<p>2
t�1
</p>
<p>:
</p>
<p>The limiting distributions from Proposition 15.3 are established in Problem 15.4.
</p>
<p>Proposition 15.3 (Dickey-Fuller Test) Let the I(1) process fxtg (�xt D et) satisfy
the assumptions from Proposition 14.2. It then holds
</p>
<p>(a) for Oa from regression (15.8) without intercept that
</p>
<p>n .Oa � 1/ d!
W2.1/ � &#13;e.0/
</p>
<p>!2e
</p>
<p>2
R 1
0
</p>
<p>W2.s/ ds
;
</p>
<p>(b) and for the t-statistic that
</p>
<p>ta
d!
</p>
<p>p
!2ep
&#13;e.0/
</p>
<p>R 1
0 W.s/ dW.s/C
</p>
<p>!2e�&#13;e.0/
2!2eqR 1
</p>
<p>0 W
2.s/ ds
</p>
<p>;
</p>
<p>as n ! 1.
</p>
<p>In this elegant form these limiting distributions were first given by Phillips (1987).2
</p>
<p>Note that the distributions depend on two parameters &#13;e.0/ and !
2
e called &ldquo;nuisance
</p>
<p>parameters&rdquo; in this context. They are a nuisance because we have to somehow
</p>
<p>deal with them (remove their effect) without being economically interested in their
</p>
<p>values. Particularly, if et D "t is a white noise process, then the first limit simplifies
to the so-called Dickey-Fuller distribution, cf. (1.11) and (1.12):
</p>
<p>n .Oa � 1/ d! W
2.1/ � 1
</p>
<p>2
R 1
0
</p>
<p>W2.s/ ds
D
R 1
0
</p>
<p>W.s/ dW.s/
R 1
0
</p>
<p>W2.s/ ds
:
</p>
<p>This expression does not depend on unknown nuisance parameters anymore; hence,
</p>
<p>quantiles can be simulated and approximated. One rejects for small (too strongly
</p>
<p>negative) values as the test is one-sided against the alternative of stationarity
</p>
<p>(jaj &lt; 1). Similarly, for !2e D &#13;e.0/ the limit of the t-statistic simplifies to a ratio
</p>
<p>2Through numerous works by Peter Phillips the functional central limit theory has found its
way into econometrics. This kind of limiting distributions was then celebrated as &ldquo;non-standard
asymptotics&rdquo;; meanwhile it has of course become standard.</p>
<p/>
</div>
<div class="page"><p/>
<p>338 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>free of nuisance parameters:
</p>
<p>DF D
R 1
0
</p>
<p>W.s/ dW.s/qR 1
0 W
</p>
<p>2.s/ ds
</p>
<p>: (15.9)
</p>
<p>The numerator equals that one of DF a from (1.11).
</p>
<p>In the relevant case that fetg is serially correlated, one can run two paths in
practice. Firstly, the test statistics can be appropriately modified by estimators for!2e
and &#13;e.0/. Phillips (1987) and Phillips and Perron (1988) paved this way. Secondly,
</p>
<p>one frequently calculates the regression augmented by K lags (ADF test):
</p>
<p>xt D Oa xt�1 C
KX
</p>
<p>kD1
Ǫk �xt�k C O"t ; t D K C 1; : : : ; n ;
</p>
<p>or with � D a � 1
</p>
<p>�xt D O� xt�1 C
KX
</p>
<p>kD1
Ǫk �xt�k C O"t ; t D K C 1; : : : ; n :
</p>
<p>If K is so large that the error term is free of serial correlation, then the t-statistic
</p>
<p>belonging to the test on a D 1, i.e. � D 0, converges to the Dickey-Fuller
distribution, and available tabulated percentiles serve as critical values; for further
</p>
<p>details see Said and Dickey (1984) and Chang and Park (2002). In practice, one
</p>
<p>would run a regression with intercept. This leaves the functional shape of the
</p>
<p>limiting distributions unaffected; only replace the WP W by a so-called demeaned
</p>
<p>WP, see also Problem 15.5.
</p>
<p>KPSS Test for Stationarity
</p>
<p>Now, the null and the alternative hypotheses are interchanged. The null hypothesis
</p>
<p>of the test suggested by Kwiatkowski, Phillips, Schmidt, and Shin (1992) claims
</p>
<p>that the time series fytg is integrated of order 0 while it exhibits a random walk
component under the alternative (hence, it is I(1)). Actually, this is a test for
</p>
<p>parameter constancy. The model reads
</p>
<p>yt D ct C et ; t D 1; : : : ; n ;
</p>
<p>with the hypotheses
</p>
<p>H0 W ct D c D constant;
</p>
<p>H1 W ct is a random walk.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.3 Integration Tests 339
</p>
<p>Under the null hypothesis, the intercept is again estimated by LS:
</p>
<p>yt D Oc C Oet ; Oc D y ;
</p>
<p>Oet D yt � y D et � e :
</p>
<p>From this, the partial sum process fStg is obtained:
</p>
<p>St WD
tX
</p>
<p>jD1
Oej :
</p>
<p>Again, we assume that the long-run variance is consistently estimated from the
</p>
<p>residuals Oet under H0. Then, the test statistic is formulated as
</p>
<p>� D n
�2
</p>
<p>O!2e
</p>
<p>nX
</p>
<p>tD1
S2t :
</p>
<p>The limiting distribution under the null hypothesis of stationarity is provided in
</p>
<p>Problem 15.6.
</p>
<p>Proposition 15.4 (KPSS Test) Let the I(0) process fetg satisfy the assumptions
from Proposition 14.2, and yt D c C et. It then holds that
</p>
<p>�
d!
Z 1
</p>
<p>0
</p>
<p>.W.s/� s W.1//2ds DW CM :
</p>
<p>as n ! 1.
</p>
<p>This expression does not depend on unknown nuisance parameters, and critical
</p>
<p>values are tabulated. In econometrics one often speaks about the KPSS distribution
</p>
<p>although this distribution has a long tradition in statistics where it also trades
</p>
<p>under the name of the Cram&eacute;r-von-Mises (CM) distribution. Quantiles were first
</p>
<p>tabulated by Anderson and Darling (1952). The limit CM is constructed from a
</p>
<p>Brownian bridge with W.1/ � 1W.1/ D 0, which reflects of course that Sn D 0 by
construction.
</p>
<p>Linear Time Trends
</p>
<p>Many economic and financial time series are driven by a linear time trend in the
</p>
<p>mean. Consider a trend stationary process xt D ˇt C et, which is not integrated of
order 1. Still, it holds that
</p>
<p>xt D xt�1 C ˇ C et � et�1 :</p>
<p/>
</div>
<div class="page"><p/>
<p>340 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>Hence, it is not surprising that a regression of xt on xt�1 results in a Dickey-Fuller
statistic not rejecting the false null hypothesis of integration of order 1. In order
</p>
<p>to avoid the confusion of a stochastic trend (unit root process integrated of order
</p>
<p>1) and a linear time trend, one has to include time as explanatory variable in the
</p>
<p>lag-augmented regression estimated by LS (we also add now a constant intercept)3:
</p>
<p>xt D Qc C Qı t C Qa xt�1 C
KX
</p>
<p>kD1
Q̨k �xt�k C Q"t ; t D K C 1; : : : ; n :
</p>
<p>Under the null hypothesis that fxtg is integrated of order 1 (possibly with drift), the
t-statistic associated with Qa � 1 obeys the following limiting distribution:
</p>
<p>eDF D
R 1
0
</p>
<p>QW.t/ d QW.t/qR 1
0
</p>
<p>QW2.t/ dt
: (15.10)
</p>
<p>The functional form is identical to that from (15.9), only that the WP is replaced by
</p>
<p>a so-called detrended WP QW defined for instance in Park and Phillips (1988, p. 474):
</p>
<p>QW.t/ D W.t/ �
Z 1
</p>
<p>0
</p>
<p>W.s/ds C 12
�Z 1
</p>
<p>0
</p>
<p>sW.s/ds � 1
2
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>��
1
</p>
<p>2
� t
�
:
</p>
<p>We call eDF also the detrended Dickey-Fuller distribution; critical values are
</p>
<p>tabulated in the literature.
</p>
<p>Similarly, the KPSS test may be modified to account for a linear time trend.
</p>
<p>Simply replace the demeaned series Oet D yt � y by the detrended one:
</p>
<p>Qet D yt � Qc � Q̌t ; QSt D
tX
</p>
<p>jD1
Qej :
</p>
<p>Computing the KPSS statistic Q� from Qet results asymptotically in a detrended
Cram&eacute;r-von-Mises distribution (eCM say) as long as the null hypothesis of (trend)
</p>
<p>stationarity holds true. Details on eCM and critical values thereof are given in
</p>
<p>Kwiatkowski et al. (1992): Q� d! eCM with
</p>
<p>eCM D
Z 1
</p>
<p>0
</p>
<p>�
W.s/C .2s � 3s2/W.1/C .6s2 � 6s/
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.r/dr
</p>
<p>�2
ds : (15.11)
</p>
<p>3Equivalently, one might feed detrended data into the ADF regression above.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Nonsense Regression 341
</p>
<p>15.4 Nonsense Regression
</p>
<p>Nonsense or spurious regressions occur if two integrated processes are regressed
</p>
<p>on each other without being cointegrated. Hence, we need to start by defining
</p>
<p>cointegration and by briefly recapping the standard statistics from the regression
</p>
<p>model.
</p>
<p>Cointegration
</p>
<p>The starting point for the econometric analysis of integrated time series is the
</p>
<p>concept of cointegration, which is also rooted in the equilibrium paradigm of
</p>
<p>economic theory. The idea of cointegration was introduced by Granger (1981) and
</p>
<p>has firmly been embedded in econometrics by the work of Engle and Granger
</p>
<p>(1987).
</p>
<p>Let us consider two integrated processes fxtg and fytg integrated of order 1.
Sometimes we assume that there is a linear combination with b &curren; 0 such that
</p>
<p>yt � bxt DW vt (15.12)
</p>
<p>is integrated of order 0. Here, y D b x is interpreted as a long-run equilibrium
relation, as postulated by economic theory, from which, however, the empirical
</p>
<p>observations deviate at a given point in time t by vt. If there is no linear combination
</p>
<p>of two I(1) processes that is stationary, then fxtg and fytg are called not cointegrated.
</p>
<p>Estimators and Statistics in the RegressionModel
</p>
<p>We consider the regression model without intercept (for the sake of simplicity) that
</p>
<p>is estimated by means of the least squares (LS) method:
</p>
<p>yt D Ǒ xt C Out ; t D 1; : : : ; n : (15.13)
</p>
<p>In this section we work under the assumption that fxtg and fytg are integrated of
order 1 but not cointegrated. Hence, each linear combination, ut D yt � ˇ xt, is
necessarily I(1) as well. Let fytg and fxtg be both components of fztg:
</p>
<p>zt D
�
</p>
<p>yt
</p>
<p>xt
</p>
<p>�
:
</p>
<p>Hence, it holds with (14.8):
</p>
<p>n�0:5ybsnc ) B1.s/ and n�0:5xbsnc ) B2.s/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>342 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>If the regressand yt and the regressor xt are stochastically independent, then this
</p>
<p>property is transferred to the limiting processes B1 and B2. Nevertheless, we will
</p>
<p>show that then Ǒ from (15.13) does not tend to the true value that is zero. Instead,
a (significant) relation between the independent variables is spuriously obtained.
</p>
<p>Since Granger and Newbold (1974), this circumstance is called spurious or nonsense
</p>
<p>regression.
</p>
<p>The LS estimator from the regression without intercept reads
</p>
<p>Ǒ D
Pn
</p>
<p>tD1 xt ytPn
tD1 x
</p>
<p>2
t
</p>
<p>:
</p>
<p>The t-statistic4 belonging to the test on the parameter value 0 is based on the
</p>
<p>difference of estimator and hypothetical value divided by the estimated standard
</p>
<p>error of the estimator:
</p>
<p>tˇ D
Ǒ � 0
sˇ
</p>
<p>with s2ˇ D
s2Pn
</p>
<p>tD1 x
2
t
</p>
<p>; s2 D 1
n
</p>
<p>nX
</p>
<p>tD1
Ou2t :
</p>
<p>As a measure of fit, the (uncentered) coefficient of determination is frequently
</p>
<p>calculated,5
</p>
<p>R2uc D 1 �
Pn
</p>
<p>tD1 Ou2tPn
tD1 y
</p>
<p>2
t
</p>
<p>:
</p>
<p>Finally, the Durbin-Watson statistic is a well-established measure for the first order
</p>
<p>residual autocorrelation,
</p>
<p>dw D
Pn
</p>
<p>tD2 .Out � Out�1/
2
</p>
<p>Pn
tD1 Ou2t
</p>
<p>� 2
 
1 �
</p>
<p>1
n
</p>
<p>Pn
tD2 Out Out�1
</p>
<p>1
n
</p>
<p>Pn
tD1 Ou2t
</p>
<p>!
:
</p>
<p>Let us briefly recall the behavior of these measures if we worked with I(0)
</p>
<p>variables x and y not being correlated (ˇ D 0). Then, Ǒ would tend to 0, the t-
statistic would converge to a normal distribution and the coefficient of determination
</p>
<p>would tend to zero. Finally, the Durbin-Watson statistic would converge to 2 .1 �
�1/ &gt; 0, where �1 denotes the first order autocorrelation coefficient of the regression
</p>
<p>errors. In the case of nonsense regressions, we obtain qualitatively entirely different
</p>
<p>asymptotic results.
</p>
<p>4For the following calculation of s2 we divide by n without correcting for degrees of freedom,
which does not matter asymptotically (n ! 1/.
5&ldquo;Uncentered&rdquo;, as the regression is calculated without intercept.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.4 Nonsense Regression 343
</p>
<p>Asymptotics
</p>
<p>Due to Proposition 14.4 (b) it holds for the denominator of the LS estimator
</p>
<p>n�2
nX
</p>
<p>tD1
x2t
</p>
<p>d!
Z 1
</p>
<p>0
</p>
<p>B22.s/ ds;
</p>
<p>and for the numerator we obtain
</p>
<p>n�2
nX
</p>
<p>tD1
xt yt
</p>
<p>d!
Z 1
</p>
<p>0
</p>
<p>B2.s/B1.s/ ds :
</p>
<p>Both results put together yield
</p>
<p>Ǒ d!
R 1
0
</p>
<p>B1.s/B2.s/ dsR 1
0
</p>
<p>B22.s/ ds
DW ˇ1 :
</p>
<p>In particular, if yt and xt are stochastically independent, then Ǒ does not tend to the
true value 0 but to the random variable ˇ1. And as if that was not enough, the t-
statistic belonging to the test on the true parameter value ˇ D 0 tends to infinity
in absolute value! Hence, in this situation t-statistics highly significantly reject the
</p>
<p>true null hypothesis of no correlation and therefore report absurd relations as being
</p>
<p>significant. This phenomenon was experimentally discovered for small samples by
</p>
<p>Granger and Newbold (1974) and asymptotically proved by Phillips (1986). For
</p>
<p>n ! 1 it namely holds that n�0:5 tˇ has a well-defined limiting distribution. In
the problem section we prove in addition the further properties of the following
</p>
<p>proposition.
</p>
<p>Proposition 15.5 (Nonsense Regression) For I(1) processes fxtg and fytg it holds
in case of no cointegration with the notation introduced that
</p>
<p>.a/ Ǒ d!
1R
0
</p>
<p>B1.s/B2.s/ds
</p>
<p>1R
0
</p>
<p>B22.s/ds
</p>
<p>DW ˇ1 ;
</p>
<p>.b/ R2uc
d! ˇ21
</p>
<p>1R
0
</p>
<p>B22.s/ ds
</p>
<p>1R
0
</p>
<p>B21.s/ ds
</p>
<p>DW R21 ;
</p>
<p>.c/ n�1 s2
d!
</p>
<p>1R
0
</p>
<p>B21.s/ ds
�
1 � R21
</p>
<p>�
DW s21 ;
</p>
<p>.d/ n�0:5 tˇ
d!
</p>
<p>ˇ1
</p>
<p>s
1R
0
</p>
<p>B22.s/ds
</p>
<p>s1
;
</p>
<p>.e/ dw
p! 0 ;
</p>
<p>as n ! 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>344 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>As already has been emphasized: The results (a), (b) and (d) justify to speak of a
</p>
<p>nonsense regression. A first hint at the lack of cointegration is obtained from the first
</p>
<p>order residual autocorrelation: For nonsense or spurious regressions, the Durbin-
</p>
<p>Watson statistic tends to zero.
</p>
<p>Example 15.3 (Hendry, 1980) Hendry (1980) shows the real danger of nonsense
</p>
<p>regressions. In his polemic example, the price development (measured by the
</p>
<p>consumer price index P) is to be explained. For this purpose, first a money supply
</p>
<p>variable M is used. Then a second variable C is considered (for which we could think
</p>
<p>of consumption), and it appears that this time series explains the price development
</p>
<p>better than M does. However, there can in fact be no talk of explanation; this is
</p>
<p>a nonsense correlation as behind C the cumulated rainfalls are hidden (hence, the
</p>
<p>precipitation amount)! By the way, note that P and C are not (only) integrated of
</p>
<p>order 1 but in addition exhibit a deterministic time trend, which only aggravates the
</p>
<p>problem of nonsense regression. �
</p>
<p>If integrated variables are not cointegrated, they have to be analyzed in differences,
</p>
<p>i.e. �yt is regressed on �xt, resulting in the familiar stationary regression model.
</p>
<p>Naturally, not all integrated economic variables lead to nonsense regressions. This
</p>
<p>does not happen under cointegration, which brings us to the final chapter.
</p>
<p>15.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>15.1 Show
</p>
<p>1
</p>
<p>nkC1
</p>
<p>nX
</p>
<p>tD1
tk ! 1
</p>
<p>k C 1 for k 2 N
</p>
<p>as n ! 1.
</p>
<p>15.2 Prove Proposition 15.1.
</p>
<p>15.3 Prove Proposition 15.2.
</p>
<p>15.4 Prove Proposition 15.3.
</p>
<p>15.5 Derive an expression for the limiting distribution of n.a � 1/ under a D 1 if a
model with intercept is estimated in the Dickey-Fuller test:
</p>
<p>xt D c C a xt�1 C "t ; t D 1; : : : ; n :
</p>
<p>Assume �xt D "t to be white noise.</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Problems and Solutions 345
</p>
<p>15.6 Prove Proposition 15.4.
</p>
<p>15.7 Prove the statements (b), (c) and (d) from Proposition 15.5.
</p>
<p>15.8 Prove statement (e) from Proposition 15.5.
</p>
<p>Solutions
</p>
<p>15.1 Define the continuous and hence Riemann-integrable function f on Œ0; 1&#141; with
</p>
<p>antiderivative F:
</p>
<p>f .x/ D xk with F.x/ D x
kC1
</p>
<p>k C 1 :
</p>
<p>Further, we work with the equidistant partition
</p>
<p>Œ0; 1&#141; D
n[
</p>
<p>iD1
Œti�1; ti&#141; ; ti D
</p>
<p>i
</p>
<p>n
:
</p>
<p>Hence,
</p>
<p>1
</p>
<p>nkC1
</p>
<p>nX
</p>
<p>tD1
tk D
</p>
<p>nX
</p>
<p>tD1
</p>
<p>� t
n
</p>
<p>�k 1
n
D
</p>
<p>nX
</p>
<p>iD1
f .ti/ .ti � ti�1/
</p>
<p>!
Z 1
</p>
<p>0
</p>
<p>f .t/ dt D F.1/� F.0/ ;
</p>
<p>which proves the claim.
</p>
<p>15.2 According to the hints in the text, it holds
</p>
<p>Ǒ D
Pn
</p>
<p>tD1 t.ˇt C et/Pn
tD1 t
</p>
<p>2
D ˇ C
</p>
<p>Pn
tD1 tet
</p>
<p>n3
</p>
<p>3
C n2
</p>
<p>2
C n
</p>
<p>6
</p>
<p>or
</p>
<p>n1:5. Ǒ � ˇ/ D n
�1:5Pn
</p>
<p>tD1 tet
1
3
C 1
</p>
<p>2n
C 1
</p>
<p>6n2
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>346 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>The denominator tends to 1
3
</p>
<p>whereas Proposition 14.2 (b) guarantees the following
</p>
<p>limiting distribution:
</p>
<p>n1:5. Ǒ � ˇ/ d!
!e
R 1
0
</p>
<p>sdW.s/
1
3
</p>
<p>:
</p>
<p>From Example 9.2 it follows that
</p>
<p>n1:5
Ǒ � ˇ
3!e
</p>
<p>d!
Z 1
</p>
<p>0
</p>
<p>sdW.s/ � N
�
0;
1
</p>
<p>3
</p>
<p>�
:
</p>
<p>If !e is replaced by a consistent estimator,
</p>
<p>O!e
p! !e;
</p>
<p>then the result from (15.5) is established.
</p>
<p>15.3 The I(1) case is treated in an analogous way as the trend stationary case, see
</p>
<p>Proposition 14.2 (c):
</p>
<p>n0:5. Ǒ � ˇ/ D n
�2:5Pn
</p>
<p>tD1 txt
1
3
C 1
</p>
<p>2n
C 1
</p>
<p>6n2
</p>
<p>d!
!e
R 1
0 sW.s/ ds
</p>
<p>1
3
</p>
<p>:
</p>
<p>From Corollary 8.1 (c) it follows for c D 0
Z 1
</p>
<p>0
</p>
<p>s W.s/ ds � N
�
0;
</p>
<p>2
</p>
<p>15
</p>
<p>�
:
</p>
<p>Hence, (15.6) is proved.
</p>
<p>15.4 Under H0, the LS estimator is given as:
</p>
<p>Oa D
Pn
</p>
<p>tD1 xt�1xtPn
tD1 x
</p>
<p>2
t�1
</p>
<p>D 1C
Pn
</p>
<p>tD1 xt�1etPn
tD1 x
</p>
<p>2
t�1
</p>
<p>:
</p>
<p>Insofar it holds
</p>
<p>n.Oa � 1/ D n
�1Pn
</p>
<p>tD1 xt�1et
n�2
</p>
<p>Pn
tD1 x
</p>
<p>2
t�1
</p>
<p>:</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Problems and Solutions 347
</p>
<p>Thus, numerator as well as denominator are in a form accessible for Proposi-
</p>
<p>tion 14.2,
</p>
<p>n.Oa � 1/ d!
!2e
2
.W2.1/ � &#13;e.0/
</p>
<p>!2e
/
</p>
<p>!2e
R 1
0 W
</p>
<p>2.s/ds
D
R 1
0 W.s/dW.s/C
</p>
<p>!2e �&#13;e.0/
2!2eR 1
</p>
<p>0 W
2.s/ds
</p>
<p>;
</p>
<p>which is the first distribution to be established.
</p>
<p>To find the second limit, one has to handle the standard error sa of Oa. It is based
on the residual variance estimation
</p>
<p>s2 D 1
n
</p>
<p>nX
</p>
<p>tD1
bet2 D
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
.xt � Oa xt�1/2
</p>
<p>D 1
n
</p>
<p>nX
</p>
<p>tD1
..1 � Oa/xt�1 C et/2
</p>
<p>D .1 � Oa/
2
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
x2t�1 C
</p>
<p>2.1� Oa/
n
</p>
<p>nX
</p>
<p>tD1
xt�1et C
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
e2t
</p>
<p>D n2.1 � Oa/2
Pn
</p>
<p>tD1 x
2
t�1
</p>
<p>n3
C 2n.1� Oa/
</p>
<p>Pn
tD1 xt�1et
</p>
<p>n2
C 1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
e2t
</p>
<p>p! 0C 2 � 0C &#13;e.0/;
</p>
<p>where Proposition 14.2 (e), (f) and n�1
Pn
</p>
<p>tD1 e
2
t ! &#13;e.0/ as well as the circumstance
</p>
<p>that n.1 � Oa/ converges in distribution were used. All in all, it hence holds for the
t-statistic:
</p>
<p>ta D
Oa � 1
</p>
<p>sa
D
.Oa � 1/
</p>
<p>qPn
tD1 x
</p>
<p>2
t�1
</p>
<p>s
</p>
<p>D
n.Oa � 1/
</p>
<p>q
n�2
</p>
<p>Pn
tD1 x
</p>
<p>2
t�1
</p>
<p>s
</p>
<p>d!
</p>
<p>�R 1
0
</p>
<p>W.s/dW.s/C !
2
e�&#13;e.0/
2!2e
</p>
<p>�q
!2e
R 1
0
</p>
<p>W2.s/ds
p
&#13;e.0/
</p>
<p>R 1
0
</p>
<p>W2.s/ds
</p>
<p>D !ep
&#13;e.0/
</p>
<p>R 1
0
</p>
<p>W.s/dW.s/C !
2
e�&#13;e.0/
2!2eqR 1
</p>
<p>0
W2.s/ds
</p>
<p>:
</p>
<p>This completes the proof.</p>
<p/>
</div>
<div class="page"><p/>
<p>348 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>15.5 With the means
</p>
<p>x D 1
n
</p>
<p>nX
</p>
<p>tD1
xt; x�1 D
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
xt�1; " D
</p>
<p>1
</p>
<p>n
</p>
<p>nX
</p>
<p>tD1
"t
</p>
<p>it holds for the LS estimator when including an intercept under a D 1:
</p>
<p>a D
Pn
</p>
<p>tD1.xt�1 � x�1/xtPn
tD1.xt�1 � x�1/2
</p>
<p>D
Pn
</p>
<p>tD1.xt�1 � x�1/.xt�1 C "t/Pn
tD1.xt�1 � x�1/2
</p>
<p>D
Pn
</p>
<p>tD1.xt�1 � x�1/.xt�1 � x�1/Pn
tD1.xt�1 � x�1/2
</p>
<p>C
Pn
</p>
<p>tD1.xt�1 � x�1/"tPn
tD1.xt�1 � x�1/2
</p>
<p>:
</p>
<p>Thus, we obtain
</p>
<p>n.a � 1/ D
n�1
</p>
<p>�Pn
tD1 xt�1"t � x�1
</p>
<p>Pn
tD1 "t
</p>
<p>�
</p>
<p>n�2
�Pn
</p>
<p>tD1 x
2
t�1 � n.x�1/2
</p>
<p>�
</p>
<p>D n
�1Pn
</p>
<p>tD1 xt�1"t � n�0:5x�1n�0:5xn
n�2
</p>
<p>Pn
tD1 x
</p>
<p>2
t�1 � .n�0:5x�1/
</p>
<p>2
:
</p>
<p>For "t � WN.0; �2/ with !2" D �2 D &#13;".0/ it therefore holds due to
Proposition 14.2 (a), (e) and (f) and according to Proposition 14.1 for s D 1:
</p>
<p>n.a � 1/ d!
�2
R 1
0
</p>
<p>W.s/dW.s/ � �
R 1
0
</p>
<p>W.s/ds �W.1/
</p>
<p>�2
R 1
0
</p>
<p>W2.s/ds � .�
R 1
0
</p>
<p>W.s/ds/2
</p>
<p>D
R 1
0
</p>
<p>W.s/dW.s/ � W.1/
R 1
0
</p>
<p>W.s/ds
R 1
0
</p>
<p>W2.s/ds � .
R 1
0
</p>
<p>W.s/ds/2
:
</p>
<p>If one defines the demeaned WP,
</p>
<p>W.s/ WD W.s/ �
Z 1
</p>
<p>0
</p>
<p>W.r/dr;
</p>
<p>then one observes two interesting identities:
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.s/dW.s/ D
Z 1
</p>
<p>0
</p>
<p>W.s/dW.s/
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>W.s/dW.s/ �
Z 1
</p>
<p>0
</p>
<p>W.r/dr
</p>
<p>Z 1
</p>
<p>0
</p>
<p>dW.s/
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>W.s/dW.s/ �
Z 1
</p>
<p>0
</p>
<p>W.r/drW.1/;</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Problems and Solutions 349
</p>
<p>and
</p>
<p>Z 1
</p>
<p>0
</p>
<p>.W.s//2ds D
Z 1
</p>
<p>0
</p>
<p> 
W2.s/ � 2W.s/
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.r/dr C
�Z 1
</p>
<p>0
</p>
<p>W.r/dr
</p>
<p>�2!
ds
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>W2.s/ds � 2
Z 1
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>Z 1
</p>
<p>0
</p>
<p>W.r/dr C
�Z 1
</p>
<p>0
</p>
<p>W.r/dr
</p>
<p>�2
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>W2.s/ds �
�Z 1
</p>
<p>0
</p>
<p>W.s/ds
</p>
<p>�2
:
</p>
<p>Hence, the limiting distribution in the case of a regression with intercept (which is
</p>
<p>equivalent to running a regression of demeaned variables!) can also be written as:
</p>
<p>n.a � 1/ d!
R 1
0
</p>
<p>W.s/dW.s/
R 1
0
.W.s//2ds
</p>
<p>:
</p>
<p>Thus, one obtains the same functional form as in the case without intercept, save
</p>
<p>that W.s/ is replaced by W.s/.
</p>
<p>15.6 For the partial sum St we obtain under the null hypothesis:
</p>
<p>St D
tX
</p>
<p>jD1
Oej D
</p>
<p>tX
</p>
<p>jD1
.ej � Ne/:
</p>
<p>The adequately normalized squared sum yields
</p>
<p>n�2
nX
</p>
<p>tD1
S2t D n�1
</p>
<p>nX
</p>
<p>tD1
</p>
<p>Z t
n
</p>
<p>t�1
n
</p>
<p>S2t�1ds C
S2n
</p>
<p>n2
</p>
<p>D
nX
</p>
<p>tD1
</p>
<p>t
nZ
</p>
<p>t�1
n
</p>
<p>�
Sbsncp
</p>
<p>n
</p>
<p>�2
ds C S
</p>
<p>2
n
</p>
<p>n2
;
</p>
<p>where S0 D 0 was used and Sbsnc is the step function from Proposition 14.2:
</p>
<p>Sbsnc D
bsncX
</p>
<p>jD1
.ej � Ne/; s 2
</p>
<p>�
t � 1
</p>
<p>n
;
</p>
<p>t
</p>
<p>n
</p>
<p>�
:</p>
<p/>
</div>
<div class="page"><p/>
<p>350 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>Note that Sn is zero by construction:
</p>
<p>Sn D
nX
</p>
<p>jD1
.ej � Ne/ D nNe � nNe D 0:
</p>
<p>Hence, we obtain
</p>
<p>n�2
nX
</p>
<p>tD1
S2t D
</p>
<p>Z 1
</p>
<p>0
</p>
<p>�
Sbsncp
</p>
<p>n
</p>
<p>�2
ds:
</p>
<p>As there is a continuous functional on the right-hand side, Proposition 14.3 with
</p>
<p>Proposition 14.2 yields:
</p>
<p>n�2
</p>
<p>!2e
</p>
<p>nX
</p>
<p>tD1
S2t
</p>
<p>d!
Z 1
</p>
<p>0
</p>
<p>.W.s/ � sW.1//2 ds:
</p>
<p>If !2e is replaced by a consistent estimator, then we just obtain the given limiting
</p>
<p>distribution CM as a functional of a Brownian bridge. This completes the proof.
</p>
<p>15.7 The limit ˇ1 of Ǒ given in Proposition 15.5 is adopted from the text.
The s2 is based on the LS residuals Out D yt � Ǒxt. Hence, the sum of squared
</p>
<p>residuals becomes
</p>
<p>nX
</p>
<p>tD1
Ou2t D
</p>
<p>nX
</p>
<p>tD1
y2t � 2 Ǒ
</p>
<p>nX
</p>
<p>tD1
yt xt C Ǒ2
</p>
<p>nX
</p>
<p>tD1
x2t
</p>
<p>D
nX
</p>
<p>tD1
y2t � 2
</p>
<p>.
Pn
</p>
<p>tD1 yt xt/
2
</p>
<p>Pn
tD1 x
</p>
<p>2
t
</p>
<p>C
�Pn
</p>
<p>tD1 yt xtPn
tD1 x
</p>
<p>2
t
</p>
<p>�2 nX
</p>
<p>tD1
x2t
</p>
<p>D
nX
</p>
<p>tD1
y2t �
</p>
<p>.
Pn
</p>
<p>tD1 yt xt/
2
</p>
<p>Pn
tD1 x
</p>
<p>2
t
</p>
<p>D
nX
</p>
<p>tD1
y2t � Ǒ2
</p>
<p>nX
</p>
<p>tD1
x2t :
</p>
<p>Thus, again with Proposition 14.4 (b), one immediately obtains for the uncentered
</p>
<p>coefficient of determination:
</p>
<p>R2uc D 1 �
n�2
</p>
<p>Pn
tD1 Ou2t
</p>
<p>n�2
Pn
</p>
<p>tD1 y
2
t
</p>
<p>D
Ǒ2n�2
</p>
<p>Pn
tD1 x
</p>
<p>2
t
</p>
<p>n�2
Pn
</p>
<p>tD1 y
2
t</p>
<p/>
</div>
<div class="page"><p/>
<p>15.5 Problems and Solutions 351
</p>
<p>d!
ˇ21
</p>
<p>R 1
0
</p>
<p>B22.s/dsR 1
0
</p>
<p>B21.s/ ds
</p>
<p>DW R21:
</p>
<p>The relation used above between the sum of squared residuals and the coefficient
</p>
<p>of determination further yields
</p>
<p>n�1s2 D 1
n2
</p>
<p>nX
</p>
<p>tD1
Ou2t
</p>
<p>D .1 � R2uc/ n�2
nX
</p>
<p>tD1
y2t
</p>
<p>d! .1 � R21/
Z 1
</p>
<p>0
</p>
<p>B21.s/ds
</p>
<p>DW s21:
</p>
<p>Moreover, the asymptotics of s2 enables us to state the behavior of the t-statistic
</p>
<p>as well. The required normalization is obvious:
</p>
<p>n�0:5tˇ D
Ǒpn�2PntD1 x2t
</p>
<p>n�0:5s
</p>
<p>d! ˇ1
s1
</p>
<p>sZ 1
</p>
<p>0
</p>
<p>B22.s/ds:
</p>
<p>15.8 Due to Out D yt � Ǒ xt, the numerator of the Durbin-Watson statistic yields:
</p>
<p>n�1
nX
</p>
<p>tD2
.Out � Out�1/2 D n�1
</p>
<p>nX
</p>
<p>tD2
</p>
<p>�
�yt � Ǒ�xt
</p>
<p>�2
</p>
<p>D n�1
nX
</p>
<p>tD2
</p>
<p>�
w1;t � Ǒw2;t
</p>
<p>�2
</p>
<p>D n�1
nX
</p>
<p>tD2
w21;t �
</p>
<p>2 Ǒ
n
</p>
<p>nX
</p>
<p>tD2
w1;t w2;t C
</p>
<p>Ǒ2
n
</p>
<p>nX
</p>
<p>tD2
w22;t
</p>
<p>d! &#13;1.0/� 2ˇ1 � Cov.w1;t;w2;t/C ˇ21 &#13;2.0/;</p>
<p/>
</div>
<div class="page"><p/>
<p>352 15 Trends, Integration Tests and Nonsense Regressions
</p>
<p>where &#13;i.0/ denote the variances of the processes fwi;tg, i D 1; 2. By definition one
obtains
</p>
<p>n dw D n
�1Pn
</p>
<p>tD2.Out � Out�1/2
n�2
</p>
<p>Pn
tD1 Ou2t
</p>
<p>d! &#13;1.0/ � 2ˇ1 � Cov.w1;t;w2;t/C ˇ
2
1 &#13;2.0/
</p>
<p>s21
:
</p>
<p>Consequently, dw tends to zero in probability.
</p>
<p>References
</p>
<p>Anderson, T. W., &amp; Darling, D. A. (1952). Asymptotic theory of certain &ldquo;Goodness of Fit&rdquo; criteria
based on stochastic processes. Annals of Mathematical Statistics, 23, 193&ndash;212.
</p>
<p>Andrews, D. W. K. (1991). Heteroskedasticity and autocorrelation consistent covariance matrix
estimation. Econometrica, 59, 817&ndash;858.
</p>
<p>Chang, Y., &amp; Park, J. (2002). On the asymptotics of ADF tests for unit roots. Econometric Reviews,
21, 431&ndash;447.
</p>
<p>Dickey, D. A., &amp; Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series
with a unit root. Journal of the American Statistical Association, 74, 427&ndash;431.
</p>
<p>Durlauf, S. N., &amp; Phillips, P. C. B. (1988). Trends versus random walks in time series analysis.
Econometrica, 56, 1333&ndash;1354.
</p>
<p>Elliott, G., Rothenberg, T. J., &amp; Stock, J. H. (1996). Efficient tests for an autoregressive unit root.
Econometrica, 64, 813&ndash;836.
</p>
<p>Engle, R. F., &amp; Granger, C. W. J. (1987). Co-integration and error correction: Representation,
estimation, and testing. Econometrica, 55, 251&ndash;276.
</p>
<p>Granger, C. W. J. (1981). Some properties of time series data and their use in econometric model
specification. Journal of Econometrics, 16, 121&ndash;130.
</p>
<p>Granger, C. W. J., &amp; Newbold P. (1974). Spurious regressions in econometrics. Journal of
Econometrics, 2, 111&ndash;120.
</p>
<p>Hamilton, J. (1994). Time series analysis. Princeton: Princeton University Press.
Hassler, U. (2000). Simple regressions with linear time trends. Journal of Time Series Analysis, 21,
</p>
<p>27&ndash;32.
Hendry, D. F. (1980). Econometrics &ndash; alchemy or science? Economica, 47, 387&ndash;406.
Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., &amp; Shin Y. (1992). Testing the null hypothesis of
</p>
<p>stationarity against the alternative of a unit root. Journal of Econometrics, 54, 159&ndash;178.
Newey, W. K., &amp; West, K. D. (1987). A Simple, positive semi-definite, heteroskedasticity and
</p>
<p>autocorrelation consistent covariance matrix. Econometrica, 55, 703&ndash;708.
Park, J. Y., &amp; Phillips, P. C. B. (1988). Statistical inference in regressions with integrated processes:
</p>
<p>Part I. Econometric Theory, 4, 468&ndash;497.
Phillips, P. C. B. (1986). Understanding spurious regressions in econometrics. Journal of Econo-
</p>
<p>metrics, 33, 311&ndash;340.
Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica, 55, 277&ndash;301.
Phillips, P. C. B, &amp; Perron, P. (1988). Testing for a unit root in time series regression. Biometrika,
</p>
<p>75, 335&ndash;346.
Said, S. E., &amp; Dickey, D. A. (1984). Testing for unit roots in autoregressive-moving average models
</p>
<p>of unknown order. Biometrika, 71, 599&ndash;607.</p>
<p/>
</div>
<div class="page"><p/>
<p>16Cointegration Analysis
</p>
<p>16.1 Summary
</p>
<p>This chapter is addressed to the analysis of cointegrated variables. Properties like
</p>
<p>superconsistency of the LS estimator and conditions for asymptotic normality are
</p>
<p>extensively discussed. Error-correction is the reverse of cointegration, which is
</p>
<p>why we provide an introduction to the analysis of error-correction models as well.
</p>
<p>In particular, we discuss cointegration testing. In 2003, Clive W.J. Granger was
</p>
<p>awarded the Nobel prize for introducing the concept of cointegration. Finally, we
</p>
<p>stress once more the effect of linear time trends underlying the series.
</p>
<p>16.2 Error-Correction and Cointegration
</p>
<p>Before cointegration had been launched, so-called error-correction models
</p>
<p>impressed due to their empirical performance, cf. e.g. Davidson, Hendry, Srba,
</p>
<p>and Yeo (1978). Today we know that these models are just the other side of
</p>
<p>the cointegration coin. By way of example, the key statement of Granger&rsquo;s
</p>
<p>representation theorem from Engle and Granger (1987) is illustrated, which
</p>
<p>demonstrates the fact that error-correction and cointegration are equivalent.
</p>
<p>Autoregressive Distributed LagModel
</p>
<p>Let us consider a dynamic regression model in which fytg has an autoregressive
structure on the one hand and which is explained by (lagged) exogenous variables
</p>
<p>xt�j on the other:
</p>
<p>yt D a1 yt�1 C � � � C ap yt�p C c0 xt C c1 xt�1 C � � � C c` xt�` C "t :
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1_16
</p>
<p>353</p>
<p/>
</div>
<div class="page"><p/>
<p>354 16 Cointegration Analysis
</p>
<p>Hence, this is an extension of the AR(p) process. Because of the additional
</p>
<p>exogenous explanatory variables, we sometimes speak of ARX(p, `) models,
</p>
<p>although such processes are more often called autoregressive distributed lag
</p>
<p>models, ARDL(p, `). We assume that fxtg is integrated of order one. In order to have
cointegration with fytg, the ARDL model has to be stable. We adopt the stability
condition from Proposition 3.4:
</p>
<p>1 � a1 z � � � � � ap zp D 0 ) jzj &gt; 1 :
</p>
<p>Example 16.1 (ARDL(2,2)) With p D ` D 2 we consider
</p>
<p>yt D a1 yt�1 C a2 yt�2 C c0 xt C c1 xt�1 C c2 xt�2 C "t :
</p>
<p>Due to the assumed stability, the parameter b can be defined:
</p>
<p>b WD c0 C c1 C c2
1 � a1 � a2
</p>
<p>:
</p>
<p>In fact, the denominator is not only different from zero but positive if stability is
</p>
<p>given (which we again know from Proposition 3.4): 1 � a1 � a2 &gt; 0. Thus, the
following parameter &#13; is negative:
</p>
<p>&#13; WD �.1 � a1 � a2/ &lt; 0 :
</p>
<p>Elementary manipulations lead to the reparameterization (cf. Problem 16.1)
</p>
<p>�yt D &#13; Œ yt�1 � b xt�1&#141; � a2�yt�1 C c0�xt � c2�xt�1 C "t : (16.1)
</p>
<p>In this equation differences of y are related to their own lags and differences of
</p>
<p>x. In addition, they depend on a linear combination of lagged levels (in square
</p>
<p>brackets). This last aspect is the one constituting error-correction models. The
</p>
<p>cointegration relation y D b x is understood as a long-run equilibrium relation
and vt�1 D yt�1 � b xt�1 as a deviation from it in t � 1. This deviation from the
equilibrium again influences the increments of yt. The involved linear combination
</p>
<p>of yt�1 and xt�1 needs to be stationary because f�ytg is stationary by assumption.
Hence, in the example it is obvious that such a relation between differences and
</p>
<p>levels implies cointegration. Indeed, it is the lagged deviation from the equilibrium
</p>
<p>vt�1 influencing the increments of�yt with a negative sign. If yt�1 is greater than the
equilibrium value, vt�1 &gt; 0, then this affects the change from yt�1 to yt negatively,
i.e. y is corrected towards the equilibrium, and vice versa for values below the
</p>
<p>equilibrium value. What economists know as deviation from the equilibrium is
</p>
<p>called &ldquo;error&rdquo; (in the sense of a deviation from a set target) in engineering, which
</p>
<p>explains the name error-correction model. �</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Error-Correction and Cointegration 355
</p>
<p>The example can be generalized. Cointegrated ARDL models of arbitrary order can
</p>
<p>always be formulated as error-correction models. This is not surprising against the
</p>
<p>background of Granger&rsquo;s representation theorem.
</p>
<p>Granger&rsquo;s Representation Theorem
</p>
<p>Error-correction adjustment is the downside to cointegration. The relation between
</p>
<p>cointegration and error-correction is explained by the following proposition where
</p>
<p>we, however, will not spell out all the technical details. The result goes back to
</p>
<p>Granger, cf. Engle and Granger (1987) or Johansen (1995, Theorem 4.2).
</p>
<p>Proposition 16.1 (Representation Theorem) Let fytg and fxtg be integrated of
order one. They are cointegrated if and only if they have an error-correction
</p>
<p>representation,
</p>
<p>�yt D &#13; vt�1 C
pX
</p>
<p>jD1
aj�yt�j C
</p>
<p>X̀
</p>
<p>jD1
˛j �xt�j C "t ; (16.2)
</p>
<p>�xt D &#13;x vt�1 C
pxX
</p>
<p>jD1
a
.x/
j �yt�j C
</p>
<p>`xX
</p>
<p>jD1
˛
.x/
j �xt�j C "x;t ; (16.3)
</p>
<p>vt D yt � b xt � I.0/ ; b &curren; 0 ; (16.4)
</p>
<p>where at least one of the so-called adjustment coefficients &#13; or &#13;x is different from
</p>
<p>zero.
</p>
<p>Of course, not all aj (a
.x/
j ) and ˛j (˛
</p>
<p>.x/
j ) need to be different from zero. The error
</p>
<p>sequences f"tg and f"x;tg are white noise and may be contemporaneously correlated.
Frequently in practice, additional contemporaneous differences of the respective
</p>
<p>other variable are hence incorporated on the right-hand side. For Eq. (16.2) this
</p>
<p>means e.g. the inclusion of ˛0�xt. Then, one sometimes also speaks of the
</p>
<p>conditional or structural error-correction equation.
</p>
<p>Cointegration and the Long-Run Variance Matrix
</p>
<p>In (14.6) the symmetric long-run variance matrix ˝ of a stationary vector has been
</p>
<p>defined. We now consider an I(1) vector z0t D .z1;t; z2;t/ such that �zt D .w1;t;w2;t/0
is integrated of order zero, which is why ˝ cannot be equal to the zero matrix.
</p>
<p>Nevertheless, the matrix does not have to be invertible. It rather holds: If the
</p>
<p>vector fztg is cointegrated, then ˝ has the reduced rank one and is not invertible.
Equivalently, this means: If˝ is invertible, then fz1;tg and fz2;tg are not cointegrated.
To this, we consider the following example.</p>
<p/>
</div>
<div class="page"><p/>
<p>356 16 Cointegration Analysis
</p>
<p>Example 16.2 (˝ under Cointegration) In this example, let fz2;tg be a random
walk, the cointegration parameter be one, and the deviation from the equilibrium
</p>
<p>vt D "1;t be iid and independent of f�z2;tg:
</p>
<p>z1;t D z2;t C "1;t ;
</p>
<p>z2;t D z2;t�1 C "2;t :
</p>
<p>Hence, if f"1;tg and f"2;tg are independent with variances �21 and �22 , then one shows
with w1;t D �z1;t D "2;t C "1;t � "1;t�1 and w2;t D �z2;t D "2;t:
</p>
<p>&#13;w.0/ D
�
�22 C 2 �21 �22
</p>
<p>�22 �
2
2
</p>
<p>�
and &#13;w.1/ D
</p>
<p>�
��21 0
0 0
</p>
<p>�
:
</p>
<p>For h &gt; 1, &#13;w.h/ D 0. Thus, it holds:
</p>
<p>˝w D �22
�
1 1
</p>
<p>1 1
</p>
<p>�
;
</p>
<p>i.e. the matrix is of rank one and not invertible. �
</p>
<p>Conversely, it holds as well that full rank of ˝ follows from the absence of
</p>
<p>cointegration. This is to be illustrated by the following example.
</p>
<p>Example 16.3 (˝ without Cointegration) Now, let fz1;tg and fz2;tg be two random
walks independent of each other:
</p>
<p>z1;t D z1;t�1 C "1;t ;
z2;t D z2;t�1 C "2;t :
</p>
<p>Since f"1;tg and f"2;tg are independent with variances �21 and �22 , one shows with
w1;t D "1;t and w2;t D "2;t:
</p>
<p>&#13;w.0/ D
�
�21 0
</p>
<p>0 �22
</p>
<p>�
:
</p>
<p>For h &gt; 0, &#13;w.h/ D 0. Thus, it holds:
</p>
<p>˝w D &#13;w.0/ ;
</p>
<p>where this matrix has full rank 2 in the case of positive variances and is thus
</p>
<p>invertible. �
</p>
<p>Hence, the presence of cointegration of the I(1) vector fztg depends on the matrix
˝ . The examples show that no cointegration of fztg is equivalent to the full rank of
˝ , cf. Phillips (1986).</p>
<p/>
</div>
<div class="page"><p/>
<p>16.2 Error-Correction and Cointegration 357
</p>
<p>Linearly Independent Cointegration Vectors
</p>
<p>For more than two I(1) variables linearly independent cointegration vectors can
</p>
<p>exist. In such a situation there can be no talk of &ldquo;the true&rdquo; cointegration vector. Each
</p>
<p>and every linear combination of independent cointegration vectors is itself again a
</p>
<p>cointegration vector. Although this cannot occur under our assumption of a bivariate
</p>
<p>vector, we want to become aware of this problem by means of a three-dimensional
</p>
<p>example.
</p>
<p>Example 16.4 (Three Interest Rates) Assume z1, z2 and z3 to be interest rates for
</p>
<p>one-month, two-month and three-month loans integrated of order one. Then one
</p>
<p>expects (due to the expectations hypothesis of the term structure) the interest rate
</p>
<p>differentials z1 � z2 and z2 � z3 to provide stable relations:
</p>
<p>z1;t � z2;t D v1;t � I.0/;
z2;t � z3;t D v2;t � I.0/:
</p>
<p>Here, b01 D .1;�1; 0/ and b02 D .0; 1;�1/ are linearly independent, and both are
cointegrating vectors for z0t D .z1;t; z2;t; z3;t/ as v1;t and v2;t are both assumed to be
I(0):
</p>
<p>�
b01
b02
</p>
<p>�0
@
</p>
<p>z1;t
</p>
<p>z2;t
</p>
<p>z3;t
</p>
<p>1
A D
</p>
<p>�
v1;t
v2;t
</p>
<p>�
:
</p>
<p>Hence, the uniqueness of the cointegration vector is lost. It is rather that b1 and b2
form a basis for the cointegration space. Each vector contained in the plane they
</p>
<p>span, i.e. each linear combination of b1 and b2, is itself again a cointegration vector.
</p>
<p>For example for
</p>
<p>ˇ˛ D b1 C .1� ˛/b2 D
</p>
<p>0
@
</p>
<p>1
</p>
<p>�˛
˛ � 1
</p>
<p>1
A
</p>
<p>one obtains a stationary relation comprising all three variables from zt,
</p>
<p>z1;t D ˛z2;t C .1 � ˛/z3;t C v1;t C .1 � ˛/v2;t ;
</p>
<p>where v1;t C .1 � ˛/v2;t is I(0). In particular for ˛ D 0 one sees that z1;t and z3;t
alone are also cointegrated with the cointegration vector ˇ00 D b01Cb02 D .1; 0;�1/.
The cointegration vectors b1, b2 as well as ˇ0 provide economically reasonable and
</p>
<p>theoretically secured statements on the interest rate differentials. However, ˇ˛ for</p>
<p/>
</div>
<div class="page"><p/>
<p>358 16 Cointegration Analysis
</p>
<p>an arbitrary ˛ is just as good a cointegrating vector. Hence,
</p>
<p>z1 D ˛z2 C .1 � ˛/z3
</p>
<p>for ˛ &curren; 1 or ˛ &curren; 0 is as well a &ldquo;true&rdquo; long-run equilibrium relation, even if, in
contrast to the interest rate differentials, it is not readily amenable to an economic
</p>
<p>interpretation. These problems with the interpretation of more than one linearly
</p>
<p>independent cointegration vectors are of a fundamental nature and cannot be solved
</p>
<p>unless one makes a priori (economically plausible) assumptions on the form of the
</p>
<p>cointegrating vectors. The cointegration analysis is not the life belt saving us from
</p>
<p>the lack of identification: With purely statistical methods one generally cannot make
</p>
<p>economic statements. �
</p>
<p>For only two I(1) variables fytg and fxtg, linearly independent cointegration vectors
cannot exist. We show this by contradiction. Hence, let us assume with b1 &curren; b2
from R that two linearly independent relations exist. We collect them row-wise in
</p>
<p>the matrix B:
</p>
<p>B D
�
1 �b1
1 �b2
</p>
<p>�
:
</p>
<p>Then, it holds by assumption that
</p>
<p>B
</p>
<p>�
yt
</p>
<p>xt
</p>
<p>�
D
�
</p>
<p>yt � b1xt
yt � b2xt
</p>
<p>�
D
�
v1;t
</p>
<p>v2;t
</p>
<p>�
</p>
<p>is a vector of I(0) variables fv1;tg and fv2;tg. Due to the independence of the
cointegration vectors, B is invertible:
</p>
<p>�
yt
</p>
<p>xt
</p>
<p>�
D B�1
</p>
<p>�
v1;t
v2;t
</p>
<p>�
:
</p>
<p>This yields fytg and fxtg as linear combinations of fvi;tg, i D 1; 2, from which
it follows that fytg and fxtg themselves have to be I(0), which contradicts the
assumption.
</p>
<p>16.3 Cointegration Regressions
</p>
<p>In the case of bivariate cointegration, the LS estimator of a static regression of yt
on xt tends to the true value with the sample size (i.e. with rate n). For this fast rate
</p>
<p>of convergence the term superconsistency has been coined in the literature. At the
</p>
<p>same time limiting normality only arises under additional assumptions.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Cointegration Regressions 359
</p>
<p>Superconsistent Estimation
</p>
<p>We consider the LS estimator regressing yt on xt under the assumption (16.4) that
</p>
<p>cointegration is present. For the sake of simplicity, we again do not allow for an
</p>
<p>intercept (which is why we assume that the I(1) processes have the starting value
</p>
<p>zero):
</p>
<p>yt D Ob xt C Ovt ; t D 1; : : : ; n : (16.5)
</p>
<p>Then we write for the LS estimator:
</p>
<p>Ob � b D
</p>
<p>nP
tD1
</p>
<p>xtvt
</p>
<p>nP
tD1
</p>
<p>x2t
</p>
<p>or
</p>
<p>n
�
Ob � b
</p>
<p>�
D
</p>
<p>n�1
nP
</p>
<p>tD1
xtvt
</p>
<p>n�2
nP
</p>
<p>tD1
x2t
</p>
<p>:
</p>
<p>To be able to apply the functional limit theory from Chap. 14, we now define
</p>
<p>wt D
�
vt
�xt
</p>
<p>�
; i.e. zt D
</p>
<p>0
@
</p>
<p>tP
iD1
vi
</p>
<p>xt
</p>
<p>1
A (16.6)
</p>
<p>instead of z0t D .yt; xt/ as in Sect. 15.4 without cointegration. Then it holds with the
results from Proposition 14.4(b) and (c):
</p>
<p>n.Ob � b/ d�!
</p>
<p>1R
0
</p>
<p>B2.s/dB1.s/C
1P
</p>
<p>hD0
E.�xtvtCh/
</p>
<p>1R
0
</p>
<p>B22.s/ds
</p>
<p>:
</p>
<p>As the LS estimator tends to the true value with the sample size n instead of with
</p>
<p>only n0:5 as for the stationary regression model, since Stock (1987) and Engle and
</p>
<p>Granger (1987) it has become common usage to speak of superconsistency of the
</p>
<p>static cointegration estimator from (16.5); however, this result has been known from
</p>
<p>Phillips and Durlauf (1986) already.
</p>
<p>Note that the estimation of b is consistent despite possible correlation between
</p>
<p>error term vt and regressor xt (or �xt). Insofar the cointegration regression knocks
</p>
<p>out the simultaneity bias (or &ldquo;Haavelmo bias&rdquo;): Superconsistency is a strong</p>
<p/>
</div>
<div class="page"><p/>
<p>360 16 Cointegration Analysis
</p>
<p>asymptotic argument for single equation regressions despite possibly existing
</p>
<p>dependencies through simultaneous relations between the individual equations,
</p>
<p>i.e. despite correlation between regressors and error term. According to this, the
</p>
<p>cointegration approach can be understood as a reaction to the simultaneous equation
</p>
<p>methodology of former decades. At the same time, it is not clear anymore which
</p>
<p>variable constitutes the endogenous left-hand side and which quantity identifies the
</p>
<p>exogenous regressor. Beside (16.4), it also holds as a &ldquo;true relation&rdquo; that
</p>
<p>xt D
yt
</p>
<p>b
� vt
</p>
<p>b
:
</p>
<p>Hence, if xt is regressed on yt, then one would obtain analogously a superconsistent
</p>
<p>estimator for b�1. This vehemently contrasts the results of the stationary standard
econometrics where the (asymptotic) validity of LS crucially depends on the correct
</p>
<p>specification of the single equation and exogeneity assumptions.
</p>
<p>Further Asymptotic Properties
</p>
<p>In Problems 16.2 and 16.3 we prove the further properties of the following
</p>
<p>proposition. Here, the standard errors of the t-statistic, the uncentered coefficient
</p>
<p>of determination and the Durbin-Watson statistic are defined as in Sect. 15.4.
</p>
<p>Proposition 16.2 (CointegrationRegression) For cointegrated I(1) processes fxtg
and fytg it holds with (16.4) and the notation introduced that
</p>
<p>.a/ n.Ob � b/ d�!
1R
0
</p>
<p>B2.s/dB1.s/C�xv
1R
0
</p>
<p>B22.s/ds
</p>
<p>;
</p>
<p>.b/ n.1 � R2uc/
d! &#13;1.0/
</p>
<p>b2
1R
0
</p>
<p>B22.s/ds
</p>
<p>;
</p>
<p>.c/ s2
p! &#13;1.0/ ;
</p>
<p>.d/ tb D Ob�bsb
d!
</p>
<p>1R
0
</p>
<p>B2.s/dB1.s/C�xv
s
&#13;1.0/
</p>
<p>1R
0
</p>
<p>B22.s/ds
</p>
<p>;
</p>
<p>.e/ dw
p! 2 .1� �v.1// ;
</p>
<p>where
</p>
<p>�xv WD
1X
</p>
<p>hD0
E.�xtvtCh/ and &#13;1.0/ WD Var.vt/
</p>
<p>as n ! 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Cointegration Regressions 361
</p>
<p>Three remarks are to help with the interpretation. (i) Note again that supercon-
</p>
<p>sistency holds even if the regressors xt correlate with the error terms vt. This
</p>
<p>nice property, however, comes at a price: Unfortunately, the limiting distributions
</p>
<p>from Proposition 16.2(a) and (d) are (without further assumptions) generally not
</p>
<p>Gaussian. (ii) As a rule, for regressions involving trending (integrated) variables
</p>
<p>one empirically observes values of the coefficient of determination near one, which
</p>
<p>is explained by Proposition 16.2(b). Due to that, for trending (integrated) time
</p>
<p>series the coefficient of determination cannot be interpreted as usual: Since fytg
does not have a constant variance, the coefficient of determination does not give
</p>
<p>the percentage of the variance explained by the regression. (iii) At no point it was
</p>
<p>assumed that the error terms, fvtg, are white noise. For the first order residual
autocorrelation, it holds
</p>
<p>O� Ov.1/ D
Pn�1
</p>
<p>tD1 Ovt OvtC1Pn
tD1 Ov2t
</p>
<p>p! �v.1/ D
E.vtvtC1/
</p>
<p>Var.vt/
;
</p>
<p>which is just reflected by the behavior of the Durbin-Watson statistic.
</p>
<p>Asymptotic Normality
</p>
<p>The price for the superconsistency without exogeneity assumption is that the
</p>
<p>limiting distribution of the t-statistic is generally not Gaussian anymore. However,
</p>
<p>if vt and �xs are stochastically independent for all t and s, then Kr&auml;mer (1986)
</p>
<p>shows that asymptotic normality arises. This assumption, however, is stronger than
</p>
<p>necessary. It suffices to require, first, �xv D 0 and, second, that the Brownian
motions B1 and B2 are independent. For independence of B1 and B2 we only need
</p>
<p>!12 D
1X
</p>
<p>hD�1
E.�xtvtCh/ D 0 (16.7)
</p>
<p>as under this condition it holds that Bi D !iWi. Due to Proposition 10.4 the
following corollary is obtained (also cf. Problem 16.4).
</p>
<p>Corollary 16.1 (Asymptotic Normality) If�xv from Proposition 16.2 is zero, then
</p>
<p>it holds under (16.7) that
</p>
<p>tb
d! N
</p>
<p>�
0;
</p>
<p>!21
&#13;1.0/
</p>
<p>�
;
</p>
<p>as n ! 1.
</p>
<p>If one has consistent estimators for the variance and the long-run variance, then
</p>
<p>the t-statistic can be modified as follows and can be applied with standard normal</p>
<p/>
</div>
<div class="page"><p/>
<p>362 16 Cointegration Analysis
</p>
<p>distribution asymptotics under Corollary 16.1:
</p>
<p>�b WD
s
</p>
<p>O&#13;1.0/
O!21
</p>
<p>tb
d! N .0; 1/:
</p>
<p>For the estimation of the long-run variance, we refer to the remarks in Sect. 15.2, in
</p>
<p>particular Example 15.2. As fvtg itself is not observable, O&#13;1.0/ and O!21 have to be
calculated from Ovt D yt � Obxt.
</p>
<p>Efficient Estimation
</p>
<p>The assumptions �xv D 0 and !12 D 0 required for Corollary 16.1 are often not
met in practice. We now consider modifications of LS resulting in limiting normality
</p>
<p>that get around such restrictions. To that end we have a closer look at the LS limit
</p>
<p>from Proposition 16.2 called now L.Ob/:
</p>
<p>n .Ob � b/ d! L.Ob/ :
</p>
<p>We define the process B1�2,
</p>
<p>B1�2.s/ WD B1.s/ � !12!�22 B2.s/; (16.8)
</p>
<p>in such a way that it does not correlate with B2:
</p>
<p>E.B1�2.r/B2.s// D min.r; s/!12 � min.r; s/!12!�22 !22 D 0:
</p>
<p>Because of normality the two processes are thus independent. The variance of the
</p>
<p>new process is
</p>
<p>Var.B1�2.s// D E.B21�2.s// D s!21�2 ; !21�2 WD !21 � !212!�22 :
</p>
<p>With
</p>
<p>B1.s/ D B1�2.s/C B2.s/!�22 !12
</p>
<p>we may now decompose the LS limit as follows:
</p>
<p>L.Ob/ D
�Z 1
</p>
<p>0
</p>
<p>B22.s/ds
</p>
<p>��1 �Z 1
</p>
<p>0
</p>
<p>B2.s/dB1�2.s/C
Z 1
</p>
<p>0
</p>
<p>B2.s/dB2.s/!
�2
2 !12 C�xv
</p>
<p>�
</p>
<p>D L1 C L2 C L3 :</p>
<p/>
</div>
<div class="page"><p/>
<p>16.3 Cointegration Regressions 363
</p>
<p>The first component, L1, is conditionally normal with mean zero, i.e.
</p>
<p>L1jB2 � N
 
0; !21�2
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>B22.s/ds
</p>
<p>��1!
;
</p>
<p>which is true by Proposition 10.4. The second component defined as a multiple ofR 1
0
</p>
<p>B2.s/dB2.s/ D .B22.1/ � 1/=2 is stochastic and introduces skewness into L.Ob/,
while finally this distribution is shifted deterministically by �xv . Consequently, for
</p>
<p>arbitrary " &gt; 0 one has
</p>
<p>P.jL1j &lt; "/ &gt; P.jL.Ob/j &lt; "/ ;
</p>
<p>see Saikkonen (1991, Theorem 3.1) Saikkonen. In that sense, L1 is more concen-
</p>
<p>trated around zero than L.Ob/ in general, such that intuitively the LS estimator is
closest to zero for !12 D �xv D 0. This is the intuition for the following definition:
Let ObC denote a cointegration estimator with
</p>
<p>n .ObC � b/ d! L1 where L1jB2 � N
 
0; !2
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>B22.s/ds
</p>
<p>��1!
; (16.9)
</p>
<p>for some positive constant !; then ObC is said to be efficient (see Saikkonen 1991 for
a more general discussion). To further justify this notion of efficiency we note that
</p>
<p>full information maximum likelihood estimation of a cointegrated system results in
</p>
<p>exactly this distribution, see Phillips (1991).
</p>
<p>Efficient cointegration regressions are not only interesting because they achieve
</p>
<p>the lower bound for the standard error; more importantly, related t-type statistics
</p>
<p>are asymptotically normal, which allows for standard inference. Suppose we have
</p>
<p>an efficient estimator satisfying (16.9), and that O! (typically computed from
cointegration residuals) is consistent for !. Then we define the t-type statistic
</p>
<p>tC D
ObC � b
</p>
<p>O!
</p>
<p>vuut
nX
</p>
<p>tD1
x2t :
</p>
<p>From the previous discussion it follows, see Phillips and Park (1988):
</p>
<p>tC
d! N .0; 1/ :
</p>
<p>Consequently, the caveat of superconsistent LS cointegration regression, lacking
</p>
<p>normality in general, is overcome by efficient estimators.
</p>
<p>Let us repeat once more: LS cointegration estimation is efficient under the not
</p>
<p>very realistic assumption that !12 D �xv D 0. Several modifications of LS</p>
<p/>
</div>
<div class="page"><p/>
<p>364 16 Cointegration Analysis
</p>
<p>achieving efficiency without this assumption have been proposed. First, the so-
</p>
<p>called dynamic LS estimator suggested independently by Saikkonen (1991) and
</p>
<p>Stock and Watson (1993) is settled in the time domain, see also Phillips and Loretan
</p>
<p>(1991); second, so-called frequency domain based modifications of LS have been
</p>
<p>suggested by Phillips and Hansen (1990) (&ldquo;fully modified LS&rdquo;) or Park (1992)
</p>
<p>(&ldquo;canonical cointegrating regression&rdquo;); they all meet (16.9) and tC � N .0; 1/,
asymptotically.
</p>
<p>Linear Time Trends
</p>
<p>In Sect. 15.2 we considered linear time trends and I(1) processes, i.e. so-called
</p>
<p>integrated processes with drift:
</p>
<p>xt D �C xt�1 C et ; � &curren; 0: (16.10)
</p>
<p>By repeated substitution one obtains
</p>
<p>xt D x0 C � t C
tX
</p>
<p>jD1
ej;
</p>
<p>i.e. fxtg consists of a linear trend of the slope � and an I(1) component; and of a
starting value whose influence can be neglected such that we set x0 D 0 w.l.o.g.
In addition, let the cointegration relation (16.4) hold true. Consequently, fytg as
well exhibits a linear trend of the slope b�. The cointegration relation (16.4)
</p>
<p>simultaneously eliminates the deterministic linear time trend and the stochastic I(1)
</p>
<p>trend from both series. In this case the static LS regression from (16.5) yields an
</p>
<p>even faster rate of convergence (n1:5 instead of n) and simultaneously, the limiting
</p>
<p>distribution is Gaussian. The following proposition is a special case of the more
</p>
<p>general results from West (1988). We prove it in Problem 16.5.
</p>
<p>Proposition 16.3 (West) We assume cointegrated I(1) processes fxtg and fytg with
drift (i.e. (16.10) with (16.4)). Then it holds for the regression (16.5) without
</p>
<p>intercept that
</p>
<p>n1:5.Ob � b/ d! N
�
0;
</p>
<p>3!21
�2
</p>
<p>�
</p>
<p>as n ! 1, where !21 is the long-run variance of fvtg from (16.4).
</p>
<p>For the sake of completeness, note that the asymptotics from Proposition 16.3 is
</p>
<p>qualitatively retained if the regression is run with intercept. However, the variance of
</p>
<p>the Gaussian distribution is changed. It becomes 12!21=�
2. For practical inference,
</p>
<p>�2 has to be estimated from fxtg or f�xtg, while !21 can be estimated consistently</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Cointegration Testing 365
</p>
<p>from the cointegration residuals Ovt. For � D 1 the limiting distribution from
Proposition 16.3 equals that from (15.5), which is not coincidental, see also the
</p>
<p>proof in Problem 16.5: The scalar I(1) regressor with drift is dominated by the
</p>
<p>linear time trend; hence, the cointegration regression amounts to a trend stationary
</p>
<p>regression.
</p>
<p>Note, however, that Proposition 16.3 holds only in our special case that xt is a
</p>
<p>scalar I(1) variable. If one has a vector of I(1) regressors with drift instead of a
</p>
<p>scalar variable, then the asymptotic normality in general does not hold anymore,
</p>
<p>and the very fast convergence with rate n1:5 is lost as well. Instead, Hansen (1992)
</p>
<p>proved results in line with Proposition 16.2 if there are several I(1) regressors of
</p>
<p>which at least one has a drift.
</p>
<p>16.4 Cointegration Testing
</p>
<p>If one regresses nonstationary (integrated) time series on each other, the inter-
</p>
<p>pretation of the regression outcome largely depends on whether the series are
</p>
<p>cointegrated or not. Hence, one has to test for the absence or presence of cointe-
</p>
<p>gration.
</p>
<p>Residual-Based Dickey-Fuller Test
</p>
<p>The idea of the following test for the null hypothesis of no cointegration dates
</p>
<p>back to Engle and Granger (1987), although a rigorous asymptotic treatment was
</p>
<p>provided later by Phillips and Ouliaris (1990). The idea is very simple. Without
</p>
<p>cointegration any linear combination of I(1) variables results in a series that too
</p>
<p>has a unit root. Hence, the Dickey-Fuller test is applied to LS residuals, which are
</p>
<p>computed from a regression with intercept:
</p>
<p>Out D yt � Ǫ � Ǒxt :
</p>
<p>Due to the included intercept, fOutg are zero mean by construction. Hence, the DF
regression in the second step may be run w.l.o.g. without intercept:
</p>
<p>Out D a Out�1 C et ; t D 1; : : : ; n :
</p>
<p>The LS estimator a converges to 1 under the null hypothesis of a residual unit root
</p>
<p>with the rate known from Sect. 15.3. However, Ǒ does not converge to 0, but a
limit characterized in Proposition 15.5. Consequently, the asymptotic distribution of
</p>
<p>n.a � 1/ does not only depend on one WP but rather on two. Let ta.2/, involving 2
I(1) processes, denote the t-statistic related to a � 1. Then the limit depends on two</p>
<p/>
</div>
<div class="page"><p/>
<p>366 16 Cointegration Analysis
</p>
<p>standard Wiener processes W1 and W2:
</p>
<p>ta.2/
d! DF .W1;W2/ : (16.11)
</p>
<p>Interestingly, this limit is free of nuisance parameters as long as f�ytg and f�xtg
are white noise; in particular, it does not depend on the eventual correlation
</p>
<p>between f�ytg and f�xtg; see also Problem 16.7. If f�ytg and f�xtg are not white
noise processes, then a may be computed from a lag-augmented regression, or a
</p>
<p>modification to the test statistic in line with Phillips (1987) has to be applied. Critical
</p>
<p>values or p-values are most often taken from MacKinnon (1991, 1996). Here, we do
</p>
<p>not present a definition of the functional shape of the limit DF .W1;W2/; rather, to
</p>
<p>give at least an idea thereof, we consider now explicitly the less complicated case
</p>
<p>without constant. So, Out and Ǒ are now from a regression without intercept,
</p>
<p>Out D yt � Ǒxt :
</p>
<p>We use a new notation to denote the subsequent DF regression
</p>
<p>Out D Ma Out�1 C M"t ; t D 1; : : : ; n :
</p>
<p>In Problem 16.7 the following result is given.
</p>
<p>Proposition 16.4 (Phillips &amp; Ouliaris) Let fztg with z0t D .yt; xt/ be a random
walk such that f�ytg and f�xsg are white noise processes not correlated for t &curren; s,
although we do allow for contemporaneous correlation. In case of no cointegration
</p>
<p>it holds with the notation introduced above that
</p>
<p>n.Ma � 1/ d!
R 1
0
</p>
<p>U.t/ dU.t/
R 1
0
</p>
<p>U2.t/ dt
; n ! 1 ;
</p>
<p>with
</p>
<p>U.t/ WD W1.t/ �
R 1
0 W1.s/W2.s/ dsR 1
</p>
<p>0 W
2
2 .s/ ds
</p>
<p>W2.t/ ;
</p>
<p>where W1 and W2 are two independent Wiener processes.
</p>
<p>The functional shape of this limit corresponds exactly to the one in (1.11); only that
</p>
<p>the WP W is replaced by U, which is, however, no longer a WP. Not surprisingly,
</p>
<p>the new process U is defined as residual from a projection of W1 (corresponding to
</p>
<p>y) on W2 (corresponding to x). Once more this shows the power and elegance of the
</p>
<p>functional limit theory approach introduced in Chap. 14.</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Cointegration Testing 367
</p>
<p>Residual-Based KPSS Test
</p>
<p>It comes in natural to apply also the KPSS test to regression residuals. As in
</p>
<p>Sect. 15.3 the hypotheses are now exchanged: We test for the null hypothesis
</p>
<p>of cointegration against the alternative of no cointegration. Working with LS
</p>
<p>cointegration residuals, Ovt D yt � Obxt, we have under the null hypothesis
</p>
<p>Ovt D vt � .Ob � b/ xt ;
</p>
<p>where
</p>
<p>n .Ob � b/ d! b1
</p>
<p>with the limiting distribution b1 characterized in Proposition 16.2. Interestingly, we
observe by a FCLT (Proposition 14.1) that
</p>
<p>n0:5 .Ob � b/ xbrnc ) b1B2.r/ ;
</p>
<p>with B2 being the Brownian motion behind fxtg from z0t D
�Pt
</p>
<p>jD1 vj; xt
�
</p>
<p>. Therefore,
</p>
<p>it holds that .Ob�b/ xt converges to zero with growing sample size, and the empirical
residuals are proxies of the unobserved cointegration deviation: Ovt � vt. Hence, it
is tempting to believe that a KPSS test applied to the sequence f Ovtg behaves as if
applied to fvtg. This, however, is not correct, as we will demonstrate next, since
the limit characterized in Proposition 15.4 is not recovered when working with
</p>
<p>cointegration residuals.
</p>
<p>The KPSS test builds on the partial sum process St D
Pt
</p>
<p>jD1 Ovj. Mimicking the
proof of Proposition 14.2(a) in Problem 14.3, we obtain the following FCLT for the
</p>
<p>partial sum process:
</p>
<p>n�0:5Sbrnc D n�0:5
brncX
</p>
<p>jD1
vj � n .Ob � b/ n�1:5
</p>
<p>brncX
</p>
<p>jD1
xj
</p>
<p>) B1.r/ � b1
Z r
</p>
<p>0
</p>
<p>B2.s/ ds :
</p>
<p>Notwithstanding that Ovt � vt we must thus not jump at the conclusion that the
residual effect is negligible: The more careful analysis showed that the limit of the
</p>
<p>partial sum process depends on the distribution b1 arising from the cointegration
regression.
</p>
<p>What is more, we know that the LS limit b1 from Proposition 16.2 is plagued by
the nuisance parameters �xv and !12, except for the special case of Corollary 16.1.
</p>
<p>Therefore, Shin (1994) suggested to apply the KPSS test not with LS residuals but</p>
<p/>
</div>
<div class="page"><p/>
<p>368 16 Cointegration Analysis
</p>
<p>with residuals from an efficient regression (now with intercept),
</p>
<p>SCt D
tX
</p>
<p>jD1
OvCj ; OvCj D yt � OaC � ObCxt ;
</p>
<p>where ObC is efficient in the sense of (16.9). Efficient cointegration regressions
rely on removing �xv and !12 consistently. Hence, Shin (1994) showed that the
</p>
<p>limiting distribution of the KPSS test applied to efficient residuals is free of nuisance
</p>
<p>parameters, and he provided critical values. Let �C.2/ denote the residual-based
KPSS statistic building on vCj D yt � OaC � ObCxt, thus involving two I(1) variables.
Under the null hypothesis of cointegration it holds asymptotically that (Shin, 1994,
</p>
<p>Thm. 2)
</p>
<p>�C.2/
d! CM.W1;W2/ ;
</p>
<p>where
</p>
<p>CM.W1;W2/ D
Z 1
</p>
<p>0
</p>
<p>"
W1.s/ � sW1.1/ �
</p>
<p>R s
0
</p>
<p>W2.r/dr
R 1
0
</p>
<p>W2.r/dW1.r/R 1
0
</p>
<p>W22.r/dr
</p>
<p>#2
ds ;
</p>
<p>(16.12)
</p>
<p>and W is again short for a demeaned Wiener process. For related work see Harris and
</p>
<p>Inder (1994) or Leybourne and McCabe (1994), although the latter paper considered
</p>
<p>only the case of LS residuals.
</p>
<p>Error-Correction Test
</p>
<p>The third test we look into is not residual-based. The analysis rather relies on the
</p>
<p>error-correction equation (16.2):
</p>
<p>�yt D &#13; vt�1 C differences C "t :
</p>
<p>The fact that we restrict the analysis to the error-correction equation of fytg and that
we ignore Eq. (16.3) has to be justified by the assumption
</p>
<p>&#13;x D 0 : (16.13)
</p>
<p>This assumptions implies that, when cointegration is present, only �yt reacts
</p>
<p>to the deviation from the equilibrium of the previous period. Because of the
</p>
<p>assumption (16.13), absence of cointegration means &#13; D 0, which is the null
hypothesis. In order that there is an adjustment to the equilibrium in the case of</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Cointegration Testing 369
</p>
<p>cointegration, &#13; &lt; 0 under the alternative:
</p>
<p>H0 W &#13; D 0 vs. H1 W &#13; &lt; 0:
</p>
<p>To provide further intuition for the test statistic, we rewrite the error-correction
</p>
<p>equation (16.2) by inserting the definition of vt�1:
</p>
<p>�yt D &#13;.yt�1 � bxt�1/C
pX
</p>
<p>jD1
aj�yt�j C
</p>
<p>X̀
</p>
<p>jD1
˛j �xt�j C "t
</p>
<p>D &#13;yt�1 C �xt�1 C
pX
</p>
<p>jD1
aj�yt�j C
</p>
<p>X̀
</p>
<p>jD1
˛j �xt�j C "t : (16.14)
</p>
<p>Here, we defined
</p>
<p>� D �&#13;b ;
</p>
<p>where the null hypothesis of course implies � D 0. Hence, one may test the
null hypothesis by means of an F-type test statistic for &#13; D � D 0, which has
been investigated by Boswijk (1994). Alternatively, one may employ a t-type test
</p>
<p>specifically for &#13; D 0 only as proposed by Banerjee, Dolado, and Mestre (1998).
The following proposition characterizes the asymptotic behavior of the LS estimator
</p>
<p>O&#13; , cf. Banerjee, Dolado, and Mestre (1998, Proposition 1). The Wiener processes
W1 and W2 are adopted from Proposition 14.4 with z
</p>
<p>0
t D .yt; xt/. In order to get
</p>
<p>a limiting distribution free of nuisance parameters, we assume that �xt and "s are
</p>
<p>uncorrelated at arbitrary points in time:
</p>
<p>E.�xt"s/ D 0 I (16.15)
</p>
<p>see also the proof in Problem 16.6.
</p>
<p>Proposition 16.5 (BDM) Let the I(1) processes fxtg and fytg be not cointegrated,
and let the exogeneity assumption (16.15) be fulfilled. Then, it holds for the LS
</p>
<p>estimator from the regression (16.14) that
</p>
<p>n O&#13; d!
</p>
<p>1R
0
</p>
<p>W22 .s/ ds
1R
0
</p>
<p>W1 .s/ dW1 .s/ �
1R
0
</p>
<p>W1 .s/W2 .s/ ds
1R
0
</p>
<p>W2 .s/ dW1 .s/
</p>
<p>1R
0
</p>
<p>W22 .s/ds
1R
0
</p>
<p>W21 .s/ds �
 
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p>!2
</p>
<p>as n ! 1.</p>
<p/>
</div>
<div class="page"><p/>
<p>370 16 Cointegration Analysis
</p>
<p>Obviously, this limiting distribution can be reshaped into the following form (which
</p>
<p>lends itself for a multivariate generalization with vectors fxtg):
</p>
<p>n O&#13; d!
</p>
<p>1R
0
</p>
<p>W1.s/dW1.s/ �
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p> 
1R
0
</p>
<p>W22 .s/ds
</p>
<p>!�1
1R
0
</p>
<p>W2.s/dW1.s/
</p>
<p>1R
0
</p>
<p>W21 .s/ds �
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p> 
1R
0
</p>
<p>W22 .s/ds
</p>
<p>!�1
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p>:
</p>
<p>In fact, Banerjee et al. (1998) suggest the computation of the t-statistic relating to
</p>
<p>O&#13; : t&#13; .2/. As limiting distribution under H0 one obtains:
</p>
<p>t&#13; .2/
d! BDM.W1;W2/ (16.16)
</p>
<p>D
</p>
<p>1R
0
</p>
<p>W1.s/dW1.s/ �
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p> 
1R
0
</p>
<p>W22 .s/ds
</p>
<p>!�1
1R
0
</p>
<p>W2.s/dW1.s/
</p>
<p>vuut 1R
0
</p>
<p>W21 .s/ds �
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p> 
1R
0
</p>
<p>W22 .s/ds
</p>
<p>!�1
1R
0
</p>
<p>W1.s/W2.s/ds
</p>
<p>:
</p>
<p>Again, in most practical situations an intercept will be included in (16.14). If the
</p>
<p>corresponding test statistic is called t&#13; .2/, then it holds under the null hypothesis that
</p>
<p>t&#13; .2/
d! BDM.W1;W2/ : (16.17)
</p>
<p>This limit has the same functional shape as BDM.W1;W2/, only that Wi have to be
</p>
<p>replaced by the demeaned analogs W i, i D 1; 2.
Simulated critical values for conducting the tests can be found in Banerjee et al.
</p>
<p>(1998). One rejects for small values. From Ericsson and MacKinnon (2002) p-values
</p>
<p>are available, too.
</p>
<p>Linear Time Trends
</p>
<p>It has been mentioned in this and the previous chapter that in practice one would
</p>
<p>run regressions with an intercept to account for non-zero means of the series. But
</p>
<p>how should one proceed if the mean function follows a linear time trend, i.e. if the
</p>
<p>series are I(1) with drift? This is a quite realistic assumption for many economic and
</p>
<p>financial time series where positive growth rates are plausible. One might consider
</p>
<p>the analysis of detrended data, see Sect. 15.2. Note that the regression of detrended
</p>
<p>series is equivalent to including a linear time trend in the regression (see Frisch &amp;
</p>
<p>Waugh, 1933):
</p>
<p>yt D Ǫ C Oı t C Ǒ xt C Out :</p>
<p/>
</div>
<div class="page"><p/>
<p>16.4 Cointegration Testing 371
</p>
<p>This is why we call such regressions also detrended regressions. Similarly, one
</p>
<p>may augment the error-correction regression (16.14) by a linear time trend (and a
</p>
<p>constant). Many economists, however, do not run detrended regression or detrend
</p>
<p>the series even if the data display a linear time trend by eyeball inspection.
</p>
<p>Economically, it is often more meaningful to &ldquo;explain&rdquo; one trend by another instead
</p>
<p>of regressing deviations from linear trends on each other. Also statistically the
</p>
<p>regression of detrended data may not seem advisable since power losses are to be
</p>
<p>expected (see Hamilton, 1994, Sect. 19.2).
</p>
<p>Running regressions with intercept only, i.e. without detrending, in the presence
</p>
<p>of linear time trends in the regressors has some subtle implications, however. Gener-
</p>
<p>ally, the presence of a linear time trend in the data not accounted for in the regression
</p>
<p>will affect the limiting distributions. Just remember that a simple (or bivariate)
</p>
<p>cointegration regression in the presence of a linear time trend (Proposition 16.3)
</p>
<p>resembles more the detrending of a trend stationary process (Proposition 15.1) than
</p>
<p>a cointegration regression without linear trend (Proposition 16.2). More precisely, a
</p>
<p>linear time trend in fxtg will dominate the stochastic unit root in the following sense:
If fxtg is I(1) with drift, E.�xt/ D �C et, � &curren; 0, or
</p>
<p>xt D x0 C � t C
tX
</p>
<p>jD1
ej ; t D 1; : : : n ;
</p>
<p>then this process grows with rate n (and not n0:5, see Proposition 14.1):
</p>
<p>xbrnc
n
</p>
<p>) 0C � r C 0 ; � &curren; 0 :
</p>
<p>This provides an intuition for the following finding by Hansen (1992, Theorem 7):
</p>
<p>If fxtg is I(1) with drift and fytg and fxtg are not cointegrated, and if a regression-
based DF test for no cointegration is computed from a regression with intercept
</p>
<p>but without detrending, then the limiting distribution of the t-type DF statistic is
</p>
<p>not given by DF .W1;W2/ from (16.11), but rather by the detrended univariate
</p>
<p>distribution eDF given in (15.10). A corresponding result was established for the
</p>
<p>error-correction test by Hassler (2000a): If fxtg and fytg are I(1) but not cointegrated,
and if fxtg is integrated with drift and the error-correction test for no cointegration is
computed from a regression with intercept but without detrending, then the limiting
</p>
<p>distribution of the t-statistic is not given by BDM.W1;W2/ from (16.17), but by
</p>
<p>the detrended Dickey-Fuller distribution eDF . And similarly: If fxtg is I(1) with drift
and cointegrated with fytg, and if a regression-based KPSS test for cointegration
is computed from a regression with intercept only, then the limiting distribution
</p>
<p>of the KPSS statistic is not given by CM.W1;W2/ from (16.12), but rather by the
</p>
<p>detrended univariate distribution eCM given in (15.11); see Hassler (2000b). Hence,
</p>
<p>we have the following proposition.
</p>
<p>Proposition 16.6 (Hansen &amp; Hassler) Consider the test statistics ta.2/, t&#13; .2/
</p>
<p>or �C.2/ computed without detrending to test for the null hypothesis of (no)</p>
<p/>
</div>
<div class="page"><p/>
<p>372 16 Cointegration Analysis
</p>
<p>cointegration of the I.1/ processes fxtg and fytg. With eDF from (15.10) and eCM
from (15.11) it holds under the respective null hypotheses that
</p>
<p>.a/ ta.2/
d�!
(
DF.W1;W2/ ; if E.�xt/ D 0
</p>
<p>eDF ; if E.�xt/ &curren; 0
;
</p>
<p>.b/ t&#13; .2/
d�!
(
BDM.W1;W2/ ; if E.�xt/ D 0
</p>
<p>eDF ; if E.�xt/ &curren; 0
;
</p>
<p>.c/ �C.2/
d�!
(
CM.W1;W2/ ; if E.�xt/ D 0
</p>
<p>eCM ; if E.�xt/ &curren; 0
;
</p>
<p>as n ! 1.
</p>
<p>Proposition 16.6 is not restricted to bivariate regressions, but carries over to the
</p>
<p>general multiple regression case as follows:
</p>
<p>Consider single-equation regressions estimated by LS (or efficient variants
</p>
<p>thereof); regressions with intercept only on k I(1) regressors, of which at least one
</p>
<p>has a drift, result under the null hypothesis (of cointegration or no cointegration,
</p>
<p>respectively) in a limit as if one runs a detrended regression on k�1 I(1) regressors.
For k D 1 this reproduces Proposition 16.6. For a proof for the residual-based DF
</p>
<p>test with k &gt; 1 see again Hansen (1992), and also the lucid discussion by Hamilton
</p>
<p>(1994, p. 596, 597); for a proof for the error-correction test see Hassler (2000a), and
</p>
<p>for the residual-based KPSS test see Hassler (2001) for k &gt; 1.
</p>
<p>In view of Proposition 16.6 one may identify two strategies when testing
</p>
<p>cointegration from regressions with intercept only; we restrict the discussion to
</p>
<p>the case of a scalar regressor xt. First, one might ignore the possibility of linear
</p>
<p>trends and always work with critical values from DF.W1;W2/, BDM.W1;W2/ or
</p>
<p>CM.W1;W2/ provided for the case of regressions with intercept only under no drift;
</p>
<p>we call this strategy SI (I for &ldquo;ignoring&rdquo;), and of course it is not correct if fxtg
displays a linear trend in mean. Second, one may always account for the possibility
</p>
<p>of linear time trends and work with critical values from eDF or eCM; let us call
</p>
<p>this strategy SA (A for &ldquo;account&rdquo;), and note that it is only appropriate if fxtg is
indeed dominated by a linear time trend. In a numerical example we discuss the
</p>
<p>consequences of SI and SA.
</p>
<p>Example 16.5 (Testing under the Suspicion of Time Trends) We consider tests at a
</p>
<p>nominal significance level of 5 %. Let Qc1 and c2 denote critical values from eDF
or eCM and DF.W1;W2/, BDM.W1;W2/ or CM.W1;W2/, respectively. For the
</p>
<p>residual-based DF test by Phillips and Ouliaris (1990) we take asymptotic critical
</p>
<p>values from MacKinnon (1991): Qc1 D �3:41 and c2 D �3:34. Coincidently, these
critical values are not very distant. Strategy SI results in a slightly too liberal test
</p>
<p>(rejecting more often than with 5 % probability) in the presence of a drift, while SA
is mildly conservative (rejecting less often than in 5 % of all cases) in the absence of</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Problems and Solutions 373
</p>
<p>a linear trend in the regressor. So, for the residual-based DF test, Proposition 16.6 is
</p>
<p>not so relevant, since the distributions happen to differ not that much, and Qc1 � c2.
For the error-correction test by Banerjee et al. (1998), however, matters are not
</p>
<p>quite so harmless, since stronger size distortions are caused by a larger difference
</p>
<p>of the asymptotic critical values: Qc1 D �3:41 and c2 D �3:19. With the residual-
based KPSS test, things change qualitatively and quantitatively. Critical values from
</p>
<p>Kwiatkowski, Phillips, Schmidt, and Shin (1992) and Shin (1994) are Qc1 D 0:146
and c2 D 0:314, and thus differ dramatically. Since one rejects for too large values,
strategy SI implies in the presence of a linear time trend a very conservative test; it
</p>
<p>will hardly reject the null hypothesis, which comes at a price of power of course.
</p>
<p>The other way round, without linear time trends strategy SA will reject the true null
</p>
<p>hypothesis much too often resulting in an intolerably liberal test. �
</p>
<p>Clearly, none of the strategies SI or SA is generally acceptable when testing under
</p>
<p>the possibility of linear time trends. Fortunately, one often has strong a priori beliefs
</p>
<p>regarding the absence or presence of a linear time trend in the regressor. If one is
</p>
<p>convinced that a linear time trend is present in fxtg, then one would apply e.g. ta.2/
or t&#13; .2/ with critical values from eDF ; if one believes that there is no linear time
</p>
<p>trend behind fxtg, then critical values from DF.W1;W2/ or BDM.W1;W2/must be
recommended.
</p>
<p>If one is not sure about the absence or presence of a linear time trend in the data,
</p>
<p>then there are (at least) two more strategies beyond SI or SA one may employ. As
</p>
<p>a third strategy, one may always test from detrended data. This clearly circumvents
</p>
<p>size distortions, but comes at a price of power losses as has been acknowledged
</p>
<p>for instance by Hansen (1992) or Hamilton (1994, p. 598). Fourth, one may rely
</p>
<p>on a pretest whether the regressor follows a linear trend or not. If a linear time
</p>
<p>trend in fxtg is significant, then one would apply e.g. �C.2/ with critical values
from eCM; if not, then critical values from CM.W1;W2/ should be applied. Such
</p>
<p>a pretesting strategy, however, will be troubled in small samples by the problem of
</p>
<p>controlling the significance level when carrying out a sequence of conditional tests
</p>
<p>(multiple testing). A recommendation whether the strategy of generally detrending
</p>
<p>or the strategy of pretesting is to be preferred, when the presence or absence of a
</p>
<p>linear time trend is debatable, will require future research.
</p>
<p>16.5 Problems and Solutions
</p>
<p>Problems
</p>
<p>16.1 Show the equivalence of (16.1) and the ARDL(2,2) parameterization from
</p>
<p>Example 16.1.
</p>
<p>16.2 Prove statements (b), (c) and (d) from Proposition 16.2.
</p>
<p>16.3 Prove statement (e) from Proposition 16.2.</p>
<p/>
</div>
<div class="page"><p/>
<p>374 16 Cointegration Analysis
</p>
<p>16.4 Prove Corollary 16.1.
</p>
<p>16.5 Prove Proposition 16.3.
</p>
<p>16.6 Prove Proposition 16.5 for the special case that no lagged differences are
</p>
<p>required to obtain white noise errors f"tg:
</p>
<p>�yt D &#13;yt�1 C �xt�1 C "t :
</p>
<p>16.7 Prove Proposition 16.4.
</p>
<p>Solutions
</p>
<p>16.1 Using &#13; and b, one obtains with � D 1 � L from (16.1):
</p>
<p>yt D yt�1 � .1 � a1 � a2/yt�1 C .c0 C c1 C c2/xt�1
�a2.yt�1 � yt�2/C c0.xt � xt�1/ � c2.xt�1 � xt�2/C "t
</p>
<p>D a1yt�1 C a2yt�2 C c0xt C c1xt�1 C c2xt�2 C "t :
</p>
<p>Hence, the claim is already proved.
</p>
<p>16.2 We proceed in the same way as in the proof of Proposition 15.5, only that we
</p>
<p>work under (16.6) when appealing to Proposition 14.4. We start with
</p>
<p>n
�
1 � R2uc
</p>
<p>�
D
</p>
<p>n�1
nP
</p>
<p>tD1
Ov2t
</p>
<p>n�2
nP
</p>
<p>tD1
y2t
</p>
<p>D s
2
</p>
<p>n�2
nP
</p>
<p>tD1
y2t
</p>
<p>:
</p>
<p>The numerator on the right-hand side is just
</p>
<p>s2 D n�1
nX
</p>
<p>tD1
</p>
<p>�
yt � Ob xt
</p>
<p>�2
</p>
<p>D n�1
nX
</p>
<p>tD1
</p>
<p>�
b xt � Ob xt C vt
</p>
<p>�2
</p>
<p>D n�1
nX
</p>
<p>tD1
</p>
<p>h
.b � Ob/2 x2t C 2 .b � Ob/xt vt C v2t
</p>
<p>i
</p>
<p>D n .b � Ob/2
</p>
<p>nP
tD1
</p>
<p>x2t
</p>
<p>n2
C 2 .b � Ob/
</p>
<p>nP
tD1
</p>
<p>xt vt
</p>
<p>n
C
</p>
<p>nP
tD1
v2t
</p>
<p>n
:</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Problems and Solutions 375
</p>
<p>The first of the three remaining terms tends to zero as
P
</p>
<p>x2t is of order n
2 and
</p>
<p>.b � Ob/ is of order n�1; correspondingly, the second expression tends to zero asP
xt vt grows with n; finally, the third term converges to Var.vt/ as a law of large
</p>
<p>numbers holds for fv2t g. This proves Proposition 16.2(c). By the same arguments,
one establishes:
</p>
<p>n�2
nX
</p>
<p>tD1
y2t D n�2
</p>
<p>nX
</p>
<p>tD1
</p>
<p>�
b2 x2t C 2 b xt vt C v2t
</p>
<p>� d! b2
Z 1
</p>
<p>0
</p>
<p>B22.s/ ds C 0C 0 :
</p>
<p>Hence, Proposition 16.2(b) is proved as well.
</p>
<p>Finally, the behavior of the t-statistic with s2b D s2=
P
</p>
<p>x2t is clear again by
</p>
<p>Proposition 14.4:
</p>
<p>tb D
Ob � b
</p>
<p>sb
D
</p>
<p>1
n
</p>
<p>nP
tD1
</p>
<p>xt vt
</p>
<p>s
</p>
<p>s
1
n2
</p>
<p>nP
tD1
</p>
<p>x2t
</p>
<p>d�!
</p>
<p>1R
0
</p>
<p>B2.s/dB1.s/C
1P
</p>
<p>hD0
E .�xt vtCh/
</p>
<p>s
&#13;1.0/
</p>
<p>1R
0
</p>
<p>B22.s/ds
</p>
<p>:
</p>
<p>16.3 In order to analyze the behavior of the Durbin-Watson statistic, we only need
</p>
<p>to study the numerator,
</p>
<p>n�1
nX
</p>
<p>tD2
. Ovt � Ovt�1/2 D n�1
</p>
<p>nX
</p>
<p>tD2
</p>
<p>�
.b � Ob/� xt C�vt
</p>
<p>�2
</p>
<p>D .b � Ob/2
</p>
<p>nP
tD2
.� xt/
</p>
<p>2
</p>
<p>n
C 2.b � Ob/
</p>
<p>nP
tD2
� xt �vt
</p>
<p>n
C
</p>
<p>nP
tD2
.� vt/
</p>
<p>2
</p>
<p>n
:
</p>
<p>As .b � Ob/ tends to zero, there remains asymptotically
</p>
<p>n�1
nX
</p>
<p>tD2
.� Ovt/2
</p>
<p>p�! Var.� vt/ D 2Var.vt/ � 2Cov.vt; vt�1/ :
</p>
<p>Hence, it holds as claimed:
</p>
<p>dw D
n�1
</p>
<p>nP
tD2
.� Ov2t /
</p>
<p>s2
</p>
<p>p�! 2 .1 � �v.1// ;
</p>
<p>as s2 approaches Var.vt/ with n growing.
</p>
<p>16.4 The result will follow from Proposition 16.2. By (16.7), we have !12 D 0.
Due to the resulting diagonality of ˝ , ˝0:5 is diagonal as well, cf. ˝2 from</p>
<p/>
</div>
<div class="page"><p/>
<p>376 16 Cointegration Analysis
</p>
<p>Example 14.6. Hence it holds:
</p>
<p>�
B1.s/
</p>
<p>B2.s/
</p>
<p>�
D
�
!1W1.s/
</p>
<p>!2W2.s/
</p>
<p>�
;
</p>
<p>and !2 cancels from the limiting distribution of Proposition 16.2(d) (using the
</p>
<p>second assumption�xv D 0):
</p>
<p>!2
1R
0
</p>
<p>W2.s/dW1.s/!1
</p>
<p>s
&#13;1.0/!
</p>
<p>2
2
</p>
<p>1R
0
</p>
<p>W22 .s/ds
</p>
<p>D !1p
&#13;1.0/
</p>
<p>1R
0
</p>
<p>W2.s/dW1.s/
</p>
<p>s
1R
0
</p>
<p>W22 .s/ds
</p>
<p>:
</p>
<p>According to Proposition 10.4, the stochastic quotient on the right-hand side follows
</p>
<p>a standard normal distribution. Hence, it holds that
</p>
<p>p
&#13;1.0/
</p>
<p>!1
tb
</p>
<p>d! N .0; 1/ ;
</p>
<p>which proves the corollary.
</p>
<p>16.5 By assumption, it holds:
</p>
<p>n�3
nX
</p>
<p>tD1
x2t D n�3
</p>
<p>2
64�2
</p>
<p>nX
</p>
<p>tD1
t2 C 2�
</p>
<p>nX
</p>
<p>tD1
</p>
<p>0
@t
</p>
<p>tX
</p>
<p>jD1
ej
</p>
<p>1
AC
</p>
<p>nX
</p>
<p>tD1
</p>
<p>0
@
</p>
<p>tX
</p>
<p>jD1
ej
</p>
<p>1
A
2
3
75 :
</p>
<p>We know from Proposition 14.2(c) and (e) that the second and the third expression
</p>
<p>in square brackets have to be divided by n2:5 and n2, respectively, such that they
</p>
<p>converge. However, in front of the square bracket there is n�3, such that it holds
with (15.2):
</p>
<p>n�3
nX
</p>
<p>tD1
x2t
</p>
<p>d! �
2
</p>
<p>3
:
</p>
<p>Hence we have the denominator of the LS estimator under control:
</p>
<p>Ob D b C
Pn
</p>
<p>tD1 xt vtPn
tD1 x
</p>
<p>2
t
</p>
<p>:
</p>
<p>In order to crack the numerator, we consider
</p>
<p>xbsnc
n
</p>
<p>D x0
n
</p>
<p>C �bsnc
n
</p>
<p>C
Pbsnc
</p>
<p>jD1 ej
</p>
<p>n
;</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Problems and Solutions 377
</p>
<p>and due to Proposition 14.1 it holds
</p>
<p>xbsnc
n
</p>
<p>� �bsnc
n
</p>
<p>d! � s ;
</p>
<p>and xt is dominated by a linear trend, i.e. xt behaves just as �t. Thus, as for the
</p>
<p>detrending in the trend stationary case, we obtain with a standard Wiener process V
</p>
<p>that (see Sect. 15.2):
</p>
<p>n1:5.Ob � b/ D
n�1:5
</p>
<p>nP
tD1
</p>
<p>xtvt
</p>
<p>n�3
nP
</p>
<p>tD1
x2t
</p>
<p>d!
�!1
</p>
<p>1R
0
</p>
<p>sdV.s/
</p>
<p>�2=3
</p>
<p>D 3 !1
�
</p>
<p>1Z
</p>
<p>0
</p>
<p>sdV.s/
</p>
<p>� N
�
0;
3 !21
�2
</p>
<p>�
;
</p>
<p>where !1V.s/ is the Brownian motion corresponding to
Pt
</p>
<p>jD1 vj, and !
2
1 is the long-
</p>
<p>run variance of fvtg. The normality of the integral follows from Example 9.2. Hence,
the claim is proved.
</p>
<p>16.6 Because of the simplifying assumption we consider the regression of (16.14)
</p>
<p>without differences. As LS estimator for the vector
</p>
<p> WD
�
&#13;
</p>
<p>�
</p>
<p>�
</p>
<p>one hence obtains for a sample t D 1; : : : ; n:
</p>
<p>O D D�1
nX
</p>
<p>tD1
</p>
<p>�
yt�1
xt�1
</p>
<p>�
�yt ; (16.18)
</p>
<p>where D has the form
</p>
<p>D D
</p>
<p>0
BB@
</p>
<p>nP
tD1
</p>
<p>y2t�1
nP
</p>
<p>tD1
yt�1xt�1
</p>
<p>nP
tD1
</p>
<p>xt�1yt�1
nP
</p>
<p>tD1
x2t�1
</p>
<p>1
CCA :</p>
<p/>
</div>
<div class="page"><p/>
<p>378 16 Cointegration Analysis
</p>
<p>Plugging in �yt D "t under H0, we obtain
</p>
<p>O D D�1
nX
</p>
<p>tD1
</p>
<p>�
yt�1
xt�1
</p>
<p>�
"t :
</p>
<p>In the case of no cointegration, for using Proposition 14.4 we choose
</p>
<p>zt D
�
</p>
<p>yt
</p>
<p>xt
</p>
<p>�
:
</p>
<p>Then it holds for the matrix D:
</p>
<p>n�2D
d!
</p>
<p>0
BBB@
</p>
<p>1R
0
</p>
<p>B21 .s/ ds
1R
0
</p>
<p>B1 .s/B2 .s/ ds
</p>
<p>1R
0
</p>
<p>B1 .s/B2 .s/ ds
1R
0
</p>
<p>B22 .s/ ds
</p>
<p>1
CCCA : (16.19)
</p>
<p>For the inverse, this implies
</p>
<p>n2D�1
d! 1
</p>
<p>det
</p>
<p>0
BBB@
</p>
<p>1R
0
</p>
<p>B22 .s/ ds �
1R
0
</p>
<p>B1 .s/B2 .s/ ds
</p>
<p>�
1R
0
</p>
<p>B1 .s/B2 .s/ ds
1R
0
</p>
<p>B21 .s/ ds
</p>
<p>1
CCCA ;
</p>
<p>where &ldquo;det&rdquo; stands for the determinant of the limiting matrix from (16.19). Here,
</p>
<p>the known inversion formula for .2 � 2/-matrices was applied:
</p>
<p>�
a b
</p>
<p>c d
</p>
<p>��1
D 1
</p>
<p>det
</p>
<p>�
d �b
�c a
</p>
<p>�
; det D a d � b c : (16.20)
</p>
<p>Note that Proposition 14.4(b) was applied with zt�1 instead of zt. This is unproblem-
atic and can be justified by similar arguments like those leading to Corollary 14.1 in
</p>
<p>the univariate case.
</p>
<p>Next, we analyze with Proposition 14.4(c)
</p>
<p>n�1
nX
</p>
<p>tD1
</p>
<p>�
yt�1
xt�1
</p>
<p>�
�yt D n�1
</p>
<p>nX
</p>
<p>tD1
.zt � wt/w1;t
</p>
<p>d!
1Z
</p>
<p>0
</p>
<p>B .s/ dB1 .s/C
1X
</p>
<p>hD0
E .wtw1;tCh/ � E .wtw1;t/</p>
<p/>
</div>
<div class="page"><p/>
<p>16.5 Problems and Solutions 379
</p>
<p>D
1Z
</p>
<p>0
</p>
<p>B .s/ dB1 .s/C
1X
</p>
<p>hD1
E .wtw1;tCh/
</p>
<p>D
1Z
</p>
<p>0
</p>
<p>B .s/ dB1 .s/C 0 ;
</p>
<p>where we used that w1;t D �yt D "t is free from serial correlation and is
uncorrelated with �xs at each point in time, see (16.15):
</p>
<p>E .wtw1;tCh/ D 0 ; h &gt; 0 :
</p>
<p>Thus, under the null hypothesis of no cointegration, we obtain that O&#13; tends to zero.
For this purpose we consider the first row of the limit of n2D�1 multiplied byR
</p>
<p>B .s/ dB1 .s/:
</p>
<p>n O&#13; d!
</p>
<p>1R
0
</p>
<p>B22 .s/ ds
1R
0
</p>
<p>B1 .s/ dB1 .s/ �
1R
0
</p>
<p>B1 .s/B2 .s/ ds
1R
0
</p>
<p>B2 .s/ dB1 .s/
</p>
<p>det
:
</p>
<p>This is almost the claim as &ldquo;det&rdquo; is defined as the determinant of the limit of n�2D.
Finally, note that Bi D !iWi holds as �xt D w2;t and �ys D "s D w1;s are
uncorrelated. Thus, the long-run variances cancel from the limiting distribution and
</p>
<p>one obtains the required result.
</p>
<p>16.7 Under the null hypothesis of no cointegration we define z0t D .yt; xt/ with
long-run variance matrix of full rank. The corresponding vector Brownian motion
</p>
<p>B0 D .B1; B2/ can be written in terms of independent WPs W 0 D .W1; W2/ as
</p>
<p>B.t/ D T W.t/ D
 
</p>
<p>t11W1.t/C !12!2 W2.t/
!2W2.t/
</p>
<p>!
:
</p>
<p>Here, T is the triangular decomposition given in (14.7) with TT 0 D ˝ . The limit of
Ǒ from Proposition 15.5(a) hence becomes
</p>
<p>!2ˇ1 D
R 1
0
</p>
<p>B1.s/W2.s/dsR 1
0
</p>
<p>W22 .s/ds
D
</p>
<p>t11
R 1
0
</p>
<p>W1.s/W2.s/ds C !12!2
R 1
0
</p>
<p>W22 .s/dsR 1
0
</p>
<p>W22 .s/ds
: (16.21)
</p>
<p>With this result one obtains a FCLT for the residuals Out D yt � Ǒxt:
</p>
<p>n�0:5 Oubrnc ) B1.r/ � ˇ1B2.r/
D .1;�ˇ1/ T W.r/</p>
<p/>
</div>
<div class="page"><p/>
<p>380 16 Cointegration Analysis
</p>
<p>D t11
</p>
<p> 
W1.r/ �
</p>
<p>R 1
0
</p>
<p>W1.s/W2.s/ dsR 1
0
</p>
<p>W22 .s/ ds
W2.r/
</p>
<p>!
</p>
<p>D t11U.r/ :
</p>
<p>Unfortunately, however, Proposition 14.2 does not apply directly since U.r/ is not
</p>
<p>a WP. Still, with the techniques we used to prove Proposition 14.2(e) and (f) in
</p>
<p>Problems 14.3 and 14.5, we can establish (omitting details)
</p>
<p>n�2
nX
</p>
<p>tD1
Ou2t�1
</p>
<p>d! t211
Z 1
</p>
<p>0
</p>
<p>U2.t/dt ;
</p>
<p>n�1
nX
</p>
<p>tD1
Out�1�Out
</p>
<p>d! t211
U2.1/
</p>
<p>2
� 1
2
</p>
<p>�
!21 � 2ˇ1!12 C ˇ21!22
</p>
<p>�
;
</p>
<p>where the last limit arises because f�ztg is white noise such that!2i and!12 coincide
with the (co)variances. Further, note by B D TW that
</p>
<p>!12ˇ1 D
t11!12
</p>
<p>!2
</p>
<p>R 1
0
</p>
<p>W1.s/W2.s/dsR 1
0
</p>
<p>W22 .s/ds
C !
</p>
<p>2
12
</p>
<p>!22
: (16.22)
</p>
<p>Remember from (14.7) that
</p>
<p>t211 D !21 �
!212
</p>
<p>!22
:
</p>
<p>Consequently, for
</p>
<p>Ma D 1C
Pn
</p>
<p>tD1 Out�1�OutPn
tD1 Ou2t�1
</p>
<p>we get by (16.21) and (16.22) that
</p>
<p>n .Ma � 1/ d!
</p>
<p>U2.1/
</p>
<p>2
� 1
</p>
<p>2
</p>
<p> 
1C
</p>
<p>� R 1
0 W1.s/W2.s/dsR 1
</p>
<p>0 W
2
2 .s/ds
</p>
<p>�2!
</p>
<p>R 1
0
</p>
<p>U2.t/dt
:
</p>
<p>The numerator of this limit may be condensed. Use the product rule from
</p>
<p>Example 11.5 to obtain
</p>
<p>W1.1/W2.1/ D
Z 1
</p>
<p>0
</p>
<p>W1.s/dW2.s/C
Z 1
</p>
<p>0
</p>
<p>W2.s/dW1.s/ :</p>
<p/>
</div>
<div class="page"><p/>
<p>References 381
</p>
<p>It follows that
</p>
<p>U2.1/
</p>
<p>2
D W
</p>
<p>2
1 .1/
</p>
<p>2
�
R 1
0
</p>
<p>W1.s/W2.s/dsR 1
0
</p>
<p>W22 .s/ds
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>W1.s/dW2.s/C
Z 1
</p>
<p>0
</p>
<p>W2.s/dW1.s/
</p>
<p>�
</p>
<p>C
 R 1
</p>
<p>0
W1.s/W2.s/dsR 1
0
</p>
<p>W22 .s/ds
</p>
<p>!2
W22 .1/
</p>
<p>2
:
</p>
<p>Once more by Ito&rsquo;s lemma
W2i .1/
</p>
<p>2
D
R 1
0
</p>
<p>Wi.s/dWi.s/C 12 , such that
</p>
<p>U2.1/
</p>
<p>2
D
Z 1
</p>
<p>0
</p>
<p>W1.s/dW1.s/C
1
</p>
<p>2
</p>
<p>�
R 1
0 W1.s/W2.s/dsR 1
</p>
<p>0 W
2
2 .s/ds
</p>
<p>�Z 1
</p>
<p>0
</p>
<p>W1.s/dW2.s/C
Z 1
</p>
<p>0
</p>
<p>W2.s/dW1.s/
</p>
<p>�
</p>
<p>C
 R 1
</p>
<p>0 W1.s/W2.s/dsR 1
0 W
</p>
<p>2
2 .s/ds
</p>
<p>!2 �Z 1
</p>
<p>0
</p>
<p>W2.s/dW2.s/C
1
</p>
<p>2
</p>
<p>�
</p>
<p>D
Z 1
</p>
<p>0
</p>
<p>U.t/ dU.t/C 1
2
</p>
<p>0
@1C
</p>
<p> R 1
0
</p>
<p>W1.s/W2.s/dsR 1
0
</p>
<p>W22 .s/ds
</p>
<p>!21
A :
</p>
<p>This provides the expression for the limiting distribution given in Proposition 16.4
</p>
<p>as required.
</p>
<p>References
</p>
<p>Banerjee, A., Dolado, J. J., &amp; Mestre R. (1998). Error-correction mechanism tests for cointegration
in a single-equation framework. Journal of Time Series Analysis, 19, 267&ndash;283.
</p>
<p>Boswijk, H. P. (1994). Testing for an unstable root in conditional and structural error correction
models. Journal of Econometrics, 63, 37&ndash;60.
</p>
<p>Davidson, J., Hendry, D. F., Srba, F., &amp; Yeo S. (1978). Econometric modelling of the aggregate
time-series relationship between consumers&rsquo; expenditure and income in the United Kingdom.
Economic Journal, 88, 661&ndash;692.
</p>
<p>Engle, R. F., &amp; Granger, C. W. J. (1987). Co-integration and error correction: Representation,
estimation, and testing. Econometrica, 55, 251&ndash;276.
</p>
<p>Ericsson, N. R., &amp; MacKinnon, J. G. (2002). Distributions of error correction tests for cointegra-
tion. Econometrics Journal, 5, 285&ndash;318.
</p>
<p>Frisch, R., &amp; Waugh, F. V. (1933). Partial time regressions as compared with individual trends.
Econometrica, 1, 387&ndash;401.
</p>
<p>Hamilton, J. (1994). Time series analysis. Princeton: Princeton University Press.
Hansen, B. E. (1992). Efficient estimation and testing of cointegrating vectors in the presence of
</p>
<p>deterministic trends. Journal of Econometrics, 53, 87&ndash;121.</p>
<p/>
</div>
<div class="page"><p/>
<p>382 16 Cointegration Analysis
</p>
<p>Harris, D., &amp; Inder, B. (1994). A test of the null hypothesis of cointegration. In C. P. Hargreaves
(Ed.), Nonstationary time series analysis and cointegration (pp. 133&ndash;152). Oxford/New York:
Oxford University Press.
</p>
<p>Hassler, U. (2000a). Cointegration testing in single error-correction equations in the presence of
linear time trends. Oxford Bulletin of Economics and Statistics, 62, 621&ndash;632.
</p>
<p>Hassler, U. (2000b). The KPSS test for cointegration in case of bivariate regressions with linear
trends. Econometric Theory, 16, 451&ndash;453.
</p>
<p>Hassler, U. (2001). The effect of linear time trends on the KPSS test for cointegration. Journal of
Time Series Analysis, 22, 283&ndash;292.
</p>
<p>Johansen, S. (1995). Likelihood-based inference in cointegrated vector autoregressive models.
Oxford/New York: Oxford University Press.
</p>
<p>Kr&auml;mer, W. (1986). Least squares regression when the independent variable follows an ARIMA
process. Journal of the American Statistical Association, 81, 150&ndash;154.
</p>
<p>Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., &amp; Shin Y. (1992). Testing the null hypothesis of
stationarity against the alternative of a unit root. Journal of Econometrics, 54, 159&ndash;178.
</p>
<p>Leybourne, S. J., &amp; McCabe, B. P. M. (1994). A simple test for cointegration. Oxford Bulletin of
Economics and Statistics, 56, 97&ndash;103.
</p>
<p>MacKinnon, J. G. (1991). Critical values for co-integration tests. In R. F. Engle, &amp; C. W. J. Granger
(Eds.), Long-run economic relationships (pp. 267&ndash;276). Oxford/New York: Oxford University
Press.
</p>
<p>MacKinnon, J. G. (1996). Numerical distribution functions for unit root and cointegration tests.
Journal of Applied Econometrics, 11, 601&ndash;618.
</p>
<p>Park, J. Y. (1992). Canonical cointegrating regressions. Econometrica, 60, 119&ndash;143.
</p>
<p>Phillips, P. C. B. (1986). Understanding spurious regressions in econometrics. Journal of Econo-
metrics, 33, 311&ndash;340.
</p>
<p>Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica, 55, 277&ndash;301.
Phillips, P. C. B. (1991). Optimal inference in cointegrated systems. Econometrica, 59, 283&ndash;306.
Phillips, P. C. B., &amp; Durlauf, S. N. (1986). Multiple time series regression with integrated
</p>
<p>processes. Review of Economic Studies, LIII, 473&ndash;495.
Phillips, P. C. B., &amp; Hansen, B. E. (1990). Statistical inference in instrumental variables regression
</p>
<p>with I(1) processes. Review of Economic Studies, 57, 99&ndash;125.
Phillips, P. C. B., &amp; Loretan, M. (1991). Estimating long-run economic equilibria. Review of
</p>
<p>Economic Studies, 58, 407&ndash;436.
Phillips, P. C. B., &amp; Ouliaris, S. (1990). Asymptotic properties of residual based tests for
</p>
<p>cointegration. Econometrica, 58, 165&ndash;193.
Phillips, P. C. B., &amp; Park, J. Y. (1988). Asymptotic equivalence of ordinary least squares and
</p>
<p>generalized least squares in regressions with integrated regressors. Journal of the American
Statistical Association, 83, 111&ndash;115.
</p>
<p>Saikkonen, P. (1991). Asymptotically efficient estimation of cointegration regressions. Economet-
ric Theory, 7, 1&ndash;21.
</p>
<p>Shin, Y. (1994). A residual-based test of the Null of cointegration against the alternative of no
cointegration. Econometric Theory, 10, 91&ndash;115.
</p>
<p>Stock, J. H. (1987). Asymptotic properties of least squares estimators of cointegrating vectors.
Econometrica, 55, 1035&ndash;1056.
</p>
<p>Stock, J. H., &amp; Watson, M. W. (1993). A simple estimator of cointegrating vectors in higher order
integrated systems. Econometrica, 61, 783&ndash;820.
</p>
<p>West, K. D. (1988). Asymptotic normality, when regressors have a unit root. Econometrica, 56,
1397&ndash;1418.</p>
<p/>
</div>
<div class="page"><p/>
<p>References
</p>
<p>Anderson, T. W., &amp; Darling, D. A. (1952). Asymptotic theory of certain &ldquo;Goodness of Fit&rdquo; criteria
based on stochastic processes. Annals of Mathematical Statistics, 23, 193&ndash;212.
</p>
<p>Andrews, D. W. K. (1991). Heteroskedasticity and autocorrelation consistent covariance matrix
estimation. Econometrica, 59, 817&ndash;858.
</p>
<p>Andrews, D. W. K., &amp; Chen, H.-Y. (1994). Approximately median-unbiased estimation of
autoregressive models. Journal of Business &amp; Economic Statistics, 12, 187&ndash;204.
</p>
<p>Baillie, R. T. (1996). Long memory processes and fractional integration in econometrics. Journal
of Econometrics, 73, 5&ndash;59.
</p>
<p>Banerjee, A., Dolado, J. J., Galbraith, J. W., &amp; Hendry, D. F. (1993). Co-integration, error
correction, and the econometric analysis of non-stationary data. Oxford/New York: Oxford
University Press.
</p>
<p>Banerjee, A., Dolado, J. J., &amp; Mestre R. (1998). Error-correction mechanism tests for cointegration
in a single-equation framework. Journal of Time Series Analysis, 19, 267&ndash;283.
</p>
<p>Bickel, P. J., &amp; Doksum, K. A. (2001). Mathematical statistics: Basic ideas and selected topics,
volume 1 (2nd ed.). Upper Saddle River: Prentice-Hall.
</p>
<p>Billingsley, P. (1968). Convergence of probability measures. New York: Wiley.
Billingsley, P. (1986). Probability and measure (2nd ed.). New York: Wiley.
Black, F., &amp; Scholes, M. (1973). The pricing of options and corporate liabilities. The Journal of
</p>
<p>Political Economy, 81, 637&ndash;654.
Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econo-
</p>
<p>metrics, 31, 307&ndash;327.
Bondon, P., &amp; Palma, W. (2007). A class of antipersistent processes. Journal of Time Series
</p>
<p>Analysis, 28, 261&ndash;273.
</p>
<p>Boswijk, H. P. (1994). Testing for an unstable root in conditional and structural error correction
models. Journal of Econometrics, 63, 37&ndash;60.
</p>
<p>Breiman, L. (1992). Probability (2nd ed.). Philadelphia: Society for Industrial and Applied
Mathematics.
</p>
<p>Brennan, M. J, &amp; Schwartz, E. S. (1980). Analyzing convertible bonds. The Journal of Financial
and Quantitative Analysis, 15, 907&ndash;929.
</p>
<p>Brockwell, P. J., &amp; Davis, R. A. (1991). Time series: Theory and methods (2nd ed.). New York:
Springer.
</p>
<p>Broze, L., Scaillet, O., &amp; Zako&iuml;an, J.-M. (1995). Testing for continuous-time models of the short-
term interest rate. Journal of Empirical Finance, 2, 199&ndash;223.
</p>
<p>Campbell, J. Y., &amp; Mankiw, N. G. (1987). Are output fluctuations transitory? Quarterly Journal of
Economics, 102, 857&ndash;880.
</p>
<p>Chan, K. C., Karolyi, G. A., Longstaff, F. A., &amp; Sanders, A. B. (1992). An empirical comparision
of alternative models of the short-term interest rate. The Journal of Finance, XLVII, 1209&ndash;1227.
</p>
<p>Chang, Y., &amp; Park, J. (2002). On the asymptotics of ADF tests for unit roots. Econometric Reviews,
21, 431&ndash;447.
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1
</p>
<p>383</p>
<p/>
</div>
<div class="page"><p/>
<p>384 References
</p>
<p>Cochrane, J. H. (1988). How big is the random walk in GNP? Journal of Political Economy, 96,
893&ndash;920.
</p>
<p>Cogley, T., &amp; Sargent T. S. (2005). Drifts and volatilities: Monetary policies and outcomes in the
post WWII US. Review of Economic Dynamics, 8, 262&ndash;302.
</p>
<p>Constantinides, G. M., &amp; Ingersoll, J. E., Jr. (1984). Optimal bond trading with personal taxes.
Journal of Financial Economics, 13, 299&ndash;335.
</p>
<p>Cox, J. C., Ingersoll, J. E., Jr., &amp; Ross S. A. (1980). An analysis of variable rate loan contracts. The
Journal of Finance, 35, 389&ndash;403.
</p>
<p>Cox, J. C., Ingersoll, J. E., Jr., &amp; Ross S. A. (1985). A theory of the term structure of interest rates.
Econometrica, 53, 385&ndash;407.
</p>
<p>Davidson, J. (1994). Stochastic limit theory: An introduction for econometricians.
Oxford/New York: Oxford University Press.
</p>
<p>Davidson, J., Hendry, D. F., Srba, F., &amp; Yeo S. (1978). Econometric modelling of the aggregate
time-series relationship between consumers&rsquo; expenditure and income in the United Kingdom.
Economic Journal, 88, 661&ndash;692.
</p>
<p>Demetrescu, M., Kuzin, V., &amp; Hassler, U. (2008). Long memory testing in the time domain.
Econometric Theory, 24, 176&ndash;215.
</p>
<p>Dhrymes, Ph. J. (2000). Mathematics for econometrics (3rd ed.). New York: Springer.
Dickey, D. A., &amp; Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series
</p>
<p>with a unit root. Journal of the American Statistical Association, 74, 427&ndash;431.
Donsker, M. D. (1951). An invariance principle for certain probability limit theorems. Memoirs of
</p>
<p>the American Mathematical Society, 6, 1&ndash;12.
Dothan, L. U. (1978). On the term structure of interest rates. Journal of Financial Economics, 6,
</p>
<p>59&ndash;69.
Durlauf, S. N., &amp; Phillips, P. C. B. (1988). Trends versus random walks in time series analysis.
</p>
<p>Econometrica, 56, 1333&ndash;1354.
Elliott, G., Rothenberg, T. J., &amp; Stock, J. H. (1996). Efficient tests for an autoregressive unit root.
</p>
<p>Econometrica, 64, 813&ndash;836.
Engle, R. F. (1982). Autoregressive conditional heteroskedasticity with estimates of the variance
</p>
<p>of U.K. inflation. Econometrica, 50, 987&ndash;1008.
Engle, R. F. (2002). New frontiers for ARCH models. Journal of Applied Econometrics, 17,
</p>
<p>425&ndash;446.
Engle, R. F., &amp; Bollerslev T. (1986). Modelling the persistence of conditional variances. Econo-
</p>
<p>metric Reviews, 5, 1&ndash;50.
Engle, R. F., &amp; Granger, C. W. J. (1987). Co-integration and error correction: Representation,
</p>
<p>estimation, and testing. Econometrica, 55, 251&ndash;276.
Engle, R. F., Lilien, D. M., &amp; Robins, R. P. (1987). Estimating time-varying risk premia in the term
</p>
<p>structure: the ARCH-M model. Econometrica, 55, 391&ndash;407.
Ericsson, N. R., &amp; MacKinnon, J. G. (2002). Distributions of error correction tests for cointegra-
</p>
<p>tion. Econometrics Journal, 5, 285&ndash;318.
Frisch, R., &amp; Waugh, F. V. (1933). Partial time regressions as compared with individual trends.
</p>
<p>Econometrica, 1, 387&ndash;401.
Fuller, W. A. (1996). Introduction to statistical time series (2nd ed.). New York: Wiley.
Giraitis, L., Koul, H. L., &amp; Surgailis, D. (2012). Large sample inference for long memory processes.
</p>
<p>London: Imperial College Press.
Gourieroux, Chr., &amp; Jasiak, J. (2001). Financial econometrics: Problems, models, and methods.
</p>
<p>Princeton: Princeton University Press.
Gradshteyn, I. S., &amp; Ryzhik, I. M. (2000). Table of integrals, series, and products (6th ed.).
</p>
<p>London/San Diego: Academic Press.
Granger, C. W. J. (1981). Some properties of time series data and their use in econometric model
</p>
<p>specification. Journal of Econometrics, 16, 121&ndash;130.
Granger, C. W. J., &amp; Joyeux, R. (1980). An Introduction to long-memory time series models and
</p>
<p>fractional differencing. Journal of Time Series Analysis, 1, 15&ndash;29.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 385
</p>
<p>Granger, C. W. J., &amp; Newbold P. (1974). Spurious regressions in econometrics. Journal of
Econometrics, 2, 111&ndash;120.
</p>
<p>Grimmett, G. R., &amp; Stirzaker, D. R. (2001). Probability and random processes (3rd ed.). Oxford:
Oxford University Press.
</p>
<p>Hamilton, J. (1994). Time series analysis. Princeton: Princeton University Press.
Hansen, B. E. (1992). Efficient estimation and testing of cointegrating vectors in the presence of
</p>
<p>deterministic trends. Journal of Econometrics, 53, 87&ndash;121.
Hansen, B. E. (1992a). Convergence to stochastic integrals for dependent heterogeneous processes.
</p>
<p>Econometric Theory, 8, 489&ndash;500.
Harris, D., &amp; Inder, B. (1994). A test of the null hypothesis of cointegration. In C. P. Hargreaves
</p>
<p>(Ed.), Nonstationary time series analysis and cointegration (pp. 133&ndash;152). Oxford/New York:
Oxford University Press.
</p>
<p>Hassler, U. (2000). Simple regressions with linear time trends. Journal of Time Series Analysis, 21,
27&ndash;32.
</p>
<p>Hassler, U. (2000a). Cointegration testing in single error-correction equations in the presence of
linear time trends. Oxford Bulletin of Economics and Statistics, 62, 621&ndash;632.
</p>
<p>Hassler, U. (2000b). The KPSS test for cointegration in case of bivariate regressions with linear
trends. Econometric Theory, 16, 451&ndash;453.
</p>
<p>Hassler, U. (2001). The effect of linear time trends on the KPSS test for cointegration. Journal of
Time Series Analysis, 22, 283&ndash;292.
</p>
<p>Hassler, U. (2012). Impulse responses of antipersistent processes. Economics Letters, 116,
454&ndash;456.
</p>
<p>Hassler, U. (2014). Persistence under temporal aggregation and differencing. Economics Letters,
</p>
<p>124, 318&ndash;322.
Hassler, U., &amp; Hosseinkouchack, M. (2014). Effect of the order of fractional integration on impulse
</p>
<p>responses. Economics Letters, 125, 311&ndash;314.
Hassler, U., &amp; Kokoszka (2010). Impulse responses of fractionally integrated processes with long
</p>
<p>memory. Econometric Theory, 26, 1855&ndash;1861.
Hendry, D. F. (1980). Econometrics &ndash; alchemy or science? Economica, 47, 387&ndash;406.
Hosking, J. R. M. (1981). Fractional differencing. Biometrika, 68, 165&ndash;176.
Johansen, S. (1995). Likelihood-based inference in cointegrated vector autoregressive models.
</p>
<p>Oxford/New York: Oxford University Press.
Johnson, N. L., Kotz, S., &amp; Balakrishnan, N. (1994). Continuous univariate distributions, Volume
</p>
<p>1 (2nd ed.). New York: Wiley.
Kirchg&auml;ssner, G., Wolters, J., &amp; Hassler, U. (2013). Introduction to modern time series analysis
</p>
<p>(2nd ed.). Berlin/New York: Springer.
Klebaner, F. C. (2005). Introduction to stochastic calculus with applications (2nd ed.). London:
</p>
<p>Imperical College Press.
Kr&auml;mer, W. (1986). Least squares regression when the independent variable follows an ARIMA
</p>
<p>process. Journal of the American Statistical Association, 81, 150&ndash;154.
Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., &amp; Shin Y. (1992). Testing the null hypothesis of
</p>
<p>stationarity against the alternative of a unit root. Journal of Econometrics, 54, 159&ndash;178.
Leybourne, S. J., &amp; McCabe, B. P. M. (1994). A simple test for cointegration. Oxford Bulletin of
</p>
<p>Economics and Statistics, 56, 97&ndash;103.
Maasoumi, E., &amp; McAleer, M. (2008). Realized volatility and long memory: An overview.
</p>
<p>Econometric Reviews, 27, 1&ndash;9.
MacKinnon, J. G. (1991). Critical values for co-integration tests. In R. F. Engle, &amp; C. W. J. Granger
</p>
<p>(Eds.), Long-run economic relationships (pp. 267&ndash;276). Oxford/New York: Oxford University
Press.
</p>
<p>MacKinnon, J. G. (1996). Numerical distribution functions for unit root and cointegration tests.
Journal of Applied Econometrics, 11, 601&ndash;618.
</p>
<p>Marsh, T. A., &amp; Rosenfeld, E. R. (1983). Stochastic processes for interest rates and equilibrium
bond prices. The Journal of Finance, XXXVIII, 635&ndash;646.</p>
<p/>
</div>
<div class="page"><p/>
<p>386 References
</p>
<p>Merton, R. C. (1973). Theory of rational option pricing. The Bell Journal of Economics and
Management Science, 4, 141&ndash;183.
</p>
<p>Mikosch, Th. (1998). Elementary stochastic calculus with finance in view. Singapore: World
Scientific Publishing.
</p>
<p>Mills, T. C., &amp; Markellos, R. N. (2008). The econometric modelling of financial time series (3rd
ed.). Cambridge/New York: Cambridge University Press.
</p>
<p>Nelson, D. B. (1991). Conditional heteroskedasticity in asset returns: A new approach. Economet-
rica, 59, 347&ndash;370.
</p>
<p>Nelson, D. B., &amp; Cao, Ch. Q. (1992). Inequality constraints in the univariate GARCH model.
Journal of Business &amp; Economic Statistics, 10, 229&ndash;235.
</p>
<p>Newey, W. K., &amp; West, K. D. (1987). A Simple, positive semi-definite, heteroskedasticity and
autocorrelation consistent covariance matrix. Econometrica, 55, 703&ndash;708.
</p>
<p>&Oslash;ksendal, B. (2003). Stochastic differential equations: An introduction with applications (6th ed.).
Berlin/New York: Springer.
</p>
<p>Park, J. Y. (1992). Canonical cointegrating regressions. Econometrica, 60, 119&ndash;143.
Park, J. Y., &amp; Phillips, P. C. B. (1988). Statistical inference in regressions with integrated processes:
</p>
<p>Part I. Econometric Theory, 4, 468&ndash;497.
Phillips, P. C. B. (1986). Understanding spurious regressions in econometrics. Journal of Econo-
</p>
<p>metrics, 33, 311&ndash;340.
Phillips, P. C. B. (1987). Time series regression with a unit root. Econometrica, 55, 277&ndash;301.
Phillips, P. C. B. (1988). Weak convergence of sample covariance matrices to stochastic integrals
</p>
<p>via martingale approximations. Econometric Theory, 4, 528&ndash;533.
Phillips, P. C. B. (1991). Optimal inference in cointegrated systems. Econometrica, 59, 283&ndash;306.
</p>
<p>Phillips, P. C. B., &amp; Durlauf, S. N. (1986). Multiple time series regression with integrated
processes. Review of Economic Studies, LIII, 473&ndash;495.
</p>
<p>Phillips, P. C. B., &amp; Hansen, B. E. (1990). Statistical inference in instrumental variables regression
with I(1) processes. Review of Economic Studies, 57, 99&ndash;125.
</p>
<p>Phillips, P. C. B., &amp; Loretan, M. (1991). Estimating long-run economic equilibria. Review of
Economic Studies, 58, 407&ndash;436.
</p>
<p>Phillips, P. C. B., &amp; Ouliaris, S. (1990). Asymptotic properties of residual based tests for
cointegration. Econometrica, 58, 165&ndash;193.
</p>
<p>Phillips, P. C. B., &amp; Park, J. Y. (1988). Asymptotic equivalence of ordinary least squares and
generalized least squares in regressions with integrated regressors. Journal of the American
Statistical Association, 83, 111&ndash;115.
</p>
<p>Phillips, P. C. B, &amp; Perron, P. (1988). Testing for a unit root in time series regression. Biometrika,
75, 335&ndash;346.
</p>
<p>Phillips, P. C. B., &amp; Solo, V. (1992). Asymptotics for linear processes. The Annals of Statistics, 20,
971&ndash;1001.
</p>
<p>P&ouml;tscher, B. M., &amp; Prucha, I. R. (2001). Basic elements of asymptotic theory. In B. H. Baltagi
(Ed.), A companion to theoretical econometrics (pp. 201&ndash;229). Malden: Blackwell.
</p>
<p>Ross, S. (2010). A first course in probability (8th ed.). Upper Saddle River: Prentice-Hall.
Rudin, W. (1976). Principles of mathematical analyis (3rd ed.). New York: McGraw-Hill.
Said, S. E., &amp; Dickey, D. A. (1984). Testing for unit roots in autoregressive-moving average models
</p>
<p>of unknown order. Biometrika, 71, 599&ndash;607.
Saikkonen, P. (1991). Asymptotically efficient estimation of cointegration regressions. Economet-
</p>
<p>ric Theory, 7, 1&ndash;21.
Shin, Y. (1994). A residual-based test of the Null of cointegration against the alternative of no
</p>
<p>cointegration. Econometric Theory, 10, 91&ndash;115.
Soong, T. T. (1973). Random differential equations in science and engineering. New York:
</p>
<p>Academic Press.
Stock, J. H. (1987). Asymptotic properties of least squares estimators of cointegrating vectors.
</p>
<p>Econometrica, 55, 1035&ndash;1056.
Stock, J. H., &amp; Watson, M. W. (1993). A simple estimator of cointegrating vectors in higher order
</p>
<p>integrated systems. Econometrica, 61, 783&ndash;820.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 387
</p>
<p>Syds&aelig;ter, K., Str&oslash;m, A., &amp; Berck, P. (1999). Economists&rsquo; mathematical manual (3rd ed.).
Berlin/New York: Springer.
</p>
<p>Tanaka, K. (1996). Time series analysis: Nonstationary and noninvertible distribution theory.
New York: Wiley.
</p>
<p>Taylor, S.J. (1994). Modeling stochastic volatlity: A review and comparative study. Mathematical
Finance, 4, 183&ndash;204.
</p>
<p>Trench, W. F. (2013). Introduction to real analysis. Free Hyperlinked Edition 2.04 December 2013.
Downloaded on 10th May 2014 from http://digitalcommons.trinity.edu/mono/7.
</p>
<p>Tsay, R. S. (2005). Analysis of financial time series (2nd ed.). New York: Wiley.
Tse, Y. K. (1995). Some international evidence on the stochastic behavior of interest rates. Journal
</p>
<p>of International Money and Finance, 14, 721&ndash;738.
Vasicek, O. (1977). An equilibrium characterization of the term structure. Journal of Financial
</p>
<p>Economics, 5, 177&ndash;188.
West, K. D. (1988). Asymptotic normality, when regressors have a unit root. Econometrica, 56,
</p>
<p>1397&ndash;1418.
White, H. (2001). Asymptotic theory for econometricians (2nd ed.). London/San Diego: Academic
</p>
<p>Press.
Wold, H. O. A. (1938). A study in the analysis of stationary time series. Stockholm: Almquist &amp;
</p>
<p>Wiksell.</p>
<p/>
<div class="annotation"><a href="http://digitalcommons.trinity.edu/mono/7">http://digitalcommons.trinity.edu/mono/7</a></div>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>Algebra, 14
� -, 14
Borel-, 16
</p>
<p>ARCH model, 130
EGARCH, 140
GARCH, 135
GARCH-M, 139
IGARCH, 137
</p>
<p>Autocorrelation, 31
Autocovariance, 31
Autoregressive distributed lag model, 354
</p>
<p>Brownian bridge, 163, 339
Brownian motion, 156
</p>
<p>with drift, 162
geometric, 165, 268
</p>
<p>Cadlag, 311
Causally invertible, 54
Cholesky decomposition, 319
Coefficient of determination, 342
Cointegration, 341, 355
Comparison of coefficients, 53, 71
Continuous differentiability, 200
Convergence, 155
</p>
<p>Cauchy criterion, 188
in distribution, 155, 190, 314
in mean square, 181, 186
in probability, 189
weak, 308, 313
</p>
<p>Correlation coefficient, 24
Covariance, 23
Cycle, 78
</p>
<p>annual, 83
cosine, 78
semi-annual, 83
sine, 224
</p>
<p>Density function, 17
Detrended regression, 371
Detrending, 331, 371, 373
Dickey-Fuller test, 336
Difference equation, 56, 60
</p>
<p>deterministic, 60
stochastic, 56
</p>
<p>Differential equation
with constant coefficients, 265
deterministic, 264
homogeneous, 264, 266
stochastic (see Stochastic differential
</p>
<p>equation)
Diffusion, 221, 243, 261
Distribution, 22
</p>
<p>conditional, 27
joint, 22
marginal, 22
multivariate, 30
</p>
<p>Distribution function, 16
Drift. See Integrated process
Durbin-Watson statistic, 342
</p>
<p>Error-correction model, 353, 355
Event, 13
Expectation
</p>
<p>conditional, 27
Expected value, 18, 267
</p>
<p>Filter, 51, 80, 85
causal, 51
difference, 52
</p>
<p>Fractional
differences, 106
integration, 107
</p>
<p>noise, 108
Frequency, 78
</p>
<p>&copy; Springer International Publishing Switzerland 2016
U. Hassler, Stochastic Processes and Calculus, Springer Texts in Business
and Economics, DOI 10.1007/978-3-319-23428-1
</p>
<p>389</p>
<p/>
</div>
<div class="page"><p/>
<p>390 Index
</p>
<p>Functional, 312
Functional central limit theorem, 307
</p>
<p>Gamma function, 107, 119, 121, 175
Gaussian distribution
</p>
<p>asymptotic, 361
Gaussian process, 30
</p>
<p>Impulse response, 50, 87, 104
Index set, 29
Inequality
</p>
<p>Cauchy-Schwarz, 25
Chebyshev&rsquo;s, 20
Jensen&rsquo;s, 26
Markov&rsquo;s, 20
triangle, 25, 312
</p>
<p>Information set, 32, 128
Integrated process, 305, 306, 317
</p>
<p>with drift, 334, 364
of order -1, I(-1), 306
</p>
<p>of order 0, I(0), 306
of order 1, I(1), 306
</p>
<p>Invariance principle, 308
Ito integral, 215
</p>
<p>autocovariance, 217
expected value, 216, 217
general, 219
variance, 216, 218
</p>
<p>Ito&rsquo;s lemma
bivariate with one factor, 245
for diffusions, 243
for Wiener processes, 240
multivariate, 250
with time as a dependent variable, 248
</p>
<p>Ito sum, 214
</p>
<p>KPSS test, 338
Kurtosis, 19
</p>
<p>Lag operator, 51
Lag polynomial, 53
</p>
<p>causally invertible, 54
invertible, 54
</p>
<p>Least squares estimator, 4, 331
Leverage effect, 141
L&rsquo;Hospital&rsquo;s rule, 205
Linear time trend, 331
Long memory, 104, 110
</p>
<p>Long-run variance, 303
consistent estimation, 335
matrix, 317
</p>
<p>Markov property, 59
Martingale, 32
</p>
<p>difference, 33, 129
Mean squared error, 187
Measurability, 15
Metric, 311
</p>
<p>supremum, 312
Moments, 18
</p>
<p>centered, 18
</p>
<p>Normal distribution, 21
bivariate, 24
log-, 165
</p>
<p>Ornstein-Uhlenbeck process, 204, 248, 285
properties, 205
</p>
<p>Partial integration, 200
Partition, 153, 180
</p>
<p>adequate, 180
disjoint, 153
equidistant, 153, 180
</p>
<p>Period, 78
Persistence, 50, 59, 86, 104
</p>
<p>anti-, 111
strong, 108
</p>
<p>Power transfer function, 80, 85
Probability, 13, 14
</p>
<p>space, 14
Process
</p>
<p>ARCH (see ARCH model)
ARMA, 64
autoregressive, 56
continuous-time, 30
discrete-time, 30
integrated (see Integrated process)
invertible, 65, 88, 109
linear, 49
Markov, 32
moving average, 45
normal, 30
pure random, 31
stationary, 30
stochastic, 29
strictly stationary, 30
weakly stationary, 31</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 391
</p>
<p>Random variable, 15
continuous, 17
integrable, 26
</p>
<p>Random walk, 151
continuous-valued, 153
discrete-valued, 152
</p>
<p>Residuals, 334, 335
Riemann integral, 181
</p>
<p>autocovariance, 185
expected value, 182
Gaussian distribution, 183
variance, 184
</p>
<p>Riemann-Stieltjes sum, 200
Riemann sum, 180
</p>
<p>Schur criterion, 56
Set of outcomes, 13
Skewness, 19
Spectral density function, 81
Spectrum, 80, 81
Stieltjes integral, 200, 220
</p>
<p>autocovariance, 204
</p>
<p>definition, 199
expected value, 202
Gaussian distribution, 202
variance, 202
</p>
<p>Stochastically independent, 23
Stochastic differential equation
</p>
<p>with constant coefficients, 268
with linear coefficients, 263
inhomogeneous linear with additive noise,
</p>
<p>268
moments of the solution, 267
uniqueness of solution, 262
</p>
<p>Stratonovich integral, 217
Superconsistency, 358
</p>
<p>Theorem
Donsker&rsquo;s, 308
Fubini&rsquo;s, 22, 182
Slutsky&rsquo;s, 315
</p>
<p>Time series, 29
Trend component, 81
Trend stationary, 332, 333
</p>
<p>Unit root, 306
</p>
<p>Variance, 18
Variation, 222
</p>
<p>absolute, 222
quadratic, 225
</p>
<p>Volatility, 127
</p>
<p>White noise, 31, 81
Wiener process, 156
</p>
<p>demeaned, 309
hitting time, 160
integrated, 185
maximum, 167
reflected, 164
scale invariance, 159
zero crossing, 161
</p>
<p>W.l.o.g., 60
Wold decomposition, 51</p>
<p/>
</div>
<ul>	<li>Preface</li>
<ul>	<li>Scope of the Book</li>
	<li>Note to Students and Instructors</li>
	<li>References</li>
</ul>
	<li>Acknowledgments</li>
	<li>Contents</li>
	<li>List of Figures</li>
	<li>1 Introduction</li>
<ul>	<li>1.1 Summary</li>
	<li>1.2 Finance</li>
<ul>	<li>Stock Prices</li>
	<li>Interest Rates</li>
	<li>Empirical Returns</li>
</ul>
	<li>1.3 Econometrics</li>
<ul>	<li>Random Walks</li>
	<li>Dickey-Fuller Distribution</li>
	<li>Autocorrelation</li>
</ul>
	<li>1.4 Mathematics</li>
<ul>	<li>Ito Integrals</li>
	<li>Ito's Lemma</li>
</ul>
	<li>1.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>Part I Time Series Modeling</li>
<ul>	<li>2 Basic Concepts from Probability Theory</li>
<ul>	<li>2.1 Summary</li>
	<li>2.2 Random Variables</li>
<ul>	<li>Probability Space</li>
	<li>Random Variable</li>
	<li>Continuous Random Variables</li>
	<li>Expected Value and Higher Moments</li>
	<li>Markov's and Chebyshev's Inequality</li>
</ul>
	<li>2.3 Joint and Conditional Distributions</li>
<ul>	<li>Joint Distribution and Independence</li>
	<li>Covariance</li>
	<li>Cauchy-Schwarz Inequality</li>
	<li>Conditional Distributions</li>
	<li>Conditional Expectation</li>
</ul>
	<li>2.4 Stochastic Processes (SP)</li>
<ul>	<li>Definition</li>
	<li>Stationary and Gaussian Processes</li>
	<li>Markov Processes and Martingales</li>
	<li>Martingale Differences</li>
</ul>
	<li>2.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>3 Autoregressive Moving Average Processes (ARMA)</li>
<ul>	<li>3.1 Summary</li>
	<li>3.2 Moving Average Processes</li>
<ul>	<li>MA(1)</li>
	<li>MA(q)</li>
	<li>MA(&infin;) Processes</li>
	<li>Impulse Responses</li>
</ul>
	<li>3.3 Lag Polynomials and Invertibility</li>
<ul>	<li>Causal Linear Filters</li>
	<li>Invertibility of Lag Polynomials</li>
	<li>Causally Invertible Polynomials</li>
</ul>
	<li>3.4 Autoregressive and Mixed Processes</li>
<ul>	<li>AR(1)</li>
	<li>AR(p)</li>
	<li>AR(2)</li>
	<li>Autoregressive Moving Average Processes</li>
	<li>ARMA(1,1)</li>
</ul>
	<li>3.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>4 Spectra of Stationary Processes</li>
<ul>	<li>4.1 Summary</li>
	<li>4.2 Definition and Interpretation</li>
<ul>	<li>Periodic Cycles</li>
	<li>Definition</li>
	<li>Interpretation</li>
	<li>Examples</li>
</ul>
	<li>4.3 Filtered Processes</li>
<ul>	<li>Filtered Processes</li>
	<li>Persistence</li>
	<li>ARMA Spectra</li>
</ul>
	<li>4.4 Examples of ARMA Spectra</li>
<ul>	<li>Summation over the Diagonal</li>
	<li>AR(1) Spectra</li>
	<li>AR(2) Spectra</li>
	<li>ARMA(1,1) Spectra</li>
	<li>Multiplicative Seasonal AR Process</li>
</ul>
	<li>4.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>5 Long Memory and Fractional Integration</li>
<ul>	<li>5.1 Summary</li>
	<li>5.2 Persistence and Long Memory</li>
<ul>	<li>Persistence</li>
	<li>Fractional Differencing and Integration</li>
</ul>
	<li>5.3 Fractionally Integrated Noise</li>
<ul>	<li>Fractional Noise and Long Memory</li>
	<li>Long Memory in the Frequency Domain</li>
</ul>
	<li>5.4 Generalizations</li>
<ul>	<li>Fractionally Integrated ARMA Processes (ARFIMA)</li>
	<li>Semiparametric Models</li>
	<li>Nonstationary Processes</li>
</ul>
	<li>5.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>6 Processes with Autoregressive Conditional Heteroskedasticity (ARCH)</li>
<ul>	<li>6.1 Summary</li>
	<li>6.2 Time-Dependent Heteroskedasticity</li>
<ul>	<li>Heteroskedasticity as a Function of the Past</li>
	<li>Heuristics</li>
</ul>
	<li>6.3 ARCH Models</li>
<ul>	<li>Conditional Moments</li>
	<li>Stationarity</li>
	<li>Correlation of the Squares</li>
	<li>Skewness and Kurtosis</li>
</ul>
	<li>6.4 Generalizations</li>
<ul>	<li>GARCH</li>
	<li>IGARCH </li>
	<li>GARCH-M </li>
	<li>EGARCH</li>
	<li>YAARCH</li>
</ul>
	<li>6.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
</ul>
	<li>Part II Stochastic Integrals</li>
<ul>	<li>7 Wiener Processes (WP)</li>
<ul>	<li>7.1 Summary</li>
	<li>7.2 From Random Walk to Wiener Process</li>
<ul>	<li>Random Walks</li>
	<li>Wiener Process</li>
	<li>Formal Definition</li>
</ul>
	<li>7.3 Properties</li>
<ul>	<li>Pathwise Properties</li>
	<li>Markov and Martingale Property</li>
	<li>Scale Invariance</li>
	<li>Hitting Time</li>
	<li>Zero Crossing</li>
</ul>
	<li>7.4 Functions of Wiener Processes</li>
<ul>	<li>Brownian Motion B(t)</li>
	<li>Brownian Motion with Drift X(t) = μ t + σ W(t) </li>
	<li>Brownian Bridge X(t) = B(t) - t  B(1)</li>
	<li>Reflected Wiener Process X(t) = |W(t)|</li>
	<li>Geometric Brownian Motion X(t) = eμt + σ W(t) </li>
	<li>Maximum of a WP X(t) = max0 &le;s &le;t W(s)</li>
	<li>Integrated Wiener Process X(t) = 0t W(s)  ds</li>
</ul>
	<li>7.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>8 Riemann Integrals</li>
<ul>	<li>8.1 Summary</li>
	<li>8.2 Definition and Fubini's Theorem</li>
<ul>	<li>Partition</li>
	<li>Definition and Existence</li>
	<li>Fubini's Theorem</li>
	<li>General Rules</li>
</ul>
	<li>8.3 Riemann Integration of Wiener Processes</li>
<ul>	<li>Normal Distribution</li>
	<li>Autocovariance Function</li>
	<li>Examples</li>
</ul>
	<li>8.4 Convergence in Mean Square</li>
<ul>	<li>Definition and Properties</li>
	<li>Convergence to a Constant</li>
	<li>Test of Convergence</li>
	<li>Further Modes of Convergence</li>
</ul>
	<li>8.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>9 Stieltjes Integrals</li>
<ul>	<li>9.1 Summary</li>
	<li>9.2 Definition and Partial Integration</li>
<ul>	<li>Definition</li>
	<li>Integration by Parts</li>
</ul>
	<li>9.3 Gaussian Distribution and Autocovariances</li>
<ul>	<li>Gaussian Distribution</li>
	<li>Autocovariance Function</li>
</ul>
	<li>9.4 Standard Ornstein-Uhlenbeck Process</li>
<ul>	<li>Definition</li>
	<li>Properties</li>
	<li>Simulation</li>
</ul>
	<li>9.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>Reference</li>
</ul>
	<li>10 Ito Integrals</li>
<ul>	<li>10.1 Summary</li>
	<li>10.2 A Special Case</li>
<ul>	<li>Problems with the Definition</li>
	<li>Ito Integral</li>
	<li>Stratonovich Integral</li>
</ul>
	<li>10.3 General Ito Integrals</li>
<ul>	<li>Definition and Moments</li>
	<li>Distribution and Further Properties</li>
	<li>Diffusions</li>
</ul>
	<li>10.4 (Quadratic) Variation</li>
<ul>	<li>(Absolute) Variation</li>
	<li>Quadratic Variation</li>
	<li>Wiener Processes</li>
	<li>Symbolic Notation</li>
</ul>
	<li>10.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>11 Ito's Lemma</li>
<ul>	<li>11.1 Summary</li>
	<li>11.2 The Univariate Case</li>
<ul>	<li>For Wiener Processes</li>
	<li>Explanation and Proof</li>
	<li>For Diffusions</li>
	<li>On the Proof</li>
</ul>
	<li>11.3 Bivariate Diffusions with One WP</li>
<ul>	<li>One-Factor Case</li>
	<li>Time as a Dependent Variable</li>
	<li>K-Variate Diffusions</li>
</ul>
	<li>11.4 Generalization for Independent WP</li>
<ul>	<li>The General Case</li>
	<li>The 2-Factor Case</li>
</ul>
	<li>11.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>Reference</li>
</ul>
</ul>
	<li>Part III Applications</li>
<ul>	<li>12 Stochastic Differential Equations (SDE)</li>
<ul>	<li>12.1 Summary</li>
	<li>12.2 Definition and Existence</li>
<ul>	<li>Diffusions</li>
	<li>Linear Coefficients</li>
	<li>Deterministic Case</li>
</ul>
	<li>12.3 Linear Stochastic Differential Equations</li>
<ul>	<li>Homogeneous Solution</li>
	<li>General Solution</li>
	<li>Expected Value and Variance</li>
	<li>Inhomogeneous Linear SDE with Additive Noise</li>
	<li>Proof of Proposition 12.2</li>
</ul>
	<li>12.4 Numerical Solutions</li>
<ul>	<li>Euler Approximation</li>
</ul>
	<li>12.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>13 Interest Rate Models</li>
<ul>	<li>13.1 Summary</li>
	<li>13.2 Ornstein-Uhlenbeck Process (OUP)</li>
<ul>	<li>Vasicek</li>
	<li>Simulations</li>
</ul>
	<li>13.3 Positive Linear Interest Rate Models</li>
<ul>	<li>Sufficient Condition</li>
	<li>Dothan</li>
	<li>Brennan-Schwartz</li>
	<li>Simulations</li>
</ul>
	<li>13.4 Nonlinear Models</li>
<ul>	<li>Cox, Ingersoll &amp; Ross [CIR]</li>
	<li>Further Models and Parameter Estimation</li>
</ul>
	<li>13.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>14 Asymptotics of Integrated Processes</li>
<ul>	<li>14.1 Summary</li>
	<li>14.2 Limiting Distributions of Integrated Processes</li>
<ul>	<li>Long-Run Variance</li>
	<li>Integrated Processes</li>
	<li>The Functional Central Limit Theorem (FCLT)</li>
	<li>First Implications</li>
</ul>
	<li>14.3 Weak Convergence of Functions</li>
<ul>	<li>Metric Function Spaces</li>
	<li>Continuous Functionals</li>
	<li>Weak Convergence</li>
	<li>Continuous Mapping Theorem</li>
</ul>
	<li>14.4 Multivariate Limit Theory</li>
<ul>	<li>Integrated Vectors</li>
	<li>Functional Limit Theory</li>
</ul>
	<li>14.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>15 Trends, Integration Tests and Nonsense Regressions</li>
<ul>	<li>15.1 Summary</li>
	<li>15.2 Trend Regressions</li>
<ul>	<li>Detrending</li>
	<li>Trend Stationarity</li>
	<li>I(1) with Drift</li>
	<li>Consistent Estimation of the Long-Run Variance</li>
</ul>
	<li>15.3 Integration Tests</li>
<ul>	<li>Dickey-Fuller [DF] Test for Nonstationarity</li>
	<li>KPSS Test for Stationarity</li>
	<li>Linear Time Trends</li>
</ul>
	<li>15.4 Nonsense Regression</li>
<ul>	<li>Cointegration</li>
	<li>Estimators and Statistics in the Regression Model</li>
	<li>Asymptotics</li>
</ul>
	<li>15.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>16 Cointegration Analysis</li>
<ul>	<li>16.1 Summary</li>
	<li>16.2 Error-Correction and Cointegration</li>
<ul>	<li>Autoregressive Distributed Lag Model</li>
	<li>Granger's Representation Theorem</li>
	<li>Cointegration and the Long-Run Variance Matrix</li>
	<li>Linearly Independent Cointegration Vectors</li>
</ul>
	<li>16.3 Cointegration Regressions</li>
<ul>	<li>Superconsistent Estimation</li>
	<li>Further Asymptotic Properties</li>
	<li>Asymptotic Normality</li>
	<li>Efficient Estimation</li>
	<li>Linear Time Trends</li>
</ul>
	<li>16.4 Cointegration Testing</li>
<ul>	<li>Residual-Based Dickey-Fuller Test</li>
	<li>Residual-Based KPSS Test</li>
	<li>Error-Correction Test</li>
	<li>Linear Time Trends</li>
</ul>
	<li>16.5 Problems and Solutions</li>
<ul>	<li>Problems</li>
	<li>Solutions</li>
</ul>
	<li>References</li>
</ul>
	<li>References</li>
</ul>
	<li>Index</li>
</ul>
</body></html>