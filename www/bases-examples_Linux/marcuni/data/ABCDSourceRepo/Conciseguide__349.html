<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title></title>
</head>
<body><div class="page"><p/>
<p>Undergraduate Topics in Computer Science
</p>
<p>Concise Guide 
</p>
<p>to Databases
</p>
<p>Peter Lake
</p>
<p>Paul Crowther
</p>
<p>A Practical Introduction</p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Topics in Computer
Science</p>
<p/>
</div>
<div class="page"><p/>
<p>Undergraduate Topics in Computer Science (UTiCS) delivers high-quality instructional content for un-
dergraduates studying in all areas of computing and information science. From core foundational and
theoretical material to final-year topics and applications, UTiCS books take a fresh, concise, and mod-
ern approach and are ideal for self-study or for a one- or two-semester course. The texts are all authored
by established experts in their fields, reviewed by an international advisory board, and contain numer-
ous examples and problems. Many include fully worked solutions.
</p>
<p>For further volumes:
www.springer.com/series/7592</p>
<p/>
<div class="annotation"><a href="http://www.springer.com/series/7592">http://www.springer.com/series/7592</a></div>
</div>
<div class="page"><p/>
<p>Peter Lake ï¿½ Paul Crowther
</p>
<p>Concise Guide
to Databases
</p>
<p>A Practical Introduction
</p>
<p>Foreword by Professor Richard Hill</p>
<p/>
</div>
<div class="page"><p/>
<p>Peter Lake
Sheffield Hallam University
Sheffield, UK
</p>
<p>Paul Crowther
Sheffield Hallam University
Sheffield, UK
</p>
<p>Series editor
</p>
<p>Ian Mackie
</p>
<p>Advisory board
</p>
<p>Samson Abramsky, University of Oxford, Oxford, UK
Karin Breitman, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil
Chris Hankin, Imperial College London, London, UK
Dexter Kozen, Cornell University, Ithaca, USA
Andrew Pitts, University of Cambridge, Cambridge, UK
Hanne Riis Nielson, Technical University of Denmark, Kongens Lyngby, Denmark
Steven Skiena, Stony Brook University, Stony Brook, USA
Iain Stewart, University of Durham, Durham, UK
</p>
<p>ISSN 1863-7310 ISSN 2197-1781 (electronic)
Undergraduate Topics in Computer Science
ISBN 978-1-4471-5600-0 ISBN 978-1-4471-5601-7 (eBook)
DOI 10.1007/978-1-4471-5601-7
Springer London Heidelberg New York Dordrecht
</p>
<p>Library of Congress Control Number: 2013955488
</p>
<p>&copy; Springer-Verlag London 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher&rsquo;s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
</p>
<p>Printed on acid-free paper
</p>
<p>Springer is part of Springer Science+Business Media (www.springer.com)</p>
<p/>
<div class="annotation"><a href="http://www.springer.com">http://www.springer.com</a></div>
<div class="annotation"><a href="http://www.springer.com/mycopy">http://www.springer.com/mycopy</a></div>
</div>
<div class="page"><p/>
<p>Dedicated to our mate Andy McEwan
</p>
<p>Paul and Peter</p>
<p/>
</div>
<div class="page"><p/>
<p>Foreword
</p>
<p>From tablets of stone through to libraries of parchments; from paper-based files to
the electronic era, there is not one aspect of modern business that has avoided the
need to collect, collate, organize and report upon data. The proliferation of databases
and database technologies within modern times, has now been further secured by the
use of the Internet to enable database integration on a massive scale.
</p>
<p>In amongst the innovation, the basic concepts remain. The need to organize&mdash;
a topic that Codd reminded us could be best done by relational models&mdash;is now
being challenged, as processor power and storage space become cheap and utility-
like with the advent of Cloud Computing infrastructure. But a glance at the past does
much to inform future thinking, and this book serves to prepare the foundations of
a mature approach to using database technologies in the 21st Century.
</p>
<p>In many cases, both established and emerging database technologies are readily
available and free to use. As such they may appear free to implement, which fuels
rapid adoption of technology that may not have been proven sufficiently, without the
formal governance that other business norms might impose. This creates an exciting,
risky, domain where commercial models can make or lose money depending upon
how they embrace and realize the potential of the technology. Conversely, there is
also an opportunity to solve problems when things go awry&mdash;and the accelerated
innovation that we now witness, presents more opportunities and pitfalls, if we do
not possess the requisite understanding of how databases should serve our needs.
</p>
<p>Proficiency in the field of databases is a combination of technical understanding,
conceptual knowledge and business acumen. All of these traits are underpinned by
education, and the need for professionals to continually update their knowledge.
Since professionals not only face the challenge of when to introduce a technology,
but also when not to adopt, it is important to understand the impact of failure as well
as success. This book takes readers through the essential basics, before charting a
path towards technical skill acquisition in the real-life context of business.
</p>
<p>Professor Richard HillHead of Subject, Computing and Mathematics
University of Derby, Derby, UK
June 2013
</p>
<p>vii</p>
<p/>
</div>
<div class="page"><p/>
<p>viii Foreword
</p>
<p>About Richard Hill:
</p>
<p>Richard Hill, PhD, is Professor of Intelligent Systems and Head of Department in the School of
</p>
<p>Computing and Mathematics, at the University of Derby, UK. Professor Hill has published widely
</p>
<p>in the areas of multi agent systems, computational intelligence, intelligent cloud computing and
</p>
<p>emerging technologies for distributed systems, and has organised a number of international confer-
</p>
<p>ences. Latterly, Professor Hill has edited and co-authored several book collections and textbooks,
</p>
<p>including &lsquo;Guide to Cloud Computing: Principles and Practice&rsquo;, published by Springer UK.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface
</p>
<p>Overview and Goals
</p>
<p>Databases are not new and there are many text books available which cover various
database types, especially relational. What is changing, however, is that Relational
Database Management Systems (RDBMS) are no longer the only database solution.
In an era where Big Data is the current buzzword and Data Scientists are tomorrow&rsquo;s
big earners, it is important to take a wider view of database technology.
</p>
<p>Key objectives for this book include:
&bull; Present an understanding of the key technologies involved in Database Systems
</p>
<p>in general and place those technologies in an historic context
&bull; Explore the potential use of a variety of database types in a business environ-
</p>
<p>ment
&bull; Point out areas for further research in a fast moving domain
&bull; Equip readers with an understanding of the important aspects of a database
</p>
<p>professional&rsquo;s job
&bull; Provide some hands-on experience to further assist in the understanding of the
</p>
<p>technologies involved
</p>
<p>Organisation and Features
</p>
<p>This book is organised into three parts:
&bull; Part I introduces database concepts and places them in both a historic and busi-
</p>
<p>ness context;
&bull; Part II provides insights into some of the major database types around today
</p>
<p>and also provides some hands-on tutorials in the areas concerned;
&bull; Part III is devoted to issues and challenges which face Database Professionals.
</p>
<p>Target Audiences
</p>
<p>This book has been written specifically to support the following audiences:
</p>
<p>Advanced undergraduate students and postgraduate students should find the
combination of theoretical and practical examples database usage of interest. We
imagine this text would be of particular relevance for modern Computer Science,
</p>
<p>ix</p>
<p/>
</div>
<div class="page"><p/>
<p>x Preface
</p>
<p>Software Engineering, and Information Technology courses. However, any course
that makes reference to databases, and in particular to the latest developments in
computing will find this text book of use. As such, University Instructors may adopt
the book as a core text.
</p>
<p>Especially in Part II, this book adopts a learning-by-doing approach, with the
extensive worked examples explaining how to use the variety of databases available
to address today&rsquo;s business needs. Practising Database Professionals, and Applica-
tion Developers will also be able to use this book to review the current state of the
database domain.
</p>
<p>Suggested Uses
</p>
<p>A Concise Guide to Databases can be used as a solid introduction to the concept of
databases. The book is suitable as both a comprehensive introduction to databases,
as well as a reference text as the reader develops their skills and abilities through
practical application of the ideas. For University Instructors, we suggest the follow-
ing programme of study for a twelve-week semester format:
&bull; Weeks 1&ndash;3: Part I
&bull; Weeks 4&ndash;8: Part II
&bull; Weeks 9&ndash;12: Part III
&bull; Week 12: Assessment
</p>
<p>Review Questions
</p>
<p>Each chapter concludes with a set of review questions that make specific reference
to the content presented in the chapter, plus an additional set of further questions
that will require further research. The review questions are designed in such a way
that the reader will be able to tackle them based on the chapter contents. They are
followed by discussion questions, that often require research, extended reading of
other material or discussion and collaboration. These can be used as classroom dis-
cussion topics by tutors or used as the basis of summative assignments.
</p>
<p>Hands-on Exercises
</p>
<p>The technology chapters include extended hands-on exercises. Readers will then
progressively engage in more complex activities, building skills and knowledge
along the way. Such an approach ensures that a solid foundation is built before more
advanced topics are undertaken. Some of the material here is Open Source, whilst
some examples are Oracle specific, but even these latter can be applied to other SQL
databases.</p>
<p/>
</div>
<div class="page"><p/>
<p>Preface xi
</p>
<p>Chapter Summary
</p>
<p>A brief summary of each of the twelve chapters is as follows:
Chapter 1: Data is the lifeblood of all business systems and we place the use of
</p>
<p>data in its historical context and review some of the key concepts in handling data.
Chapter 2: Provides an examination of the way that data has been handled
</p>
<p>throughout history, using databases of a variety of types.
Chapter 3: Considers how we actually store data. Turning information into a
</p>
<p>series of 1s and 0s is at the heart of every current database system and so an under-
standing of issues like physical storage and distribution are important concepts to
understand.
</p>
<p>Chapter 4: The de facto standard database solution is, without doubt, the rela-
tional database. In this chapter we look at how RDBMS works and provide worked
examples.
</p>
<p>Chapter 5: The NoSQL movement is still relatively new. Databases which store
data without schemas and which do not necessarily provide transactional security
may seem like a bad idea to experienced relational database practitioners, but these
tools do certainly have their place in today&rsquo;s data rich society. We review the area
in general and then look at specific examples of a Column-based and a Document-
based database, with hands-on tutorials for each.
</p>
<p>Chapter 6: Look at many leading database vendors&rsquo; web sites and you will see
that we are in the Big Data era. We explore what this actually means and, using a
tutorial, review one of the key concepts in this era&mdash;that of MapReduce.
</p>
<p>Chapter 7: Object databases were once thought of as the next important design
for databases. When used by developers using Object programming they can seem
very appealing still. There are half-way house solutions also available&mdash;Oracle, for
example, has an Object-Relational option. We explore this area with more tutorial
material.
</p>
<p>Chapter 8: Reading data from disk is far slower than reading from RAM. Com-
puting technologies now exist that can allow databases to run entirely in memory,
making for very rapid data processing. These databases may well become the norm
as RAM becomes cheaper and hard disk technology becomes less able to improve
in performance.
</p>
<p>Chapter 9: Once you have designed your database, especially when supporting
a web- or cloud-based solution, you need to be sure that it can grow if the business
that the application supports is successful. Scalability is about ensuring that you can
cope with many concurrent users, or huge amounts of data, or both.
</p>
<p>Chapter 10: Once your system is built, you need to be able to have it available for
use permanently (or as close to permanently as can be achieved within the financial
resources at your disposal). We review key concepts such as back-up, recovery, and
disaster recovery.
</p>
<p>Chapter 11: For a DBA the dreaded phone call is &ldquo;my report is running very
slowly&rdquo;. For a start, what is mean by slowly? What is the user used to? Then there
is the problem of how you establish where the problem is&mdash;is it hardware related?
Or Network related? At the Server or Client end? The solution may be indexes, or</p>
<p/>
</div>
<div class="page"><p/>
<p>xii Preface
</p>
<p>partitions: we review a variety of performance related techniques. We include some
tutorial material which explores some performance management tools.
</p>
<p>Chapter 12: Data is one of an organisation&rsquo;s most important assets. It needs to
be protected from people wanting to either take it, or bring the system down. We
look at physical and software-related weaknesses and review approaches to making
our databases secure.
</p>
<p>Peter Lake
Paul Crowther
</p>
<p>Sheffield, UK</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents
</p>
<p>Part I Databases in Context
</p>
<p>1 Data, an Organisational Asset . . . . . . . . . . . . . . . . . . . . . 3
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 In the Beginning . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 The Rise of Organisations . . . . . . . . . . . . . . . . . . . . 4
1.4 The Challenges of Multi-site Operation . . . . . . . . . . . . . 4
1.5 Internationalisation . . . . . . . . . . . . . . . . . . . . . . . . 5
1.6 Industrialisation . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.7 Mass Transport . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.8 Communication . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.9 Stocks and Shares . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.10 Corporate Takeovers . . . . . . . . . . . . . . . . . . . . . . . 10
1.11 The Challenges of Multi National Operations . . . . . . . . . . 11
1.12 The Data Asset . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.13 Electronic Storage . . . . . . . . . . . . . . . . . . . . . . . . 13
1.14 Big Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.15 Assets in the Cloud . . . . . . . . . . . . . . . . . . . . . . . . 16
1.16 Data, Data Everywhere . . . . . . . . . . . . . . . . . . . . . . 17
1.17 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.18 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
</p>
<p>1.18.1 Review Questions . . . . . . . . . . . . . . . . . . . . 18
1.18.2 Group Work Research Activities . . . . . . . . . . . . 19
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
</p>
<p>2 A History of Databases . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 The Digital Age . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.3 Sequential Systems . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4 Random Access . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.5 Origins of Modern Databases . . . . . . . . . . . . . . . . . . . 24
2.6 Transaction Processing and ACID . . . . . . . . . . . . . . . . 25
2.7 Two-Phase Commit . . . . . . . . . . . . . . . . . . . . . . . . 26
</p>
<p>xiii</p>
<p/>
</div>
<div class="page"><p/>
<p>xiv Contents
</p>
<p>2.8 Hierarchical Databases . . . . . . . . . . . . . . . . . . . . . . 27
2.9 Network Databases . . . . . . . . . . . . . . . . . . . . . . . . 27
2.10 Relational Databases . . . . . . . . . . . . . . . . . . . . . . . 28
2.11 Object Oriented Databases . . . . . . . . . . . . . . . . . . . . 30
2.12 Data Warehouse . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.13 The Gartner Hype Cycle . . . . . . . . . . . . . . . . . . . . . 32
2.14 Big Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.15 Data in the Cloud . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.16 The Need for Speed . . . . . . . . . . . . . . . . . . . . . . . . 34
2.17 In-Memory Database . . . . . . . . . . . . . . . . . . . . . . . 34
2.18 NoSQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.19 Spatial Databases . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.20 Databases on Personal Computers . . . . . . . . . . . . . . . . 36
2.21 Distributed Databases . . . . . . . . . . . . . . . . . . . . . . . 36
2.22 XML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.23 Temporal Databases . . . . . . . . . . . . . . . . . . . . . . . 38
2.24 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.25 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>2.25.1 Review Questions . . . . . . . . . . . . . . . . . . . . 39
2.25.2 Group Work Research Activities . . . . . . . . . . . . 39
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
</p>
<p>3 Physical Storage and Distribution . . . . . . . . . . . . . . . . . . . 41
3.1 The Fundamental Building Block . . . . . . . . . . . . . . . . 41
3.2 Overall Database Architecture . . . . . . . . . . . . . . . . . . 42
</p>
<p>3.2.1 In-Memory Structures . . . . . . . . . . . . . . . . . . 42
3.2.2 Walking Through a Straightforward Read . . . . . . . . 43
3.2.3 Server Processes . . . . . . . . . . . . . . . . . . . . . 45
3.2.4 Permanent Structures . . . . . . . . . . . . . . . . . . 46
</p>
<p>3.3 Data Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.3.1 Row Chaining and Migration . . . . . . . . . . . . . . 52
3.3.2 Non-relational Databases . . . . . . . . . . . . . . . . 52
</p>
<p>3.4 How Logical Data Structures Map to Physical . . . . . . . . . . 52
3.5 Control, Redo and Undo . . . . . . . . . . . . . . . . . . . . . 52
3.6 Log and Trace Files . . . . . . . . . . . . . . . . . . . . . . . . 54
3.7 Stages of Start-up and Shutdown . . . . . . . . . . . . . . . . . 54
3.8 Locking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.9 Moving Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.10 Import and Export . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>3.10.1 Data Is Important . . . . . . . . . . . . . . . . . . . . 61
3.11 Distributed Databases . . . . . . . . . . . . . . . . . . . . . . . 61
3.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.13 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.14 Group Work Research Activities . . . . . . . . . . . . . . . . . 64
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xv
</p>
<p>Part II Database Types
</p>
<p>4 Relational Databases . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.1 Origins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.2 Normalisation . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
</p>
<p>4.2.1 First Normal Form (1NF) . . . . . . . . . . . . . . . . 71
4.3 Second Normal Form (2NF) . . . . . . . . . . . . . . . . . . . 72
4.4 Third Normal Form (3NF) . . . . . . . . . . . . . . . . . . . . 73
4.5 Beyond Third Normal Form . . . . . . . . . . . . . . . . . . . 75
4.6 Entity Modelling . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.7 Use Case Modelling . . . . . . . . . . . . . . . . . . . . . . . 76
4.8 Further Modelling Techniques . . . . . . . . . . . . . . . . . . 82
4.9 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.10 Converting a Design into a Relational Database . . . . . . . . . 85
4.11 Worked Example . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.12 Create the Tables . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.13 CRUDing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.14 Populate the Tables . . . . . . . . . . . . . . . . . . . . . . . . 90
4.15 Retrieve Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.16 Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
4.17 More Complex Data Retrieval . . . . . . . . . . . . . . . . . . 93
4.18 UPDATE and DELETE . . . . . . . . . . . . . . . . . . . . . . 94
4.19 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.20 Group Work Research Activity . . . . . . . . . . . . . . . . . . 95
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
</p>
<p>5 NoSQL Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
5.1 Databases and the Web . . . . . . . . . . . . . . . . . . . . . . 97
5.2 The NoSQL Movement . . . . . . . . . . . . . . . . . . . . . . 98
</p>
<p>5.2.1 What Is Meant by NoSQL? . . . . . . . . . . . . . . . 100
5.3 Differences in Philosophy . . . . . . . . . . . . . . . . . . . . 101
5.4 Basically Available, Soft State, Eventually Consistent (BASE) . 103
5.5 Column-Based Approach . . . . . . . . . . . . . . . . . . . . . 103
5.6 Examples of Column-Based Using Cassandra . . . . . . . . . . 104
</p>
<p>5.6.1 Cassandra&rsquo;s Basic Building Blocks . . . . . . . . . . . 106
5.6.2 Data Sources . . . . . . . . . . . . . . . . . . . . . . . 107
5.6.3 Getting Started . . . . . . . . . . . . . . . . . . . . . . 107
5.6.4 Creating the Column Family . . . . . . . . . . . . . . 110
5.6.5 Inserting Data . . . . . . . . . . . . . . . . . . . . . . 112
5.6.6 Retrieving Data . . . . . . . . . . . . . . . . . . . . . 112
5.6.7 Deleting Data and Removing Structures . . . . . . . . 114
5.6.8 Command Line Script . . . . . . . . . . . . . . . . . . 115
5.6.9 Shutdown . . . . . . . . . . . . . . . . . . . . . . . . 116
</p>
<p>5.7 CQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.7.1 Interactive CQL . . . . . . . . . . . . . . . . . . . . . 118</p>
<p/>
</div>
<div class="page"><p/>
<p>xvi Contents
</p>
<p>5.7.2 IF You Want to Check How Well You Now Know
Cassandra . . . . . . . . . . . . . . . . . . . . . . . . . 119
</p>
<p>5.7.3 Timings . . . . . . . . . . . . . . . . . . . . . . . . . 120
5.8 Document-Based Approach . . . . . . . . . . . . . . . . . . . 120
</p>
<p>5.8.1 Examples of Document-Based Using MongoDB . . . . 122
5.8.2 Data Sources . . . . . . . . . . . . . . . . . . . . . . . 122
5.8.3 Getting Started . . . . . . . . . . . . . . . . . . . . . . 122
5.8.4 Navigation . . . . . . . . . . . . . . . . . . . . . . . . 123
5.8.5 Creating a Collection . . . . . . . . . . . . . . . . . . 123
5.8.6 Simple Inserting and Reading of Data . . . . . . . . . . 125
5.8.7 More on Retrieving Data . . . . . . . . . . . . . . . . 127
5.8.8 Indexing . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.8.9 Updating Data . . . . . . . . . . . . . . . . . . . . . . 130
5.8.10 Moving Bulk Data into Mongo . . . . . . . . . . . . . 130
</p>
<p>5.9 IF You Want to Check How Well You Now Know
MongoDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.9.1 Timings . . . . . . . . . . . . . . . . . . . . . . . . . 131
</p>
<p>5.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.11 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.12 Group Work Research Activities . . . . . . . . . . . . . . . . . 132
</p>
<p>5.12.1 Sample Solutions . . . . . . . . . . . . . . . . . . . . 133
5.12.2 MongoDB Crib . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
</p>
<p>6 Big Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
6.1 What Is Big Data? . . . . . . . . . . . . . . . . . . . . . . . . 135
6.2 The Datacentric View of Big Data . . . . . . . . . . . . . . . . 137
</p>
<p>6.2.1 The Four Vs . . . . . . . . . . . . . . . . . . . . . . . 138
6.2.2 The Cloud Effect . . . . . . . . . . . . . . . . . . . . . 140
</p>
<p>6.3 The Analytics View of Big Data . . . . . . . . . . . . . . . . . 141
6.3.1 So Why Isn&rsquo;t Big Data Just Called Data
</p>
<p>Warehousing 2? . . . . . . . . . . . . . . . . . . . . . 142
6.3.2 What Is a Data Scientist? . . . . . . . . . . . . . . . . 145
6.3.3 What Is Data Analysis for Big Data? . . . . . . . . . . 147
</p>
<p>6.4 Big Data Tools . . . . . . . . . . . . . . . . . . . . . . . . . . 147
6.4.1 MapReduce . . . . . . . . . . . . . . . . . . . . . . . 148
6.4.2 Hadoop . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.4.3 Hive, Pig and Other Tools . . . . . . . . . . . . . . . . 150
</p>
<p>6.5 Getting Hands-on with MapReduce . . . . . . . . . . . . . . . 151
6.6 Using MongoDB&rsquo;s db.collection.mapReduce() Method . . . . . 152
</p>
<p>6.6.1 And If You Have Time to Test Your MongoDB and JS
Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
</p>
<p>6.6.2 Sample Solutions . . . . . . . . . . . . . . . . . . . . 156
6.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.8 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 158</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xvii
</p>
<p>6.9 Group Work Research Activities . . . . . . . . . . . . . . . . . 158
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
</p>
<p>7 Object and Object Relational Databases . . . . . . . . . . . . . . . 161
7.1 Querying Data . . . . . . . . . . . . . . . . . . . . . . . . . . 161
7.2 Problems with Relational Databases . . . . . . . . . . . . . . . 161
7.3 What Is an Object? . . . . . . . . . . . . . . . . . . . . . . . . 163
7.4 An Object Oriented Solution . . . . . . . . . . . . . . . . . . . 164
7.5 XML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
7.6 Object Relational . . . . . . . . . . . . . . . . . . . . . . . . . 168
7.7 What Is Object Relational? . . . . . . . . . . . . . . . . . . . . 169
7.8 Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
7.9 Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
7.10 Hierarchies and Inheritance . . . . . . . . . . . . . . . . . . . . 174
7.11 Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
7.12 Encapsulation and Polymorphism . . . . . . . . . . . . . . . . 177
7.13 Polymorphism . . . . . . . . . . . . . . . . . . . . . . . . . . 178
7.14 Support for Object Oriented and Object Relational Database
</p>
<p>Development . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
7.15 Will Object Technology Ever Become Predominant in Database
</p>
<p>Systems? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
7.15.1 Review Questions . . . . . . . . . . . . . . . . . . . . 181
7.15.2 Group Work Research Activities . . . . . . . . . . . . 181
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>8 In-Memory Databases . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.2 Origins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.3 Online Transaction Processing Versus Online Analytical
</p>
<p>Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
8.4 Interim Solution&mdash;Create a RAM Disk . . . . . . . . . . . . . . 185
8.5 Interim Solution&mdash;Solid State Drive (SSD) . . . . . . . . . . . 186
8.6 In-Memory Databases&mdash;Some Misconceptions . . . . . . . . . 187
8.7 In-Memory Relational Database&mdash;The Oracle TimesTen
</p>
<p>Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.8 In-Memory Column Based Storage&mdash;The SAP HANA
</p>
<p>Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.9 In-Memory On-line Transaction Processing&mdash;The Starcounter
</p>
<p>Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
8.10 Applications Suited to In-Memory Databases . . . . . . . . . . 193
8.11 In Memory Databases and Personal Computers . . . . . . . . . 193
8.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
8.13 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 195
8.14 Group Work Research Activities . . . . . . . . . . . . . . . . . 195
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196</p>
<p/>
</div>
<div class="page"><p/>
<p>xviii Contents
</p>
<p>Part III What Database Professionals Worry About
</p>
<p>9 Database Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . 201
9.1 What Do We Mean by Scalability? . . . . . . . . . . . . . . . . 201
9.2 Coping with Growing Numbers of Users . . . . . . . . . . . . . 202
</p>
<p>9.2.1 So Why Can&rsquo;t Access Cope with Lots of Users? . . . . 203
9.2.2 Client/Server . . . . . . . . . . . . . . . . . . . . . . . 204
9.2.3 Scalability Eras . . . . . . . . . . . . . . . . . . . . . 206
</p>
<p>9.3 Coping with Growing Volumes of Data . . . . . . . . . . . . . 208
9.3.1 E-Commerce and Cloud Applications Need Scalable
</p>
<p>Solutions . . . . . . . . . . . . . . . . . . . . . . . . . 209
9.3.2 Vertical and Horizontal Scaling . . . . . . . . . . . . . 210
9.3.3 Database Scaling Issues . . . . . . . . . . . . . . . . . 210
9.3.4 Single Server Solutions . . . . . . . . . . . . . . . . . 212
9.3.5 Distributed RDBMS: Shared Nothing vs. Shared Disk . 213
9.3.6 Horizontal Scaling Solutions . . . . . . . . . . . . . . 215
9.3.7 Scaling Down for Mobile . . . . . . . . . . . . . . . . 217
</p>
<p>9.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
9.5 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 218
9.6 Group Work Research Activities . . . . . . . . . . . . . . . . . 218
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
</p>
<p>10 Database Availability . . . . . . . . . . . . . . . . . . . . . . . . . . 221
10.1 What Do We Mean by Availability? . . . . . . . . . . . . . . . 221
10.2 Keeping the System Running&mdash;Immediate Solutions to Short
</p>
<p>Term Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 222
10.2.1 What Can Go Wrong? . . . . . . . . . . . . . . . . . . 223
</p>
<p>10.3 Back-up and Recovery . . . . . . . . . . . . . . . . . . . . . . 229
10.3.1 MTBF and MTTR . . . . . . . . . . . . . . . . . . . . 231
</p>
<p>10.4 Disaster Recovery (DR) . . . . . . . . . . . . . . . . . . . . . 235
10.4.1 Business Impact . . . . . . . . . . . . . . . . . . . . . 236
10.4.2 High Availability for Critical Systems . . . . . . . . . . 237
10.4.3 Trade-offs and Balances . . . . . . . . . . . . . . . . . 238
10.4.4 The Challenge or Opportunity of Mobile . . . . . . . . 238
</p>
<p>10.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
10.6 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 239
10.7 Group Work Research Activities . . . . . . . . . . . . . . . . . 239
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
</p>
<p>11 Database Performance . . . . . . . . . . . . . . . . . . . . . . . . . 241
11.1 What Do We Mean by Performance? . . . . . . . . . . . . . . . 241
11.2 A Simplified RDBMS Architecture . . . . . . . . . . . . . . . 243
11.3 Physical Storage . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>11.3.1 Block Size . . . . . . . . . . . . . . . . . . . . . . . . 245
11.3.2 Disk Arrays and RAID . . . . . . . . . . . . . . . . . 246
11.3.3 Alternatives to the HDD . . . . . . . . . . . . . . . . . 246</p>
<p/>
</div>
<div class="page"><p/>
<p>Contents xix
</p>
<p>11.3.4 Operating System (OS) . . . . . . . . . . . . . . . . . 247
11.3.5 Database Server Processes . . . . . . . . . . . . . . . . 248
11.3.6 Archive Manager . . . . . . . . . . . . . . . . . . . . 248
11.3.7 Schema Level: Data Types, Location and Volumes . . . 249
11.3.8 SQL Optimisation . . . . . . . . . . . . . . . . . . . . 250
11.3.9 Indexes . . . . . . . . . . . . . . . . . . . . . . . . . . 251
11.3.10 Network . . . . . . . . . . . . . . . . . . . . . . . . . 255
11.3.11 Application . . . . . . . . . . . . . . . . . . . . . . . 256
</p>
<p>11.4 Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
11.4.1 What Is Database Tuning? . . . . . . . . . . . . . . . . 258
11.4.2 Benchmarking . . . . . . . . . . . . . . . . . . . . . . 259
11.4.3 Another Perspective . . . . . . . . . . . . . . . . . . . 259
</p>
<p>11.5 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
11.5.1 Tuning and Performance Tools . . . . . . . . . . . . . 261
11.5.2 Using the Built-in Advisers . . . . . . . . . . . . . . . 275
11.5.3 Over to You! . . . . . . . . . . . . . . . . . . . . . . . 276
</p>
<p>11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
11.7 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 277
11.8 Group Work Research Activities . . . . . . . . . . . . . . . . . 277
Appendix Creation Scripts and Hints . . . . . . . . . . . . . . . . . 277
</p>
<p>A.1 Hints on the Over to You Section . . . . . . . . . . . . 279
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
</p>
<p>12 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
12.2 Physical Security . . . . . . . . . . . . . . . . . . . . . . . . . 284
12.3 Software Security&mdash;Threats . . . . . . . . . . . . . . . . . . . 286
12.4 Privilege Abuse . . . . . . . . . . . . . . . . . . . . . . . . . . 286
12.5 Platform Weaknesses . . . . . . . . . . . . . . . . . . . . . . . 291
12.6 SQL Injection . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
12.7 Weak Audit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
12.8 Protocol Vulnerabilities . . . . . . . . . . . . . . . . . . . . . . 294
12.9 Authentication Vulnerabilities . . . . . . . . . . . . . . . . . . 295
12.10 Backup Data Exposure . . . . . . . . . . . . . . . . . . . . . . 295
12.11 Mobile Device Based Threats . . . . . . . . . . . . . . . . . . 296
12.12 Security Issues in Cloud Based Databases . . . . . . . . . . . . 296
12.13 Policies and Procedures . . . . . . . . . . . . . . . . . . . . . . 298
12.14 A Security Checklist . . . . . . . . . . . . . . . . . . . . . . . 298
12.15 Review Questions . . . . . . . . . . . . . . . . . . . . . . . . . 299
12.16 Group Work Research Activities . . . . . . . . . . . . . . . . . 300
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
</p>
<p>Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303</p>
<p/>
</div>
<div class="page"><p/>
<p>Part I
</p>
<p>Databases in Context</p>
<p/>
</div>
<div class="page"><p/>
<p>1Data, an Organisational Asset
</p>
<p>What the reader will learn:
</p>
<p>&bull; The rise of the organisation
&bull; The evolution of data usage and processing in organisations
&bull; Technological change and its impact on data
&bull; Data storage, retrieval and analysis&mdash;the road to competitive advantage
&bull; Data exploitation and privacy
</p>
<p>1.1 Introduction
</p>
<p>Today an organisation depends on data for its very survival. Data is used to make
strategic decisions about the future direction an organisation will take and for that
the data must be both current and accurate. Because data is an asset to a company
it can be given a value and it can be traded. This chapter will look at data as an
organisational asset tracing its importance in recording, analysing and planning as
organisations grew and became more sophisticated. The impact of industrialisation
on data requirements will be examined and finally with the advent of electronic
data storage and processing, the emergence of data as an important component of
a company&rsquo;s assets. Finally there will be a discussion of big data the current issues
surrounding privacy when everything that can be recorded is recorded.
</p>
<p>There are many examples of data being an organisational asset, but we will start
with a specific well known example. Amazon, is an on-line retailer which originally
sold books. It has now become an organisation dealing with a large variety of goods
and services both as the primary seller and as a facilitator for other retailers. Any
web user can search for items using key words and can restrict their search in various
ways. Once you buy an item from Amazon you can create an account. Amazon uses
the information about what you bought (and searched for) to suggest other items
you might like to buy. It also cross references this with what other people bought.
So if you bought a wireless networking card for your desktop computer, you would
also get a list of what other people bought with it. Google is exploiting data in your
transaction in real time to influence your shopping decisions.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_1,
&copy; Springer-Verlag London 2013
</p>
<p>3</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_1">http://dx.doi.org/10.1007/978-1-4471-5601-7_1</a></div>
</div>
<div class="page"><p/>
<p>4 1 Data, an Organisational Asset
</p>
<p>This is an example of how an organisation uses data in an on-line transaction in a
real time way. It is the current stage of an evolutionary process of storing, processing
and exploiting data which began with the first record keeping.
</p>
<p>1.2 In the Beginning
</p>
<p>Religious orders and governments were the first large organisations to gather and
actively exploit data to raise revenue. Recorded data has been known to exist since
at least 2800 BC in ancient Egypt. These included records of the earliest known
forms of taxation. Records were held on limestone flakes and papyrus. The Rosetta
Stone, famous for holding the key to translating hieroglyphics (the same information
was written in three languages on the stone, one of which could be understood and
was used to translate the other two) was created to show a temples exemption from
taxes.
</p>
<p>1.3 The Rise of Organisations
</p>
<p>Before considering electronic data and its use in modern commercial organisations
we need to consider how non-government organisations developed and how their
data needs grew more and more sophisticated. In any government or business organ-
isation, there are always records. At its most basic you need to know who owes you
money and who you owe money to. More sophisticated records would include the
value of infrastructure such as buildings and machinery as well as its depreciation&mdash;
loss of value&mdash;due to wear and tear plus the costs of maintenance. Where the organ-
isation consists of more than a single trader records of employees need to be kept.
</p>
<p>In Britain pre-dissolution monasteries maintained excellent records and accounts
of the estates they controlled (something that was also of value to Henry VIII and
which Thomas Cromwell exploited from 1534 as part of the Dissolution). Lands
and money were donated to the monasteries with many of them becoming large
landowners. Income from these estates made many of them extremely wealthy.
This growth in wealth along with numbers of lay brothers who filled ancillary roles
and provided labour created a complexity that could only be managed by keeping
records. The monk in charge of these records and the finances of the monastery was
the bursar who oversaw the maintenance of detailed records of both income and ex-
penditure of the estate. In many cases this was structured within specific categories,
for example capital expenditure (on buildings). In today&rsquo;s terminology the bursar
would be called the database administrator.
</p>
<p>1.4 The Challenges of Multi-site Operation
</p>
<p>The monasteries in sixteenth century England were independent organisations
loosely federated by their allegiance to the Roman Catholic Church based in Rome.
As mentioned, they often owned vast estates with many farms or granges which</p>
<p/>
</div>
<div class="page"><p/>
<p>1.5 Internationalisation 5
</p>
<p>could be considerable distances (for the day) from the monastery. These granges
needed to keep local records, but also needed to submit accounts to the monastery.
This could be regarded as a form of distributed data where there was a need for
day to day local account keeping along with a centralised master account system.
A problem was maintaining data integrity. Once accounts from the granges had been
submitted to the monastery counting house, records could be updated but there was
always a time delay. At the time this was not regarded as important because at the
time there was no competitive advantage in very rapid processing of records.
</p>
<p>1.5 Internationalisation
</p>
<p>Despite the previous example most early organisational data was held centrally be-
cause the reach of the organisation tended to be geographically small. This changed
with exploration and colonisation. There needed to be data kept in multiple loca-
tions. With European colonisation of other parts of the world, trading companies
had to start keeping data locally as well as at head office. Some of these companies
were huge, for example the East India Company came into being in 1600 and by
the eighteenth century effectively controlled large parts of India and had its own
army and navy. Its base was London, but it had multiple trading centres in India and
China. The London base was called East India House and it housed the company
records. These and the people needed to maintain them kept growing with the result
that the building was continually being expanded. The demand for more space and
the poor condition of the original building lead was the justification for a new build-
ing. This was completed in 1729. Even this was not enough and as the company
continued to grow adjoining properties were acquired.
</p>
<p>What the company developed was effectively a distributed database. Day to day
operations in the colony were handled locally with records being transported back to
head office where the ever expanding central records were updated and processed.
Since long distance data transmission was by ship and there was a risk of the ship
not making it home, there was always going to be a problem with data currency and
integrity. This could be solved by a certain level of data redundancy. For example
rather than relying on one ship for your records, you replicated everything and sent
it by two or more ships. You also distributed valuable cargo between your ships. If
one sank, all the records, including about what was on the missing ship survived.
This was an example of backup and security. It was also a form of insurance.
</p>
<p>The East India Company was wound up in 1858 primarily due to public and
government concern about its activities. It had been blamed in part for triggering
American war of Independence (the tea in the Boston tea Party was company stock)
and laying the foundations for the First Opium War where Indian opium was used
to trade for tea. However the British government took over many of the operations
of the Company in the building became the India Office.
</p>
<p>The East India Company has been used to illustrate the development of a organi-
sation with international reach, but it was by no means an isolated example. Around
the time the company was wound up technological advances including the develop-
ment of the telegraph and faster and more reliable transport improved the efficiency</p>
<p/>
</div>
<div class="page"><p/>
<p>6 1 Data, an Organisational Asset
</p>
<p>of national and international organisations. With more data which was increasingly
more current better planning and control could be implemented. The following sec-
tions look at the new technologies and their impact on data and the organisation.
</p>
<p>1.6 Industrialisation
</p>
<p>Between the late eighteenth and early nineteenth there was a change from small
scale hand manufacturing to large scale mass manufacturing. This is usually referred
to as the industrial revolution although there were several phases covering not just
manufacturing but also the supporting infrastructure. In this period, manufacturing
progressively moved from a cottage industry to a factory based enterprise. This
required detailed records to effectively manage a large scale operation.
</p>
<p>Materials required for the industrial process were much larger than previously re-
quired in cottage industries. Although the term was not really used until the 1950&rsquo;s
in business this was supply chain logistics. Consumables had to be bought and trans-
ported. It is costly to have stock in a warehouse; therefore you want to have as little
raw material on hand as possible but no so little as to affect production. Likewise
finished goods need to be shipped as soon as possible. Labour needed to be hired
and managed. All of this required record keeping to plan and manage operations,
in other words data. It also lead to a transport revolution, first with canals and then
with railways to move primarily goods and later in the case of the railways, people.
Both of these had their own data requirements.
</p>
<p>The sophistication of the industrialisation process kept increasing through the
nineteenth and twentieth century. The introduction of the assembly line further in-
creased efficiency but it was dependent on the line being supplied. This is a supply
process known as logistics (although the term was not used for business until the
mid-1950&rsquo;s) and requires: &ldquo;having the right item in the right quantity at the right
time at the right place for the right price in the right condition. . . &rdquo; (Mallik 2010).
This required processing of data to make sure the right decisions on where and when
to buy materials was made. Even today, a break in supply of components can shut
down an entire operation. In Australia, BorgWarner supplied gearboxes to most of
the country&rsquo;s vehicle manufacturers. Only two days&rsquo; supply of these components
were held at the assembly factories, so a strike at BorgWarner effectively shut down
the rest of automotive manufacturing centre.
</p>
<p>1.7 Mass Transport
</p>
<p>Another impact of the industrial revolution was on transport. Up until the nineteenth
century transport had been either by horse or ship. Neither was particularly fast and
most people rarely travelled far outside their local community. Mass movement of
goods inland tended to be by boat but was restricted to navigable rivers. Although
canals for navigation were not new, it was the Bridgewater Canal opened in 1761 for
the transport of coal that proved the efficiency of this form of transport. The success</p>
<p/>
</div>
<div class="page"><p/>
<p>1.8 Communication 7
</p>
<p>of Bridgewater Canal lead to many more being developed and resulted in dramatic
falls in the transport costs of bulky materials. The dominance of canals was relatively
short. The rapid development of the Railway network from 1840, initially for freight
but also for passengers meant that meant material goods and people could be moved
great distances quickly and cheaply. The railway system was however dependent on
data for its successful (and safe) operation. Trains had to be scheduled which meant
timetables had to be developed. There were several forms of these. One was for
passengers who wanted to know when a train was leaving. There was also a roster
for engines, carriages and their crews&mdash;basically another timetable but one which
had to be updated more often because of availability. To make sure you had a place
on the train (particularly if you were a first class passenger) meant seat reservations
had to be implemented. This is a description of just a small part of the records and
data processing needed for running railways in the Victorian era.
</p>
<p>Things didn&rsquo;t really change until the advent of mass air travel. Consider an air-
line booking systems. Today you can book a seat with an airline specifying the date
and time of your flights and even your seat on the plane by accessing a website. Be-
fore networked computers the system was paper based. At head office there would
be a record of each flight scheduled for a specific period in the future. Based on
experience, travel agents would be assigned a number of seats on each plane which
they could sell over the counter. If they ran out of seats or they had not been al-
located seats on a particular flight they would have to phone the flight centre to
request a seat. If there were some in reserve, once again they could book it directly,
otherwise, the customer had to be logged and told they would be contacted once
a check of available seats had been made. The booking centre would then have to
call around other agents who had been assigned seats and see if any were still left.
Depending on the outcome, the customer would be contacted. This was a cumber-
some system and with the growth in air travel it became increasingly sophisticated.
Introduction of computerised booking meant the number of staff needed to run it
declined dramatically.
</p>
<p>There was also another effect of the computerisation of data. Once you had
booked on a plane, you had a record of who the customer was and where they were
going which was easily retrievable. If they were a frequent traveller you could start
building a profile of where they tended to go and how often. That meant you could
be sent targeted marketing material. Initially this was by post, but now is most likely
to be via e-mail.
</p>
<p>1.8 Communication
</p>
<p>Before the advent of technology to transmit information electrically (and it was
electrical rather than electronic) transmission of data was by hand. Faster technolo-
gies, such as the semaphore system and other line of sight communication systems
did exist. In Tasmania (then Van Diemen&rsquo;s Land) a semaphore system built in 1811
could be used to report an escape from the penal settlement at Port Arthur within
seven minutes. Port Arthur is approximately 100 kilometres (60 miles) from Hobart</p>
<p/>
</div>
<div class="page"><p/>
<p>8 1 Data, an Organisational Asset
</p>
<p>Fig. 1.1 1865 map showing proposed submarine and land telegraphs around the world. (Bill
Burns: Altlantic-Cable.com Website)
</p>
<p>by road meaning such a message could take a day or more to get through by hand.
The big disadvantage of the system was it was only useable in daylight in clear
conditions. The semaphore system in Tasmania was discontinued in 1880 with the
installation of a telegraph system (Fig. 1.1).
</p>
<p>The electric telegraph which was developed in the early nineteenth century meant
data could be transmitted much more quickly and reliably using a standardised
code&mdash;Morse code. This was a series of dots and dashes which represented numbers
and letters. More commonly used letters had shorter codes then longer ones, for ex-
ample T was a single dash while J was a dot and three dashes. Accuracy could still
be a problem however. A solution to this was to send important information twice
and then investigate any discrepancies. Another solution was to introduce a check
number where the final digit was a function of the preceding digits. This meant
a calculation had to be done at both the sending and receiving end slowing down
transmission. This issue was solved in computers using a similar process&mdash;the main
difference being it was automated and quicker.
</p>
<p>Although the electric telegraph was superseded by telephone and radio technol-
ogy, Morse code continues to be used because it is less sensitive to poor signal
conditions. Aeronautical systems such as VHF omnidirectional radio range (VOR)
constantly send identification signals in the code.</p>
<p/>
<div class="annotation"><a href="http://Altlantic-Cable.com">http://Altlantic-Cable.com</a></div>
</div>
<div class="page"><p/>
<p>1.9 Stocks and Shares 9
</p>
<p>An important invention using telegraph technology was invented in 1870. The
ticker tape machine was a device that allowed information about stocks and shares
to be transmitted via the telegraph system. Unlike the telegraph Morse code tapper
key, the sending device had a keyboard (which originally looked like a piano key-
board) to send plain language text. At the receiving end the information was printed
in readable text on a thin strip of paper. Speeds were typically one character per
second so abbreviations for company names were used. This was the first example
of dedicated electrical commercial information transmission and it survived into the
1960&rsquo;s.
</p>
<p>The next two communication technologies which had a major impact on data
transmission were the telephone and the radio (although the latter still relied more on
the Morse code than voice). Major systems were built around the use of telephones
as already illustrated by the airline booking system example. Direct recording of the
majority of data sent this way was rare however. It was almost always transcribed
by an operator which always resulted in a certain level of transcription errors.
</p>
<p>Direct data transmission between remote users, computers and other computers
started in the late 1950&rsquo;s and was originally devised because of cold war concerns.
However, as early as 1964 SABRE (Semi-automated Business Research Environ-
ment) was used by the airline industry to keep bookings correct in real time and was
accessible to travel agents around the world. The amount of transcription and hence
errors was also reduced by this system as the user was entering data directly into the
system and automated checks could be applied.
</p>
<p>1.9 Stocks and Shares
</p>
<p>Up to this point the discussion has been about organisations that either produce
things or supply the infrastructure to enable production along with the supporting
role of records and data processing. However one of the biggest users and produc-
ers of data has always been financial sector. This sector makes money by buying
and selling holdings in other companies, in other words shares, lending and storing
money for individuals and organisations (banks) and indemnifying individuals and
organisations against loss (insurance).
</p>
<p>Stock Exchange The London Stock Exchange was set up in 1801 although trade
in stocks and shares had been going on for a much longer time. For example John
Castaing, operating out of a coffee house in London&rsquo;s Exchange Alley published the
prices of a few commodities (including salt and coal) several times a week in 1698.
The new exchange was different in that it was regulated and only licensed traders
dealing in listed stocks could operate. The new exchange kept records of trades and
the price they could be traded at. This could be used along with information on
supply and demand to make decisions on whether to buy or sell stock&mdash;what we
now call market intelligence.</p>
<p/>
</div>
<div class="page"><p/>
<p>10 1 Data, an Organisational Asset
</p>
<p>Banking Modern banking had its origins in the city states of Renaissance Italy
such as Venice, Genoa and Florence. In the United Kingdom lack of trust in the
government (for example the appropriation of &pound;200,000 of private money in 1640
by Charles I) led merchants to deposit gold and silver with the Worshipful Company
of Goldsmiths. Originally the goldsmiths were an artisan company but over time
incorporated silversmiths and jewellers.
</p>
<p>Goldsmiths discovered they now had an asset (money) they could lend at an
interest. This turned out to be quite profitable. The next logical step was to actively
solicit deposits on which they paid interest, but which could be loaned out at a
higher interest. A side effect of this was deposit receipts started being used in lieu
of payment by coin. This in turn meant Goldsmiths didn&rsquo;t have to have to lend real
money&mdash;a deposit receipt would be treated in the same way. You could therefore
lend more than you actually had. This system is still in place today but is regulated.
</p>
<p>The basis of banking is records and data. Originally these were kept at your
physical bank branch and arrangements had to be made if you wished to make a
transaction (particularly a withdrawal) at another branch. Banks opened their doors
quite late and closed quite early in comparison to other institutions because of the
need to keep complex accounts including applying interest calculations. The speed
of manual data processing was limiting the availability of the banking system to
users.
</p>
<p>Insurance This was a way of spreading risks against loss, especially of ships and
cargo. The concept is not a new one with Chinese merchants distributing cargo
between several ships so that the loss of one would not mean total loss. Modern
insurance had roots in the Great Fire of London of 1666 in that in 1680 an office
opened to insure brick and timber framed buildings against loss by fire. The concept
was you paid a premium (which was invested) based on the risk of your house. It
assumed lots of people would do that so if a house burned down, the cost of the loss
would be covered out of the pool of money received in insurance premiums. A side
effect was insurance companies setting up their own fire brigades to fight fires in
buildings insured by them.
</p>
<p>Once again this required a lot of record keeping including who was insured, what
their premiums were and what the risk of them making a claim was. Like today some
areas and some individuals were more risky than others. Another aspect was keeping
control of how the premiums were invested. Insurance companies had to store large
amounts of money, and just like the banks, were able to invest it to make more
money. Insurance companies therefore became important players in both the stock
exchange and banking. It is also no surprise that the first commercial organisation
to buy a computer was an insurance company.
</p>
<p>1.10 Corporate Takeovers
</p>
<p>Companies are often the target of takeover bids, or attempt takeovers of other com-
panies. The part that usually reaches the headlines is the amount the company is
bought or sold for and only rarely the issues of assimilating the takeover target. One</p>
<p/>
</div>
<div class="page"><p/>
<p>1.11 The Challenges of Multi National Operations 11
</p>
<p>reason company will become a takeover target is because their shares are seen as
undervalued. However a company may become a target because the buyer wants to
either expand their range of goods and services, gain access to more customers, en-
ter into a new regional market or a combination of all three. After a takeover issues
with corporate data rarely make the headlines but there are almost always issues
with assimilating an organisations systems.
</p>
<p>The data stored by one company will have one or more of the following incom-
patibilities:
&bull; different vendor supplied system(s)
&bull; different data formats
&bull; different database design
&bull; duplication of key data (for example customer numbers)
and that is only part of the of a potential list. Given these issues, a number of options
are available:
&bull; run more than one system (Channel 4 took this approach).
&bull; load the data from one system into another. This may require programming to
</p>
<p>automate the system, but data verification will be necessary.
</p>
<p>1.11 The Challenges of Multi National Operations
</p>
<p>Many of the world&rsquo;s big companies are multinational, or transnational. This is a step
on from the idea of internationalisation previously mentioned. It is possible that the
location of the company head office is dictated by tax issues and very little actual
data processing takes place there. Amazon is one example of this.
</p>
<p>&ldquo;Amazon.co.uk is thought to have classified itself as a service provider to its Luxembourg
business, Amazon EU Sarl, in order to reduce its tax bill. Its UK business employed 4,200
people at the end of 2012, compared with 380 in Luxembourg.&rdquo;
(BBC News http://www.bbc.co.uk/news/business-22549434 accessed 18/06/2013.)
</p>
<p>Further the product being bought may have been manufactured in Taiwan, pur-
chased by a customer in Australia using the Amazon UK website referencing data
stored on a server in the United States. The single purchase may therefore involve
multiple subsidiary companies operating under a multitude of different national leg-
islation.
</p>
<p>What appears on the surface to be a single organisation is, therefore, often a fed-
eration of companies with a complex ownership and management structure. Data in
the Amazon example has to be distributed both geographically and organisationally.
You buy a book from Amazon UK which is where the transaction is recorded. The
local warehouse were the product is located may run its own segment of the corpo-
rate database to locate stock and generate packing lists. Actually payment is then
routed through Luxembourg company which would also have a local database seg-
ment of the corporate database. The data therefore becomes distributed amongst a
number of businesses. Distributed database systems and transaction processing will
be discussed in more detail in the next chapter.</p>
<p/>
<div class="annotation"><a href="http://Amazon.co.uk">http://Amazon.co.uk</a></div>
<div class="annotation"><a href="http://www.bbc.co.uk/news/business-22549434">http://www.bbc.co.uk/news/business-22549434</a></div>
</div>
<div class="page"><p/>
<p>12 1 Data, an Organisational Asset
</p>
<p>1.12 The Data Asset
</p>
<p>As already mentioned corporate account data was initially held in paper ledgers
and journals. This was often supplemented by indexed files&mdash;small cards arranged
in some order containing information. For example, each customer would have an
index card recording their name and address and possibly other comments.
</p>
<p>Index cards appeared in the eighteenth century. The first recorded use of them
was by the botanist Carl Linnaeus who needed a system to record species he was
studying. He wanted something that was easy to insert new data into and easy to
retrieve existing data. The zenith of the card system was its use in libraries. Here
there were a number of card indexes which indexed the physical book collection.
Usually there were indexes for author, title and subject. Details of the book along
with its physical shelf location were recorded. From the late nineteenth century shelf
locations were organised by the Dewey decimal classification which was founded
on subject.
</p>
<p>Variations of card indexes, for example recipe cards are still in use today al-
though electronic databases applications are making inroads and library catalogues
are almost all now electronic.
</p>
<p>Index cards were usually only organised in one way. Personal data would nor-
mally be sorted on surname for example. This was not a problem when only one
record was wanted and the data happened to be sorted in a way compatible with
the search. Once you wanted to search on more than field, then you had a problem.
This was partially overcome by the use of edge-notched cards. These consisted of
a border with small holes punched in it, then a body where information could be
written. The cards could be classified by clipping the holes so they extended to the
edge of the card. By using one or more needles inserted through the holes, cards
could be lifted out&mdash;those that remained were the search results. This system meant
the cards need not be in any fixed order although they all had to be oriented the same
way. The orientation was achieved by bevelling one corner, a system also used on
Hollerith punch cards). Any card not in the correct order would then be immediately
obvious. This system had one major drawback&mdash;it only worked on relatively small
numbers of cards and it was impossible to do instant data analysis.
</p>
<p>These examples were all for manual data storage and retrieval. It worked but it
was slow and unsuitable for handling large amounts of data that had to be processed
on a regular basis. Census data was a particular problem and in 1890 Herman Hol-
lerith came up with a proposal for storing census data on punched cards which could
be read by a machine.
</p>
<p>The idea of using punched cards was not new in 1890. They had first appeared in
the early eighteenth century for controlling textile looms&mdash;the pattern to be woven
was stored on the card which automatically &lsquo;programmed&rsquo; the machine. The most
often quoted examples are the punched cards of the Jacquard loom of 1801.
</p>
<p>The Hollerith card was at the centre of a technology that created, sorted and tab-
ulated data. Hollerith created the Tabulating Machine Company which joined with
three other companies to become the Computing Tabulating Recording Company
in 1901. This company was eventually renamed International Business Machines</p>
<p/>
</div>
<div class="page"><p/>
<p>1.13 Electronic Storage 13
</p>
<p>(IBM) in 1924 and went on to become a dominant player in the mainframe and
early personal computer industry.
</p>
<p>The original Hollerith card had 12 rows and 24 columns but in 1928 the number
of columns was increased to 80. Initially these were only used for numeric data,
with the bottom ten holes representing the digits 0 through to 9. The top two rows
were used for indicating positive and negative. Alphabetic data was stored by using
a combination of holes, so a hole in row 1 and row 4 represented the letter &lsquo;A&rsquo;
(Fig. 1.2).
</p>
<p>The 80 column card became an industry standard and also influenced other early
computing technology. For example early monitors tended to have 80 column dis-
plays and the coding forms for the COBOL programming language was also organ-
ised in 80 columns because early programs were also entered via punched cards.
</p>
<p>This paper technology had a number of significant issues:
&bull; It was bulky.
&bull; Individual cards could not be updated&mdash;they had to be re-punched which slowed
</p>
<p>down turnaround. There are stories of operators sticking chats back into the
holes.
</p>
<p>&bull; Order (particularly of program code stored on cards) was important&mdash;the au-
thors boss once came off his motorbike while carrying two boxes of punch cards
on the back. It was quicker to reproduce the cards than to try sorting them.
</p>
<p>&bull; It was sensitive to environmental conditions. High humidity caused the cards to
swell and jam the processing machines.
</p>
<p>&bull; Care had to be taken to avoid rodent attacks.
&bull; They were a fire hazard.
</p>
<p>Card processing reached its peak with the popular IBM 360 series of computers.
During the 1970&rsquo;s card technology became less and less common and had all but
disappeared by the 1980&rsquo;s replaced by tape and disk technology.
</p>
<p>1.13 Electronic Storage
</p>
<p>Punched card technology had an initial advantage over electronic technology&mdash;it
was very cheap. However tape and disk technology were available at the same time.
These technologies became progressively cheaper, faster and able to store more and
more data in a compact form.
</p>
<p>Magnetic tape was used on the first commercial computer, UNIVAC 1, however
it was IBM&rsquo;s tape technology that became the industry standard during the 1950&rsquo;s.
Initially this was a half inch wide seven track tape which gave way to nine track
tape with the introduction of the IBM 360 series of computers. Depending on the
recording density between 5 MB and 140 MB data could be stored on a single tape.
Data is written to the tape in blocks which contain a number of records. Between
each block was an inter-block gap. This was needed because the memory buffer on
the tape drive had a limited size and once full had to be transferred to the central
processing unit. The tape had to be stopped while this was happening, then acceler-
ated back to read speed for the next block. This resulted in the characteristic jerky
movement of early tape drives.</p>
<p/>
</div>
<div class="page"><p/>
<p>14 1 Data, an Organisational Asset
</p>
<p>F
ig
.1
</p>
<p>.2
80
</p>
<p>co
lu
</p>
<p>m
n
</p>
<p>pu
nc
</p>
<p>h
ca
</p>
<p>rd
.N
</p>
<p>ot
e:
</p>
<p>in
th
</p>
<p>is
ex
</p>
<p>am
pl
</p>
<p>e
th
</p>
<p>e
m
</p>
<p>ea
ni
</p>
<p>ng
of
</p>
<p>ho
le
</p>
<p>s
is
</p>
<p>pr
in
</p>
<p>te
d
</p>
<p>ab
ov
</p>
<p>e
ea
</p>
<p>ch
co
</p>
<p>lu
m
</p>
<p>n,
so
</p>
<p>th
is
</p>
<p>ar
ra
</p>
<p>y
of
</p>
<p>ho
le
</p>
<p>s
re
</p>
<p>pr
es
</p>
<p>en
ts
</p>
<p>11
5,
</p>
<p>25
.1
</p>
<p>,1
/(
</p>
<p>or
ig
</p>
<p>in
al
</p>
<p>im
ag
</p>
<p>e
by
</p>
<p>H
ar
</p>
<p>ke
av
</p>
<p>ai
la
</p>
<p>bl
e
</p>
<p>at
ht
</p>
<p>tp
:/
</p>
<p>/c
om
</p>
<p>m
on
</p>
<p>s.
w
</p>
<p>ik
im
</p>
<p>ed
ia
</p>
<p>.o
rg
</p>
<p>/w
ik
</p>
<p>i/
F
</p>
<p>il
e:
</p>
<p>P
un
</p>
<p>ch
_c
</p>
<p>ar
d_
</p>
<p>80
_c
</p>
<p>ol
um
</p>
<p>ns
_(
</p>
<p>2)
.j
</p>
<p>pg
ac
</p>
<p>ce
ss
</p>
<p>ed
07
</p>
<p>/0
8/
</p>
<p>20
13
</p>
<p>)</p>
<p/>
<div class="annotation"><a href="http://commons.wikimedia.org/wiki/File:Punch_card_80_columns_(2).jpg">http://commons.wikimedia.org/wiki/File:Punch_card_80_columns_(2).jpg</a></div>
</div>
<div class="page"><p/>
<p>1.14 Big Data 15
</p>
<p>Magnetic tape stores data sequentially, so in that regard it is very similar to cards.
It is also a relatively cheap technology which makes it very convenient for making
backups of systems. It does however have a number of problems.
1. The tape and the recording heads are in contact with each other which means
</p>
<p>over time both wear with increasing read/write errors as they get older. It also
means tape heads have to be cleaned on a regular basis.
</p>
<p>2. Although less bulky than cards, they are still bulky and prone to damage.
3. Sequential access limits the applications it can be used for.
</p>
<p>The final type of magnetic technology is disk based. This was also introduced
in the 1950&rsquo;s by IBM and is a random access device. When first introduced they
were the most expensive storage device and had relatively low capacities beginning
at 3.7 Mb. However, right from the beginning storage capacities went up while price
came down. Data could now be accessed directly on the disk&mdash;a big advantage over
tape technology where to find an individual record you had to read, on average half
the records on a tape, something you would never do. Storage on disk meant the data
had to be organised and this lead to the development of theoretical, then practical
frameworks which became databases. The evolution of database technology will be
discussed in the next chapter.
</p>
<p>Although disk technology was the last new technology, it too is being superseded
for new data storage and retrieval applications by main memory. As mentioned in
the previous paragraph, the cost of disk storage has come down, but so has the cost
of computer memory. Early core memory&mdash;so called because it was made up of
magnetic rings or cores was very expensive and bulky. The introduction of semi-
conductor memory resulted in massive reductions in size and price and increase in
speed. Initially a major problem semiconductor memory was that it was volatile&mdash;if
the power went off you lost all the data stored in memory. Today that is no longer
an issue. Chapter 3 will talk in detail about physical aspects of data retrieval.
</p>
<p>It is now feasible to store a database in a computer&rsquo;s memory&mdash;in-memory
databases which will be discussed in Chap. 8. This gives the advantages of very fast
processing not limited by the input/output speed restrictions of disk drives. Disk
backup is still required for backup and recovery operations.
</p>
<p>1.14 Big Data
</p>
<p>Whenever you use a loyalty card, for example a Nectar card in Britain or a Wal-Mart
card in the USA to gain points which can be used to purchase goods and services
you also &lsquo;give&rsquo; the card owner a lot of information about your buying habits. What
you bought, where you bought it, when you bought it and how you bought it are all
captured at the point of sale. Because you will have given them other information
such as your age, address and gender, this can also be cross-referenced and other
information generated, for example, how far from home your purchase was made
and was it likely to have been made on your way to or from work. Now think of
how many people go through all Tesco supermarkets a day using their loyalty cards
and you get an idea of how much data is generated. The new term for this is big</p>
<p/>
</div>
<div class="page"><p/>
<p>16 1 Data, an Organisational Asset
</p>
<p>data&mdash;recognition of the fact that it is difficult to process using traditional database
management tools. IBM say
</p>
<p>&ldquo;Big data is more than simply a matter of size; it is an opportunity to find insights in new and
emerging types of data and content, to make your business more agile, and to answer ques-
tions that were previously considered beyond your reach. Until now, there was no practical
way to harvest this opportunity.&rdquo;
(http://www-01.ibm.com/software/data/bigdata/ accessed 22/06/2013).
</p>
<p>The concept is not new however and grew out of the field of data warehousing.
A data warehouse is a repository for organisational data uploaded from its opera-
tional system. Point of sale transactions as described in the Tesco example above are
linked to other systems such as inventory to generate restocking orders but copies of
all transactions are place in the data warehouse. The warehouse is not a database be-
cause its organisation is different and data is added, retrieved and analysed, but not
modified. It is therefore a write once, read many system which continually grows.
Processing data like this can yield unexpected patterns. For example it was discov-
ered that male shoppers under 30 had a tendency to buy nappies (or diapers) and
beer in single shopping run. It turned out fathers were being sent to stock up on nap-
pies and took the opportunity to buy beer at the same time. The supermarket chain
involved used this information to reconfigure isles so nappies and beer were close
together increasing the chance of male customers&rsquo; impulse buying beer.
</p>
<p>Big data technologies are also of interest to government, particularly intelligence
agencies. The National Security Agency (NSA) has built what is arguably the largest
data storage facility in the world, the Utah Data Centre. We explore Big Data in more
detail in Chap. 6.
</p>
<p>1.15 Assets in the Cloud
</p>
<p>In 2006 Amazon announced the release of Amazon Elastic Compute Cloud (EC2).
This was one of the first commercial Cloud applications where, rather than storing
data on your own server, you stored it somewhere else. That &lsquo;somewhere else&rsquo; was
something you didn&rsquo;t need to worry about (although many organisations did), you
just bought space and time from the cloud provider who provided the resource you
wanted on a server who&rsquo;s physical location could be anywhere in the world. The ad-
vantage of cloud computing was that you didn&rsquo;t have to worry about capacity or even
backup and recovery. There are numerous examples of cloud computing being used
for operational systems. For example G-Cloud (http://gcloud.civilservice.gov.uk/) is
a system used by the United Kingdom civil service to make services widely avail-
able and to distribute a range of material through its CloudStore.
</p>
<p>With Cloud computing an organisation no longer has to store its data asset itself,
it can be left with a trusted provider in much the same way an organisation deposits
money (an asset) with a bank. It does raise issues of security and privacy, some of
which are addressed in the next section and in Chap. 12.</p>
<p/>
<div class="annotation"><a href="http://www-01.ibm.com/software/data/bigdata/">http://www-01.ibm.com/software/data/bigdata/</a></div>
<div class="annotation"><a href="http://gcloud.civilservice.gov.uk/">http://gcloud.civilservice.gov.uk/</a></div>
</div>
<div class="page"><p/>
<p>1.16 Data, Data Everywhere 17
</p>
<p>1.16 Data, Data Everywhere
</p>
<p>An issue which has emerged in the last few years is the question: Is technology and
more specifically databases destroying privacy? This claim has gained momentum
with revelations about the use the data at the Utah Data Centre will be put to. Claims
of assaults on privacy are not new. In 1890 in the United States of America William
Brandeis and Samuel Warren writing in the Harvard Law Review claimed there
was an unprecedented assault on privacy. The media they were referring to was the
tabloid press and cheap photography. Up until that point the issue of privacy had
not been a major issue&mdash;publicity was the exception rather than the norm. Privacy
could be ensured by physical walls and the distance from neighbours. That is not to
say spying did not occur.
</p>
<p>The introduction of a national postal service brought the first real concerns about
data privacy. Once you posted a letter it was almost impossible to ensure no one tam-
pered with it, even if it was sealed. That ultimately resulted in laws outlawing tam-
pering and establishing a legal right to privacy in correspondence. The thing about
this kind of written data was that although it could be copied, that was time consum-
ing. Also letters (and telegraphs) that were not intercepted could be destroyed after
they were read which also ensured their confidentiality.
</p>
<p>Today we exist in a world where everything that can be recorded is recorded and
correcting or deleting all copies of data is difficult if not impossible. Aaron Bady
writing in the MIT Technology Review in 2011 states:
</p>
<p>&ldquo;We shouldn&rsquo;t worry about particular technologies of broadcasting or snooping&mdash;for in-
stance the way Facebook trumpets our personal information or deep packet inspection al-
lows government to trawl through oceans of internet data. The most important change is not
the particular technology but, rather, the increase in the number of pathways through which
information flows&rdquo;
(Bady 2011, &lsquo;World Without Walls&rsquo;, MIT Technology Review, Vol. 114(6) p. 68).
</p>
<p>Bady says that the baseline of our information environment has become a ten-
dency towards total availability and recall&mdash;indefinitely storing and circulating ev-
erything. We often add to the circulation ourselves by agreeing to allow the sharing
of our details. This is often in the &lsquo;fine print&rsquo; of the terms and conditions of loy-
alty card application forms or when you set up an account with an on-line retailer.
All this means that collecting standardizing, processing and selling vast pools of
data has become big business&mdash;data as a true, tradable international organisational
asset. It is not just legitimately acquired data that is being sold. Cases of files of
personal information that could be used for a variety of fraudulent activities being
traded have been reported. In an example from the BBC in 2009 it was reported:
&lsquo;Staff at mobile phone company T-Mobile passed on millions of records from thou-
sands of customers to third party brokers, the firm has confirmed.&rsquo; (17/11/2009,
http://news.bbc.co.uk/1/hi/8364421.stm accessed 22/06/2013). In actual fact this
case was illegal rather fraudulent with the information being used for direct mar-
keting. The case resulted in prosecutions.
</p>
<p>It should be noted, that data is not just an asset for private organisations, Gov-
ernment intelligence agencies increasingly see data as an intelligence asset. Using
data mining techniques, automated systems can monitor data, tracking and analysing</p>
<p/>
<div class="annotation"><a href="http://news.bbc.co.uk/1/hi/8364421.stm">http://news.bbc.co.uk/1/hi/8364421.stm</a></div>
</div>
<div class="page"><p/>
<p>18 1 Data, an Organisational Asset
</p>
<p>anomalies. It is another question as to who decides what an anomaly is. In the United
States a lot of the impetus for this type of analysis was the 9/11 2001 terrorist attacks
where the investigating commission blamed a &lsquo;failure to connect the dots&rsquo; as part of
the failure to predict the attacks.
</p>
<p>Governments can gain access not only to their citizen&rsquo;s data, but often to the data
of citizens in other countries, particularly given the growing popularity of cloud stor-
age where the physical servers may be located anywhere, but come under the laws
of the country where they are located. This has caused international tensions. In
June 2013 The European commission&rsquo;s vice-president, Viviane Reding said &ldquo;Direct
access of US law enforcement to the data of EU citizens on servers of US compa-
</p>
<p>nies should be excluded unless in clearly defined, exceptional and judicially review-
</p>
<p>able situations&rdquo; (http://www.guardian.co.uk/world/2013/jun/11/europe-us-privacy
last accessed 22/06/2013). This was in response to revelations from a whistle-blower
in the American National Security Agency (NSA) that the organisation was conduct-
ing widespread surveillance of United States citizens and possibly citizens of other
countries using data stored on servers in America.
</p>
<p>1.17 Summary
</p>
<p>In this chapter we have concentrated on data and its value to organisations. Evidence
of commercial data has been found in ancient organisations, but much of this chapter
has been devoted to the rise of the modern organisation. In parallel to the increasing
sophistication of organisations and their need for data we looked at the technologies
which helped facilitate its storage and transport noting the rapid changes which
started in the nineteenth century and which are still continuing.
</p>
<p>We concluded with looking at the vast amount of data which both governments
and commercial organisations are amassing and how they are using it. We also
looked at the increasing public concerns about privacy.
</p>
<p>In the following chapters we will look first at how electronic data is stored and
processed. This will show the evolution of database management systems. After that
various aspects of database technology will be looked at in more depth.
</p>
<p>1.18 Exercises
</p>
<p>1.18.1 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
</p>
<p>1. When were the first financial records kept and in what format
2. What is a ticker tape machine and what was its importance in data communi-
</p>
<p>cations?
3. Why were punched cards a popular technology and why did their use decline?
4. List the data issues that can occur when one company takes over another
5. Why is privacy more a concern now than 200 years ago?</p>
<p/>
<div class="annotation"><a href="http://www.guardian.co.uk/world/2013/jun/11/europe-us-privacy">http://www.guardian.co.uk/world/2013/jun/11/europe-us-privacy</a></div>
</div>
<div class="page"><p/>
<p>References 19
</p>
<p>1.18.2 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Activity 1 Make a list of the loyalty cards you have. Go the web site associated
with each of them (they almost certainly will have a web site) and for each find:
&bull; terms and conditions.
&bull; are they allowed to share your data with other organisations?
&bull; have you (can you) opted out of that sharing?
&bull; do they state what they are going to do with any data stored on you?
&bull; what are their privacy policies?
</p>
<p>Activity 2 There have been a number of articles about &lsquo;going off grid&rsquo; where you
do not leave a digital footprint of your activities or movement. John Platt in his
2012 article &lsquo;Going off the grid: Why more people are choosing to live life un-
plugged&rsquo; (available at http://www.mnn.com/lifestyle/responsible-living/stories/
going-off-the-grid-why-more-people-are-choosing-to-live-life-un last accessed
2/7/2013) says there are lots of grids: car grid, supermarket grid and the bank
grid are among those mentioned.
&bull; How difficult is it to go completely off grid?
&bull; Is it desirable to go completely off grid?
&bull; How many grids could you opt out of?&mdash;make a list with advantages and
</p>
<p>disadvantages for each you choose to opt out of
</p>
<p>References
</p>
<p>Bady A (2011) World without walls. MIT Technol Rev 114(6):66&ndash;71
Mallik S (2010) In: Bidgoil H (ed) The handbook of technology management: supply chain man-
</p>
<p>agement, marketing and advertising, and global management, vol 2. Wiley, Hoboken, p 104
</p>
<p>Further Reading
</p>
<p>Anderson KB, Erik D, Salinger MA (2008) Identity theft. J Econ Perspect 22(2):171&ndash;192
Bocij P, Greasley A, Hickie S (2008) Business information systems: technology, development and
</p>
<p>management, 4th edn. Prentice Hall, Harrow. Chap. 3
Brandeis W, Samuel Warren S (1890) The right to privacy. Harvard Law Rev 4(5). Available at
</p>
<p>http://groups.csail.mit.edu/mac/classes/6.805/articles/privacy/Privacy_brand_warr2.html, last
accessed 12/06/2013
</p>
<p>Lawson P (1993) The East India Company: a history. Longman, London
London Stock Exchange (2013) Our history. http://www.londonstockexchange.com/about-the-
</p>
<p>exchange/company-overview/our-history/our-history.htm, last accessed 22/06/2013
McKinsey and Company (2011) Big data: the next frontier for innovation, competition, and produc-
</p>
<p>tivity. http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_
for_innovation, last accessed 22/06/2013
</p>
<p>National Archives (2013) Dissolution of the monasteries 1536&ndash;1540. http://www.nationalarchives.
gov.uk/records/research-guides/dissolution-of-the-monasteries.htm, last accessed 22/06/2013.
This site gives links to monastic records of the time</p>
<p/>
<div class="annotation"><a href="http://www.mnn.com/lifestyle/responsible-living/stories/going-off-the-grid-why-more-people-are-choosing-to-live-life-un">http://www.mnn.com/lifestyle/responsible-living/stories/going-off-the-grid-why-more-people-are-choosing-to-live-life-un</a></div>
<div class="annotation"><a href="http://www.mnn.com/lifestyle/responsible-living/stories/going-off-the-grid-why-more-people-are-choosing-to-live-life-un">http://www.mnn.com/lifestyle/responsible-living/stories/going-off-the-grid-why-more-people-are-choosing-to-live-life-un</a></div>
<div class="annotation"><a href="http://groups.csail.mit.edu/mac/classes/6.805/articles/privacy/Privacy_brand_warr2.html">http://groups.csail.mit.edu/mac/classes/6.805/articles/privacy/Privacy_brand_warr2.html</a></div>
<div class="annotation"><a href="http://www.londonstockexchange.com/about-the-exchange/company-overview/our-history/our-history.htm">http://www.londonstockexchange.com/about-the-exchange/company-overview/our-history/our-history.htm</a></div>
<div class="annotation"><a href="http://www.londonstockexchange.com/about-the-exchange/company-overview/our-history/our-history.htm">http://www.londonstockexchange.com/about-the-exchange/company-overview/our-history/our-history.htm</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://www.nationalarchives.gov.uk/records/research-guides/dissolution-of-the-monasteries.htm">http://www.nationalarchives.gov.uk/records/research-guides/dissolution-of-the-monasteries.htm</a></div>
<div class="annotation"><a href="http://www.nationalarchives.gov.uk/records/research-guides/dissolution-of-the-monasteries.htm">http://www.nationalarchives.gov.uk/records/research-guides/dissolution-of-the-monasteries.htm</a></div>
</div>
<div class="page"><p/>
<p>2A History of Databases
</p>
<p>What the reader will learn:
&bull; The Origins of databases
&bull; Databases of the 1960&rsquo;s and 1970&rsquo;s
&bull; Current mainstream database technologies&mdash;relational versus object orientation
&bull; The need for speed in database systems
&bull; Distributed databases and the challenges of transaction processing with dis-
</p>
<p>tributed data
</p>
<p>2.1 Introduction
</p>
<p>In Chap. 1 we discussed data as an organisational asset. We saw data, usually in
the form of records has been with us since at least ancient Egyptian times. We also
so that the big drivers of the need to keep detailed records were trade and taxation.
Basically you needed to keep track of who owed you how much for what. This meant
you not only needed a way of recording it, you also needed a way of retrieving
it and updating it. Ultimately this lead to the development of double entry book
keeping which emerged in the 13th and 14th centuries. Retrieving data was another
issue and in paper based systems indexes were developed to ease and speed this
process. Producing reports from data was manual, time consuming and error prone
although various types of mechanical calculators were becoming common by the
mid nineteenth century to aid the process. Because of the time taken to produce
them, major reports were usually published once a month coinciding reconciliation
of accounts. An end of year financial position was also produced. On demand reports
were unheard of. As a result the end of the month and the end of the year were
always extremely busy times in the accounts section of an organisation.
</p>
<p>2.2 The Digital Age
</p>
<p>The first commercial computer UNIVAC 1 was delivered in 1951 to the US Census
Bureau (Fig. 2.1). In 1954 Metropolitan Life became the first financial company to
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_2,
&copy; Springer-Verlag London 2013
</p>
<p>21</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_2">http://dx.doi.org/10.1007/978-1-4471-5601-7_2</a></div>
</div>
<div class="page"><p/>
<p>22 2 A History of Databases
</p>
<p>Fig. 2.1 UNIVAC 1 (original
at http://www.
computer-history.info/Page4.
dir/pages/Univac.dir/, last
accessed 07/08/2013)
</p>
<p>purchase one of the machines. This was a significant step forward giving Metropoli-
tan Life a first mover advantage over other insurance companies. The UNIVAC was
followed by IBM&rsquo;s 701 in 1953 and 650 in 1954. At the time it was envisaged by
IBM that only 50 machines would be built and installed. By the end of production of
the IBM 650 in 1962 nearly 2,000 had been installed. Data could now be processed
much faster and with fewer errors compared with manual systems and a greater
range of reports which were more up to date could be produced. One unfortunate
side effect was regarding things as &lsquo;must be right, it came from the computer&rsquo; and
conversely &lsquo;I can&rsquo;t do anything about it, it is on the computer&rsquo;. Fortunately both
attitudes are no longer as prevalent.
</p>
<p>2.3 Sequential Systems
</p>
<p>Initial computer systems processing data were based on the pre-existing manual
systems. These were sequential systems, where individual files were composed of
records organised in some predetermined order. Although not a database because
there was no one integrated data source, electronic file processing was the first step
towards an electronic data based information system. Processing required you to
start at the first record and continue to the end. This was quite good for producing
payroll and monthly accounts, but much less useful when trying to recall an indi-
vidual record. As a result it was not uncommon to have the latest printout of the file
which could be manually searched if required kept on hand until a new version was
produced. Printing was relatively slow by today&rsquo;s standards, so usually only one
copy was produced although multi part paper using interleaved carbon paper was
used sometimes when multiple copies could be justified. Sequential file processing
also meant there had to be cut offs&mdash;times when no new updates could be accepted
before the programs were run.</p>
<p/>
<div class="annotation"><a href="http://www.computer-history.info/Page4.dir/pages/Univac.dir/">http://www.computer-history.info/Page4.dir/pages/Univac.dir/</a></div>
<div class="annotation"><a href="http://www.computer-history.info/Page4.dir/pages/Univac.dir/">http://www.computer-history.info/Page4.dir/pages/Univac.dir/</a></div>
<div class="annotation"><a href="http://www.computer-history.info/Page4.dir/pages/Univac.dir/">http://www.computer-history.info/Page4.dir/pages/Univac.dir/</a></div>
</div>
<div class="page"><p/>
<p>2.4 Random Access 23
</p>
<p>Fig. 2.2 A tape based system
</p>
<p>Although disk storage was available it was expensive so early systems tended
to be based on 80 column punched cards, punched paper tape or magnetic tape
storage, which meant input, update and output had to be done in very specific
ways. IBM used card input which was copied onto magnetic tape. This data had
to be sorted. Once that was done it could be merged with and used to update ex-
isting data (Fig. 2.2). This was usually held on the &lsquo;old&rsquo; master tape, the tape con-
taining the processed master records from the previous run of the system (when
it was designated the &lsquo;new&rsquo; master). Once updated a new master tape was pro-
duced which would become the old master in the next run. Processing had to be
scheduled. In our example, the first run would be sorting, the second process-
ing. Since only one program could be loaded and run at a time with these sys-
tems demand for computer time grew rapidly. As well as data processing time
was needed to develop, compile and test new computer programmes. Ultimately
many computer systems were being scheduled for up to 24 hours a day, 7 days a
week.
</p>
<p>There were also very strict rules on tape storage so if a master tape were corrupted
it could be recreated by using archived old masters and the transaction tapes. If there
were any issues with one of the transactions, it would not be processed, but would
be copied to an exception file on another tape. This would be later examined to find
out what had gone wrong. Normally the problem would rectified by adding a new
transaction to be processed at the next run of the program.
</p>
<p>2.4 RandomAccess
</p>
<p>IBM introduced hard disk drives which allowed direct access to data in 1956, but
these were relatively low capacity and expensive compared to tape system. By 1961
the systems had become cheaper and it was possible to add extra drives to your
system. The advantage of a disk drive was you could go directly to a record in the file
which meant you could do real time transaction processing. That is now the basis of</p>
<p/>
</div>
<div class="page"><p/>
<p>24 2 A History of Databases
</p>
<p>processing for nearly all commercial computer systems. At the same time operating
systems were developed to allow multiple users and multiple programs to active at
the same time removing some of the restrictions imposed by tight scheduling needed
on single process machines.
</p>
<p>2.5 Origins of Modern Databases
</p>
<p>It was inevitable that data would be organised in some way to make storage and re-
trieval more efficient. However, while they were developing these systems there was
also a move by the manufacturers to lock customers into their proprietary products.
Many failed forcing early adopters of the &lsquo;wrong&rsquo; systems to migrate to other widely
adopted systems, usually at great expense. For example Honeywell developed a lan-
guage called FACT (Fully Automated Compiler Technique) which was designed for
implementing corporate systems with associated file structures. The last major user
was the Australian Department of Defence in the 1960&rsquo;s and 70&rsquo;s. They took several
years and a large budget to convert to UNIVAC&rsquo;s DMS 1100 system which will be
described below.
</p>
<p>UNIVAC and IBM competed to develop the first database where records were
linked in a way that was not sequential. UNIVAC had decided to adopt the COBOL
(Common Business Oriented Language) programming language and therefore also
adopted the CODASYL (COnference on DAta SYstems Languages) conventions for
developing their database. CODASYL was a consortium formed in 1959 to develop
a common programming language which became the basis for COBOL. Interest-
ingly despite Honeywell being a member of the CODASYL group, they tried to
put forward their FACT language as tried and functioning alternative to the untried
COBOL. As shown this strategy was not successful. In 1967 CODASYL renamed
itself the Database Task Group (DBTG) and developed a series of extensions to
COBOL to handle databases. Honeywell, Univac and Digital Equipment Corpora-
tion (DEC) were among those who adopted this standard for their database imple-
mentations.
</p>
<p>IBM on the other hand had developed its own programming language PL/1 (Pro-
gramming Language 1) and a database implementation known as IMS (Information
Management System) for the extremely popular IBM Series 360 computer family.
The UNIVAC system was a network database and the IBM a strictly hierarchical
one. Both were navigational systems were you accessed the system at one point
then navigated down a hierarchy before conducting a sequential search to find the
record you wanted. Both systems relied on pointers.
</p>
<p>The following sections will look at transaction processing which underlies many
database systems, then different database technologies will be briefly examined in
roughly the order they appeared. Many of these will be dealt with in more detail in
later chapters.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.6 Transaction Processing and ACID 25
</p>
<p>2.6 Transaction Processing and ACID
</p>
<p>Many database applications deal with transaction processing, therefore we need to
look at what a transaction is. Basically a transaction is one or more operations that
make up a single task. Operations fall into one of four categories; Create, Read,
Update or Delete (so called CRUDing). As an example you decide to make a with-
drawal at a cash machine. There are several transactions involved here, first you need
to be authenticated by the system. We will concentrate on the actual withdrawal. The
system checks you have enough money in your account (Read); it then reduces the
amount in your account by the amount requested (Update) and issues the money
and a receipt. It also logs the operation recording your details, the time, location and
amount of the withdrawal (Create). Several things can happen that cause the trans-
action to abort. You may not have enough money in the account or the machine may
not have the right denomination of notes to satisfy your request. In these cases the
operations are undone or rolled back so your account (in other words the database)
is returned to the state it was in before you started.
</p>
<p>ACID stands for atomicity, consistency, isolation and durability and is funda-
mental to database transaction processing. Earlier in this chapter we saw how trans-
actions were processed in sequential file systems, but with multiuser systems which
rely on a single database, transaction processing becomes a critical part of process-
ing. Elements of this discussion will be expanded on in the chapters on availability
and security.
</p>
<p>Atomicity refers to transactions being applied in an &lsquo;all or nothing&rsquo; manner. This
means if any part of a transaction fails, the entire transaction is deemed to have
failed and the database is returned to the state it was in before the transaction started.
This returning to the original state is known as rollback. On the other hand, if the
transaction completes successfully the database is permanently updated&mdash;a process
known as commit.
</p>
<p>Consistency means data written to a database must be valid according to defined
rules. In a relational database this includes constraints which determine the valid-
ity of data entered. For example, if an attempt was made to create an invoice and
a unassigned customer id was used the transaction would fail (you can&rsquo;t have an
invoice which does not have an associated customer). This would also trigger the
atomicity feature rolling back the transaction.
</p>
<p>Isolation ensures that transactions being executed in parallel would result in a fi-
nal state identical to that which would be arrived at if the transactions were executed
serially. This is important for on line systems. For example, consider two users si-
multaneously attempting to buy the last seat on an airline flight. Both would initially
see that a seat was available, but only one can buy it. The first to commit to buying
by pressing the purchase button and having the purchase approved would cause the
other users transaction to stop and any entered data (like name and address) to be
rolled back.</p>
<p/>
</div>
<div class="page"><p/>
<p>26 2 A History of Databases
</p>
<p>Durability is the property by which once a transaction has been committed, it
will stay committed. From the previous example, if our buyer of the last seat on the
aeroplane has finished their transaction, received a success message but the system
crashes for whatever reason before the final update is applied, the user would rea-
sonably think their purchase was a success. Details of the transaction would have to
be stored in a non-volatile area to allow them to be processed when the system was
restored.
</p>
<p>2.7 Two-Phase Commit
</p>
<p>Two-phase commit is a way of dealing with transactions where the database is held
on more than one server. This is called a distributed database and these will be
discussed in more detail below. However a discussion on the transaction process-
ing implications belongs here. In a distributed database processing a transaction has
implications in terms of the A (atomicity) and C (consistency) of ACID since the
database management system must coordinate the committing or rolling back of
transactions as a self-contained unit on all database components no matter where
they are physically located. The two phases are the request phase and the commit
phase. In the request phase a coordinating process sends a query to commit message
to all other processes. The expected response is &lsquo;YES&rsquo; otherwise the transaction
aborts. In the commit phase the coordinator sends a message to the other processes
which attempt to complete the commit (make changes permanent). If any one pro-
cess fails, all processes will execute a rollback.
</p>
<p>Oracle, although calling it a two-phase commit has defined three phases: prepare,
commit and forget which we will look at in more detail. In the prepare phase all
nodes referenced in a distributed transaction are told to prepare to commit by the
initiating node which becomes the global coordinator.
</p>
<p>Each node then records information in the redo logs so it can commit or rollback.
It also places a distributed lock on tables to be modified so no reads can take place.
The node reports back to the global coordinator that it is prepared to commit, is
read-only or abort. Read-only means no data on the node can be modified by the
request so no preparation is necessary and will not participate in the commit phase.
If the response is abort it means the node cannot successfully prepare. That node
then does a rollback of its local part of the transaction. Once one node has issued
a abort, the action propagates among the rest of the nodes which also rollback the
transaction guaranteeing the A and C of ACID.
</p>
<p>If all nodes involved in the transaction respond with prepared, the process moves
into the commit phase. The step in this phase is for the global coordinator node to
commit. If that is successful the global coordinator instructs all the other nodes to
commit the transaction. Each node then does a commit and updates the local redo
log before sending a message that they have committed. In the case of a failure to
commit, a message is sent and all sites rollback.
</p>
<p>The forget phase is a clean-up stage. Once all participating nodes notify the
global coordinator they have committed, a message is sent to erase status infor-
mation about the transaction.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.8 Hierarchical Databases 27
</p>
<p>Fig. 2.3 Hierarchical database
</p>
<p>SQLServer has a similar two phase commit strategy, but includes the third (for-
get) phase at the end of the second phase.
</p>
<p>2.8 Hierarchical Databases
</p>
<p>Although hierarchical databases are no longer common, it is worth spending some
time on a discussion of them because IMS, a hierarchical database system is still one
of IBM&rsquo;s highest revenue products and is still being actively developed. It is however
a mainframe software product and may owe some of its longevity to organisations
being locked in to the product from the early days of mainstream computing. The
host language for IMS is usually IBM&rsquo;s PL/1 but COBOL is also common.
</p>
<p>Like a networked databases, the structure of a hierarchical database relies on
pointers. A major difference is that it must be navigated from the top of the tree
(Fig. 2.3).
</p>
<p>2.9 Network Databases
</p>
<p>In a network database such as UNIVAC&rsquo;s DMS 1100, you have one record which
is the parent record. In the example in Fig. 2.4 this is a customer record. This is
defined with a number of attributes (for example Customer ID, Name, Address).
Linked to this are a number of child records, in this case orders which would also
have a number of attributes. It is up to the database design to decide how they are
linked. The default was often to &lsquo;next&rsquo; pointers where the parent pointed to the
first child, the first child had a pointer to the second child and so on. The final
child would have a pointer back to the parent. If faster access was required, &lsquo;prior&rsquo;
pointers could be defined allowing navigation in a forward and backward direction.
Finally if even more flexibility was required &lsquo;direct&rsquo; pointers could be defined which
pointed directly from a child record back to the parent. The trade-off was between
speed of access and speed of updates, particularly when inserting new child records
and deleting records. In these cases pointers had to be updated.</p>
<p/>
</div>
<div class="page"><p/>
<p>28 2 A History of Databases
</p>
<p>Fig. 2.4 Network database
</p>
<p>The whole of the definition of the database and the manipulation of it was hosted
by the COBOL programming language which was extended to deal with DMS 1100
database applications. It is the main reason why the COBOL language is still in use
today. It is worth noting that deficiencies in the language led to fears there would
be major computer (which actually meant database) failure at midnight on 31st De-
cember 1999. This was referred to as the Y2K or millennium bug and related to the
way dates were stored (it was feared some systems would think the year was 1900
as only the last two digits of the year were stored). The fact that the feared failures
never happened has been put down to the maintenance effort that went into systems
in the preceding few years.
</p>
<p>2.10 Relational Databases
</p>
<p>These will be discussed in detail in Chap. 4. They arose out of Edgar Codd&rsquo;s 1970
paper &ldquo;A Relational Model of Data for Large Shared Data Banks&rdquo; (Codd 1970).
What became Oracle Corporation used this as the basis of what became the biggest
corporate relational database management system. It was also designed to be plat-
form independent, so it didn&rsquo;t matter what hardware you were using.
</p>
<p>The basis of a relational system is a series of tables of records each with spe-
cific attributes linked by a series of joins. These joins are created using foreign keys
which are attribute(s) containing the same data as another tables primary key. A pri-
mary key is a unique identifier of a record in a table. This approach to data storage
was very efficient in terms of the disk space used and the speed of access to records.
</p>
<p>In Fig. 2.5 we have a simple database consisting of 2 tables. One contains em-
ployee records, the other contains records relating to departments. The Department
ID can be seen as the link (join) between the two tables being a primary key in the</p>
<p/>
</div>
<div class="page"><p/>
<p>2.10 Relational Databases 29
</p>
<p>Fig. 2.5 Relational database consisting of two tables or relations
</p>
<p>Department table and a foreign key in the Employee table. There is also a one to
many relationship illustrated which means for every record in the Department table
there are many records in the Employee table. The converse is also true in that for
every record in the Employee table there is one and only one record in the Depart-
ment table.
</p>
<p>The data in a relational database is manipulated by the structured query language
(SQL). This was formalised by ANSI (American National Standards Institute) in
1986. The have been seven revisions of SQL86, the most recent revision being
SQL2011 or ISO/IEC 9075:2011 (International Organization for Standardization
/International Electrotechnical Commission). SQL has a direct relationship to rela-
tional algebra. This describes tables (in relational algebra terms&mdash;relations), records
(tuples) and the relationship between them. For example:
</p>
<p>P1 = &bull;type_property=&lsquo;House&rsquo;(Property_for_Rent)
</p>
<p>would be interpreted as find all the records in the property for rent table where the
type of rental property is a house and would be written in SQL as:
</p>
<p>SELECT &lowast; FROM property_for_rent WHERE type_property = &lsquo;House&rsquo;;
</p>
<p>Oracle and Microsoft SQL server are examples of relational database systems. Mi-
crosoft Access has many of the features of a relational database system including
the ability to manipulate data using SQL, but is not strictly regarded as a relational
database management system.</p>
<p/>
</div>
<div class="page"><p/>
<p>30 2 A History of Databases
</p>
<p>2.11 Object Oriented Databases
</p>
<p>Most programming today is done in an object oriented language such as Java or
C++. These introduce a rich environment where data and the procedures and func-
tions need to manipulate it are stored together. Often a relational database is seen
by object oriented programmers as a single persistent object on which a number of
operations can be performed. However there are more and more reasons why this is
becoming a narrow view.
</p>
<p>One of the first issues confronting databases is the rise of non-character (alphanu-
meric) data. Increasingly images, sound files, maps and video need to be stored, ma-
nipulated and retrieved. Even traditional data is being looked at in other ways than
by traditional table joins. Object oriented structures such as hierarchies, aggregation
and pointers are being introduced. This has led to a number of innovations, but also
to fragmentation of standards.
</p>
<p>From the mid 1980&rsquo;s a number of object oriented database management sys-
tems (OODBMS) were developed but never gained traction in the wider business
environment. They did become popular in niche markets such as the geographic
and engineering sectors where there was a need for graphical data to be stored and
manipulated. One of the big issues with object oriented databases is, as mentioned
creating a standard. This was attempted by the Object Data Management Group
(ODMG) which published five revisions to its standard. This group was wound up
in 2001. The function of ODMG has, been taken over by the Object Management
Group (OMG). Although there is talk of development of a 4th generation standard
for object databases, this has not yet happened
</p>
<p>An issue with developing object oriented systems was that a lot of expertise and
systems had been developed around relational databases and SQL had become, for
better or worse, the universal query language. The second approach to object ori-
entation therefore was to extend relational databases to incorporate object oriented
features. These are known as object relational databases. They are manipulated by
SQL commands which have been extended to deal with object structures.
</p>
<p>Chapter 7 will look in detail at both Object and Object Relational Databases.
</p>
<p>2.12 DataWarehouse
</p>
<p>A problem with an operational database is that individual records within it are con-
tinually being updated, therefore it is difficult to do an analysis based on historical
data unless you actively store it. The concept behind a data warehouse is that it can
store data generated by different systems in the organisation, not just transactions
form the database. This data can then be analysed and the results of the analysis
used to make decisions and inform the strategic direction of an organisation. Data
warehouses have already been mentioned in Chap. 1 where they were discussed in
their role of an organisational asset. Here we look briefly at some of their technical
details.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.12 Data Warehouse 31
</p>
<p>Fig. 2.6 Star schema
</p>
<p>A data warehouse is a central repository of data in an organisation storing his-
torical data and being constantly added to by current transactions. The data can be
analysed to produce trend reports. It can also be analysed using data mining tech-
niques for generating strategic information. Unlike data in a relational database, data
in a data warehouse is organised in a de-normalised format. There are three main
formats:
&bull; Star schema: tables are either fact tables which record data about specific events
</p>
<p>such as a sales transaction and dimension tables which information relating to
the attributes in the fact table (Fig. 2.6)
</p>
<p>&bull; Snowflake schemas are also based around a fact table which has dimension
tables linked to it. However these dimension tables may also have further tables
linked to them giving the appearance of a snowflake (Fig. 2.7)
</p>
<p>&bull; Starflake schema which is a snowflake schema where only some of the dimen-
sion tables have been de-normalised
</p>
<p>Data mining is a process of extracting information from a data warehouse and
transforming it into an understandable structure. The term data analytics is starting
to be used in preference to data mining. There are a number of steps in the process:
&bull; Data cleansing were erroneous and duplicate data is removed. It is not uncom-
</p>
<p>mon in raw data to have duplicate data where there is only a minor difference,
for example two customer records may be identical, but one contains a suburb
name as part of the address and one does not. One needs removing.
</p>
<p>&bull; Initial Analysis where the quality and distribution of the data is determined.
This is done to determine if there is any bias or other problems in the data
distribution
</p>
<p>&bull; Main analysis where statistical models are applied to answer some question,
usually asked management.</p>
<p/>
</div>
<div class="page"><p/>
<p>32 2 A History of Databases
</p>
<p>Fig. 2.7 Snowflake schema
</p>
<p>Data mining is just one method in a group collectively known as Business Intelli-
gence that transform data into useful business information. Others included business
process management and business analytics. A further use for data in a data ware-
house is to generate a decision system based on patterns in the data. This process is
known as machine learning. There are two main branches: neural networks which
use data to train a computer program simulating neural connections in the brain for
pattern recognition and rule based systems where a decision tree is generated.
</p>
<p>2.13 The Gartner Hype Cycle
</p>
<p>Gartner began publishing its annual hype cycle report for emerging technologies in
2007. Technologies such as Big Data and Cloud Computing both feature so we will
have a brief look at the hype cycle before continuing. The hype cycle is based on
media &lsquo;hype&rsquo; about new technologies and classifies them as being in one of five
zones.
&bull; The Technology Trigger which occurs when a technological innovation or
</p>
<p>product generates significant press interest
&bull; The Peak of Inflated Expectations where often unrealistic expectations are
</p>
<p>raised
&bull; The Trough of Disillusionment where, because of failing to meet expectations
</p>
<p>there is a loss of interest</p>
<p/>
</div>
<div class="page"><p/>
<p>2.14 Big Data 33
</p>
<p>&bull; The Slope of Enlightenment where organisations which have persisted with
the technology despite setbacks begin to see benefits and practical applications
</p>
<p>&bull; The Plateau of Productivity where the benefits of the technology have been
demonstrated and it becomes widely accepted
</p>
<p>In practice it is not really a cycle but a continuum. Gartner has produced many
different hype cycles covering many different topics from advertising through to
wireless network infrastructure (see Gartner Inc. 2013).
</p>
<p>One of the exercises at the end of this chapter involves examining hype cycles.
By completing this you will see what they look like and how technologies move
across the cycle on a year by year basis.
</p>
<p>2.14 Big Data
</p>
<p>As mentioned in Chap. 1, the amount of data available that an organisation can
collect and analyse has been growing rapidly. The first reference to big data on
Gartner&rsquo;s Hype Cycle of emerging technologies was in 2011 when it was referred
to as &lsquo;Big Data and Extreme Information Processing and Management&rsquo;. As a term
it grew out of data warehousing recognising the growth of storage needed to handle
massive datasets some organisations were generating.
</p>
<p>The definition of &ldquo;Big data&rdquo; is often left intentionally vague, for example
McKinsey&rsquo;s 2011 report defines it as referring &lsquo;. . . to datasets whose size is be-
yond the ability of typical database software tools to capture, store, manage, and
analyze.&rsquo; (Manyika et al. 2011, p. 1). This is because different organisations have
different abilities to store and capture data. Context and application are therefore
important. Big Data will be explored in depth in Chap. 6.
</p>
<p>2.15 Data in the Cloud
</p>
<p>The 2012 Gartner Hype Cycle also identifies database platform as a service (as part
of cloud computing) and like in-memory databases it has been identified as over-
hyped. However like in-memory databases it is also thought to be within five years
of maturity.
</p>
<p>Cloud computing is distributed computing where the application and data a user
may be working with is located somewhere on the internet. Amazon Elastic Com-
pute Cloud (Amazon EC2) was one of the first commercial cloud vendors supplying
resizable computing and data capacity in the cloud. The Amazon cloud is actually a
large number of servers located around the world although three of the biggest data
centres are located in the United States, each with thousands of servers in multiple
buildings. An estimate in 2012 suggested Amazon had over 500,000 servers in to-
tal. As a user you can buy both processing and data storage when you need it. Many
other vendors including British Telecom (BT) are now offering cloud services.
</p>
<p>The impact on database systems is you no longer have to worry about local stor-
age issues because you can purchase a data platform as a service in the cloud. It also
means whatever an organisations definition of big data is, there is a cost effective</p>
<p/>
</div>
<div class="page"><p/>
<p>34 2 A History of Databases
</p>
<p>way of storing and analysing that data. Although concerns have been raised about
security including backup and recovery of cloud services, they are probably better
than policies and practice in place at most small to medium sized businesses.
</p>
<p>2.16 The Need for Speed
</p>
<p>One of the technologies facilitating databases was hard disk drives. These provided
fast direct access to data. As already stated, the cost of this technology has been
falling and speed increasing. Despite this the biggest bottleneck in data processing
is the time it takes to access data on a disk. The time taken has three components:
seek time which is how long it takes the read/write head to move over the disk and
rotational latency, basically the average time it takes the disk to rotate so the record
you want is actually under the read/write head. Data then has to be moved from the
disk to the processor&mdash;the data transfer rate.
</p>
<p>For many applications this is not a problem, but for others performance is an
issue. For example stockbrokers use systems called High Frequency Trading or
micro-trading where decisions are made in real time based on transaction data. This
is automated and based on trading algorithms which use input from market quotes
and transactions to identify trends. Requirements for speed like this are beyond disk
based systems but there are ways to get around the problem. The first is to move the
entire database into memory; the second is to use a different access strategy to that
provided by the database management system; finally there is a combination of the
two.
</p>
<p>The 2012 Gartner Hype cycle for emerging technologies identifies in-memory
database management systems and in-memory analysis as part of its analysis. How-
ever both of these are sliding into what Gartner term the &lsquo;trough of disillusionment&rsquo;
meaning they have probably been overhyped. On the other hand they are also pre-
dicted to be within 5 years of maturity which Gartner calls the &lsquo;plateau of produc-
tivity&rsquo;
</p>
<p>2.17 In-Memory Database
</p>
<p>Early computer systems tried to optimise the use of main memory because it was
both small and expensive. This has now changed with continued development of
semi-conductor memory, specifically non-volatile memory. This was a major devel-
opment as it meant data stored in memory was not lost if there was a power failure.
It is now possible to hold a complete database in a computer&rsquo;s memory. There is
no one dominant technology at the moment. Oracle, for example has taken the ap-
proach of loading a relational database in its TimesTen system. The problem with
this approach is a relational database is designed for data stored on disk and helps
optimise the time taken to transfer data to memory. This is no longer an issue as the
data is already in memory meaning other ways of organising and accessing data can
be implemented. In-memory databases will be looked at in detail in Chap. 8.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.18 NoSQL 35
</p>
<p>2.18 NoSQL
</p>
<p>One problem with the relational database model was that it was designed for charac-
ter based data which could be modelled in terms of attributes and records translated
to columns and rows in a table. It has also been suggested it does not scale well
which is a problem in a world where everyone is talking about big data. NoSQL
(some say Not Only SQL) is an umbrella term for a number of approaches which
do not use traditional query languages such as SQL and OQL (Object Query Lan-
guage). There are a number of reasons for the rise in interest on No SQL. The first
is speed and the poor fit of traditional query languages to technologies such as in-
memory databases mentioned above. Secondly there is the form of the data which
people want to store, analyse and retrieve. Tweets from Twitter are data that do not
easily fit a relational or object oriented structure. Instead column based approaches
were a column is the smallest unit of storage or a document based approach where
data is denormalised can be applied. NoSQL will be discussed in Chap. 5.
</p>
<p>2.19 Spatial Databases
</p>
<p>Spatial databases form the basis of Geographic Information Systems (GIS). Most
databases have some geographic information stored in them. At the very least this
will be addresses. However there is much more information available and combining
it in many ways can provide important planning information. This process is called
overlaying. For example you may have a topographic map of an area which gives
height above sea level at specific points. This can then have a soil, climate and
vegetation map overlaid on it to.
</p>
<p>An issue with spatial databases is that as well as storing &lsquo;traditional&rsquo; data you
also have to store positional and shape data. As an example, let&rsquo;s take a road. This
can be stored as all the pixels making up the road (know as a raster representation)
or it could be stored as a series of points (minimum two if it is a straight road) which
can be used to plot its location (vector representation). Along with a representation
of its shape and location you may also want to store information such as its name
and even information about its construction and condition. As well as plotting this
object you may want to know its location in relation to service infrastructure such
as sewer lines, for example where the road crosses a major sewer or where access
points to it are.
</p>
<p>Despite expecting GISs&rsquo; to be object oriented, for the most part they are rela-
tional. For example the biggest vendor in this field is ESRI which uses a proprietary
relational database called ArcSDE. Oracle Spatial is a rapidly emerging player and
has the advantage of offering this product as an option to its mainstream database
management system (currently 11g). As Oracle Spatial is part of the wider group of
Oracle products, it means there are associated object oriented features and a devel-
opment environment available.</p>
<p/>
</div>
<div class="page"><p/>
<p>36 2 A History of Databases
</p>
<p>2.20 Databases on Personal Computers
</p>
<p>Before 1980 most database systems were found on mainframe computers, but the
advent of personal computers and the recognition that they were viable business
productivity tools lead to the development of programs designed to run on desktop
machines. The first widely used electronic spreadsheet to be developed was VisiCalc
in 1979 which spelled a rather rapid end to the paper versions. Word processors and
databases followed soon after.
</p>
<p>In the early days of personal computers there was a proliferation of manufactur-
ers many of whom had their own proprietary operating systems and software. The
introduction of IBM&rsquo;s personal computer in 1980 using Microsoft&rsquo;s operating sys-
tem resulted in a shake out of the industry. Most manufacturers who remained in the
market started producing machines based on the IBM PC&rsquo;s architecture (PC clones)
and running Microsoft software. In 1991 the LINUX operating system was released
as an alternative to Microsoft&rsquo;s operating system. The largest of the other companies
was Apple which vigorously opposed the development of clones of its machines.
</p>
<p>There are several databases systems which run on personal computers. One of
the earliest was DBase developed by Ashton Tate in 1980. Versions of this ran on
most of the personal computers available at the time. The ownership of the product
has changed several times. The product itself has developed into and object oriented
database whose usage is not common among business users, but is still quite popular
among niche users.
</p>
<p>Microsoft Access was a much later product being released in 1992. It was bun-
dled with the Microsoft Office suite in 1995 and has become one of the most com-
mon desktop databases in use today. One of its attractions is that its tables and
relationships can be viewed graphically but it can also be manipulated by most SQL
statements. It is however not a true relational database management system because
of the way table joins are managed. It is compatible with Microsoft&rsquo;s relational
database system SQL Server. One of its major disadvantages is that it does not scale
well.
</p>
<p>An open source personal computer databases is MySQL released in 1995.
This is a relational database and is very popular. However although the current
open source market leader NoSQL and NewSQL based products are gaining on
it according to Matt Asya&rsquo;s in his 2011 article &lsquo;MySQL&rsquo;s growing NoSQL prob-
lem&rsquo;. (Available at http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/ last
accessed 04/07/2013.)
</p>
<p>2.21 Distributed Databases
</p>
<p>In Chap. 1 we saw the evolution of organisations often lead to them being located on
multiple sites and sometimes consisting of multiple federated but independent com-
ponents. Therefore rather than all the data in an organisation being held on a single
centralised database, data is sometimes distributed so operational data is held on the
site where it is being used. In the case of a relational database this requires tables or</p>
<p/>
<div class="annotation"><a href="http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/">http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/</a></div>
</div>
<div class="page"><p/>
<p>2.22 XML 37
</p>
<p>parts of tables to be fragmented and in some case replicated. Early versions of dis-
tribution was happening in the mid 1970&rsquo;s, for example the Australian Department
of Defence had a centralised database system but bases around Australia replicated
segments based on their local needs on mini computers. This was not a real time
system, rather the central systems and the distributed machines synchronised sev-
eral times a day.
</p>
<p>Today it is possible to have components or fragments of a database on a net-
work of servers whose operation is transparent to users. There are four main rea-
sons to fragment or distribute databases across multiple sites or nodes. The first is
efficiency where data is stored where it is most frequently used. This cuts the over-
head of data transmission. Obviously you don&rsquo;t store data that is not used by any
local application. This also leads to the second reason&mdash;security, since data not re-
quired at local nodes is not stored there it makes local unauthorised access more
difficult. The third reason is usage. Most applications work with views rather than
entire tables, therefore the data required by the view is stored locally rather than
the whole table. The final reason is parallelism where similar transactions can be
processed in parallel at different sites. Where multiple sites are involved in a trans-
action a two phase commit process is required and is described in the following
section.
</p>
<p>There are some disadvantages however. The database management system has
to be extended to deal with distributed data. Poor fragmentation can result in badly
degraded performance due to constant data transfer between nodes (servers). This
may also lead to integrity problems if part of the network goes down. There needs
to be a more complex transaction processing regime to make sure data integrity is
maintained.
</p>
<p>2.22 XML
</p>
<p>XML stands for eXtensible Markup Language and is used for encoding a document
in a form that is both human and machine readable. It is not intended to give a
full description of XML here. What is important from a database point of view is
that you can define a database in terms of XML which is important for document
databases. There are two forms of XML schema which constrains what information
can be stored in a database and constrain the data types of the stored information. It
must be noted an XML document can be created without an associated schema, but
those will not be considered here.
</p>
<p>Document Type Definition (DTD) This is an optional part of a XML document
and performs the task of constraining and typing the information in the document.
This constraining is not like the basic types of integer and characters (VARCHAR)
we have already seen. Instead it constrains the appearance of sub-elements and at-
tributes within an element. An example DTD may look like:</p>
<p/>
</div>
<div class="page"><p/>
<p>38 2 A History of Databases
</p>
<p>&lt;!DOCTYPE account [
&lt;!ELEMENT invoice (invoice-number total)&gt;
(!ATTLIST invoice
</p>
<p>account-number ID #REQUIRED
owner IDREF #REQUIRED&gt;
</p>
<p>&lt;!ELEMENT customer (customer-name customer street customer-city)&gt;
(!ATTLIST customer
</p>
<p>customer-id ID #REQUIRED
accounts IDREF #REQUIRED
</p>
<p>...
</p>
<p>]&gt;
</p>
<p>where an attribute of type ID must be unique and a IDREF is a reference to an
element and must contain a value that appears in an ID element. This effectively
gives a one to many relationship.
</p>
<p>DTD have limitations however. They cannot specify data types and are written in
their own language, not XML. Also they do not support newer XML features such
as namespaces. Namespaces are used to contain definitions of identifiers. Once a
namespace is set up a developer will be confident of identifiers definitions. The
same identifier in a different namespace could have different definitions.
</p>
<p>XML Schema This is a more sophisticated schema language designed to over-
come the deficiencies of DTD at the same time providing backwards (and forwards)
compatibility. It also allows user defined types to be created and text that appears in
elements to be constrained to more familiar types such as numeric including specific
format definitions. Most importantly it is specified by XML syntax.
</p>
<p>Once a database is set up using XML it can be queried using a number of lan-
guages:
&bull; XPath is designed for path expressions and is the basis for the following two
</p>
<p>languages.
&bull; XSLT is a transformation language. It was originally designed to control for-
</p>
<p>matting of XML to HTML but it can be used to generate queries.
&bull; XQuery is the current proposed standard for querying XML data.
</p>
<p>It should be noted that many database management systems including Oracle
are compatible with XML and can format output and receive input as XML. As
mentioned in Chap. 1, there is a tendency to store anything that can be stored. Much
of this data is in the form of complete documents. XML databases are an ideal way
to store these therefore they are likely to become more and more common.
</p>
<p>2.23 Temporal Databases
</p>
<p>A temporal database is a database with built-in support for handling data involving
time. A lot of data has a time element to it just as we have seen data often has a
geographic element to it. Unlike the geographic element however, the time attributes
are less often stored. The exception is transaction logs were temporal data is critical.</p>
<p/>
</div>
<div class="page"><p/>
<p>2.24 Summary 39
</p>
<p>Two aspects of time are valid time&mdash;the time during which a fact is true in the
real world and transactional time where it is true in a database. For example when
an electricity meter reading is taken, at the instance it is taken it is true in real time.
It is then entered into the supplier&rsquo;s database along with the time the reading was
taken. This is the beginning of when it is true in transactional time. It remains true
until the time the next reading is entered. It means the reading has start and an end
point (the time of the next reading) during which it is true.
</p>
<p>2.24 Summary
</p>
<p>In this chapter we have looked at the origins of organisational data processing be-
ginning with systems based around sequential file processing. We then looked at the
origins and development of database technology as random access devices became
cheaper, faster and more reliable. The impact of Edgar Codd&rsquo;s influential work and
the rise to dominance of relational database systems was examined in detail and will
be the subject of further parts of this book. Competing with the relational database
model is the object oriented model which is gaining traction because the amount of
non-structured and non-character based data is increasing.
</p>
<p>We looked at developing requirements for databases. New applications demand
faster processing of data held in a database. Cheaper, stable computer memory is
facilitating this so a database can be held in-memory. As well as a need for speed
there is a need for volume as typified by Big Data and the tools need to analyse it.
Lastly we looked at distributed databases placing data where it is needed.
</p>
<p>2.25 Exercises
</p>
<p>2.25.1 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
</p>
<p>&bull; What is the difference between a Hierarchical and a Network database?
&bull; Define the following relational database terms: relation, tuple, attribute, rela-
</p>
<p>tionship.
&bull; What is ACID and why is it important?
&bull; Describe the stages in the two phase commit process.
&bull; Why have object oriented databases not become the standard commercial
</p>
<p>database management systems?
</p>
<p>2.25.2 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.</p>
<p/>
</div>
<div class="page"><p/>
<p>40 2 A History of Databases
</p>
<p>Activity 1 Search for the yearly Gartner Hype Cycle on-line. The earliest of these
is from 2009. Look for database related technologies. When did they start to ap-
pear? From 2011 Gartner also produced Hype for the Cloud. Look at this as well
and identify relevant technologies. Are the technologies you identified moving
along the hype cycle year on year? When did they first appear on the hype cycle?
</p>
<p>The most recent version of the hype cycle is likely to be a document you have to
pay for, but earlier versions are freely available.
</p>
<p>Activity 2 Retrieve M. Asya&rsquo;s (2011) &lsquo;MySQL&rsquo;s growing NoSQL problem&rsquo; arti-
cle. (Available at http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/ last
accessed 04/07/2013.) This makes predictions on the growth of MySQL, NoSQL
and NewSQL. Research whether Asya&rsquo;s predictions are on track and if there are
significant variations what is causing them?
</p>
<p>References
</p>
<p>Codd EF (1970) A relational model of data for large shared data banks. Republished in Com-
mun ACM 26(1):64&ndash;69 (1983). Available on-line at http://dl.acm.org/citation.cfm?id=358007,
accessed 21/06/2013
</p>
<p>Gartner Inc (2013) Research methodologies: hype cycles. Available online at http://www.
gartner.com/technology/research/methodologies/hype-cycle.jsp, accessed 21/06/2013
</p>
<p>Manyika J, Chui M, Brown B, Bughin J, Dobbs R, Roxburgh C, Hung Byers A (2011) Big
data: the next frontier for innovation, competition, and productivity. McKinsey Global Insti-
tute. http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_
innovation, accessed 23/06/2013
</p>
<p>Further Reading
</p>
<p>ODBMS.ORG (2006) 4th generation standard for object databases on its way. http://odbms.org/
About/News/20060218.aspx, accessed 23/06/2013
</p>
<p>&Ouml;zsu MT, Valduriez P (2010) Principles of distributed database systems, 3rd edn. Springer, Berlin.
Available at www.stanford.edu/class/cs347/reading/textbook.pdf, accessed 25/06/2013</p>
<p/>
<div class="annotation"><a href="http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/">http://www.theregister.co.uk/2012/05/25/nosql_vs_mysql/</a></div>
<div class="annotation"><a href="http://dl.acm.org/citation.cfm?id=358007">http://dl.acm.org/citation.cfm?id=358007</a></div>
<div class="annotation"><a href="http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp">http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp</a></div>
<div class="annotation"><a href="http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp">http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://odbms.org/About/News/20060218.aspx">http://odbms.org/About/News/20060218.aspx</a></div>
<div class="annotation"><a href="http://odbms.org/About/News/20060218.aspx">http://odbms.org/About/News/20060218.aspx</a></div>
<div class="annotation"><a href="http://www.stanford.edu/class/cs347/reading/textbook.pdf">http://www.stanford.edu/class/cs347/reading/textbook.pdf</a></div>
</div>
<div class="page"><p/>
<p>3Physical Storage and Distribution
</p>
<p>What the reader will learn:
</p>
<p>&bull; That databases need to be able to make the information they store permanent.
&bull; That the biggest factor upon the way a database functions is the physical envi-
</p>
<p>ronment it runs in.
&bull; That whilst there are alternative approaches to database architecture design,
</p>
<p>storage on a disk is likely to be at the core of the physical system.
&bull; That hardware failure is a fact of life in computing and database architecture is
</p>
<p>about minimising the impact of any such failure.
&bull; That processing and data can be distributed in response to availability or perfor-
</p>
<p>mance requirements
</p>
<p>3.1 The Fundamental Building Block
</p>
<p>At the time of writing this book D-Wave were hitting the news for having sold
one of their Quantum Computers to Google and NASA. As reported in the New
Scientist (Aron 2013), this computer is likely to have cost more than $10 million,
and works not with the bits we have become so used to in our digital era, but quibits
which can be in different states simultaneously, rather than just the on and off we
have become used to. If, like the author, your brain can&rsquo;t cope with quantum theory,
don&rsquo;t worry&mdash;we will here be concentrating on more everyday computing. However,
as always, database professionals do need to keep their eyes on what is happening
in the field of computer science if they are to continue to provide the best service
possible to their organisation. Perhaps in a few years time this chapter will be out of
date as we all become used to quantum computing!
</p>
<p>However, in the meantime, we can safely make the assertion that a modern
computer-based database system&rsquo;s primary purpose is to store and retrieve binary
digits (bits). All the data in even the most highly encrypted critical systems, resolves
itself into a series of 1s and 0s, or Ons and Offs.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_3,
&copy; Springer-Verlag London 2013
</p>
<p>41</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_3">http://dx.doi.org/10.1007/978-1-4471-5601-7_3</a></div>
</div>
<div class="page"><p/>
<p>42 3 Physical Storage and Distribution
</p>
<p>Since only simple, boolean data such as true/false or Yes/No can be stored in a
bit, eight bits are connected together to form a unit called a byte which can store an
unsigned integer of between 0 and 255. If the first bit is used to sign the subsequent
value (+ or &minus;), then the byte can store 0&ndash;127.
</p>
<p>Various encoding systems have been used to allow the numbers in a byte to rep-
resent characters. Perhaps the most famous, and still in use, with western charac-
ter sets, is ASCII. Text datatypes are relative simple, but even objects like audio
or images can be stored in a database. Oracle, for example, has the Blob (Binary
Large Object) which is a variable-length string of bits up to 2,147,483,647 charac-
ters long.
</p>
<p>3.2 Overall Database Architecture
</p>
<p>As we see by reviewing the chapters in this book, there are many different ap-
proaches to storing ons and offs in the form of a database management system.
Without doubt the most commonly used approach is variations around the relational
model.
</p>
<p>Whilst the NoSQL databases may approach data storage differently to RDBMSs,
two key elements exist in all common databases: the permanent storage of data,
usually on disks, and the processing of data, usually carried on in the RAM of
machine running the database. We will look at some of these structures and pro-
cesses in detail below. The examples given will be from the Oracle environment,
but most of what follows will also be true in other RDBMSs. The final section of
the chapter will review how these may differ with NoSQL databases, and distributed
databases.
</p>
<p>Information
</p>
<p>Most databases will need to carry out the four processes that make up the
acronym CRUD which is used by many database professionals:
&bull; Create
&bull; Read
&bull; Update
&bull; Delete
</p>
<p>3.2.1 In-Memory Structures
</p>
<p>One important factor in the design of database systems is that Hard Disk Drives
(HDD) are the slowest aspect of any system. It can be hundreds of time slower for
a RDBMS to read a row from a HDD than from RAM. This is largely to do with
the fact that there are mechanical moving parts involved in HDD technology, but no
such hindrance exists in RAM.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Overall Database Architecture 43
</p>
<p>Because of this databases will typically attempt to keep as much data as they
can in memory, using a variety of caching techniques to make data more read-
ily accessible to users. In RDBMS systems data is often stored in RAM once it
has been read from disk for as long as is possible (that is, whilst the RAM has
available space for the data), just in case another user wants to access the same
data.
</p>
<p>As was hinted in the last paragraph, data placed in RAM will be automatically
timed-out and removed if it isn&rsquo;t used. Oracle manages this with a LRU list (Least
Recently Used). You can force data to be permanently pinned in memory if you
never want it to be removed. This can be used, for example, to keep code for stored
procedures in memory for speedy access by users. The Oracle procedure for this is
called: DBMS_SHARED_POOL.KEEP
</p>
<p>Oracle&rsquo;s in-memory structure which holds copies of data read from disk is
called the Database Buffer Cache. It resides within an overall structure called the
System Global Area, referred to as the SGA. Users who are connected to the
system, and have appropriate privileges, can share access to the database buffer
cache. The reason for this is that some data requests are frequently repeated by
many users. The request to populate a drop-down list of Region_Codes on a
form, for example, may be in constant use, and after the first user makes the re-
quest, no subsequent user will need to wait for a disk read to get the data re-
turned.
</p>
<p>3.2.2 Walking Through a Straightforward Read
</p>
<p>Before we go any further, let us see how a request for some information might be
treated in the RDBMS. In the process we will come across other structures which
we will review in detail afterwards, but, for now, let&rsquo;s concentrate on the overall
operation.
</p>
<p>Our user, called User_A, needs to know the address of one of their customers.
They may well write something like:
</p>
<p>Select Surname, Firstname, address1, address2, address3, city, county, nation,
telephone
From Customers
Where Customer ID = 2314;
</p>
<p>Of course this query may not have been explicitly written by User_A. They may
have just made some selections on a page in an application, and the application
could have fashioned the SQL itself on their behalf.
</p>
<p>However the SQL is formed, there will be a connection between the user and the
database sitting on a server. The SQL request is sent down to the server where it</p>
<p/>
</div>
<div class="page"><p/>
<p>44 3 Physical Storage and Distribution
</p>
<p>is parsed. This process checks not only that the query syntax is valid, but also that
User_A has sufficient privileges to actually access this data.
</p>
<p>Now we know User_A is allowed to ask this question and that the query is
valid, the next question is: has anyone else asked the same question recently and,
more importantly, is the answer to the query in memory, in the SGA? If the an-
swer is yes then the process can skip the following operations and go straight
to returning the dataset&mdash;which is why, when you see what has to happen (be-
low) if the data isn&rsquo;t in the shared area, keeping data in memory is a good strat-
egy.
</p>
<p>In this example, this data is not in memory as no-one else has recently asked
for the data. Next the parsed query is passed on to an optimiser. There are
likely to be many access paths (routes to the data) available to the database. Per-
haps it would be quicker for the database to use an index to discover the ex-
act physical location of the required row, or maybe it would be quicker to just
read the whole table at one go and discard what isn&rsquo;t required (see the chap-
ter on Performance). It is the optimiser&rsquo;s job to decide which will be the fastest
route.
</p>
<p>Having decided, the optimiser creates an execution plan, listing the tasks that
need to be carried out and in what order. This query is relatively simple, but queries
with multiple joins, for example, will have quite complex plans.
</p>
<p>As we said, the required data is not in memory, so we will have to read the
row from the disk. Actually, what the disk is storing is 1s and 0s that can be
made to describe the data. The start of those 1s and 0s for our data will be
at a physical location on the HDD described by its Hex Address, and that in-
formation, in Oracle, is stored in a Rowid. A Rowid is a pseudocolumn which
contains a pointer to the row and which provides a unique identifier of that
row.
</p>
<p>The disk will be spun-up and the head reader moved to the location indi-
cated by the Rowid, and then it will read as many data-blocks as is needed to
fulfil the data requirement and place the information into the Database Buffer
Cache. It will stay there until such time as, if it is not reused, it falls to the bot-
tom of the LRU list and is lost. A copy is parcelled together as the response
dataset and sent over the connection back to the user. The process is illustrated
in Fig. 3.1.
</p>
<p>Information
</p>
<p>The term Data Dictionary is used to describe the place where the database
stores reference information about the database, such as about its users
schemas and objects.
</p>
<p>The database buffer cache is by no means the only memory structure. Caches
will exist for things like SQL statements and Data Dictionary information that is
being used at any point in time. There are several other specialist pools in an Ora-</p>
<p/>
</div>
<div class="page"><p/>
<p>3.2 Overall Database Architecture 45
</p>
<p>Fig. 3.1 The SQL query
process
</p>
<p>cle environment but we only need worry about them if we are going be an Oracle
DBA!
</p>
<p>3.2.3 Server Processes
</p>
<p>There are a number of independent processes that make up the overall database
server system. Some are to do with the management of the RDBMS, and others are
to do with the handling of the data. Again, the examples here are Oracle specific,
but the principles will be present in other RDBMS.
</p>
<p>Information
</p>
<p>The term Instance is used to describe the processes and structure that exist in
an Oracle server. In Oracle terminology, the Server is the Instance plus the
disks based information, which it calls the database.
</p>
<p>As its name suggests, the System Monitor&rsquo;s major task is to make sure the in-
stance is running OK. If need be it can perform a recovery when the instance starts
up.
</p>
<p>Process Monitor looks after the instance&rsquo;s processes, including user processes.
If, for example, a session is idle for too long, Process Monitor will kill the session
to free up resources.
</p>
<p>The Database Writer&rsquo;s task, as you can probably guess, is to make data changes
and inserts permanent by writing them down to the disks. Changes to data are ac-
tually made in the Database Buffer. Thus changed, but not yet written to disk, the
buffer is described as a &ldquo;dirty&rdquo; buffer. Because of the bottleneck that disk operations</p>
<p/>
</div>
<div class="page"><p/>
<p>46 3 Physical Storage and Distribution
</p>
<p>can create, dirty buffers are not written to disk the second that a commit is issued.
Rather they remain, flagged as dirty, until the time to write is judged by Oracle to
be right.
</p>
<p>This can come as quite a shock, especially to people coming from an Access-
like environment where the database IS what is on the disk. Here, the current
content of the server is described as &ldquo;what is on the disk&rdquo; plus dirty database
buffers.
</p>
<p>Database writer will flush dirty buffers to disk when there is insufficient clean
reusable buffer space left, or prior to a Checkpoint (see below).
</p>
<p>The Redo Log Writer manages the redo logs. Redo is discussed in more detail
later in this chapter, but suffice it here to say that Redo Logs are used to protect
against instance failure by storing information (known as change vectors) about
what changes have been made to the database. Using Redo logs it is possible to
reconstruct all the changes that have been made to the database since a backup by
replaying the changes onto the recovered database from the logs.
</p>
<p>Archiver is connected in a way to the Redo Logs process, in that it copies the
redo log files to a designated storage device before they get overwritten.
</p>
<p>The Checkpoint process is about regularly ensuring that the database is con-
sistent. It forces Database Writer to flush dirty buffers to disk so that the current
contents of the database are all in disks. At that point the System Change Number
(SCN) is incremented and that number is stored in the datafiles and the control files
(see below). This is used if Oracle needs to discover when the database was last
consistent and to discover inconsistent datafiles (perhaps because of disk corrup-
tion) since any datafile which does not have the current SCN in its header is not
consistent. This event is triggered when the redo log buffer is switched and when a
hot back-up is taken.
</p>
<p>The other process which is of vital importance is the User Process. When a
user attempts to log on to an instance they negotiate with a Listener service on
the server. The listener&rsquo;s job is to receive incoming requests to talk to the server. If
the user meets log-in requirements then the listener will create a connection. This
means that the user is connected to the server. On the server itself a process runs on
behalf of that client and runs jobs on their behalf.
</p>
<p>3.2.4 Permanent Structures
</p>
<p>The most obvious permanent structures in an Oracle Server are the datafiles that
store the database data on disks.
</p>
<p>The connection between the physical aspects of a database and its logical design
are these datafiles since they support tablespaces. And tablespaces are logical struc-
tures that store logical objects such as tables, indexes, functions and even the data
dictionary itself.
</p>
<p>An Oracle database will always create at least two tablespaces for itself: The
System tablespace contains the data dictionary; the Sysaux, which is used as an
auxiliary for the System tablespace. DBAs then can create tablespaces as they</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Data Storage 47
</p>
<p>think fit. An example of the SQL command to create a tablespace called FRED
is:
</p>
<p>CREATE TABLESPACE FRED DATAFILE &rsquo;/u02/oracle/data/FRED01.dbf&rsquo;
SIZE 50M EXTENT MANAGEMENT LOCAL AUTOALLOCATE;
</p>
<p>Here we provide a 50 Mb file called FRED01.DBF to support a tablespace called
FRED. The autoallocate parameter tells Oracle it can keep adding diskspace to this
tablespace as it grows.
</p>
<p>To have users create objects stored in that tablespace you would issue an SQL
statement like this:
</p>
<p>CREATE USER peter IDENTIFIED BY p2P2p2 DEFAULT TABLESPACE
FRED;
</p>
<p>The important thing to note is that the 1s and 0s that describe the rows in a
table created by peter will all be found in the physical operating system file called
FRED01.DBF.
</p>
<p>All the elements mentioned above work together to create an Oracle Instance
(see Fig. 3.2).
</p>
<p>Now we have an overview of how the key physical elements of a server (RAM
structures, Processes and HDD) work together to create an instance of a database
management system, we will now review some of the architecture in more de-
tail.
</p>
<p>3.3 Data Storage
</p>
<p>Remembering that the way that information is actually stored in a database is in 1s
and 0s, we should examine how these bits are managed by the database management
system. We will do this by building up from the bottom the storage of a row of
data. The example is from a very simple Customer database in which we store the
customer&rsquo;s birthday.
</p>
<p>The table structure looks like this:</p>
<p/>
</div>
<div class="page"><p/>
<p>48 3 Physical Storage and Distribution
</p>
<p>Fig. 3.2 Oracle Instance
</p>
<p>The table create script looks like this:
</p>
<p>CREATE TABLE &ldquo;CUSTOMERBIRTHDAYS&rdquo;
( &ldquo;CUSTOMERNO&rdquo; NUMBER (*,0),
</p>
<p>&ldquo;CUSTOMERFIRST&rdquo; VARCHAR(30),
&ldquo;CUSTOMERLAST&rdquo; VARCHAR(30),
&ldquo;BIRTHDAY&rdquo; DATE,
CONSTRAINT &ldquo;CUSTOMERBIRTHDAYS_PK&rdquo; PRIMARY KEY
</p>
<p>(&ldquo;CUSTOMERNO&rdquo;) ENABLE
)
</p>
<p>/
</p>
<p>CREATE INDEX &ldquo;CUSTOMERBIRTHDAYS_IDX1&rdquo; ON
&ldquo;CUSTOMERBIRTHDAYS&rdquo; (&ldquo;BIRTHDAY&rdquo;)
</p>
<p>/</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Data Storage 49
</p>
<p>And this is a sample of the data:
</p>
<p>The first thing we observe is that the columns are of different data types, except
the two text columns. The size of each column, in terms of bytes used to store it, will
therefore differ. Oracle uses 21 bytes to store numbers, so the CustomerNo column
will be 21 bytes long.
</p>
<p>The VARCHAR2 data type is a variable-length string of characters, the length
of which is set by the parameter passed in the brackets, but which can be, at most,
4000 bytes long. In our table the two name fields are 30 bytes long.
</p>
<p>The date column is 7 bytes long. Each of the bytes is used to store an element of
any date such that:
</p>
<p>Bytes 1 and 2 store the century and the year
Bytes 3&ndash;7 store Month, Day, Hour, Minute, Second
</p>
<p>Every row also has a pseudocolumn created by Oracle called the ROWID which
contains the unique identifier for the row, and its physical address on the file system
using Hexadecimal notation. On the author&rsquo;s system this is what is output from the
SQL statement:
</p>
<p>Select Rowid from CustomerBirthdays;
</p>
<p>Thankfully we mere mortals do not to understand this! But we need to be aware
that this is how Oracle maps a logical row to a physical position on a disk. However,
this information is not stored with the row data; rather it is used by the DBMS to
locate a row on a disk when required.</p>
<p/>
</div>
<div class="page"><p/>
<p>50 3 Physical Storage and Distribution
</p>
<p>Fig. 3.3 Row structure
</p>
<p>When the data in a row is stored the row has a header which indicates the num-
ber of columns in the row, and also points to other data blocks if there is chaining
involved (see below). Our row might look something like Fig. 3.3.
</p>
<p>This row is then stored in a data-block. The data block is a logical concept. It is
basically a pot which stores rows, or other objects&mdash;a collection of 1s and 0s. The
link with the physical disk is that a data block size will be a multiple of the physical
OS block size. In other words, if the OS block size was 4k, and the data block was
8k, when the disk head is sent to read some data from the disk, it will read two
consecutive OS blocks before it has read an Oracle data block. Data-block size is
set during database creation using the DB_BLOCK_SIZE parameter, and once set
it has to remain that size.
</p>
<p>Rows are inserted into a data block from the bottom, as in Fig. 3.4.
It is important to note that any deleted row will remain in the data-block, and just
</p>
<p>be marked as deleted so that it does not appear in any queries. The space will only
be reused when new rows are inserted and can be fitted in to the space left by the
deletion.
</p>
<p>Each block has a header which describes the contents and which Oracle uses
when locking rows as part of a write. The rest of the block is then either &ldquo;free</p>
<p/>
</div>
<div class="page"><p/>
<p>3.3 Data Storage 51
</p>
<p>Fig. 3.4 Data block
architecture
</p>
<p>space&rdquo; or data. Leaving free space in a data block is deliberate. Imagine our Birthday
example above. We could decide to populate the table with a series of inserts, like
this:
</p>
<p>Insert into CustomerBirthdays(CustomerNo, CustomerFirst, CustomerLast)
Values (125, &rsquo;Trevor&rsquo;, &rsquo;Jones&rsquo; );
Insert into CustomerBirthdays(CustomerNo, CustomerFirst, CustomerLast)
Values (127, &rsquo;William&rsquo;, &rsquo;Jones&rsquo; );
</p>
<p>Note that there is no Birthday recorded, but this is still a valid row and would be
stored in a data block, one row straight after the other. If we did this enough times
we would fill the data block and subsequent rows would need to be in another data
block. The problem with that is that we can&rsquo;t guarantee that the next data block will
be physically located next to the first one, and that might mean the OS might need
to do multiple reads, slowing up any queries.
</p>
<p>So, we have a block full of rows like this, when we suddenly get to find out
what the customers&rsquo; birthdays are. We do an SQL update statement, but because the
row resides in a full data-block Oracle will have to place this extra information in
another data-block. Again, this could result in needless extra reads, slowing down
any queries.
</p>
<p>So the free space is kept in a data-block so that rows can expand if need-be. Of
course, too much free space means you are wasting HDD space. Too little means
you are more likely to need to extend a row to another data-block.
</p>
<p>Once a data-block has insufficient free space to store an update, the row just has
to be split across two data-blocks. This is called chaining.</p>
<p/>
</div>
<div class="page"><p/>
<p>52 3 Physical Storage and Distribution
</p>
<p>3.3.1 Row Chaining andMigration
</p>
<p>Sometimes, such as when the contents of a row are bigger than the data-block size,
Oracle just has to use more than one data-block to store the data. This also hap-
pens when an update is bigger than the free space left in the original data-block.
The early part of the row will be stored in the first data-block; there will then be
a pointer to another data-block where more of the row is stored. This is known as
chaining. If this isn&rsquo;t big enough, the chain can continue growing until the data is
stored.
</p>
<p>If a row grows too big to fit in a data-block, but can fit into another data-block
Oracle may migrate the row to a new data-block to eliminate the chaining cause by
the update.
</p>
<p>3.3.2 Non-relational Databases
</p>
<p>As we will see in later chapters not all databases are relational and they may well
store data in a different way. As an extreme example, MongoDB has no schema and
you can write data of any size however you wish. Data is not stored in tables. But
for now let us just concentrate on RDBMSs.
</p>
<p>3.4 How Logical Data Structures Map to Physical
</p>
<p>When we design our relational databases we tend to think in terms of rows, columns
and tables. As we have seen above, these are all logical structure, not physical. But
there does have to be a mapping between those structures and the permanent storage.
</p>
<p>Again, different RDBMSs use different terms, but have similar approaches. In
Oracle terms the key elements in a logical hierarchy are:
</p>
<p>Data blocks store the actual data, or indexes, or other objects.
</p>
<p>The many data-blocks that represent the table, index, or other object, reside in
a logical container called a Segment. If segments need to grow, then physical disk
space needs to be acquired. This is managed at the tablespace level, since the ta-
blespace is supported by one or more data (OS) files.
</p>
<p>Several tablespaces make up a database. This can be represented as in Fig. 3.5.
</p>
<p>3.5 Control, Redo and Undo
</p>
<p>We now move from the actual data to some of the supporting aspects of the
RDBMS&rsquo;s processes.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.5 Control, Redo and Undo 53
</p>
<p>Fig. 3.5 Logical to physical
mapping
</p>
<p>Databases are designed with the realisation that things do go wrong with
technology. The chapter on Availability explores this in more detail, but dealing
with failures is so central to making a database robust that there are elements
of the systems that are in place only as fall-backs, just in case something goes
wrong.
</p>
<p>Remembering that the &ldquo;current&rdquo; status of the database will include a mix of
data on a HDD and in dirty buffers, the first thing the database will want is some
way of knowing when a database was last consistent; that is when all the data
changes were written to disk. Put another way, it is the point just before the old-
est dirty buffer in memory. This information is very important at shut-down so that
all data is saved before exit, and during any recovery situation after a database fail-
ure.
</p>
<p>To manage this process Oracle uses a System Change Number (SCN). This
gets incremented by every transaction that changes the state of the database after
a commit. It is managed within the SGA and just keeps incrementing for ever. In
this way Oracle can discover what order events took place in. This is important
during a restore and recovery of a failed database, for example. The Checkpoint
process we discussed earlier writes information about the SCN into a binary file
called the Control File. This is so important to Oracle that it is suggested you have
more than two duplicates, preferably each on a different HDD in case of disk fail-
ure.
</p>
<p>In order to ensure that no committed data is ever lost, Oracle keeps a log of
every change made to any data-block. These are called the Redo Logs. They contain
change vectors to enable any changes made to be replayed should the need occur, for
example in a recovery scenario. The change vector is written BEFORE the commit
is actioned. This means that if the user gets confirmation that their commit has been
processed, the data post-commit can be replaced even if it was not written to disk
before the instance failed.</p>
<p/>
</div>
<div class="page"><p/>
<p>54 3 Physical Storage and Distribution
</p>
<p>Redo logs are clearly an important part of ensuring robustness. Again, Oracle
allows these files to be multiplexed, that is two or more identical copies of log are
automatically updated in separate locations to guard against a disk failure on the
disk containing the Redo Log.
</p>
<p>The default operation for Redo Logs is that they are circular, in other words,
data is written until the log is full, and then it keeps on writing over the pre-
vious data. The length of time data is stored for will clearly depend upon how
big the Redo Log file is, and how many transactions happen in any given pe-
riod.
</p>
<p>When an instance fails the process is that you go to the last back-up and
restore from it. You then &ldquo;replay&rdquo; the changes as stored in the Redo Log. If
those changes are not recorded for any period between the back-up and now,
you will only be able to run with the database as it was backed-up&mdash;potentially
forcing irate users to replicate the work they did over the missing days. If this
is likely to be a problem then you need to save the data in the Redo Log be-
fore it gets overwritten. This is called archiving, and a back-up plus a fully
archived Redo Log means you can restore a database to any point in time you re-
quire.
</p>
<p>Oracle also maintains UNDO information, not to be confused with Redo. Undo
is the mechanism for allowing the user to issue the Rollback command&mdash;in other
words, to undo the changes they have made, and it does so by storing the old
data values. The other important task for Undo is to allow for multiple users to
be able to read the database in a read-consistent way. This means that, by using
the Undo data, snapshots of data as at a point in time can be created, meaning
that users are not blocked from accessing the data by other users who are making
changes.
</p>
<p>3.6 Log and Trace Files
</p>
<p>As Oracle completes tasks or encounters error it writes out information to an Alert
Log. This is a text file which is constantly being written to (and so can grow quite
large). Being a text file, it can be read even if the database is down. Oracle will put
the file in the directory specified by the BACKGROUND_DUMP_DEST parame-
ter.
</p>
<p>Figure 3.6 contains some Alert Log content. As well as being readable in a text
editor, you can also access it through the GUI management tool, Enterprise Man-
ager.
</p>
<p>3.7 Stages of Start-up and Shutdown
</p>
<p>With so many structures and processes to manage, it is perhaps not surprising that
starting an instance is not as simple as hitting an on/off switch. Not only do a lot</p>
<p/>
</div>
<div class="page"><p/>
<p>3.7 Stages of Start-up and Shutdown 55
</p>
<p>Fig. 3.6 Alert Log contents
</p>
<p>of things have to be in place for an instance to be usable, but there may be circum-
stances when a DBA wants the database to be available for maintenance, but not for
users to access.
</p>
<p>The start-up stages can be transparent to the DBA if they simply type the
STARTUP command. The output at the terminal simply describes a successful start-
up:</p>
<p/>
</div>
<div class="page"><p/>
<p>56 3 Physical Storage and Distribution
</p>
<p>The stages shown above are:
1. The instance has started&mdash;in other words, the program is running.
2. The required memory structures have been acquired by the instance.
3. Then, remembering that in Oracle terms the database is what is stored on disk,
</p>
<p>we are told that the disks have been mounted&mdash;in other words the instance can
talk to the disks.
</p>
<p>4. Finally the database is made available to users.
Although it is not obvious from these messages, the first thing Oracle tries
</p>
<p>to do is read the initialisation parameters from an initialisation file which is
stored in a platform-specific location. This file contains the default settings for
Oracle&rsquo;s start-up parameters, including the locations of important files and the
database name. If you want to see what the parameters have been set to, whilst
at the SQL&gt; prompt in SQLPLUS, logged in as SYS, type; SHOW PARAME-
TERS.
</p>
<p>Instead of issuing the STARTUP command, a DBA can issue the STARTUP
NOMOUNT command. This starts the instance and creates the memory structures
but does not mount the disks or open the database for users. More likely, however,
is the STARTUP MOUNT command. This means the DBA has an instance talking
to disks, but no users are able to attach, so that maintenance can be carried out.
After the work is completed the database can be opened by issuing the ALTER
DATABASE OPEN command.
</p>
<p>Closing the database is also a staged affair, but there are extra complications
which are around what happens to anyone using the database when the SHUT-
DOWN command is issued. The steps are the reverse of a start-up, and the reassur-
ance messages tell us exactly what is happening:
</p>
<p>In the example above the DBA issued the SHUTDOWN IMMEDIATE com-
mand. This will close all open sessions and rolls back all uncommitted transactions.
There are several other SHUTDOWN options. SHUTDOWN NORMAL waits for
transactions to finish and users to log off. This can take hours and despite being
called NORMAL, is therefore not used that often.</p>
<p/>
</div>
<div class="page"><p/>
<p>3.8 Locking 57
</p>
<p>If you want to be kinder to the user, but not wait for hours, you can issue the
SHUTDOWN TRANSACTIONAL which waits for transactions to complete and
then logs users off.
</p>
<p>3.8 Locking
</p>
<p>Whilst the database is open users will be accessing the rows from a variety of ta-
bles. Whilst each of them is reading the rows we do not have a problem. If the
row data is in the shared memory we can simply send a copy of it off to the
user.
</p>
<p>Problems start to occur in multi-user databases when more than one per-
son tries to write data to the same physical part of a disk. Let us look at a
few examples for the sample CUSTOMERBIRTHDAYS table we described ear-
lier.
</p>
<p>Example A Two separate users want to add new rows to this table. This is not
a problem for Oracle since it operates Row level locking rather than table level
locking. Multiple rows can therefore be added at any time. This assumes that the
primary key used by both is indeed a unique identifier.
</p>
<p>Example B One user is writing data to the table whilst a separate user wants to
read some rows from that table. Again, not a problem for Oracle since it always
ensures that Writers can&rsquo;t block Readers. To make sure the data the reader sees
is always as it was at the moment they issue the SELECT, Oracle will use the
Undo data to provide read consistency if another user changes row data at the same
time.
</p>
<p>Example C Both users are wanting to update the same row at about the same time.
This is where the problems start. At the moment that the first UPDATE statement
is seen by Oracle, issued by User_1, it places a lock on the data contained in the
row. No other user can now access that row to update it, at least until a commit
is issued, at which point the lock is released. User_2 then obtains ownership of
the lock on the row, and no-one else can gain access to that row until the User_2
commits.
</p>
<p>The potential problem in Example C is that User_2 might have to wait hours
if User_1 doesn&rsquo;t issue a commit. This situation might well end up with the DBA
being called by USER_2, and then User_1 would need to be contacted and asked to
commit or rollback, or their session could be killed by the dba to release the lock,
causing them to lose their changes.
</p>
<p>The worst case variation on Example C is called a deadlock. This happens
when both User_1 and User_2 have an unresolvable need for a lock to be re-
leased. Take the CUSTOMERBIRTHDAYS example and the input of the two</p>
<p/>
</div>
<div class="page"><p/>
<p>58 3 Physical Storage and Distribution
</p>
<p>users:
</p>
<p>Since both users are waiting for access to a row that will never be granted, this
conflict can&rsquo;t be resolved in any way other than for one of the transactions to be
rolled-back. The way to make deadlocks less likely is to commit often, and have all
users commit often.
</p>
<p>3.9 Moving Data
</p>
<p>The need to move data around is not new, and will be with us while ever
there are different DBMSs. We need to move data internally, between sys-
tems for example: when exchanging data between disparate information systems;
moving from an OLTP system to a Data Warehouse or upgrading legacy sys-
tems.
</p>
<p>Externally too, data needs to be moved between organisations: to and from sup-
pliers; to and from customers; to and from a variety of government agencies.
</p>
<p>Often this movement is enabled by dumping data from the source database, stor-
ing it in a file of some description, and then sending that file to the server running
the target database, which then reads the file and inserts the data into one or more of
its tables.
</p>
<p>Traditionally the file used was a Comma Separated Variable (CSV) file. Occa-
sionally there would be a TSV (Tab Separated Variable). In both of these the record
runs from column one until the end-of-line marker, and each field in that record is
separated from the previous one by a comma (or tab) character. Here is an exam-
ple of three rows from the CUSTOMERBIRTHDAYS table stored in a file called
Newcustomers.csv:
</p>
<p>14, Raj, Patel
17, Maurice, Ravel
19, Boris, Jackson
</p>
<p>At the target end a program with a couple of loops in it iterates through the file
looking for the end-of-line marker, and then breaking that line up by looking for</p>
<p/>
</div>
<div class="page"><p/>
<p>3.9 Moving Data 59
</p>
<p>the next comma. This sort of programming is bread and butter for most DBAs and
this simplicity meant that CSVs became the de facto standard way of passing data
between databases.
</p>
<p>In more recent times XML has become a common means for moving data be-
tween systems. It has the additional benefit of being readable by a browser, and so
more understandable to the human reader, but because of the repeated data descrip-
tion tags these files can become very large. If this becomes an issue there is also a
compacted version known as Binary XML.
</p>
<p>Database vendors recognised a need and produced tools to help. In Oracle there
is an exceedingly powerful tool called SQLLoader, which can bring data in from
a variety of formats and map the fields being read in directly to columns in the
target table on the fly by automatically creating a series of SQL insert commands.
The user no longer has to write any code, but does have to create a control file
which tells Oracle how to do the reading and mapping. An example of such a file,
which reads the above CSV and inserts the data into the appropriate table would
be:
</p>
<p>OPTIONS (ERRORS=999)
LOAD DATA
INFILE &rdquo;f:\tutorials\moving\Newcustomers.csv&rdquo;
BADFILE &rdquo;f:\tutorials\moving\Newcustomers.bad&rdquo;
INSERT
INTO TABLE CUSTOMERBIRTHDAYS
FIELDS TERMINATED BY &rsquo;,&rsquo; OPTIONALLY ENCLOSED BY &rsquo; &rdquo; &rsquo;
(
CUSTOMERNO,
CUSTOMERFIRST,
CUSTOMERLAST
)
</p>
<p>The INFILE is the location of the incoming CSV file, the BADFILE is where
Oracle puts any records that error. Then Oracle is told to INSERT records into
the table mapping the first field to CustomerNo, second to CustomerFirst, and
so on. The options at the top tells Oracle to keep processing even when it en-
counters errors, and only stop if it encounters more than 999. You would save
this into a file called something like Newcustomers.con and then pass that file-
name as a parameter when calling the SQLLoader executable from the command-
line:
</p>
<p>$sqlldr userid=peter/pword control=Newcustomer.con</p>
<p/>
</div>
<div class="page"><p/>
<p>60 3 Physical Storage and Distribution
</p>
<p>Whilst this may seem a little messy, it is certainly easier than writing code
from scratch. And the beauty of SQLLoader is that it can accept data in lots of
different formats. Another common medium, for example, is the Fixed Length
file, where the fields within the record are always at a certain position in a
file:
</p>
<p>14 Raj Patel
17 Maurice Ravel
</p>
<p>3.10 Import and Export
</p>
<p>There are sometimes occasions when we need to take a subset of the database and
move it to another database. This is often referred to as Exporting and Importing
the data. We need to be cautious with the terms Import and Export, however, since
different vendors use the words slightly differently. In Oracle parlance and Export of
data is a collection of objects, like Tables and Procedures, saved to an external file.
The file is not human-readable and is in such a format that only an Oracle Import
would be able to deal with it.
</p>
<p>As an example, let us Export the CUSTOMERBIRTHDAYS table to a file which
we will call BDAY.DMP, which then could be imported to another Oracle database
elsewhere.
</p>
<p>As with SQLLoader this process involves calling an executable from the com-
mand line and passing a parameter which points to a control file, here known as a
parameter file (PARFILE):
</p>
<p>$EXP peter/pword PARFILE=expparams.par
</p>
<p>The contents of the parameter file expparams.par might look something like this:
</p>
<p>FILE=&rdquo;f:\mydata\bday.dmp&rdquo;
TABLES=(CUSTOMERBIRTHDAYS)
GRANTS=y
INDEXES=y
ROWS=y
TRIGGERS=y
</p>
<p>This reads the table CUSTOMERBIRTHDAYS and also permissions granted
against that table; and indexes connected with that table; all the data (ROWS=n</p>
<p/>
</div>
<div class="page"><p/>
<p>3.11 Distributed Databases 61
</p>
<p>would give you just the table definition); and any triggers associated with the ta-
ble.
</p>
<p>At the target database all the DBA would need to type would be:
</p>
<p>$imp peter/pword FILE=bday.dmp
</p>
<p>Ms Access on the other hand allows you to pick a format to export to, includ-
ing.CSV format. It also allows you to Import from a variety of formats.
</p>
<p>3.10.1 Data Is Important
</p>
<p>When dealing with actual data it is all too easy to forget that each row is potentially
as important as the others. Our task has to be not only to move the data, but to move
it accurately.
</p>
<p>Think of a situation where you transfer a row from one system to another using
any of the methods alluded to above. Perhaps the count of inserted rows does not
exactly equal the count of rows exported. And perhaps a particularly slovenly DBA
doesn&rsquo;t notice and declares the data safely transferred. But what if the missing row
was in the Allergies table of a medical database and it holds the information that
Patient X is allergic to penicillin? Now imagine that Patient X is administered peni-
cillin and then dies after a severe allergic reaction. How would you feel if you were
that DBA?
</p>
<p>Data can be a matter of life and death, but even in less severe circumstances it is
likely to be important and we have a duty to look after it appropriately.
</p>
<p>In some circumstances the data we are provided with is itself unclean. We there-
fore need to ensure we understand the data we are dealing with and that when it
completes its journey it is a valid and exact representation of the information being
stored. However, it is not a DBA&rsquo;s job to guess what unclean data should be. Data
cleansing should be carried out cautiously and with the approval of the owner of the
data.
</p>
<p>3.11 Distributed Databases
</p>
<p>Up until this point we have been thinking in terms of our data being stored
in one place. As we have seen it is entirely probable that a large enterprise
database will use many individual HDDs to make a permanent record of the data.
For availability reasons (see Chap. 10) there may be some identical data stored
on another system, maybe in a geographically distant location, but those indi-
vidual HDDs are really just parts of a single, central database accessed by all
users.</p>
<p/>
</div>
<div class="page"><p/>
<p>62 3 Physical Storage and Distribution
</p>
<p>Imagine, however, that we work in an organisation with three offices: one in
Chennai (India), one in Perth (Australia), and one in Reykjavik (Iceland). Each col-
lects data about its customers in their part of the world. The company happens to be
Icelandic, so the head office is in Reykjavik.
</p>
<p>One solution in this case would be to have a single Server running in Reykjavik
and for the other offices to connect into that server in the normal client/server fash-
ion. But what if the people in Chennai want to collect slightly different information
about their customers to people in the other offices? In that case, with a single server
system the schema would have to be made flexible enough to cope with the differ-
ences. And what if the office in Perth wants to add a column to one of the tables?
Changing schemas can cause problems, but the people in Perth may think it worth
the risk, whereas the others may not.
</p>
<p>Distributed databases can get around these problems of data ownership by al-
lowing each local office to maintain its own database using a local version of the
DBMS, whilst also running a Distributed DBMS (DDBMS) which, at a meta level,
allows some users to see the database as the sum of all three.
</p>
<p>For clarity, we will here define a distributed database as:
</p>
<p>More than one logically interrelated database elements which are connected by a computer
network, and are typically in different geographic locations.
</p>
<p>What this means is that the SQL statement:
</p>
<p>select customer_name from customers;
</p>
<p>will return all the customer names from all offices for the user of the DDBMS, but
for the local DBMS user, they would see only a list of customer from their region.
</p>
<p>And the benefits are not just about allowing local flexibility. Bandwidth and net-
work communications can be more problematic across great distances. In the cen-
tralised solution, if the Perth office loses access to the internet it looses access to the
system, whereas if they are connecting to the local version, they may well be able
to carry on working.
</p>
<p>As we see in the chapter about Scalability (Chap. 9) relational databases can
begin to slow when storing large numbers of rows. A distributed database like the
one we are discussing here could greatly help performance by spreading the storage
load around the individual sites. Moreover, when the DDBMS is asked to gather
rows from all the local systems, the fact that the select statement is being run on three
different servers means that we automatically get a degree of parallel processing
which could speed up the return of the data.
</p>
<p>There are, naturally, downsides to opting for a distributed solution, not least of
which is the added complexity of such a system. A central resource looked after by
a single team of experts may be seen as easier and less expensive than having three
teams, especially as there also needs to be some effort put into the gluing together
of the various elements of the overall database. You could also argue that, because</p>
<p/>
</div>
<div class="page"><p/>
<p>3.11 Distributed Databases 63
</p>
<p>there are more remote access points, a distributed database is more vulnerable to
security problems.
</p>
<p>Distributed is not the same as replicated. As we see in the Availability chapter
having data on different servers replicated can help with Disaster Recovery, and can
allow parallel processing of queries to enhance performance. The downside is the
extra overhead involved in keeping the different systems in step, with each having
the same consistent data. Waiting for consistency can occur because of networking
problems and the user will see this as poor performance or a lack of robustness.
</p>
<p>The distributed model has come to the fore in recent years with the advent of
the Internet first, and then of Cloud computing. These days data can be spread over
thousands of servers all around the globe, and the need to query that amount of
information caused lots of headaches using traditional technology. The problems
became very apparent to Google when they were trying to search for and retrieve
information from across the Web.
</p>
<p>In the end they went back to basics and created their own file handling system
to be able to cope with the particular needs of their search engine which has to
work in a highly distributed environment (data is stored all over the world) and
with extremely large volumes of data. They called their system GFS (Google File
System), and it remains proprietary, but, as we see in the Big Data chapter, Hadoop
is an open source equivalent and it has become the de facto solution to this sort of
problem in the last couple of years.
</p>
<p>One of the important drivers in this change was the realisation that it takes up
less computing resource to move the data processing than it does to move the data
itself, especially when we are talking about such large volumes.
</p>
<p>In the Hadoop version of distributed databases, the data is chunked up and dis-
tributed to a number of nodes (which will be commodity servers). The Hadoop
equivalent to GFS is called HDFS and it looks after the distribution of these chunks
of data across the nodes. As well as managing this sharing out of the data, the Master
node also decides how to replicate the chunks on other nodes to assist with avail-
ability by building in redundancy.
</p>
<p>HDFS is designed to cope with large blocks of data, usually of at least 64 Mb in
size. The downside to this is that it is not good at handling many small files. On top
of this it is optimised for write-once read-many type applications. In short, whilst it
does answer the problem of querying large collections of distributed data, it is not
suitable for other types of application.
</p>
<p>When the user makes a request for data the Hadoop master node will establish
which nodes contain the data and instruct the Hadoop slaves on those nodes to pro-
cess the request and return the data. In this way the data is not moved at all before
retrieval and processing is carried out in the most efficient way possible.
</p>
<p>Many database vendors, such as Oracle and SAP now include Hadoop in their
portfolios, and there is little doubt that there will continue to be a strong demand for
this solution as we see digital data continue to expand exponentially in the next few
years.</p>
<p/>
</div>
<div class="page"><p/>
<p>64 3 Physical Storage and Distribution
</p>
<p>3.12 Summary
</p>
<p>This chapter has discussed the key physical building blocks of memory, processors
and disks in database systems. We explored how the limitations of those fallible
components have led to the development of performance and availability focused
additions to database management systems. We looked at the mechanisms involved
in starting and stopping a modern client/server database, and the processing steps
required to respond to a user&rsquo;s request for data.
</p>
<p>We have also seen that there are different approaches available in terms of the
ways data is stored and in the ways we choose to move data about between systems.
We looked at the differences between client/server and distributed databases and
explored their relative strengths and weaknesses.
</p>
<p>3.13 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; Why does Oracle prefer to return data to a user from the Shared Pool rather than
</p>
<p>read it from disk?
&bull; What is the purpose of the Redo Logs?
&bull; What does row chaining mean and what causes it?
&bull; When does a deadlock occur and what is a good way of reducing the risk of one
</p>
<p>occurring?
&bull; What types of application are not suited to running on Hadoop?
</p>
<p>3.14 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 The company you work for has just acquired another com-
pany. You are asked to look at ways to quickly retrieve data from the acquired com-
pany&rsquo;s Oracle based systems. Discuss the alternative approaches you might take,
listing the potential benefits and weaknesses of each approach, and clearly state any
assumptions you make.
</p>
<p>Discussion Topic 2 Five years have passed since the other company was acquired
and still key systems are not working well together. Some of the problems are around
networking issues and the wide geographic divide between the two head offices.
</p>
<p>You are asked to propose a new database design from scratch. Discuss the com-
parative benefits and problems of adopting a distributed approach with adopting a
single, central client/server system.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 65
</p>
<p>References
</p>
<p>Aron J (2013) Google and NASA team up to use quantum computer. Updated 12:36 21 May 2013.
For similar stories, visit the Quantum World Topic Guide. http://www.newscientist.com/article/
dn23554-google-and-nasa-team-up-to-use-quantumcomputer.html</p>
<p/>
<div class="annotation"><a href="http://www.newscientist.com/article/dn23554-google-and-nasa-team-up-to-use-quantumcomputer.html">http://www.newscientist.com/article/dn23554-google-and-nasa-team-up-to-use-quantumcomputer.html</a></div>
<div class="annotation"><a href="http://www.newscientist.com/article/dn23554-google-and-nasa-team-up-to-use-quantumcomputer.html">http://www.newscientist.com/article/dn23554-google-and-nasa-team-up-to-use-quantumcomputer.html</a></div>
</div>
<div class="page"><p/>
<p>Part II
</p>
<p>Database Types</p>
<p/>
</div>
<div class="page"><p/>
<p>4Relational Databases
</p>
<p>What the reader will learn:
</p>
<p>&bull; The Origins and terminology of relational databases
&bull; Database design&mdash;normalisation
&bull; Database design&mdash;entity modelling
&bull; Moving from design to implementation
&bull; The basics of Structured Query Language
</p>
<p>4.1 Origins
</p>
<p>No discussion of relational databases would be complete without a reference to
Edgar Codd&rsquo;s 1970 paper &ldquo;A Relational Model of Data for Large Shared Data
Banks&rdquo;. This was a mathematical description of what we now call Relational
databases and operations to manipulate them. This is called relational algebra. Codd
considered data could be organised into relations consisting of tuples, each with
consistent attributes. A tuple containing 6 attributes would be called a 6-tuple. Most
database professionals would translate this as meaning data can be organised into
tables consisting of records (or rows), each with consistent attributes (see Fig. 4.1).
</p>
<p>It is not intended to give a description of relational algebra here, but for those
who want more information see Date (2005), Chap. 5.
</p>
<p>Each record (or tuple) in a table is uniquely identified by a primary key which is
stored as an attribute or combination of attributes. For example a key could consist
of name, street, house number and postcode that could be put together to uniquely
identify a person (assuming two people with the same name do not live at the same
address). However, the key is more commonly an attribute created specifically for
the purpose of uniquely identifying a record, its name often ending in -ID. A quick
glance of my own data reveals I have many unique identifiers&mdash;my National Insur-
ance Number, my employee ID number, my library ID, my driving licence number
and my bank account number are but a small selection of all the unique identifiers
associated with me.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_4,
&copy; Springer-Verlag London 2013
</p>
<p>69</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_4">http://dx.doi.org/10.1007/978-1-4471-5601-7_4</a></div>
</div>
<div class="page"><p/>
<p>70 4 Relational Databases
</p>
<p>Fig. 4.1 Relational
nomenclature
</p>
<p>As well as describing the structure of the data, Codd also described a number of
operations which could be performed on it. These included selection, projection and
joins between tables. Selection and joins are common terminology in databases, but
projection defines conditions on the data you want to retrieve, for example people
with an age greater than 18.
</p>
<p>Ultimately these operations were formalised into a structured query language or
SQL. This was released in 1979 by what was to become Oracle Corporation.
</p>
<p>4.2 Normalisation
</p>
<p>Normalisation is a bottom up approach to database design concentrating on at-
tributes and dependencies. Edgar Codd introduced the concept of normalisation.
The primary aim of normalisation is to remove data redundancy, specifically repeat-
ing data in a single record. This reduced storage space and increased performance
as duplicated data was kept to a minimum and only used to connect tables together.
</p>
<p>A good example of why this is important can be seen by looking at the invoice in
Fig. 4.2. At first glance this could be regarded as a single record and in paper filing
systems it was often treated that way. However retrieving information from it was
difficult and a series of indexing systems were developed. For example, there was
often a card index with customer details record in it. This information changed very
rarely but was a quick method of finding customers details.
</p>
<p>The problem with documents like delivery notes, invoices and purchase orders is
they consist of data which is from three or more separate tables. For example there is
the customer information. This appears on every invoice for that customer. Secondly
there is what the customer ordered&mdash;this repeats&mdash;just look at your supermarket bill.
You don&rsquo;t want to store all this information every time you create a new delivery
note, so you store it separately and develop relationships between it. Once repeating
data is removed, you have your records in first normal form.
</p>
<p>So, if we look in detail at Fig. 4.2 we see that although it is a single physical
record it contains information about:
&bull; Invoice
&bull; Customer
&bull; Stock item (often called a line item).
</p>
<p>Because this is a company system there is no need to store information about the
company (there would only be a single record!)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.2 Normalisation 71
</p>
<p>Fig. 4.2 A typical delivery note included with goods in an on-line purchase
</p>
<p>Although you can divide the data into these separate tables you need a means of
linking them together. So, to tell which customer an invoice belongs to you need to
store something that identified them. These days that tends to be an identification
number or ID whereas in the past it was more likely to be name and address. Earlier I
mentioned how many different ID&rsquo;s were associated with me. How any ID numbers
do you think you have associated with you? Each one of these will be specific to
some system and you may be generating new ones everyday when you are shopping
on-line. Try making a list.
</p>
<p>4.2.1 First Normal Form (1NF)
</p>
<p>Normalisation requires you to go through a series of well-defined steps. First, re-
move repeating groups and create a new table to store these attributes. Include a
link to the table you have removed it from in the form of the key item. This will be
the foreign key. The main reason for removing repeating groups is to make sure you
don&rsquo;t end up with variable length records.
</p>
<p>It is necessary to identify a primary key to uniquely identify records in the re-
peating group you have removed. Sometimes you may have to invent one, either
for uniquely identifying a record, or because the combination of other attributes
is cumbersome. For example if an invoice number did not exist, you could use a
combination of the invoice date and billing name to uniquely identify an invoice
(although this only works if a customer has a maximum of one invoice a day).</p>
<p/>
</div>
<div class="page"><p/>
<p>72 4 Relational Databases
</p>
<p>There may be more than one unique identifier&mdash;in this case you have candidate
keys and you need to choose the most appropriate one. Sometimes it is a com-
bination of attributes. In the example, the Invoice ID is a unique identifier for the
invoice. The repeating group is the attributes of the items which were ordered on the
invoice&mdash;often called line items. In this case the unique identifier is the code (which
just happens to be the International Standard Book Number&mdash;ISBN). So you know
which invoice the line item belongs to, you need to include that as part of the pri-
mary key. You then end up with a compound key consisting of Invoice ID and Code.
Invoice ID is also a foreign key giving a link back to the invoice the line item relates
to. The 2 new tables are therefore:
</p>
<p>Invoice
Order date
Invoice ID (PK)
Invoice Date
Customer ID
Billing Name
Billing address
Shipping Name
Shipping Address
</p>
<p>Line Item
Invoice ID (PK, FK)
Code (PK)
Qty
Item Description
Price
VAT
Total
</p>
<p>4.3 Second Normal Form (2NF)
</p>
<p>Remove data that is only dependent on part of primary key (if there is no compound
key, it is probably already in 2NF) and create a new table. What you want is for
every non-key attribute to be dependent on the whole key
</p>
<p>In this case much of the Line item is only dependent on the code, so Line item
gets decomposed into Line Item and Stock Item: If you didn&rsquo;t do this you would
have to store the item description, price and VAT multiple times&mdash;once every time it
was included on an invoice&mdash;a very bad use of storage. The Line Item table therefore
becomes 2 tables with the code giving the link (foreign key) to the new Stock Item
table:
</p>
<p>Line Item
Invoice ID (PK, FK)
Code (PK, FK)
Qty
Total</p>
<p/>
</div>
<div class="page"><p/>
<p>4.4 Third Normal Form (3NF) 73
</p>
<p>Stock Item
Code (PK)
Item Description
Price
VAT
</p>
<p>4.4 Third Normal Form (3NF)
</p>
<p>Remove any data that is dependent on a non-key field and create a new table. This
happens when there is more than one candidate key and it would be unique. Create
a new table with this as the primary key. What you are trying to do here is separate
two entities which have become combined. They only appear once on the invoice
in this example, but only some of the information contained in the invoice changes
with every new invoice.
</p>
<p>In the example Name and Address are dependent on the Customer ID, so this can
be removed into a new table. The customers details are not going to change very
often, so there is no point re-entering and storing them every time a new invoice
is created. The Invoice would still need to contain the Customer ID so you would
know which customer the invoice related to:
</p>
<p>Invoice
Order date
Invoice ID (PK)
Invoice Date
Customer ID (FK)
</p>
<p>Customer
Customer ID (PK)
Billing Name
Billing address
Shipping Name
Shipping Address
</p>
<p>To achieve third normal form you also remove calculated field. In this case To-
tal can be calculated so you don&rsquo;t need to store it. There are occasions where you
might want to store data that can be calculated because of the overhead of doing the
calculation is higher than that of storing the calculated value.
</p>
<p>In Line item &lsquo;Total&rsquo; can be calculated as you know the quantity, price and VAT,
so it can be removed
</p>
<p>Line Item
Invoice ID (PK, FK)
Code (PK, FK)
Qty
</p>
<p>It is not unusual to find that once a table is in first normal form, it is also in third
normal form. Despite this it is still worth going through the steps.</p>
<p/>
</div>
<div class="page"><p/>
<p>74 4 Relational Databases
</p>
<p>Invoice Customer Line Item Stock Item
</p>
<p>Order date Customer ID (PK) Invoice ID (PK, FK) Code (PK)
Invoice ID (PK) Billing Name Code (PK, FK) Item Description
Invoice Date Billing address Qty Price
Customer ID (FK) Shipping Name VAT
</p>
<p>Shipping Address
</p>
<p>Fig. 4.3 The final tables
</p>
<p>Fig. 4.4 Entity Relationship diagram using crows foot notation to show one to many relationships
</p>
<p>The end result is there are 4 tables (see Fig. 4.3).
In all of them the tables in Fig. 4.3 all the attributes are fully dependent on the
</p>
<p>primary key and cannot be decomposed further.
Often this is drawn in as an ER (entity relationship) diagram. It should be noted
</p>
<p>there are a number of packages available to draw these diagrams. In this chapter
they are drawn using Microsoft Visio (Fig. 4.4).
</p>
<p>A number of concepts have been introduced in this example. The main one is the
idea of a key. There are a number of different types of key. The first is Candidate
Key. This is something that uniquely identifies a record. In some cases there may be
more than one, for example in our exercise we had ISBN which uniquely identified
an item, but it is possible there could have been a locally used stock code as well. It
would be up to the developer to decide which was best to use as the main identifier
of the record. That would become the Primary Key. The second type of Key is the
Foreign Key. This provides a link to another table and often has the same name as
a primary key in another table. Both primary and foreign keys can be composed of
more than attribute. The criteria is they are the minimum needed to uniquely identify
a record in the table.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.5 Beyond Third Normal Form 75
</p>
<p>4.5 Beyond Third Normal Form
</p>
<p>Most developers stop at third normal form as this resolves most of the issues as-
sociated with making data storage and retrieval efficient. However it can be taken
further to reduce other anomalies which can arise around keys, updates and temporal
issues.
</p>
<p>Bryce-Codd Normal Form (BCNF) deals with multiple overlapping candidate
keys. In these a combination of attributes may create a candidate key. A different
combination may form another. BCNF is not always achievable because it would
mean losing the dependencies determined at third normal form.
</p>
<p>Fourth Normal Form (4NF) is concerned with multivalued dependency. This can
occur in a table with three or more attributes when all the attributes in a table are
part of the composite key. It may in this case be necessary to decompose the table
into 2 or more tables.
</p>
<p>Consider the following example:
</p>
<p>Garage Speciality Suburb Served
</p>
<p>Speedy Motors Ford Nether Edge
Speedy Motors Ford Abbeydale
Speedy Motors Ford Hunters Bar
Speedy Motors Volvo Nether Edge
Speedy Motors Volvo Abbeydale
Speedy Motors Volvo Hunters Bar
A1 Service Centre Peugeot City
A1 Service Centre Volvo City
Sid&rsquo;s Super Service Station Volvo Nether Edge
Sid&rsquo;s Super Service Station Volvo Abbeydale
Sid&rsquo;s Super Service Station Peugeot Abbeydale
</p>
<p>Each of the attributes of Garage, Speciality and Suburb Served are key values and
form a composite key in third normal form, however the garage&rsquo;s speciality is not
affected by the suburb served, so this table should be split into two as the speciality
is dependent on the garage and the suburb served is dependent on the garage. As a
result we end up with two tables:
</p>
<p>Garage_Speciality
</p>
<p>Garage Speciality
</p>
<p>Speedy Motors Ford
Speedy Motors Volvo
A1 Service Centre Peugeot
A1 Service Centre Volvo
Sid&rsquo;s Super Service Station Volvo
Sid&rsquo;s Super Service Station Peugeot</p>
<p/>
</div>
<div class="page"><p/>
<p>76 4 Relational Databases
</p>
<p>Garage_Region
</p>
<p>Garage Suburb Served
</p>
<p>Speedy Motors Nether Edge
Speedy Motors Abbeydale
Speedy Motors Hunters Bar
A1 Service Centre City
Sid&rsquo;s Super Service Station Nether Edge
Sid&rsquo;s Super Service Station Abbeydale
</p>
<p>Fifth Normal Form (5NF) is again related to multivalued dependency. A table is
said to be in the 5NF if and only if every join dependency in it is implied by the
candidate keys. Rarely does a 4NF table not be in 5NF.
</p>
<p>Sixth Normal Form (6NF) relates to temporal databases and is intended to reduce
database components into irreducible components. In the example database there is
a problem with stock item. If the price of the item changes, the resulting change
would be applied to all historical invoices which is clearly incorrect. Likewise, VAT
can vary and this will be independent of price. There therefore needs to be extra
tables containing historical price data with the item code and the date they were
effective. This would also create a join to the invoice table.
</p>
<p>A disadvantage of normalisation, particularly among novice database design-
ers is that it provides a cook book approach which can be followed without any
understanding of the individual attributes and their relationships to one another.
Another issue is that of the performance hit caused by joins. In the current era
where memory and disk space is relatively cheap denormalised forms such as those
found in NoSQL (Chap. 5) and in-memory databases (Chap. 8) may be preferred
for speed.
</p>
<p>4.6 Entity Modelling
</p>
<p>An alternative approach to developing a logical design of a database is to identify
entities in the system and then map the relationships between them. This raises a
number of issues, the first of which is how do we identify entities and secondly
which ones are within the domain of the system.
</p>
<p>4.7 Use CaseModelling
</p>
<p>Use Cases are one part of the Unified Modelling Language (UML) which, although
designed for object oriented systems (see Chap. 7), can be used to develop a rela-
tional database design. Use Case diagrams are essentially a view of what an external
entity (either a user or an interfacing system) want to do with the data.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Use Case Modelling 77
</p>
<p>Fig. 4.5 Use case diagram
</p>
<p>This allows the designer to work out both what the requirements of the user are
and what entities are required in the system.
&bull; A Use Case is a statement of functionality required of the software, expressed
</p>
<p>in the format
&bull; Actor Action Subject
&bull; An Actor represents a stimulus to the software system. The stimulus can be
</p>
<p>internal or external.
&bull; An Action represents a capability of the software system.
&bull; A Subject represents the item acted upon by an Action of the software system.
</p>
<p>In the following example we may want to have a customer as an on-line user
(Fig. 4.5).
</p>
<p>From this we can identify a number of entities: Customer, Book (which could
be generalised to stock&rsquo; and account. Investigation of &lsquo;Details&rsquo; would identify a
number of attributes of the customer
</p>
<p>What we are identifying here are regular entities which will be transformed into
tables. Once these have been established, attributes can be investigated. Many will
be simple attributes, but others will be composite attributes, For example Name, and
address are both composite attributes. Name consists of Surname and given name
(there is a decision to be made as to how many given names are to be stored&mdash;
anything above 1 may be blank or null).
</p>
<p>Occasionally a multivalued attribute may be identified, for example in an em-
ployee table you might want to store qualifications. In this case a new table should
be created using Employee-ID and Qualification as a composite key. Other attributes
may include year obtained and institution.
</p>
<p>The example above is also an example of a weak entity. That means it does not
have an independent existence. If the employee gets deleted, then the associated
records in the qualification table also get deleted. Without the employee, to which
there is a one to many relationship, they have no meaning.</p>
<p/>
</div>
<div class="page"><p/>
<p>78 4 Relational Databases
</p>
<p>Fig. 4.6 Heavy entity
</p>
<p>Fig. 4.7 Many-to-many
relationship
</p>
<p>You also need to be aware of &lsquo;heavy&rsquo; entities. These are entities where there are
many null attributes. This may come about because data in one attribute may mean
that another attribute must be null. This often happens where the entity is actually a
hierarchy of entities. A decision needs to be made as to whether to keep the heavy
entity or map it so it reflects the hierarchy. An example of this comes later in this
chapter (Fig. 4.6).
</p>
<p>Once entities and their attributes have been identified binary relationships should
be identified. Initially map one-to-many relationships. In this the item at the &lsquo;many&rsquo;
end will include the primary key of the entity at the &lsquo;one&rsquo; end as a foreign key.
For example, a customer may have many orders (although an order can only be
associated with one customer). In this case the order contains the customer-id as a
foreign key.
</p>
<p>The more complex part is unravelling many-to-many relationships. For example,
an order may contain many stock items, but a stock item may occur on many or-
ders. Many-to-many relationships should not be part of a database design. They are
usually resolved by creating an associative entity.
</p>
<p>Consider the following situation. An employee may be assigned to several
projects and a project may have one or more employees assigned to it (Fig. 4.7).
</p>
<p>In this case a new entity called an associative entity needs to be created. The name
of this entity will depend on what the database is used for. It may be as simple as
&lsquo;Employee Assignment&rsquo;, or more complex, like &lsquo;Employee Time Tracking&rsquo; where
time spent and the activity could be stored as entities (Fig. 4.8).
</p>
<p>In both cases the new entity would have a composite key of Employee_ID and
Project-ID (although in rare cases a unique key may be created).
</p>
<p>Occasionally you may find you have a three (or more) way many-to-many rela-
tionship. For example a many suppliers may supply many parts to many customers.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Use Case Modelling 79
</p>
<p>Fig. 4.8 Resolved many-to-many relationship
</p>
<p>Fig. 4.9 Resolved three way relationship
</p>
<p>Also, any supplier can supply any part to any customer. This can be resolved in
much the same way as a many-to-many relationship by adding an associative en-
tity. This will have a composite key composed of the primary keys of the customer,
supplier and part tables (Fig. 4.9).
</p>
<p>Generally this will solve most of the design issues, however there may be
times when a number of other issues arise. One of these are unary relation-
ships. The most often quoted example of this (and the one used in standard
Oracle exercises) is where one employee is the manager of another employee.
In this case a manager manages many employees, but an employee has only
one manager. In practice this means a employee entity has an attribute, prob-
ably called &lsquo;manager&rsquo; which is a foreign key pointing back to the same ta-
ble. Ultimately one or more employees have a null value in this field be-
cause they have no manager (they are the top of the organisations hierarchy)
(Fig. 4.10).
</p>
<p>Rarely, there may be a unary many-to-many relationship. For example, in man-
ufacturing a product may be composed of many parts. Those parts may be used in
many products which themselves be a part of another product (Fig. 4.11).
</p>
<p>To make this work both the components of the primary key of assembly refer-
ence the primary key of part. That way the assembly has a one to one relationship
with part so the assembly can have a name, but at the same time there is another</p>
<p/>
</div>
<div class="page"><p/>
<p>80 4 Relational Databases
</p>
<p>Fig. 4.10 Unary
relationship&mdash;self join
</p>
<p>Fig. 4.11 Resolved unary many-to-many relationship
</p>
<p>Fig. 4.12 Fan trap
</p>
<p>relationship between assembly and part showing the assembly is made up of many
parts.
</p>
<p>There are two potential problems when modelling entities, the fan trap and the
chasm trap. These situations usually arise because of missing relationships. They
can often be identified by going back to the original use case diagrams and testing
the relationships to see if the required processes can be achieved (Fig. 4.12).
</p>
<p>In the above ER diagram it is not possible for an academic staff member to work
out which subject group they are in.
</p>
<p>This could be redrawn as Fig. 4.13, but this now gives us a chasm trap as not all
staff are in subject groups, but all staff belong to a faculty. The way to resolve the
problem is to add the missing relationship (Fig. 4.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>4.7 Use Case Modelling 81
</p>
<p>Fig. 4.13 Chasm trap
</p>
<p>Fig. 4.14 Resolving the traps
</p>
<p>The steps in entity modelling are therefore as follows:
1. Conduct a use case analysis to identify regular entities
</p>
<p>a. Identify composite attributes
b. Identify multivalued attributes
c. Map weak entities
d. Identity &lsquo;heavy&rsquo; entities and decide on a mapping resolution
</p>
<p>2. Map Binary relationships
a. Identify one to many relationships
b. Identify Many-to-many relationships
c. Map associative entities
</p>
<p>3. Map unary relationships
4. Check for fan and chasm traps</p>
<p/>
</div>
<div class="page"><p/>
<p>82 4 Relational Databases
</p>
<p>Fig. 4.15 Hierarchy diagram showing superclass (motor vehicle) and three subclasses
</p>
<p>4.8 Further Modelling Techniques
</p>
<p>With the advent of object oriented modelling, particularly where modellers are us-
ing UML toolkits, hierarchy models are sometimes constructed. This makes logical
sense, but they can&rsquo;t be directly implemented in a relational model. An object rela-
tional database system is the usual way this kind of model is implemented and this
will be discussed in depth in Chap. 7. However in the current context consider the
following simple hierarchy (Fig. 4.15).
</p>
<p>There are three ways this could be translated into relational tables:
</p>
<p>1. Collapse all the sub types into the super type&mdash;the heavy entity approach
(Fig. 4.16).
</p>
<p>This has the advantage of creating a very simple structure&mdash;a single table. It has
the disadvantage that for every record there will be four null fields, in other words
we have created a heavy entity. For example if we had a record for a car, the data
for a van and a motor bike could be null.
</p>
<p>This approach is best used where there are more attributes in the super type than
in the subtypes
</p>
<p>2. Combine the attributes of the super type with the subtypes (Fig. 4.17).
</p>
<p>This has the advantage that every distinct entity is now modelled separately and
there are no null fields. The main disadvantage is that relationships start to become
complex. For example if we had an entity called owner, each of the above entities
would need a foreign key attribute to join to the owner attribute.
</p>
<p>This approach is best used where the majority of the attributes are in the sub type.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.9 Notation 83
</p>
<p>Fig. 4.16 Heavy entity
approach
</p>
<p>Fig. 4.17 Attributes moved down into subtypes
</p>
<p>3. Implement the structure by making each of the types an entity in its own right
and adding appropriate keys (Fig. 4.18).
</p>
<p>This has the advantage of retaining most of the original hierarchy structure with
the main disadvantage being the introduction of an extra attribute to allow joining
of the tables.
</p>
<p>This approach is best used where the number of attributes in all the types is about
equal and the structure if the hierarchy wants to be maintained.
</p>
<p>4.9 Notation
</p>
<p>In this chapter we have been using the &lsquo;crows foot&rsquo; notation of describing relation-
ships. However many designers (and the packages they use) adopt the UML con-
vention with the cardinality of the relationship expressed at each end. The diagram
below shows the equivalences (Fig. 4.19).
</p>
<p>A further notation form by Chen (1976) shows entities and relationships using
the symbols, shown in Fig. 4.20.</p>
<p/>
</div>
<div class="page"><p/>
<p>84 4 Relational Databases
</p>
<p>Fig. 4.18 All types mapped as entities
</p>
<p>Fig. 4.19 Crows foot compared to UML notation
</p>
<p>The bookshop example therefore can be redrawn in Chen&rsquo;s notation (Fig. 4.21).
This can be compared with the modified entity-relationship diagram shown in
Fig. 4.22.
</p>
<p>Which the author finds particularly messy and complicated when compared to
the notation used everywhere else in this chapter. The only real advantage is the
diagram forces you to name relationships</p>
<p/>
</div>
<div class="page"><p/>
<p>4.10 Converting a Design into a Relational Database 85
</p>
<p>Fig. 4.20 Chen&rsquo;s notation
</p>
<p>Fig. 4.21 The example using Chen&rsquo;s notation
</p>
<p>4.10 Converting a Design into a Relational Database
</p>
<p>Once you have created your database design using either of the methods mentioned
above, you then need to convert it into a physical database. To do this there are some
other questions to be answered.
</p>
<p>Does any of the design need denormalising? You should look at any 1:1, 1:0
or 1:n relationships, particularly where n is 3 or less. This will reduce the number
of tables and hence the complexity of the design. The overhead may mean some
attributes with null values.
</p>
<p>What are the datatypes that will be used for each of the attributes? This may be
a decision influenced by the vendor package. For example not all RDMS support
boolean datatypes. Oracle supports:
CHAR: This is a fixed length datatype which is backfilled if the data stored is less
</p>
<p>than the field size, or returns and error is the data is longer than the field size</p>
<p/>
</div>
<div class="page"><p/>
<p>86 4 Relational Databases
</p>
<p>Fig. 4.22 Modified ER diagram
</p>
<p>VARCHAR2: is a variable length data type which is more efficient than CHAR.
However you still need to specify a maximum length and an error will be returned
if this is exceeded
</p>
<p>NUMBER: There may be issues with very large and very small numbers. Oracle
stores numbers up to 38 significant digits (both positive and negative). This is
more than adequate for most applications, but may be an issue if the database is
storing scientific data
</p>
<p>DATE
CLOB
BLOB
</p>
<p>No matter how you develop your tables, either through normalisation or entity
modelling, you also need to identify constraints. These will help maintain data in-
tegrity.
Primary Key constraint: This is basically identifying the primary key. It means
</p>
<p>values must be unique and they must not be blank (null)
Foreign Key constraint: Identifies the table the foreign key references
Not Null constraint: The entity must contain data, in other words it cannot be
</p>
<p>blank
Unique constraint: but that does not mean it is a primary key. You don&rsquo;t have to
</p>
<p>use this constraint on the primary key because that and NOT NULL are part of
the primary key constraint.
</p>
<p>Check constraints: These give rules as to what is valid data. They include math-
ematical constraints where data must be in a specific range and alphanumeric
conditions where data must come from a specified list or contain (or not contain)</p>
<p/>
</div>
<div class="page"><p/>
<p>4.11 Worked Example 87
</p>
<p>specific characters. There will be a more in depth discussion of this in Chap. 10
on security.
It is not uncommon for a particular entity to have several constraints.
Most database management system will allow you to name the constraints you
</p>
<p>create however if you do not name them, they will still be created and assigned a sys-
tem name which will be almost impossible to retrieve. It is therefore recommended
you name your constraints and use a systematic naming convention (for example
foreign keys are prefixed with FK_). The name and the constraint type are held in
the systems data dictionary.
</p>
<p>4.11 Worked Example
</p>
<p>The following exercise uses Oracles SQL Plus, but as few of Oracle specific op-
erators will be used as possible and the syntax works in Microsoft&rsquo;s SQL Server
except where stated. The customer details have been slightly altered to include a
postcode and a date when the customer was added to the system. In practice the
address and name attributes would probably be broken down further, for example,
surname, given name and initial for name and street number, street, city for address.
</p>
<p>4.12 Create the Tables
</p>
<p>In this example we are going to create the database seen in Fig. 4.22. This has been
modified from the original example in order to illustrate some features of SQL.
Tables can be created in any order, however it is more efficient if you create tables
which have no other tables dependent on them first. They can usually be identified
by not having the foot of the crows foot notation attached to them, alternatively they
are the 1 end of a 1:many relationship. Using this rule the first tables to be created
are Customer and Stock Item. The next is Invoice because it is only dependent on
Customer and finally Line Item because it is dependent on both Invoice and Stock
Item
</p>
<p>CREATE TABLE customer
</p>
<p>(customer_id VARCHAR(6),
</p>
<p>customer_name VARCHAR (24),
</p>
<p>customer_street_no NUMBER(3),
</p>
<p>customer_address VARCHAR (36),
</p>
<p>customer_postcode VARCHAR(6),
</p>
<p>date_entered (DATE));
</p>
<p>This syntax would create the table, but does not identify any constraints. Cus-
tomer ID should be identified as the primary key, the rest of the fields should have
data in them, that is should be NOT NULL.
</p>
<p>Therefore the CREATE statement becomes</p>
<p/>
</div>
<div class="page"><p/>
<p>88 4 Relational Databases
</p>
<p>CREATE TABLE customer
(customer_id VARCHAR(6)
customer_name VARCHAR (24) NOT NULL,
customer_street_no NUMBER (3),
customer_address VARCHAR (36) NOT NULL,
customer_postcode VARCHAR(6),
date_entered (DATE),
CONSTRAINT customer _pk PRIMARY KEY (customer_id));
</p>
<p>Constraints can be added and modified after the tables are created, but wherever
possible it is best to include them here. It is also important to name constraints so
that can be easily found in the data dictionary. If they are not named, they will still
be stored, but will be assigned a system name making them difficult to retrieve. In
this exercise a postscript convention to indicate the type of constraint is adopted:
</p>
<p>_pk primary key constraint
_fk foreign key constraint
_uk unique constraint
_ck check constraint
</p>
<p>The Stock item table is created in a similar way
</p>
<p>CREATE TABLE stock_item
(code VARCHAR(13),
item_description VARCHAR (24) NOT NULL,
price NUMBER (7,2) NOT NULL,
vat NUMBER (2) NOT NULL,
master_item VARCHAR(15),
CONSTRAINT stock_item_pk PRIAMRY KEY (code),
CONSTRAINT stock_item_fk FOREIGN KEY master_item REFER-
ENCES stock_item(code),
CONSTRAINT item_description_uk UNIQUE (item));
</p>
<p>This create statement illustrates how numbers are handled. price NUMBER
(7,2) defines price as being able to store numbers up to 9999.99. The 7 gives the
size of the field including the decimal point, and the 2 gives the number of places
after the decimal point. The VAT definition means two digits can be stored. Since
this is normally a percentage, it could have been defined as vat NUMBER (3,2).
</p>
<p>As well as a primary key constraint, this definition also contains a unique con-
straint which means no two descriptions can be the same. In theory this could have
been used as the primary key, but from a labelling and scanning perspective which
is how most items are handled today, the code (which happens to store the data on a
bar code) is the better candidate.
</p>
<p>The new master_item attribute is to be used where there may be several individual
items making up another item. For example, three books may be sold as a box set.
The books can be purchased individually or as a box set. Since not all items are
grouped, the new master_item attribute must not have the &lsquo;NOT NULL&rsquo; clause
associated with it. If they are a box set they will have a unique code. There is also a
new foreign key constraint linking the table with itself.</p>
<p/>
</div>
<div class="page"><p/>
<p>4.13 CRUDing 89
</p>
<p>The invoice table is created in a similar way, but in this case there is a foreign
key constraint which allows a join between this table and the customer table
</p>
<p>CREATE TABLE invoice
(invoice_id NUMBER (8),
invoice date DATE NOT NULL,
order_date DATE NOT NULL,
customer_id VARCHAR(6)
CONSTRAINT invoice_id_pk PRIMARY KEY (invoice_id),
CONSTRAINT customer_id_fk FOREIGN KEY (customer_id)
REFERENCES customer (customer_id));
</p>
<p>The foreign key constraint creates a link between customer and the invoice ta-
ble. It also means you cant have an invoice unless there is an associated customer
record in the customer table. Another impact of this constraint is you can&rsquo;t delete a
customer record if there are still any associated invoices.
</p>
<p>The final table to create is the line item table
</p>
<p>CREATE TABLE line_item
(invoice_id NUMBER (8),
code VARCHAR(13),
qty NUMBER (4),
CONSTRAINT line_item_pk PRIMARY KEY (invoice_id, code),
CONSTRAINT invoice_id_fk FOREIGN KEY (invoice_id)
REFERENCES invoice (invoice_id),
CONSTRAINT code_fk FOREIGN KEY (code)
REFERENCES stock_item (code));
</p>
<p>In this case there is a combined primary key and two foreign key clauses. This
is common where a many-to-many relationship is being resolved. If any of the con-
straint clauses is violated, an error will be returned. If more data than the field you
have defined is entered, an error will be returned. There are two ways of handling
these errors. One is by displaying the error directly to the user. A better way is to
have some programming code which handles the error in a more friendly way. This
aspect is beyond the scope of this discussion.
</p>
<p>4.13 CRUDing
</p>
<p>When working with a database you normally want to do four things: Create, Re-
trieve, Update and Delete data. Update and Delete operations are usually done in
conjunction with a Retrieve operation. It is always a good idea to show a user what
they are about to either change or delete before they do it and give them the option of
aborting the operation if they discover they have the wrong record. The rest of this
section will take you through a selection of structured query language statements. It
is not meant to be a complete set of examples, is provided to introduce SQL. Fur-
ther details of SQL for Oracle and SQL Server can be found in the further reading
section at the end of this chapter</p>
<p/>
</div>
<div class="page"><p/>
<p>90 4 Relational Databases
</p>
<p>4.14 Populate the Tables
</p>
<p>Once a table is created, it needs to be loaded with data. What will be demonstrated
below is how an initial load could be done. From a user point of view, the normal
day to day entry of data would be done via web forms created using a package such
as Oracles Developer or via web forms.
</p>
<p>To insert a new record into the customer table you would use the syntax:
</p>
<p>INSERT INTO customer VALUES (&lsquo;OS3457&rsquo;, &lsquo;Warren Felsky&rsquo;, 8, &lsquo;Mien Place,
</p>
<p>Sheffield&rsquo;, &lsquo;S7 4GH&rsquo;, &lsquo;20-APR-13&rsquo;);
</p>
<p>It should be noted that different database management systems have different
ways of handling time information. In the case of Oracle rather than &lsquo;20-APR-13&rsquo;,
TO_DATE (&lsquo;APR 20 2013&rsquo;, &lsquo;MON DD, YYYY&rsquo;) could be used where TO_DATE
is an Oracle built in function.
</p>
<p>Remember, you would not be able to insert records into the invoice table until
you had records with customer ID&rsquo;s in the customer table. If you tried you get a
foreign key constraint error.
</p>
<p>4.15 Retrieve Data
</p>
<p>The power of a relational database lies in the ability to easily search for data which
meets a particular condition. This condition may require data to be retrieved from
one or more tables. In the next section we will work from simple retrieval statements
to more complex ones which show the power of SQL.
</p>
<p>Perhaps the simplest statement is one where you want to retrieve all the records
in a table. In this case we will use the customer table
</p>
<p>SELECT &lowast; FROM customer;
</p>
<p>This is not a particularly useful statement as in a production system it could
retrieve hundreds of records.
</p>
<p>SELECT &lowast; FROM customer
</p>
<p>WHERE billing_name = &lsquo;Warren Felsky&rsquo;;
</p>
<p>Would return all customer records for the name Warren Felsky.
You can add multiple conditions, for example
</p>
<p>SELECT &lowast; FROM customer
</p>
<p>WHERE billing_name = &lsquo;Warren Felsky&rsquo; AND
</p>
<p>date_entered &gt; &lsquo;01-JAN-2010&rsquo;;
</p>
<p>The &lowast; in SELECT &lowast; means all columns are retrieved. It may be that you only
want data from certain columns to be to returned. To do this replace the &lowast; by the col-
umn names required. Some systems like Oracle will automatically label the output
with these column names</p>
<p/>
</div>
<div class="page"><p/>
<p>4.16 Joins 91
</p>
<p>SELECT billing_name, billing_address, billing_postcode, date entered FROM
</p>
<p>customer WHERE billing_name = &lsquo;Warren Felsky&rsquo; AND
</p>
<p>date_entered &gt; &lsquo;01-JAN-2010&rsquo;;
</p>
<p>4.16 Joins
</p>
<p>Retrieving data from one table is seldom enough. For example we might want to re-
trieve the invoice date and order date of a customer. Unless the customers id number
is know, this would require retrieving data from two tables. This is known as a table
join. There are a number of types of joins:
</p>
<p>SELECT billing_name, invoice_id, invoice_date
</p>
<p>FROM customer, invoice
</p>
<p>WHERE customer.customer_id = invoice.customer_id
</p>
<p>billing_name = &lsquo;Warren Felsky&rsquo; AND
</p>
<p>date_entered &gt; &lsquo;01-JAN-2010&rsquo;;
</p>
<p>The first part of the WHERE clause defines how the tables are going to be joined
by linking the primary key in one table (in this case the customer table) with its
equivalent foreign key in the invoice table. Because customer_id is common to
both tables it must be prefixed by the table name to avoid ambiguity.
</p>
<p>The above example is called an Equijoin. This means that only records which
have matching values in both tables will be returned.
</p>
<p>Inner Join (Also known as a simple join) returns those rows that satisfy the join
condition
</p>
<p>SELECT customer_name, invoice_id, invoice_date
</p>
<p>FROM customer
</p>
<p>INNER JOIN invoice
</p>
<p>ON customer.customer_id = invoice.customer_id;
</p>
<p>This syntax would retrieve those customers who have invoices. Any customers
without an invoice would not be shown. Note that &lsquo;INNER&rsquo; is an optional statement
and does not need to be included.
</p>
<p>In Oracle an Equijoin and an Inner Join are equivalent.
</p>
<p>Outer Join Extends the idea of an inner join by including some or all records
from the other table in the join which do not meet the join condition. There are three
types of outer joins:
</p>
<p>Left Outer Join This will return all records from the table on the &lsquo;left&rsquo; (literally
to the left of the LEFT clause) which have no matching records in the table on the
right. A null will be displayed instead. In the following example customers including
any which don&rsquo;t have any invoices associated with them will be returned. This will
have spaces (null) in the invoice_id and invoice_date output column.</p>
<p/>
</div>
<div class="page"><p/>
<p>92 4 Relational Databases
</p>
<p>SELECT customer_name, invoice_id, invoice_date
</p>
<p>FROM customer
</p>
<p>LEFT OUTER JOIN invoice
</p>
<p>ON customer.customer_id = invoice.customer_id;
</p>
<p>The word &lsquo;OUTER&rsquo; is optional in the syntax so LEFT JOIN would give the
same result.
</p>
<p>Right Outer Join This will return all records from the table on the &lsquo;right&rsquo; (literally
to the left of the RIGHT clause) which have no matching records in the table on
the right. A null will be displayed instead. In the following example all stock items
and their codes will be displayed regardless of whether they have appeared on an
invoice. Those which have no associated invoice will have a null displayed instead
of the invoice_id.
</p>
<p>SELECT invoice_id, stock_item.code, item
</p>
<p>FROM line_item
</p>
<p>RIGHT JOIN stock_item
</p>
<p>ON line_item.code = stock_item.code;
</p>
<p>Full Outer Join This will return all records from both the &lsquo;left&rsquo; and the &lsquo;right&rsquo;
tables whether there is a matching record in the other table or not. The syntax uses
FULL JOIN. Like the LEFT and RIGHT joins, any unmatched record will have
nulls displayed.
</p>
<p>Self Join Self joins are, as the name suggests a join that links a table to itself. Lets
consider the definition of the stock_item table again where we set up the conditions
for a self join:
</p>
<p>CREATE TABLE stock_item
</p>
<p>(code VARCHAR(13),
</p>
<p>item VARCHAR (24) NOT NULL,
</p>
<p>master_item VARCHAR (24),
</p>
<p>price NUMBER (7,2) NOT NULL,
</p>
<p>price NUMBER (7,2) NOT NULL,
</p>
<p>CONSTRAINT stock_item_pk PRIMARY KEY (code),
</p>
<p>CONSTRAINT stock_item_fk FOREIGN KEY master_item REFER-
</p>
<p>ENCES stock_item(code),
</p>
<p>CONSTRAINT item_description_uk UNIQUE (item));
</p>
<p>There is no specific self join syntax, but a LEFT JOIN can be used.
</p>
<p>SELECT m.item, i.item,
</p>
<p>FROM stock_item AS m LEFT JOIN stock_item AS i
</p>
<p>ON m.master_item = i.item
</p>
<p>In the example the syntax AS m and AS i is used to establish aliases to distin-
guish between instances of the table, in other words are we looking at the &lsquo;master</p>
<p/>
</div>
<div class="page"><p/>
<p>4.17 More Complex Data Retrieval 93
</p>
<p>item&rsquo; or &lsquo;ordinary&rsquo; item? Remember the master_item field may be NULL because
not all items are bundled into box sets. In this example only unmatched records
on the left will be displayed. Since these are box sets, it is unlikely there will be
any.
</p>
<p>Cartesian Join A final type of join is the. In this case every record in one table
is matched with every record in the joined table. So if you had two tables each of
10 records, you would end up with 100 records returned. This is obviously an error
and usually happens where matching join records are not specified. About the only
time this is likely to happen is if an equijoin is being used and the WHERE clause
linking the associated primary and foreign keys is left out
</p>
<p>Now we know how to retrieve data from tables and how to join tables together
we can start looking at more complex data manipulation. This includes more com-
plex WHERE clauses and grouping of data, for example, how could we find the
total value of an invoice, particularly as we dropped that attribute as part of the
normalisation process because it was a calculated field.
</p>
<p>4.17 More Complex Data Retrieval
</p>
<p>It is not unusual to know part of what we are looking for in our data base. For
example we know that there is the word &lsquo;Paris&rsquo; in the title of the book we are looking
for. Most databases allow the use of wildcard searches. A wildcard is a character that
takes the place of one or many other characters. It may be an &lowast;, or in the case Oracle
it is % for may characters and _ for a single character. So to search for Paris in our
stock_item table we can use:
</p>
<p>SELECT code, item, price
</p>
<p>FROM stock_item
</p>
<p>WHERE item LIKE &lsquo;%Paris%&rsquo;;
</p>
<p>If you wanted to see the cheapest book with Paris in the title you could add the
ORDER clause:
</p>
<p>SELECT code, item, price
</p>
<p>FROM stock_item
</p>
<p>WHERE item LIKE &lsquo;%Paris%&rsquo;
</p>
<p>ORDER BY price DESC;
</p>
<p>Omitting the DESC clause would make the order most expensive first. You can
have several levels of ordering, for example, if you wanted to sort first by price, then
by title the ORDER clause would become:
</p>
<p>. . . ORDER BY price DESC, item;
</p>
<p>A further requirement in most systems is to be able to calculate data. This is
done on a group of records that are returned. For example how many invoices do
customers have?</p>
<p/>
</div>
<div class="page"><p/>
<p>94 4 Relational Databases
</p>
<p>SELECT COUNT(invoice_id)
FROM invoice
GROUP BY customer_id;
</p>
<p>This can get quite complicated when a number of tables are involved, for exam-
ple, what is the total value of an invoice. This requires at least three of the tables in
our database and there is an argument for all four if the customers&rsquo; names is required
as well. The joins here are all INNER joins because we are only concerned equal
matches.
</p>
<p>SELECT invoice_id, COUNT (code), SUM (price)
FROM invoice JOIN line_item
ON invoice.invoice_id = line_item.invoice_id,
line_item JOIN stock_item
ON line_item.code = stock_item.code
GROUP BY invoice_id;
</p>
<p>4.18 UPDATE and DELETE
</p>
<p>The final two classes of operations in a relational database are UPDATE and
DELETE where you want to make changes to the data. Let&rsquo;s say we want to up-
date a customer&rsquo;s address. First it is a good idea to make sure you have the correct
customer so we could use:
</p>
<p>SELECT &lowast; FROM customer
WHERE billing_name = &lsquo;Warren Felsky&rsquo;;
</p>
<p>which we have seen before to retrieve all the customers with the name Warren Fel-
sky. We see from the data returned that one customer, OS3457 is the one we want.
We can then issue the UPDATE command to change the customers address:
</p>
<p>UPDATE customer
SET customer_street_no = 6,
</p>
<p>customer_address = &lsquo;McGregor St, Sheffield&rsquo;
customer_postcode = &lsquo;S11 1OD&rsquo;
</p>
<p>WHERE cutomer_id = &lsquo;OS3457&rsquo;;
</p>
<p>The where clause can contain any of the conditions you have seen so far. It may
have the effect of updating more than one row so it is essential you retrieve and ver-
ify records before you update them. It is also possible to get an integrity constraint
error if you try and update a record with a constraint, for example if you try and
update an attribute defined as a foreign key.
</p>
<p>Removing data from a table is both very simple and very dangerous. Like with
UPDATE you need to be sure that the record you are deleting is the one you want to
delete. Lets say we want to remove Warren Felskys record
</p>
<p>DELETE FROM customer
WHERE customer_name = &lsquo;Warren Felsky&rsquo;;</p>
<p/>
</div>
<div class="page"><p/>
<p>4.19 Review Questions 95
</p>
<p>Two things could go wrong here. If there is more than one warren Felsky, both
will be deleted. It would have been better to use the customer_id. Secondly, if War-
ren Felsky still had any live invoices in the system there would be an integrity con-
straint error. In other words all invoices associated with Felsky would have to be
deleted before the customer record could be removed.
</p>
<p>The database is not permanently changed until a COMMIT command is issued
and up to that point a ROLLBACK command could be issued to return the database
to its original state. Most systems do a COMMIT when a user logs off.
</p>
<p>This is related to ACID which was covered in Chap. 2 where we looked at trans-
action processing.
</p>
<p>4.19 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
</p>
<p>&bull; What is a tuple?
&bull; What is the difference between a primary key, candidate key and foreign key?
&bull; What is CRUDing?
&bull; What is a heavy entity?
&bull; What is a weak entity?
</p>
<p>4.20 GroupWork Research Activity
</p>
<p>This activity requires you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>The Agency currently rents out various types of domestic accommodation (flats
and houses) to clients. The clients (e.g. students or groups of students) may require
1 year leases or (e.g. families) longer term lets.
</p>
<p>Details are kept on each property and in particular:
Address (this must be searchable by postcode or partial post code or by district)
Owner details
Lead tenant details
Tenancy start
Tenancy end
Rent
Type of property (flat, detached house, terrace house etc.)
Furnished/Unfurnished (for Furnished, further details of the furnishings may be
kept)</p>
<p/>
</div>
<div class="page"><p/>
<p>96 4 Relational Databases
</p>
<p>Number of bedrooms
Number of bathrooms
Number of reception rooms
Optionally also:
A textual description
Photographs.
</p>
<p>A history of occupancy needs to be kept including periods were the property is
unoccupied.
</p>
<p>In order to make searching for appropriate properties easier it has been decided
that details of individual rooms within a property will also be stored. This will in-
clude:
</p>
<p>room type (bedroom/bathroom/kitchen etc.)
room dimensions (in feet and meters)
heating (e.g. radiator/fire)
for kitchens: appliances
for bathrooms: fittings
any special features e.g. patio windows.
</p>
<p>The tourist business has started to boom in the area and the Agency wishes to
move into the business of holiday lettings. This will involve much shorter lets and a
much greater requirement for up to date availability information.
1. Using normalisation, create an entity relationship diagram for the scenario.
Was it possible to go beyond third normal form?
2. Repeat the exercise but this time use entity modelling.
Were the results the same?
3. Write the create statements necessary for the scenario.
What constraints are necessary?
4. Create a query that retrieves a list of vacant properties (tenancy end date is
</p>
<p>before today&rsquo;s date) and their owner. Try grouping the properties by owner.
</p>
<p>References
</p>
<p>Chen P (1976) The entity-relationship model&mdash;towards a unified view of data. ACM Trans
Database Syst 1(1):9&ndash;36
</p>
<p>Codd EF (1970) A relational model of data for large shared data banks. Commun ACM 13(6):377&ndash;
387
</p>
<p>Date CJ (2005) Database in depth: relational theory for practitioners. O&rsquo;Reilly, Sebastopol
</p>
<p>Further Reading
</p>
<p>Chen P (2006) Suggested research directions for a new frontier: active conceptual modeling.
In: Conceptual modeling&mdash;ER 2006. Lecture notes in computer science, vol 4215. Springer,
Berlin/Heidelberg, pp 1&ndash;4
</p>
<p>Microsoft (2013) Microsoft SQL server library. Available on line at http://msdn.microsoft.
com/en-us/library/bb545450.aspx. Accessed 22/04/2013
</p>
<p>Oracle&reg; (2010) Database SQL language reference 11g release 1 (11.1). Available on
line at http://docs.oracle.com/cd/B28359_01/server.111/b28286/toc.htm#BEGIN. Accessed
22/04/2013</p>
<p/>
<div class="annotation"><a href="http://msdn.microsoft.com/en-us/library/bb545450.aspx">http://msdn.microsoft.com/en-us/library/bb545450.aspx</a></div>
<div class="annotation"><a href="http://msdn.microsoft.com/en-us/library/bb545450.aspx">http://msdn.microsoft.com/en-us/library/bb545450.aspx</a></div>
<div class="annotation"><a href="http://docs.oracle.com/cd/B28359_01/server.111/b28286/toc.htm#BEGIN">http://docs.oracle.com/cd/B28359_01/server.111/b28286/toc.htm#BEGIN</a></div>
</div>
<div class="page"><p/>
<p>5NoSQL Databases
</p>
<p>What the reader will learn:
</p>
<p>&bull; that Web 2.0 and Cloud Computing brought new challenges to the database
arena
</p>
<p>&bull; that a number of data-centric solutions that are not relational have come to the
fore
</p>
<p>&bull; how these new data storage mechanisms work by exploring three in detail
&bull; that each new data storage type addresses a particular business need
&bull; some of the strengths and weaknesses of these new approaches, together with
</p>
<p>an appreciation of why Relational may be with us for many years to come
</p>
<p>5.1 Databases and theWeb
</p>
<p>From the very earliest days of web development programmers have been using
databases to provide permanency, and a single source of truth, for their web-based
systems. In the beginning this would typically mean connecting to a Relational
database (RDBMS) back-end. This is particularly true for the many online trad-
ing systems that were developed, as the transactional nature of their raison d&rsquo;eÌtre
demanded the robustness provided by leading RDBMS to ensure reliable financial
dealings.
</p>
<p>To a degree, of course, the database itself will not care whether its clients are
connecting using a two- three- or n-tier architecture. And the functionality they
provided also quickly found a place in the far more interactive type of application
that came about as a result of Web 2.0, and Cloud computing.
</p>
<p>For many designers the choice usually was seen as one of Open Source (such as
MySQL) or vendor supplied (such as Oracle). The debate often revolved around cost
of purchase, cost of ownership, and trustworthiness. What it seldom revolved around
was whether the database should be relational or not. Universities running com-
puting courses would typically have a module called &ldquo;Database Systems&rdquo;, which
examined almost exclusively, the relational model. Naturally, therefore, as the stu-
dents left and began to develop exciting new applications for the commercial world,
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_5,
&copy; Springer-Verlag London 2013
</p>
<p>97</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_5">http://dx.doi.org/10.1007/978-1-4471-5601-7_5</a></div>
</div>
<div class="page"><p/>
<p>98 5 NoSQL Databases
</p>
<p>there was little doubt in their mind that a database problem was to be solved with a
relational solution.
</p>
<p>However, as web driven systems began to expand, particularly when mass-usage
systems such as Facebook and Twitter began to take off, it became clear that the
relational model is not good at everything. It is said in some quarters, for example,
that Relational does not scale well. And the fact that these new mass-usage systems
are global and data is typically spread across many nodes in many countries, is seen
by some as something that relational does not cope with well.
</p>
<p>As well as pointing up some potential weaknesses in the relational approach,
these new applications were often not actually transactional in nature. As we saw
in Chap. 4, the ACID test is at the heart of providing the transactional robustness.
However, it is a big processing overhead for a RDBMS to maintain the robustness
required in an ACID compliant database. And that overhead becomes extremely
difficult to manage if the data is spread over many servers all over the world. This
usually manifests itself in very poor performance.
</p>
<p>In addition to problems with the actual data storage aspect of systems, the re-
trieval of data from many nodes across the globe was also becoming problematic.
Google, for example, have built a world leading brand on their ability to search and
retrieve data quickly, and they recognised the weakness in the relational approach.
</p>
<p>In the end organisations eventually took the decision to write their own database
systems to meet these new demands. Google, famously, designed their own database
to suit their needs, called BigTable. This is a proprietary database and you cannot
buy a copy to use in your own environment, but the story is well documented, es-
pecially by Chang et al. (2006). It is NOT relational. Instead it is a distributed hash
mechanism built on their own file handling system, GFS. Although it is Google&rsquo;s
own product, and not openly available, other such databases do exist. HBase is an
open source database that claims to have a similar data model to that of BigTable,
for example. We review what Hbase and similar products allow us to do in the era
of &ldquo;Big Data&rdquo; in the next chapter.
</p>
<p>5.2 The NoSQLMovement
</p>
<p>We must not loose sight of the tremendous leaps forward in data management
and manipulation techniques that have occurred because of the powerful relational
model. This chapter will go on to review some current alternative approaches, but
readers should recognise that many corporations have invested many $millions in
their RDBMS architecture and will not, especially in a period of comparative re-
cession, rush to spend money on new database technology. Moreover, for many
organisations, the current RDBMS does exactly what is required of it.
</p>
<p>To quote from the Guide to Cloud Computing (Hill et al. 2013):
</p>
<p>[in the 1980s] . . . middle managers who needed information to help them to make busi-
ness decisions would think nothing of having to wait days for the data they needed. The
request would need to be coded, probably into Cobol, then perhaps handed to a punch-card
operator who would punch the programme, and then on to await an allotted, and very valu-
able, processing slot. Output from this request would probably be on a continuous run of</p>
<p/>
</div>
<div class="page"><p/>
<p>5.2 The NoSQL Movement 99
</p>
<p>sprocket-holed paper, along with many other outputs. A further wait might ensue whilst
awaiting the specialist paper decollator or burster.
With the advent of personal computing in the 1980s managers were able to collect and
manipulate data without waiting for the still very powerful mainframe. The Relational
Database, with its English-like SQL language also helped empower decision makers. Crit-
ical information could be delivered in a few seconds and organisations could gain com-
petitive advantage by accessing information swiftly. Moreover, new information might be
uncovered by using Data Mining techniques.
</p>
<p>But Relational databases have been with us for over 30 years. In that period we
have seen, for example, the birth and rise to predominance of the Windows Operat-
ing System at the cost of command-line interfaces. We are seeing now a move away
from traditional client-server applications towards available anywhere, browser or
App driven applications. When you consider the technological change underpin-
ning these advances, it is hardly surprising that database technologies, too, should
be reviewed and improved.
</p>
<p>As we will see in Chaps. 9 to 11, three areas of great import for any database
administrator (DBA) are Scalability, Availability and Performance.
</p>
<p>For many of the RDBMS vendors availability has always been a key selling point.
The traditional measure for assessing how available a database has been is Uptime,
often expressed as a percentage of total elapsed time. An uptime of 50 % would
mean that the database was available for only half of the elapsed time (probably
ensuring the host company went bust if it relied on the database for online trading!).
The dream target for a DBA is what is called &ldquo;Five Nines&rdquo; availability, which is
99.999 % available.
</p>
<p>It is true, however, that we are now in an era when differences between avail-
ability statistics for different vendor databases are slim. As a way of differentiating
between vendor products, therefore, except for applications which are most sensitive
to outages however small&mdash;one would hope that the system looking out for incom-
ing nuclear warheads is offline for as short a time as possible, for example&mdash;this
measure becomes less useful.
</p>
<p>Today&rsquo;s sales battlegrounds are therefore more often based around Scalability
and Performance. And it is precisely these two aspects that tend to be pushed by
NoSQL tool providers as they claim their products outperform and out-scale tra-
ditional RDBMS. Naturally the traditional vendors are fighting back and making
counter claims. Equally naturally the real position is somewhere between the two,
with most answers starting with &ldquo;It depends . . . &rdquo;.
</p>
<p>When examining the context in which database professionals work these days,
we can perhaps get a better understanding of why the new tools might excel; in short,
the task often now before them is to quickly find relevant data from terabytes of
unstructured web content which may be stored across many widely dispersed nodes.
Relational databases, with their large processing overhead in terms of maintaining
the ACID attributes of the data they store, and their reliance on potentially processor
hungry joins, are not the right tool for these needs.
</p>
<p>Relational databases can, and do, store and manipulate very large datasets. In
practice these are often vertically scaled systems; that is, they may have access to
multiple CPUs to distribute the processing burden, but they share RAM and disks.</p>
<p/>
</div>
<div class="page"><p/>
<p>100 5 NoSQL Databases
</p>
<p>NoSQL databases tend to be horizontally scaled; that is the data is stored on one of
many individual stand-alone systems which run relatively simple processes, such as
key look-ups, or read/write against a few records, against their own data. The indi-
vidual system onto which the data itself will be stored will be managed according
to a key. This is known as &ldquo;sharding&rdquo;.
</p>
<p>Cattell (2010) provides a nice clear definition of horizontal scaling:
</p>
<p>The term &ldquo;horizontal scalability&rdquo; means the ability to distribute both the data and the load of
[. . . ] simple operations over many servers, with no RAM or disk shared among the servers.
</p>
<p>5.2.1 What Is Meant by NoSQL?
</p>
<p>Unfortunately the answer to this also begins with &ldquo;It depends. . . &rdquo;. This is a new
term and is therefore going through a common phase in the process of new word
definition wherein different people treat the meaning differently.
</p>
<p>One frequent usage is merely &ldquo;any database which doesn&rsquo;t use SQL&rdquo;. The prob-
lem with that is that it seems to start from the premise that SQL is the only existing
database query tool. Object Query Language (OQL), for example has been with us
for many years but is not normally thought of as NoSQL.
</p>
<p>To add to the complexity, SQL is a well known, even well loved, query language.
So much so that NoSQL databases are beginning to provide SQL-type support to
help the poor old relational expert find data in the mysterious world of non-related
data stores. Cassandra, for example, after starting with just a command-line, java-
like query tool, now (since version 0.8) provides CQL.
</p>
<p>Another definition is that NoSQL is N.O.SQL and that the N.O. stands for Not
Only. Cassandra after this release might be seen as an example of this.
</p>
<p>In this book, whilst not particularly favouring one or other definition, for clarity
we will use NoSQL to mean:
</p>
<p>A database which does not store data using the precepts of the relational model and which
allows access to the data using query languages which do not follow the ISO 9075 standard
definition of SQL.
</p>
<p>Perhaps the best way to get an understanding of what NoSQL actually means
is to look at examples. Further on in the chapter we will be using a Column-
based database and a Document-based database. These are two types of approach
to storing data that are generally accepted to be &ldquo;NoSQL&rdquo;. A good single source
for most of the approaches available, and examples thereof, can be found at:
http://nosql-database.org/. They claim to be &ldquo;Your Ultimate Guide to the Non-
Relational Universe!&rdquo; and their definition of NoSQL, at least at the time of writing,
is:
</p>
<p>Next Generation Databases mostly addressing some of these points: being non-relational,
distributed, open-source and horizontally scalable.</p>
<p/>
<div class="annotation"><a href="http://nosql-database.org/">http://nosql-database.org/</a></div>
</div>
<div class="page"><p/>
<p>5.3 Differences in Philosophy 101
</p>
<p>5.3 Differences in Philosophy
</p>
<p>Data processing tasks today can involve vast volumes of data which may be dupli-
cated across many nodes in the Web. This can be when RDBMS start to struggle.
</p>
<p>Traditionally a DBA would consider indexing as an approach to speeding up the
location of particular data items. However, the index process is itself burdensome
for the RDBMS, especially if the data is volatile. An index is, after all, just a pointer
to the physical location of some data, so there has to be a secondary read process in-
volved in any retrieval based on indexes. In addition, traditional RDBMS databases
will have data in a number of tables which often need to be joined to respond to a
user query. This too is process intensive for the management system.
</p>
<p>When trying to impose ACID rules on data, ensuring that all data items on all
nodes are identical before the user can access them is a vital, but time consuming
step. The issue is that rows can become locked whilst they wait for network issues
to be resolved. (More information on locking and indexes is to be found in the
Performance chapter of this book (Chap. 11).)
</p>
<p>In recent years, however, some interesting papers have been published which
argue that ACID is not the only legitimate set of rules for a database to abide to.
CAP theorem, for example (Brewer 2012), makes the point that there is always a
balance between competing desires to be found when designing distributed systems
in general, and for the Web in particular. The three competing needs are said to be:
&bull; Consistency
&bull; Availability
&bull; Partition Tolerance (coping with breaks in physical network connections)
</p>
<p>Before we go any further we need to establish just what is meant by these terms,
especially since the &ldquo;Consistent&rdquo; used here does not use the same definition as the
one in the ACID approach.
</p>
<p>The consistent as used in the ACID test means that the data that is stored in
the database has all rules or constraints applied to it. In other words, the data
complies with all the business rules designed into the database.
The consistent as used in the CAP theorem means that all the data accessible
by clients, across all nodes, is the same. As we will see below, this is not always
the case in NoSQL databases. Often these databases do not have schemas and
whatever data validation rules there are have to be implemented at the client
end.
</p>
<p>The other two terms in CAP Theorem are more straight-forward:
Availability refers to the ability of the database to serve data to clients. In gen-
eral, the more redundant nodes a database has, the more available it will be since
anyone trying to gather data from a node which is &ldquo;down&rdquo; can get the data from
other nodes. The downside is that performance can suffer.
Partition Tolerance refers to the ability of the database to find alternate routes
through the network to get at data from various nodes should there be breaks in
communications. As Gilbert and Lynch (2002) suggest tolerance means:
</p>
<p>No set of failures less than total network failure is allowed to cause the system to
respond incorrectly.</p>
<p/>
</div>
<div class="page"><p/>
<p>102 5 NoSQL Databases
</p>
<p>As these rules can seem a little strange to users of stand-alone RDBMS, it is
worth looking at an example scenario and seeing how CAP plays out. Most readers
will be used to online shopping applications. Let us take an example of a simple
system that maintains data about the number of candles a hardware supplier has in
stock. It is a distributed system in that identical data is stored on two geographically
separate networks, Network 1 and Network 2.
</p>
<p>We should recognise that the importance of the repressions of what CAP The-
orem predicts is dependant upon the sort of application we are dealing with. Any
transactional system requires Atomicity&mdash;that is, a candle is either purchased or it
is not, a bank account is either updated or it is not.
</p>
<p>However, as we have seen, this sort of atomicity comes at a cost in terms of re-
source usage and performance. There may be circumstances where the transactional
security needed by banks, or shops, is not needed. Take for example, an online Dis-
cussion Board application. Just to keep the example simple we look at a two node
system.
</p>
<p>Julia goes on to the discussion board and asks a question, Question J. She is
logged on through Network 1 (though of course she isn&rsquo;t aware of that). Question J
gets replicated to Network 2. Users K, L, M write answers to network 1 over the
next few minutes. But only K and L get replicated since the connection between the
two networks breaks at the time that M is writing a response.
</p>
<p>Now hundreds of other users of the Discussion Board log on, some through Net-
work 1 and some through Network 2. Those using the latter will not see Answer M.
</p>
<p>But does it really matter? When you first think about this you might well say
&ldquo;of course it matters!&rdquo;. But just think about the users who logged on to Network 1
the second before M sent their answer. They were seeing exactly what users of
Network 2 are seeing now. And it could well be that Answer K was the most helpful
one, so they are not really any worse off because of it.
</p>
<p>If you are still not convinced, ask yourself another question&mdash;if you want to know
every user will be guaranteed access to exactly the same data, are you willing to pay
for the service? Naturally such a consistent database is achievable, but it may well
mean that you need to purchase an ACID compliant RDBMS, as opposed to using
an open source NoSQL solution. And even if that is your preferred solution, you
will then need to be prepared to see a fall in performance since no-one will be able
to see Answer M until the partition is removed.
</p>
<p>As we see in the next section there are various consistency approaches available,
depending upon the application&rsquo;s requirements. Many of the mechanisms used in a
RDBMS to ensure there is only ever one valid value stored involve preventing users
seeing data by locking rows whilst transactions are under way. For the locked-out
user this seems like the database is performing poorly, or worse, is not functioning
properly. That user might prefer to see stale data, rather than have to wait, potentially
hours, for &ldquo;actual&rdquo; data.
</p>
<p>For the reader interested in proof, Brewer&rsquo;s CAP theorem is proved in the Gilbert
and Lynch (2002) paper; Brewer&rsquo;s Conjecture and the Feasibility of Consistent,
Available, Partition-Tolerant Web Services.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.4 Basically Available, Soft State, Eventually Consistent (BASE) 103
</p>
<p>5.4 Basically Available, Soft State, Eventually Consistent
(BASE)
</p>
<p>BASE is, in effect, the operating premise for most NoSQL databases, in the same
way RDBMS databases apply the ACID rules. Hill et al. (2013) note:
</p>
<p>that we move from a world of certainty in terms of data consistency, to a world where all
we are promised is that all copies of the data will, at some point, be the same.
</p>
<p>However, even this less certain consistency can be managed in different ways.
The eventual consistency can be brought about, for example, by:
Read Repair: When a read operation happens and discovers that some data on a
</p>
<p>node is stale (that is, it has an earlier time-stamp than that on the other nodes),
the outdated data is refreshed.
</p>
<p>Delayed Repair: The application will return the value it finds, even though it may
be stale, but marks it as needing updating. The timing of that update is in the
hands of the controlling, or master database. This is known as weak consistency.
It is a performance oriented approach.
What we have seen is that we now have more control over whether we want an
</p>
<p>absolutely consistent collection of nodes, which a traditional ACID-focused DBMS
would automatically provide us, or varying degrees of consistency, with the upside
to the loss of consistency being in typically better performance.
</p>
<p>Many of the NoSQL databases available today use the BASE design philosophy.
Many also allow you to manage the degree of consistency required by any appli-
cation. We will now move on to look at how this different design philosophy is
actually implemented by reviewing some NoSQL tools. They all have different ap-
proaches to data storage because they aim to meet different needs, but they share
the notion that traditional RDBMS rules and practices are not necessarily always
the most appropriate.
</p>
<p>5.5 Column-Based Approach
</p>
<p>Tables of data stored in rows are such a common part of today&rsquo;s ICT systems that
even non-database people recognise the concept. This common understanding is
probably also helped by the omnipresence of spreadsheets, also made of rows.
</p>
<p>Many readers will already be aware of design and implementation techniques,
including normalisation and indexing, which can assist in optimising storage and
speeding the return of data. Often, this row-oriented tabular storage is taken for
granted. If there are specialist requirements to get data out of a database, for ex-
ample, looking for values from a particular column, the storage approach remains
relational, but tools like views, cubes, and star schema are used to speed the search
and retrieval process and assist the user in navigating a complex series of joins.
</p>
<p>For many Decision Support applications, examining a column, looking for
trends, averages and other statistical analysis is a frequent event. And yet the data
is stored so that rows can be read as efficiently as possible, not columns. We must</p>
<p/>
</div>
<div class="page"><p/>
<p>104 5 NoSQL Databases
</p>
<p>always remember that the seek and read from disk is one of the slowest aspects of
any database system. At the heart of the problem is that the DBMS has to read every
row, discarding unwanted information (or at least moving the read position so that
it skips unwanted data) to get to the data in the desired column.
</p>
<p>There are many relational databases which have optimisation techniques that
make this problem seem trivial. And yet, the question remains; if we are retriev-
ing data from columns more frequently than looking at several attributes from a row
of data, why don&rsquo;t we store the data in columns instead of rows?
</p>
<p>And that is exactly what the column-based approach provides us. The data items
for each attribute are stored one after another. The read process is thus made very
simple and there are only start-toread and end-reading pointers to worry about and
no jumping around to different disk segments to jump over unwanted data (see dia-
gram in Fig. 5.2 showing the same data as in Fig. 5.1).
</p>
<p>In the type of system we are discussing the column oriented approach improves
read efficiency because there is no need for a row identifier, and packing similar
data together allows compression algorithms to reduce the volume of disk storage
required. Whilst this does allow for more rapid analysis of those column values, it
does mean that the reverse problem is true&mdash;if there is a need to create a record (or
row of data), it would have to be recreated by using positional information to look
up the other data items from columns. This would clearly be very inefficient.
</p>
<p>A second slight issue is that best performance, in terms of compression, will
happen when similar values are adjacent to each other. If the data being recorded is
volatile and many inserts and updates occur, this efficiency will eventually be lost.
One insert could result in many rewrites as the data is shuffled to ensure similar
values are adjacent to each other.
</p>
<p>So when would you choose to use a column-based approach? Hill et al. (2013)
suggest:
</p>
<p>Column based databases allow for rapid location and return of data from one particular
attribute. They are potentially very slow with writing however, since data may need to be
shuffled around to allow a new data item to be inserted. As a rough guide then, traditional
transactionally orientated databases will probably fair better in a RDBMS. Column based
will probably thrive in areas where speed of access to non-volatile data is important, for ex-
ample in some Decision Support Applications. You only need to review marketing material
from commercial vendors to see that business analytics is seen as the key market, and speed
of data access the main product differentiator.
</p>
<p>To get a feel for the potential of the column-based approach we now go on to use
Cassandra. This is only one of the Column-based databases available. You should
look at http://nosql-database.org/ to review the others.
</p>
<p>5.6 Examples of Column-Based Using Cassandra
</p>
<p>The Cassandra data model is very different to RDBMS. Some of the concepts seem
alien if you have become very familiar with the relational model. It may help to start
by simply thinking of this as a mechanism for storing a key, and a value. This key-
value pair storage mechanism is extended by allowing the pairs to be nested. There</p>
<p/>
<div class="annotation"><a href="http://nosql-database.org/">http://nosql-database.org/</a></div>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 105
</p>
<p>F
ig
.5
</p>
<p>.1
T
</p>
<p>he
R
</p>
<p>ow
ap
</p>
<p>pr
oa
</p>
<p>ch</p>
<p/>
</div>
<div class="page"><p/>
<p>106 5 NoSQL Databases
</p>
<p>Fig. 5.2 The Column approach
</p>
<p>is a section in this chapter about Key-Value stores, so you might want to review that
if this is a new concept to you.
</p>
<p>5.6.1 Cassandra&rsquo;s Basic Building Blocks
</p>
<p>A keyspace in Cassandra is roughly equivalent to a database schema in a Relational
DBMS and it is a group of related column families (see below). Remembering that
we are dealing with a database that allows for horizontal scaling, there is an attribute
called placement_strategy which allows the user to define how to distribute replicas
around the nodes they will be using.
</p>
<p>A Column Family is roughly equivalent to a table in a RDBMS. Each such Fam-
ily is stored in a separate physical file which is sorted into key order. To help reduce
disk reads, and therefore improving performance, columns which are regularly ac-
cessed together should be kept together, within the same column family.
</p>
<p>A Column is the smallest unit of storage in Cassandra. A standard column is
composed of a unique name (key),value and a timestamp. The key identifies a row
in a Column Family.
</p>
<p>The timestamp is used to ensure distributed conflict resolution. It is usually de-
fined as the difference between the current time and 00:00:00 UTC on 1 January
1970, also known as Unix epoch. The level of granularity is usually in milliseconds,
or microseconds. The timestamp is provided by the client. This can be a problem
in cases where the data is volatile and the accuracy of the client timestamp is un-
certain. An external time server service can be a solution to this, but it does have
performance implications.
</p>
<p>A SuperColumn is a list of columns. Although we have to remember that Cas-
sandra is schema-less, you can think of this as a pot into which to put a variety of
columns, rather like a view in a RDBMS. It is a mechanism for containing multi-
ple columns with common look-up values. Again, thinking in relational terms, you
might have a transaction with several attributes (Price, date, salesperson, for ex-
ample). You could have a SuperColumn called TransationID which contains those
attributes, each stored as columns.
</p>
<p>A SuperColumn does not have a timestamp.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 107
</p>
<p>Fig. 5.3 Listing files in the home directory
</p>
<p>5.6.1.1 Getting Hands-on with Cassandra
</p>
<p>The tutorial material in this chapter demonstrates the use of Cassandra on Ubuntu
Linux. Cassandra will also run on other Linux servers, and can be installed on Win-
dows. These notes were created using Ubuntu Linux 11.10 with release 1.1.6 of
Cassandra.
</p>
<p>We start with the expectation that the reader has basic Unix skills, and has a
version of Linux with Cassandra installed on it.
</p>
<p>5.6.2 Data Sources
</p>
<p>The data in the follow tutorial material is from the UK Civil aviation Authority,
made publicly available as Open Data. Many other data sources are available from:
http://data.gov.uk/.
</p>
<p>5.6.3 Getting Started
</p>
<p>Assuming you have followed the default installation of Cassandra, as set out in the
appendix, you should open a terminal session and, from you home location, list what
is on your file system (Fig. 5.3).
</p>
<p>You may see different files and folders, but the one we need to concentrate on
here is the apachecassandra-1.1.6 folder, which contains the Cassandra software.
</p>
<p>Next, issue the CD command to move to the apache-cassandra-1.1.6\bin folder
and see the executable files installed (Fig. 5.4).
</p>
<p>In this tutorial we will be using the cassandra executable, which provides the
database functionality and needs to be started, and kept running whilst we work
with Cassandra. We will then interact with this programme through the client pro-
gramme; cassandra-cli. And then we will begin to use the query language; cqlsh.
</p>
<p>As we will be regularly using these programmes it may be an idea to create
scripts to launch them for us, and place them on the home folder.
</p>
<p>Start the text editor and insert the following lines. Once inserted, save to your
home folder as something like startCass.</p>
<p/>
<div class="annotation"><a href="http://data.gov.uk/">http://data.gov.uk/</a></div>
</div>
<div class="page"><p/>
<p>108 5 NoSQL Databases
</p>
<p>Fig. 5.4 Listing Cassandra related files
</p>
<p>#!/bin/bash
cd apache-cassandra-1.1.6
cd bin
./cassandra
</p>
<p>We will also want to start the client often. Using the text editor, create startCass-
Cli with the following content:
</p>
<p>#!/bin/bash
cd apache-cassandra-1.1.6
cd bin
./cassandra-cli
</p>
<p>Both of these files need to be turned into executable files. From the $ prompt you
will need to type:
</p>
<p>$chmod 777 startCass
$chmod 777 startCassCli</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 109
</p>
<p>Fig. 5.5 Cassandra running and waiting for input
</p>
<p>The first thing we need to do to use Cassandra is run the server. We start it, and
then leave it running, probably minimising so the terminal session is out of the way.
And then we start the client application.
</p>
<p>From the $ prompt in your home folder, run the script we just created above:
</p>
<p>$./startCass
</p>
<p>Once the server is started, open a second terminal session. In that one, start the
client session:
</p>
<p>$./startCassCli
</p>
<p>Your desktop should look something similar to the one in Fig. 5.5.
Minimise the terminal window with the server process running. It looks like it
</p>
<p>has hung as there is no input prompt on it, but do not worry, it is working as it
should!
</p>
<p>The input prompt in client terminal session is telling us that we have no user
(default) or keyspace defined. We can use the default user for now, but we do need
to create a keyspace for us to work in (Fig. 5.6).</p>
<p/>
</div>
<div class="page"><p/>
<p>110 5 NoSQL Databases
</p>
<p>Fig. 5.6 Creating a keyspace
</p>
<p>Note that when creating new objects Cassandra needs to ensure that all instances
across the cluster of servers now have this definition in place. As we are currently
working in standalone mode, this may seem unhelpful, but we must remember that
Cassandra was always designed to work in a distributed environment. After the
schema is signalled to be agreed by the cluster we can use the keyspace called First
to carry out our work in.
</p>
<p>5.6.4 Creating the Column Family
</p>
<p>Now we have an area to work in we can begin building structures to store our data
in&mdash;in much the same way that you can start creating tables in SQL once you have
access to a schema. Indeed, a Column Family is roughly equivalent to a table in a
RDBMS and a Column is the smallest unit of storage in Cassandra. A standard col-
umn is composed of a unique name (key),value and a timestamp. The key identifies
a row in a Column Family.
</p>
<p>The first Column Family we are going to work with is a simple list of UK airlines
and their domestic flights. The downloaded.csv is bigger than this, but we have
edited it down so that it only contains columns we will be using:
</p>
<p>Airline Name, Km Flown (x1000), Number of Flights, Number of Hours flown, and the
Number of Passengers handled.
</p>
<p>The data is shown in Fig. 5.7.
There aren&rsquo;t many rows but we are just cutting our teeth at the minute!
We will need a Column Family called DomesticFlights with columns to take this
</p>
<p>data. We will also need a KEY to uniquely identify each row. Later we will load
data straight from a.csv file, but here we are just getting used to the client tool.
</p>
<p>With the follow examples you can either type in directly to the $prompt, or cut
and paste from this tutorial onto the $prompt.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 111
</p>
<p>Fig. 5.7 Domestic Flights data
</p>
<p>Create Column Family DomesticFlights
WITH comparator = UTF8Type AND
key_validation_class = UTF8Type AND
column_metadata =
[
</p>
<p>{column_name: airline, validation_class: UTF8Type, index_type: KEYS},
{column_name: Kms, validation_class: IntegerType},
{column_name: Flights, validation_class: IntegerType},
{column_name: Hrs, validation_class: FloatType},
{column_name: Pass, validation_class: IntegerType}
</p>
<p>];
</p>
<p>Things to note from the above Column Family creation are that:
</p>
<p>The comparator is required by Cassandra because it is needs to know the order
</p>
<p>in which to store columns.
</p>
<p>Key_validation_class tells Cassandra what datatype the key is going to be. We
</p>
<p>will be using Airline so we have chosen UTF8, which is a string of characters
</p>
<p>and may have been described as a varchar in a RDBMS.
</p>
<p>The validation class is similar to the definition of a datatype in a relational
</p>
<p>database schema and provides, as its name suggests, the opportunity to vali-
</p>
<p>date data on input, only allowing data of the appropriate datatype to be writ-
</p>
<p>ten.</p>
<p/>
</div>
<div class="page"><p/>
<p>112 5 NoSQL Databases
</p>
<p>5.6.5 Inserting Data
</p>
<p>To insert data we can use the SET command. Here are two examples:
</p>
<p>set DomesticFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Kms&rsquo;] = 193; set
DomesticFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Flights&rsquo;] = 1388; set
DomesticFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Hrs&rsquo;] = 887; set
DomesticFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Pass&rsquo;] = 26585;
</p>
<p>set DomesticFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Kms&rsquo;] = 300;
set DomesticFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Flights&rsquo;] = 545;
set DomesticFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Hrs&rsquo;] = 686;
set DomesticFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Pass&rsquo;] = 30031;
</p>
<p>After you have carried this out your terminal should look something like Fig. 5.8.
</p>
<p>5.6.6 Retrieving Data
</p>
<p>To retrieve information we can use LIST or GET depending upon if we know the
Airline we want to return data for. Try these two queries:
</p>
<p>LIST DomesticFlights;
</p>
<p>GET DomesticFlights[&lsquo;BA CityFlyer&rsquo;];
</p>
<p>Outputs are shown in Fig. 5.9.
However, unlike data in a RDBMS column, where you would just use the
</p>
<p>WHERE clause on any column, you can&rsquo;t retrieve data by searching for specific
data value in Cassandra unless the data is indexed.
</p>
<p>Try this:
</p>
<p>GET DomesticFlights where Kms = 193;
</p>
<p>You will get an error message back telling you that there are No indexed columns
present. We can make this query work by adding a secondary index. This also
demonstrates the use of the UPDATE command to change Column Family meta-
data.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 113
</p>
<p>Fig. 5.8 Inserting data to Cassandra
</p>
<p>UPDATE COLUMN FAMILY DomesticFlights
WITH comparator = UTF8Type AND
key_validation_class = UTF8Type AND
column_metadata =
</p>
<p>[
{column_name: airline, validation_class: UTF8Type, index_type: KEYS},
{column_name: Kms, validation_class: IntegerType, index_type: KEYS},
{column_name: Flights, validation_class: IntegerType},</p>
<p/>
</div>
<div class="page"><p/>
<p>114 5 NoSQL Databases
</p>
<p>Fig. 5.9 Sample LIST output
</p>
<p>{column_name: Hrs, validation_class: FloatType},
{column_name: Pass, validation_class: IntegerType}
</p>
<p>];
</p>
<p>Now try the same GET command and you should get an answer back this time.
</p>
<p>5.6.7 Deleting Data and Removing Structures
</p>
<p>To remove a particular column from one row we use the DEL command. So, to
remove the Hrs column from the BA CityFlyer row we type:
</p>
<p>del DomesticFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Hrs&rsquo;];
</p>
<p>To remove the entire row we type:
</p>
<p>del DomesticFlights[&lsquo;BA CityFlyer&rsquo;];</p>
<p/>
</div>
<div class="page"><p/>
<p>5.6 Examples of Column-Based Using Cassandra 115
</p>
<p>If we want to remove the entire Column Family, or Keyspace, we use the Drop
command:
</p>
<p>drop column family DomesticFlights;
</p>
<p>Try each of the above, followed by LIST to see the effects.
</p>
<p>5.6.8 Command Line Script
</p>
<p>Just as Oracle allows you to runs SQL and PL/SQL scripts from the operating sys-
tem, so does Cassandra. This allows database administrators to automate, replicate
and schedule particular tasks.
</p>
<p>Here we will create a script that will drop our DomesticFlights Family and create
another one called DomFlights, inserting some rows, and then querying the data.
</p>
<p>Save to the Cassandra directory the following, using your editor, naming it
egscript.txt:
</p>
<p>Use First; Drop Column Family DomesticFlights;
Create Column Family DomFlights
WITH comparator = UTF8Type AND
key_validation_class = UTF8Type AND
column_metadata =
</p>
<p>[
{column_name: airline, validation_class: UTF8Type, index_type: KEYS},
{column_name: Kms, validation_class: IntegerType},
{column_name: Flights, validation_class: IntegerType},
{column_name: Hrs, validation_class: FloatType},
{column_name: Pass, validation_class: IntegerType}
</p>
<p>];
set DomFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Kms&rsquo;] = 193;
set DomFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Flights&rsquo;] = 1388;
set DomFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Hrs&rsquo;] = 887;
set DomFlights[&lsquo;Aurigny Air Services&rsquo;][&lsquo;Pass&rsquo;] = 26585;
</p>
<p>set DomFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Kms&rsquo;] = 300;
set DomFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Flights&rsquo;] = 545;
set DomFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Hrs&rsquo;] = 686;
set DomFlights[&lsquo;BA CityFlyer&rsquo;][&lsquo;Pass&rsquo;] = 30031;
LIST DomFlights;</p>
<p/>
</div>
<div class="page"><p/>
<p>116 5 NoSQL Databases
</p>
<p>Once you have saved that file, from the Linux $prompt type this command to run
it:
</p>
<p>./bin/cassandra-cli -host localhost -port 9160 -f egscript.txt
</p>
<p>You should now have the same data that we had before, but in a different Column
Family.
</p>
<p>5.6.9 Shutdown
</p>
<p>At the end of our busy day we may need to bring down the Cassandra Client. To do
this we simply use the quit; or exit; command from the client terminal session.
</p>
<p>If we want to shutdown the Cassandra server we use the CTRL + C keys.
</p>
<p>BUT do not do this unless you have finished your session since restarting can
</p>
<p>need a system reboot.
</p>
<p>5.7 CQL
</p>
<p>In earlier versions of Cassandra the client interface we have just been using was the
only way to interact with the database without writing code, for example Java, to
call the APIs. Many users wanting to experiment with the new NoSQL databases
found this a problem. Most database professionals know SQL well and an SQL-like
language was needed to remove the hurdle of forcing potential users to learn a new
language.
</p>
<p>In Cassandra 0.8 we saw the birth of Cassandra Query Language (CQL). It was
deliberately modelled on standard SQL, but naturally the commands are mapped
back to the column orientated storage model.
</p>
<p>An example of this is that you can actually use the SQL command CREATE
TABLE test. . . , despite the fact that there is no such structure as a table in Cassan-
dra. The above code generates a Column Family, and the two names can be used
interchangeably.
</p>
<p>Before we use the CQL environment interactively, as promised earlier, let us use
the enhanced script facility available using CQL. In the following example we are
going to create a Keyspace, Column Family and then insert data from a CSV file
called domDataOnly.csv. It contains the data displayed in the spreadsheet at the
start of these notes.
</p>
<p>Create a script file using your editor and save it as cqlcommands in the Cassandra
directory. It should have the following content, which you should make sure you
understand before copying it:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 CQL 117
</p>
<p>CREATE KEYSPACE Flights WITH strategy_class = SimpleStrategy
AND strategy_options:replication_factor = 1;
</p>
<p>use Flights;
</p>
<p>create ColumnFamily FlightDetails
(airline varchar PRIMARY KEY,
Kms int,
Noflights int,
Hrs float,
Pass int);
</p>
<p>copy FlightDetails (airline, Kms, Noflights, Hrs, Pass) from &lsquo;domDataOnly.csv&rsquo;;
</p>
<p>select &lowast; from FlightDetails;
</p>
<p>Note that the syntax is very SQL-like, even down to the commands ending in a
semicolon. However, it also maintains its connection with the traditional Cassandr-
aCli interface commands. Some details are slightly different. The datatypes for the
columns, for example are different. They are also case sensitive in CQL. Int will
not be allowed, for example, but int would.
</p>
<p>Another thing to point out is that, unlike the CassandraCli environment, you do
have to define the Replication Strategy. Cassandra was built from the start as a mul-
tiple node database. Copying data to different nodes, known as replication, helps to
improve availability and fault tolerance. More detail is available here: http://www.
datastax.com/docs/1.1/cluster_architecture/replication#replication-strategy.
</p>
<p>The advice in the online guide is that the replication factor (the number of times
each row is replicated) should be less than or equal to the number of nodes being
used to hold the replicated data. As this worked example expects the user to be on a
standalone, single node version, we have set the strategy to SimpleStrategy and the
replication factor to 1&mdash;in other words, only one copy will exist on the single node.
</p>
<p>The line that saves us manually inserting the data is the copy command. We
are telling CQL which Column Family is receiving the data, and then into which
Columns the data should go. These columns need to be in the same order that they
appear in the CSV file.
</p>
<p>To run the script file from the Cassandra directory type this at the $prompt:
</p>
<p>./bin/cqlsh &lt; &lsquo;cqlcommands&rsquo;
</p>
<p>You don&rsquo;t get reassurance messages about the creates, but the select shows that
the data exists. Your terminal should look like in Fig. 5.10.</p>
<p/>
<div class="annotation"><a href="http://www.datastax.com/docs/1.1/cluster_architecture/replication#replication-strategy">http://www.datastax.com/docs/1.1/cluster_architecture/replication#replication-strategy</a></div>
<div class="annotation"><a href="http://www.datastax.com/docs/1.1/cluster_architecture/replication#replication-strategy">http://www.datastax.com/docs/1.1/cluster_architecture/replication#replication-strategy</a></div>
</div>
<div class="page"><p/>
<p>118 5 NoSQL Databases
</p>
<p>Fig. 5.10 Sample CQL output
</p>
<p>5.7.1 Interactive CQL
</p>
<p>Just as with the older CassandraCli, you could launch CQL by creating a script like
this:
</p>
<p>#!/bin/bash
cd apache-cassandra-1.1.6
cd bin
./cqlsh
</p>
<p>(Don&rsquo;t forget to do a Chmod 777.)
Once you have issued the USE command you can begin querying the data we
</p>
<p>have just input. In this example we see a SELECT using the WHERE clause, and
the use of COUNT(&lowast;). These will all be straight forward to anyone who has some
SQL experience (Fig. 5.11).
</p>
<p>We still need to apply the same rules that we discovered using CassandraCli when
retrieving data&mdash;there needs to be an index on a column to search on that column.
The example above worked because we defined airline as the KEY and there is
therefore an index created for it. Let&rsquo;s try to access data by the Kms column. You
will note that the addition of a secondary index allows this to happen (Fig. 5.12).</p>
<p/>
</div>
<div class="page"><p/>
<p>5.7 CQL 119
</p>
<p>Fig. 5.11 Interactive CQL output
</p>
<p>Fig. 5.12 Using indexes in CQL
</p>
<p>By accessing the material available on the web, you could experiment further
with this data. Here is a good starting place: http://www.datastax.com/docs/1.0/
index and CQL specific material is here: http://www.datastax.com/docs/1.0/
references/cql/index.
</p>
<p>5.7.2 IF YouWant to Check HowWell You Now Know Cassandra . . .
</p>
<p>Using this datachimps dataset and Cassandra, answer the question:
How many airports are there in Great Britain north of Heathrow?
</p>
<p>You can do this with what we have just learned. You need to know that the coun-
try code is GB and that Heathrow&rsquo;s code is LHR.
</p>
<p>You should also be aware that CQL does not allow subqueries in the way that
SQL does so this process will have to be a two stage one.
</p>
<p>If you get the answer without looking for help, award yourself a pat on the back!
If you need a pointer or two, have a look at the end of the chapter!</p>
<p/>
<div class="annotation"><a href="http://www.datastax.com/docs/1.0/index">http://www.datastax.com/docs/1.0/index</a></div>
<div class="annotation"><a href="http://www.datastax.com/docs/1.0/index">http://www.datastax.com/docs/1.0/index</a></div>
<div class="annotation"><a href="http://www.datastax.com/docs/1.0/references/cql/index">http://www.datastax.com/docs/1.0/references/cql/index</a></div>
<div class="annotation"><a href="http://www.datastax.com/docs/1.0/references/cql/index">http://www.datastax.com/docs/1.0/references/cql/index</a></div>
</div>
<div class="page"><p/>
<p>120 5 NoSQL Databases
</p>
<p>Fig. 5.13 Using the &ldquo;time&rdquo; command
</p>
<p>5.7.3 Timings
</p>
<p>Now we have enough information to be able to begin comparing Cassandra to other
databases. You could, for example normalise the AirportLocations data and load it
into two related tables (Airport and Country) using MySQL. Using the Linux TIME
command you could do some comparisons of load time, and retrieval time.
</p>
<p>Figure 5.13 shows an example of TIME being used with a CQL command called
timetest which contains this code:
</p>
<p>use Flights;
select count(&lowast;) from Airports where CountryCode = &lsquo;GB&rsquo; and Lat &gt; 51;
</p>
<p>You could now try to answer the same question in MySQL, or any other environ-
ment you choose.
</p>
<p>5.8 Document-Based Approach
</p>
<p>Many of us are taught that well structured, normalised data is the only form of good
database design when we first encounter large scale database systems. Indeed this is
true for many systems. However, even in RDBMS design there is sometimes a case
for denormalising the data for performance.
</p>
<p>The document approach takes this even further. It is schemaless, meaning there
can be no &ldquo;correct&rdquo; design. The application developer using MongoDB, for exam-
ple, is therefore responsible for data quality issues, rather than relying on the cen-
tralised constraints typical of RDBMS.
</p>
<p>Why remove these seemingly sacrosanct rules? Well, performance and flexibility
are probably two of the main reasons. A large part of a RDBMS&rsquo;s processing time
is spent ensuring that the data entered is correct and so not having to check will, it is
suggested, speed up write operations. On top of that, adding fields to MongoDB is a</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Document-Based Approach 121
</p>
<p>relatively trivial task&mdash;something which often isn&rsquo;t the case in an RDBMS system.
When you have ill-structured data to store this flexibility can be a great advantage.
</p>
<p>As Hill et al. (2013) put it:
</p>
<p>Many of the document-centric databases don&rsquo;t allow data to be locked in the way that is
required for atomic transactions in RDBMS systems. Since locking is a significant per-
formance overhead this enables them to claim performance advantages over traditional
databases. The downside, of course, is that some applications absolutely require secure
transactional locking mechanisms. Document-oriented is probably not the right vehicle for
highly structured data in a transaction dependant environment, as occurs in many OLTP
systems.
The performance advantages that accrue from document-centric implementations mean that
they are often used when there are large volumes of semi-structured data, such as webpage
content, comment storage, or event logging. They may well also support sharding (spread-
ing a table&rsquo;s rows) of data across multiple nodes, again as a performance device.
</p>
<p>There is a growing number of Document Databases. Besides MongoDB, for ex-
ample, there is CouchDB, which, like MongoDB, is open source. Both databases
provide APIs for many programming languages, although they have their own in-
built client environments as well.
</p>
<p>They are both written with distributed data handling at their core. They both sup-
port the idea of Sharding, where data is spread across a number of nodes in what is
also sometimes called &ldquo;horizontal partitioning&rdquo; to allow for greater scalability. The
examples we use here are stand-alone, but when you review the architecture, bear in
mind the end product is often to be expected to run in a multi-node environment.
</p>
<p>Sharding is a Shared Nothing approach to distributed databases. Each node has
its own instance of the database running. When well implemented this allows high
levels of parallel processing when searching for data. However, there would be a risk
that the whole database (the sum of the shards) would become invalid if one node
failed. For that reason these databases will replicate shards to provide redundancy.
This in turn needs high powered replication processes to always ensure all shards
are always available.
</p>
<p>Document databases can be complex. A document can contain arrays and sub-
documents. Using JSON-like notation, here are two valid documents which could
be used in these databases. Note how the two &ldquo;documents&rdquo; do not have the same
structure and yet can be stored in the same collection.
</p>
<p>{
</p>
<p>name: { First: &ldquo;Penelope&rdquo;, Surname: &ldquo;Pitstop&rdquo; },
</p>
<p>Birthday: new Date(&ldquo;Jun 23, 1912&rdquo;),
</p>
<p>RacesWon: [ &ldquo;Alaska&rdquo;, &ldquo;Charlottesville&rdquo;]
</p>
<p>}
</p>
<p>{
</p>
<p>name: { First: &ldquo;Peter&rdquo;, Surname: &ldquo;Pefect&rdquo; },
</p>
<p>Birthday: new Date(&ldquo;May 23, 1940&rdquo;),
</p>
<p>Hometown: &ldquo;Miami&rdquo;
</p>
<p>Favourite: &ldquo;Penelope&rdquo;
</p>
<p>}</p>
<p/>
</div>
<div class="page"><p/>
<p>122 5 NoSQL Databases
</p>
<p>These two pots of information do not look similar enough to become rows in a
standard RDBMS. They don&rsquo;t share all fields for a start. And yet a document-based
approach allows that sort of flexibility. The name field in both cases is a container
for other fields. This is called document embedding. Embedded documents can be
complex. RacesWon, however, with its square brackets, is an array.
</p>
<p>5.8.1 Examples of Document-Based UsingMongoDB
</p>
<p>MongoDB chooses to store its data following the JavaScript Object Notation
(JSON) rules. JSON is a data-interchange format. You can read about it at the JSON
site: http://www.json.org/ For speed this human-readable format is turned into Bi-
nary format and stored as BSON.
</p>
<p>Architecturally RDBMS users coming to Mongo might find it helpful to think of
these comparisons:
&bull; A Mongo Database is a collection of related data, just as in an RDBMS
&bull; A Mongo Collection is a container for documents. It can be thought of as
</p>
<p>RDBMS table-like
&bull; A Mongo Document is rather like a RDBS Row
&bull; A Mongo Field is rather like a RDBS Column
&bull; A Mongo embedded document is rather like a RDBMS Join
&bull; A Mongo Primary key is the same as a RDBMS Primary Key
&bull; A Mongo Secondary index is the same as a RDBMS Secondary Index
</p>
<p>5.8.1.1 Getting Hands-on with MongoDB
The tutorial material in this chapter demonstrates the use of MongoDB on Ubuntu
Linux. Mongo will also run on other Linux servers, and can be installed on Win-
dows. These notes were created using Ubuntu Linux 11.10 with release 2.2.2 of
MongoDB.
</p>
<p>We start with the expectation that the reader has basic Unix skills, and has a
version of Linux with MongoDB installed on it.
</p>
<p>5.8.2 Data Sources
</p>
<p>The data in the follow tutorial material is from the UK Civil aviation Authority,
made publicly available as Open Data. Many other data sources are available too,
from: http://data.gov.uk/.
</p>
<p>5.8.3 Getting Started
</p>
<p>Assuming you have followed the default installation of MongoDB, as set out on
the MongoDB site: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-
ubuntu/ you should open a terminal session and, from your home location, create
a script file that we will use to launch MongoDB. It will contain a single line:</p>
<p/>
<div class="annotation"><a href="http://www.json.org/">http://www.json.org/</a></div>
<div class="annotation"><a href="http://data.gov.uk/">http://data.gov.uk/</a></div>
<div class="annotation"><a href="http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/">http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/</a></div>
<div class="annotation"><a href="http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/">http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/</a></div>
</div>
<div class="page"><p/>
<p>5.8 Document-Based Approach 123
</p>
<p>Fig. 5.14 Starting and
connecting to Mongo
</p>
<p>sudo service mongodb start
</p>
<p>Save this as startMongo. Then when you call it, it will ask for your root pass-
word. After entering the password the database server will be running, waiting for
connections from a client. Here we will be using the Mongo client that comes with
the installation. In the screenshot (Fig. 5.14) you see us starting the server and then
starting a client session by issuing the Mongo command.
</p>
<p>You will be connected to the default database called &ldquo;test&rdquo;. As we will see be-
low, the Mongo client is a JavaScript environment and gives the user access to all
standard JavaScript functionality.
</p>
<p>The service will run until you stop the service:
</p>
<p>sudo service mongodb stop
</p>
<p>5.8.4 Navigation
</p>
<p>If you need to know what database you are using you can issue the db command.
In the screenshot (Fig. 5.15) we move to a different db, creating it as we do, by
issuing the use command&mdash;use either changes your working database to an existing
database, or creates one of the name you provide. We use a call to dropDatabase to
remove a database.
</p>
<p>If you need further help there is a sizeable manual available online: http://docs.
mongodb.org/manual/.
</p>
<p>5.8.5 Creating a Collection
</p>
<p>The first Collection we are going to work with is a simple list of UK airlines and
their domestic flights. The downloaded.csv is bigger than this, but we have edited it
down so that it only contains columns we will be using:</p>
<p/>
<div class="annotation"><a href="http://docs.mongodb.org/manual/">http://docs.mongodb.org/manual/</a></div>
<div class="annotation"><a href="http://docs.mongodb.org/manual/">http://docs.mongodb.org/manual/</a></div>
</div>
<div class="page"><p/>
<p>124 5 NoSQL Databases
</p>
<p>Fig. 5.15 Using and
dropping a database
</p>
<p>Fig. 5.16 Domestic flights data
</p>
<p>Airline Name, Km Flown (x1000), Number of Flights, Number of Hours flown, and the
Number of Passengers handled.
</p>
<p>The data is shown in Fig. 5.16.
This is the same data we used in the Cassandra tutorial if you have done that
</p>
<p>already. It is not that appropriate for a real document-based application but will get
us started in using the tool, and we can look at data collection types later.
</p>
<p>MongoDB is a document oriented storage system. Each document includes one
or more key-value pairs. Each document is part of a collection and in a database but
more than one collection can exist. A collection can be seen as similar to a table in
a RDBMS.
</p>
<p>Unlike relational databases, MongoDB is schemaless and we do not need to de-
fine the datatypes for our incoming values. This can come as a bit of a shock to
people with an RDBMS background!
</p>
<p>We are going to store our documents in a collection called Flights. As with the db
name, we do not have to create the collection before we call it. As we see below, if
we issue the command to insert and the collection does not yet exist, Mongo creates
it. It can do this because there is no schema to worry about.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Document-Based Approach 125
</p>
<p>Fig. 5.17 Adding data to Mongo
</p>
<p>5.8.6 Simple Inserting and Reading of Data
</p>
<p>As we are working in a JavaScript environment we can create variables in memory
which we pass to Mongo as parameters when we want to create documents.
</p>
<p>In the screenshot (Fig. 5.17) you will see we have:
1. Created a database called Airlines
2. Created a Javascript variable called Airline12 to store information about Lo-
</p>
<p>ganair.
3. Checked the feedback screen to make sure the variable is correct
4. Used the insert() method of db to add the document to the collection
5. Checked that the data is saved by calling the find() method with no parameters
</p>
<p>to list all the collection contents
Note the lack of positive feedback from the system after the insert. And the fact
</p>
<p>that MongoDB has created its own ObjectID to uniquely identify the document.
We can now use find() to look for specific values in the collection. Try this, for
</p>
<p>example to return the same document which has a Km value of 504:
</p>
<p>db.Flights.find( {&ldquo;Km&rdquo;: 504})
</p>
<p>Naturally we ought to have a few more rows to make finds more interesting, so
cut and paste, or type a few, of the following:</p>
<p/>
</div>
<div class="page"><p/>
<p>126 5 NoSQL Databases
</p>
<p>Airline1 = { &ldquo;Name&rdquo;: &ldquo;AURIGNY AIR SERVICES&rdquo;, &ldquo;Km&rdquo;: 193, &ldquo;NoFlights&rdquo;:
1388, &ldquo;Hrs&rdquo;: 887, &ldquo;NoPass&rdquo;: 26585 }
db.Flights.insert( Airline1 )
</p>
<p>Airline2 = { &ldquo;Name&rdquo;: &ldquo;BA CITYFLYER LTD&rdquo;, &ldquo;Km&rdquo;: 300, &ldquo;NoFlights&rdquo;:
545, &ldquo;Hrs&rdquo;: 686.1, &ldquo;NoPass&rdquo;: 30031 }
db.Flights.insert( Airline2 )
</p>
<p>Airline3 = { &ldquo;Name&rdquo;: &ldquo;BLUE ISLANDS LIMITED&rdquo;, &ldquo;Km&rdquo;: 168, &ldquo;NoFlights&rdquo;:
991, &ldquo;Hrs&rdquo;: 520.8, &ldquo;NoPass&rdquo;: 15308 }
db.Flights.insert( Airline3 )
</p>
<p>Airline4 = { &ldquo;Name&rdquo;: &ldquo;BMI GROUP&rdquo;, &ldquo;Km&rdquo;: 1067, &ldquo;NoFlights&rdquo;: 2435,
&ldquo;Hrs&rdquo;: 2922.7, &ldquo;NoPass&rdquo;: 142804 }
db.Flights.insert( Airline4 )
</p>
<p>Airline5 = { &ldquo;Name&rdquo;: &ldquo;BRITISH AIRWAYS PLC&rdquo;, &ldquo;Km&rdquo;: 1510, &ldquo;NoFlights&rdquo;:
3327, &ldquo;Hrs&rdquo;: 4116.6, &ldquo;NoPass&rdquo;: 307849 }
db.Flights.insert( Airline5 )
</p>
<p>Airline6 = { &ldquo;Name&rdquo;: &ldquo;BRITISH INTERNATIONAL HEL&rdquo;, &ldquo;Km&rdquo;: 10,
&ldquo;NoFlights&rdquo;: 162, &ldquo;Hrs&rdquo;: 57.9, &ldquo;NoPass&rdquo;: 2169 }
db.Flights.insert( Airline6 )
</p>
<p>Airline7 = { &ldquo;Name&rdquo;: &ldquo;EASTERN AIRWAYS&rdquo;, &ldquo;Km&rdquo;: 496, &ldquo;NoFlights&rdquo;:
1406, &ldquo;Hrs&rdquo;: 1353, &ldquo;NoPass&rdquo;: 23074 }
db.Flights.insert( Airline7 )
</p>
<p>Airline8 = { &ldquo;Name&rdquo;: &ldquo;EASYJET AIRLINE COMPANY L&rdquo;, &ldquo;Km&rdquo;: 1826,
&ldquo;NoFlights&rdquo;: 3922, &ldquo;Hrs&rdquo;: 4297.2, &ldquo;NoPass&rdquo;: 399308 }
db.Flights.insert( Airline8 )
</p>
<p>Airline9 = { &ldquo;Name&rdquo;: &ldquo;FLYBE LTD&rdquo;, &ldquo;Km&rdquo;: 2505, &ldquo;NoFlights&rdquo;: 6755,
&ldquo;Hrs&rdquo;: 5635.4, &ldquo;NoPass&rdquo;: 297435 }
db.Flights.insert( Airline9 )
</p>
<p>Airline10 = { &ldquo;Name&rdquo;: &ldquo;ISLES OF SCILLY SKYBUS&rdquo;, &ldquo;Km&rdquo;: 12, &ldquo;NoFlights&rdquo;:
176, &ldquo;Hrs&rdquo;: 55.3, &ldquo;NoPass&rdquo;: 1200 }
db.Flights.insert( Airline10 )
</p>
<p>Airline11 = { &ldquo;Name&rdquo;: &ldquo;JET2.COM LTD&rdquo;, &ldquo;Km&rdquo;: 22, &ldquo;NoFlights&rdquo;: 71,
&ldquo;Hrs&rdquo;: 65, &ldquo;NumPass&rdquo;: 4059 }
db.Flights.insert( Airline11 )
</p>
<p>Note: all these inserts work, but look closely at the Airline11 row. Instead of
</p>
<p>NoPass, we have NumPass as the field name. This means, for example, that this
</p>
<p>would return nothing:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Document-Based Approach 127
</p>
<p>db.Flights.find({NoPass:4059})
</p>
<p>This is one of the issues with working without a schema. The overhead of having
to check that this was a valid field name would probably mean a relational database
would be slower to insert, but the trade off is that you have to be responsible for the
quality of the data.
</p>
<p>We can remove the incorrect document:
</p>
<p>db.Flights.remove({NumPass:4059})
</p>
<p>Now let us try to insert something else that may not, at first, feel right to a
RDBMS person:
</p>
<p>Try creating this variable and then inserting it:
</p>
<p>another = { &ldquo;Name&rdquo;: &ldquo;Metal Bird&rdquo;, &ldquo;Km&rdquo;: 112, &ldquo;NoFlights&rdquo;: 72, &ldquo;Hrs&rdquo;: 165,
&ldquo;Wings&rdquo;: 2, &ldquo;Animals&rdquo;: &ldquo;Elephants/Hippos&rdquo; }
</p>
<p>As we might expect after the previous example, the fact that the 5th Field is
called &ldquo;Wings&rdquo; rather than &ldquo;NoPass&rdquo; does not worry Mongo. But then we add an
extra field called &ldquo;Animals&rdquo; which does not appear in any other document. Again,
this just emphasises the schemaless nature of MongoDB.
</p>
<p>5.8.7 More on Retrieving Data
</p>
<p>To find out what collections there are in a database, you use the following method:
</p>
<p>db.getCollectionNames();
</p>
<p>We can perform some aggregation on the data we now have in the database using
some of the methods provided.
</p>
<p>The first thing we might like to do is count the number of documents we have
stored, which we call the count() method to do:
</p>
<p>db.Flights.count()
</p>
<p>Then we might want to count particular items. For example, how many of the
Airlines in the list fly more than 1000k Kms?</p>
<p/>
</div>
<div class="page"><p/>
<p>128 5 NoSQL Databases
</p>
<p>db.Flights.count({ Km: { $gt: 1000 } })
</p>
<p>So we pass a parameter to the count method which Mongo has to evaluate. Note
the syntax, with the use of &ldquo;{}&rdquo; pairs to separate out the distinct elements of the
expression. $gt means greater than.
</p>
<p>We saw earlier that.find() can be used to find specific values. These searches can
be ANDed, again with the use of &ldquo;{}&rdquo;. Have a look at the example below and see if
you can work out what will be returned. Note the use of &ldquo;[]&rdquo; with the $and to pass
an array of several expressions.
</p>
<p>db.Flights.find({ $and: [ { Km: {$gt: 2000} }, { NoPass: {$gt:140000} } ] } )
</p>
<p>There is also an $or operator that works in a similar way. Will the following
generate more or less rows?
</p>
<p>db.Flights.find({ $or: [ { Km: {$gt: 2000} }, { NoPass: {$gt:140000} } ] } )
</p>
<p>Looking for values from a list is also possible, using the $in operator. In this
example we look for two values of Km.
</p>
<p>db.Flights.find( { Km: { $in: [ 300, 496 ] } } )
</p>
<p>If you want your output sorted you can use the $sort operator. It needs a sort type
to be passed of either &minus;1 (descending) or 1 (ascending).
</p>
<p>db.Flights.find().sort({Km: -1})
</p>
<p>A little confusingly you would need to use this sort to discover the Min and Max
values in a field since Mongo uses Min and Max in a very different way elsewhere.
Here we answer the question Which Airline flew the most Kms?
</p>
<p>db.Flights.find().sort({Km: -1}).limit(1)
</p>
<p>So we sort descending but limit printed output to one row, answering the question
in doing so.</p>
<p/>
</div>
<div class="page"><p/>
<p>5.8 Document-Based Approach 129
</p>
<p>Remembering that we can have a variety of field names, we will occasionally
need to check if a field actually exists. Using the final insert from earlier for &ldquo;Metal
Bird&rdquo;, we could ask for all documents which contain a field called &ldquo;Animals&rdquo;:
</p>
<p>db.Flights.find( { Animals: { $exists: true } } )
</p>
<p>5.8.8 Indexing
</p>
<p>MongoDB allows us to add indexes to fields. As with any database we need to be
aware that whilst indexes can speed the retrieval of data, they are a heavy processing
overhead during Inserts and Updates, so we should use them with caution.
</p>
<p>Indexes in Mongo are stored in B-Trees (see Chap. 11, Performance, for a dis-
cussion of index types) and can either be placed on one field, or on multiple fields
to form a compound index. They are held on collections. The MongoDB manual
suggests that:
</p>
<p>In general, you should create indexes that support your primary, common, and user-facing
queries. Doing so requires MongoDB to scan the fewest number of documents possible.
</p>
<p>Lets assume we decide we will query the collection often using the Km in our
queries. To have secondary index on that field we would issue this command (the 1
indicating Ascending)
</p>
<p>db.Flights.ensureIndex( { Km: 1 } )
</p>
<p>Queries such as our earlier one to count documents where Km is above a level
would automatically use this index. You can also force the optimiser to use the index,
as with the example below where we hint to use the index that has been created on
the Km field. Note how the data output is sorted in Km order when you use the
index.
</p>
<p>db.Flights.find().hint( { Km: 1 } )
</p>
<p>You can review the indexes available with the getIndexes method on the collec-
tion:
</p>
<p>db.Flights.getIndexes()</p>
<p/>
</div>
<div class="page"><p/>
<p>130 5 NoSQL Databases
</p>
<p>5.8.9 Updating Data
</p>
<p>We can either update the values of a particular field, or alter the fields themselves
using the Update() method.
</p>
<p>After trying each of these examples, use the find method to check your entry has
worked.
</p>
<p>Assuming we made a mistake with the entry in the Animals field, we could locate
the document using the unique identifier, or as a result of a query, and then pass the
change as a parameter. Below, we find the first document for which the Km = 11,
and then change the value in the Animals field &ldquo;Elephants and Badgers&rdquo;:
</p>
<p>db.Flights.update( { Km: 11 }, { $set: { Animals: &ldquo;Elephants and
Badgers&rdquo; }})
</p>
<p>If we wanted to change the name of fields we could do this by replacing $set with
$rename, as below;
</p>
<p>db.Flights.update( { Km: 112 }, { $rename: { Animals: &ldquo;Creatures&rdquo; }})
</p>
<p>Finally, we may want to add a field, with a value in it:
</p>
<p>db.Flights.update ( { Km: 112 },{ $set: { Rivers: &ldquo;Don and Ouse&rdquo; }})
</p>
<p>In this case, because there is no field called &ldquo;Rivers&rdquo;, Mongo creates the field and
then adds the value &ldquo;Don and Ouse&rdquo;.
</p>
<p>5.8.10 Moving Bulk Data into Mongo
</p>
<p>CSVs can be input directly using the MongoImport utility This utility can also cope
with JSON data. The utility is called from the $prompt, not within the Mongo client.
</p>
<p>Here is an example of how to import a CSV file called AirportLocations.csv,
creating a Database called Airports and a Collection called AllAirports. Note that
we have told MongoImport that the first row contains field names in the header.
Parameters are identified with a leading double minus sign (Fig. 5.18).
</p>
<p>The CSV file being input is shown in Fig. 5.19.
</p>
<p>5.9 IF YouWant to Check HowWell You Now KnowMongoDB
. . .
</p>
<p>Using this datachimps dataset and Mongodb, answer the question:</p>
<p/>
</div>
<div class="page"><p/>
<p>5.9 IF You Want to Check HowWell You Now KnowMongoDB . . . 131
</p>
<p>Fig. 5.18 Bulk imports
</p>
<p>Fig. 5.19 The dataset used for the bulk import
</p>
<p>How many airports are there in Great Britain north of Heathrow?
</p>
<p>You can do this with what we have just learned. You need to know that the coun-
try code is GB and that Heathrow&rsquo;s code is LHR.
</p>
<p>If you get the answer without looking for help, award yourself a pat on the back!
If you need a pointer or two, have a look at the end of this chapter.
</p>
<p>5.9.1 Timings
</p>
<p>Now we have enough information to be able to begin comparing MongoDB to other
databases. You could, for example normalise the AirportLocations data and load it
into two related tables (Airport and Country) using MySQL. Using the Linux TIME
command you could do some comparisons of load time, and retrieval time.
</p>
<p>Of course MongoDb was designed from the outset as a distributed database and
real performance benefits are more likely to come from large volumes of data spread
across more than one instance in a distributed document database. Mongo refers</p>
<p/>
</div>
<div class="page"><p/>
<p>132 5 NoSQL Databases
</p>
<p>to this as Sharding. For now, however, let us just be happy with some elementary
reconnaissance!
</p>
<p>5.10 Summary
</p>
<p>In this chapter we have seen that there are many different data storage methods
available for a database. The decision as to which is the right one should be driven
by the requirements of the system being supported.
</p>
<p>We have examined column-based and a document-based examples of NoSQL
databases and seen that they support different types of applications from those trans-
actionally based relational systems we might be used to. There can be no doubt that
the advent of Web and Cloud computing generated opportunities and challenges for
data professionals and that NoSQL is a potentially useful set of tools to deal with
this new era.
</p>
<p>5.11 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What do ACID and BASE stand for, and what is the most significant difference
</p>
<p>between them?
&bull; What type of data is best stored using Cassandra?
&bull; What type of data is best stored using MongoDB?
&bull; What is meant by Sharding?
&bull; What do the letters &ldquo;CAP&rdquo; stand for, and describe what is meant by each letter.
</p>
<p>5.12 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 Once you have become familiar with the NoSQL databases in
the tutorials above, you should draw up a SWOT analysis to see what strengths and
weaknesses, threats and opportunities may be derived from adopting the database in
any organisation.
</p>
<p>Discussion Topic 2 Try to think of criteria you might use to compare different
types of database, including RDBMS and NoSQL examples, to help you decide
which might be the most appropriate for a given application. For example you might
think Performance is an important criterion. Having established your criteria, con-
sider how you would measure them. What sort of tests might you need to carry out
in order to compare the different databases?</p>
<p/>
</div>
<div class="page"><p/>
<p>5.12 Group Work Research Activities 133
</p>
<p>5.12.1 Sample Solutions
</p>
<p>Open the spreadsheet you have downloaded from Infochimps. Remember it is Tab
separated. Remove the unwanted columns and then save as a CSV file and name it
AirportLocations.csv. It should now look like this:
</p>
<p>Then, using your editor, create a CQL Script and call it NorthofHeathrow. It
should contain:
</p>
<p>use Flights;
</p>
<p>create ColumnFamily Airports
(KEY varchar PRIMARY KEY,
Lat float,
Lon float,
Fullname varchar,
Country varchar,
CountryCode varchar
</p>
<p>) ;
</p>
<p>Create index on Airports (Lat) ;
Create index on Airports (CountryCode) ;
</p>
<p>copy Airports (KEY, Lat, Lon, Fullname, Country, CountryCode) from
&lsquo;AirportLocations.csv&rsquo; ;
</p>
<p>select Lat from Airports where KEY = &lsquo;LHR&rsquo;;
</p>
<p>select count(&lowast;) from Airports where CountryCode = &lsquo;GB&rsquo; and Lat &gt; 51.5;
</p>
<p>Run the script:
</p>
<p>./bin/cqlsh &lt; &lsquo;NorthofHeathrow&rsquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>134 5 NoSQL Databases
</p>
<p>5.12.2 MongoDB Crib
</p>
<p>Open the spreadsheet you have downloaded from Infochimps. Remember it is Tab
separated. Remove the unwanted columns and then save as a CSV file and name it
AirportLocations.csv. It should now look like this:
</p>
<p>Add a row at the beginning to give the field names;
</p>
<p>Use MongoImport, as described earlier to import the data. Then issue the follow-
ing queries:
</p>
<p>db.AllAirports.find({AirportCode: &ldquo;LHR&rdquo;})
db.AllAirports.find({Lat: {$gt: 51}, CCode: &ldquo;GB&rdquo;} )
db.AllAirports.count({Lat: {$gt: 51.47}, CCode: &ldquo;GB&rdquo;} )
</p>
<p>References
</p>
<p>Brewer E (2012) CAP twelve years later: how the &ldquo;rules&rdquo; have changed. Computer 45(2):23&ndash;29.
doi:10.1109/MC.2012.37
</p>
<p>Cattell R (2010) Scalable SQL and NoSQL data stores. SIGMOD Rec 39(4)
Chang F, Dean J, Ghemawat S, Hsieh WC, Wallach DA, Burrows M, Chandra T, Fikes A, Gruber
</p>
<p>RE (2006) Bigtable: a distributed storage system for structured data. In: Proceedings of the 7th
symposium on operating systems design and implementation (OSDI &rsquo;06). USENIX Associa-
tion, Berkeley, pp 205&ndash;218
</p>
<p>Gilbert S, Lynch N (2002) Brewer&rsquo;s conjecture and the feasibility of consistent, available, partition-
tolerant web services. SIGACT News 33(2)
</p>
<p>Hill R, Hirsch L, Lake P, Moshiri S (2013) Guide to cloud computing: principles and practice.
Springer, London</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1109/MC.2012.37">http://dx.doi.org/10.1109/MC.2012.37</a></div>
</div>
<div class="page"><p/>
<p>6Big Data
</p>
<p>What the reader will learn:
</p>
<p>&bull; that Big Data is not just about data volumes
&bull; that analysing the data involved is the key to the value of Big Data
&bull; how to use tools like Hadoop to explore large data collections and generate
</p>
<p>information from data
&bull; that the structured data traditionally stored in a RDBMS is not the only valuable
</p>
<p>data source
&bull; that a data scientist needs to understand both statistical concepts and the busi-
</p>
<p>ness they are working for
</p>
<p>6.1 What Is Big Data?
</p>
<p>1 Terabyte = 1024 Gigabytes
1 Petabyte = 1024 Terabytes
1 Exabyte = 1024 Petabytes
1 Zettabyte = 1024 Exabytes
</p>
<p>And what does a zettabyte of information look like?
According to a Cisco blog (http://blogs.cisco.com/news/the-dawn-of-the-
</p>
<p>zettabyte-era-infographic/) a zettabyte is equivalent to about 250 billion DVDs, and
that would take one individual a very long time to watch. And the DVD is a good
measure since Cisco go on to predict that by 2015 the majority of global Internet
traffic (61 percent) will be in some form of video.
</p>
<p>So we do mean BIG!
EMC2 suggest that 1.8 Zettabytes is the amount of data estimated to be cre-
</p>
<p>ated in 2011. Their site has a growth ticker on it, allowing you to see the amount
of data created since January 2011 (http://uk.emc.com/leadership/programs/digital-
universe.htm). Of course these are estimates, but it helps us get a feel for the scale
involved. They go on to suggest that the world&rsquo;s information is more than doubling
every two years.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_6,
&copy; Springer-Verlag London 2013
</p>
<p>135</p>
<p/>
<div class="annotation"><a href="http://blogs.cisco.com/news/the-dawn-of-the-zettabyte-era-infographic/">http://blogs.cisco.com/news/the-dawn-of-the-zettabyte-era-infographic/</a></div>
<div class="annotation"><a href="http://blogs.cisco.com/news/the-dawn-of-the-zettabyte-era-infographic/">http://blogs.cisco.com/news/the-dawn-of-the-zettabyte-era-infographic/</a></div>
<div class="annotation"><a href="http://uk.emc.com/leadership/programs/digital-universe.htm">http://uk.emc.com/leadership/programs/digital-universe.htm</a></div>
<div class="annotation"><a href="http://uk.emc.com/leadership/programs/digital-universe.htm">http://uk.emc.com/leadership/programs/digital-universe.htm</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_6">http://dx.doi.org/10.1007/978-1-4471-5601-7_6</a></div>
</div>
<div class="page"><p/>
<p>136 6 Big Data
</p>
<p>At the beginning of IBM&rsquo;s guide to what Big Data is they say (http://www01.ibm.
com/software/data/bigdata):
</p>
<p>Every day, we create 2.5 quintillion bytes of data&mdash;so much that 90 % of the
data in the world today has been created in the last two years alone. This data
comes from everywhere: sensors used to gather climate information, posts to
social media sites, digital pictures and videos, purchase transaction records,
and cell phone GPS signals to name a few. This data is big data.
</p>
<p>But the most obvious trap to fall into is to believe that Big Data, a new term, is
only about large volumes of data.
</p>
<p>Roger Magoulas from O&rsquo;Reilly media is credited with the first usage of the term
&lsquo;Big Data&rsquo; in the way we have come to understand it, in 2005. But as a distinct, well
defined topic, it is younger even than that.
</p>
<p>However Springer&rsquo;s Very Large Databases (VLDB) Journal has been in existence
since 1992. It
</p>
<p>Examines information system architectures, the impact of technological ad-
vancements on information systems, and the development of novel database
applications.
</p>
<p>Whilst early hard disk drives were relatively small, Mainframes had been dealing
with large volumes of data since the 1950s.
</p>
<p>So handling large amounts of data isn&rsquo;t new, although the scale has doubtless
increased in the last few years. Perhaps it isn&rsquo;t really the actual size, but more to do
with whether or not we can meaningfully use and interact with the data? This is what
Forrester seem to have in mind with the definition suggested on Mike Gualtieri&rsquo;s
blog http://blogs.forrester.com/mike_gualtieri/12-12-05the_pragmatic_definition_
of_big_data:
</p>
<p>Big Data is the frontier of a firm&rsquo;s ability to store, process, and access (SPA)
all the data it needs to operate effectively, make decisions, reduce risks, and
serve customers.
</p>
<p>They go on to suggest three questions about big data:
</p>
<p>Store. Can you capture and store the data?
Process. Can you cleanse, enrich, and analyze the data?
Access. Can you retrieve, search, integrate, and visualize the data?</p>
<p/>
<div class="annotation"><a href="http://www01.ibm.com/software/data/bigdata">http://www01.ibm.com/software/data/bigdata</a></div>
<div class="annotation"><a href="http://www01.ibm.com/software/data/bigdata">http://www01.ibm.com/software/data/bigdata</a></div>
<div class="annotation"><a href="http://blogs.forrester.com/mike_gualtieri/12-12-05the_pragmatic_definition_of_big_data">http://blogs.forrester.com/mike_gualtieri/12-12-05the_pragmatic_definition_of_big_data</a></div>
<div class="annotation"><a href="http://blogs.forrester.com/mike_gualtieri/12-12-05the_pragmatic_definition_of_big_data">http://blogs.forrester.com/mike_gualtieri/12-12-05the_pragmatic_definition_of_big_data</a></div>
</div>
<div class="page"><p/>
<p>6.2 The Datacentric View of Big Data 137
</p>
<p>The Process question is also part of our definition of Big Data. It is often a pre-
sumption that Big Data cannot be handled by the standard, RDBMS-based data
systems.
</p>
<p>Many references to Big Data also come with the word Analytics tagged along
somewhere nearby. Analytics may be quite a new term, but again, it has, in real-
ity been with us for decades, sometimes called Business Analysis, sometimes Data
Analysis. We will look at some of the exciting ways Analytics has changed business
decisions later in the chapter, but we are really just talking about applying tools and
techniques to the large volumes of data now available to an organisation and making
some sense of it.
</p>
<p>So there seems to be evidence that Big Data is more than just a new buzzword.
We can see it as a loose label which covers the storage and accessibility of large
volumes of data from multiple sources in a way that allows new information to be
gleaned by applying a variety of analytic tools. Interestingly, whilst the phrase &ldquo;Big
Data&rdquo; appears all over the place on the web, it isn&rsquo;t used much in the jobs market.
Instead, people with the required skills are tending to be called Data Scientists.
</p>
<p>And now may well be the time for career minded Data Scientists to make a good
living. In one McKinsey report (http://www.mckinsey.com/insights/mgi/research/
technology_and_innovation/big_data_the_next_frontier_for_innovation) they sug-
gest:
</p>
<p>There will be a shortage of talent necessary for organizations to take advan-
tage of big data. By 2018, the United States alone could face a shortage of
140,000 to 190,000 people with deep analytical skills as well as 1.5 million
managers and analysts with the know-how to use the analysis of big data to
make effective decisions.
</p>
<p>The rest of this chapter is in three sections. We will look at Big Data from a
datacentric perspective, then from an analytics perspective, and then finally quickly
review some of the tools being used by Data Scientists.
</p>
<p>6.2 The Datacentric View of Big Data
</p>
<p>The cost of HDD storage has dropped dramatically over the last few decades, from
thousands of dollars to fractions of a dollar per Gbyte. The era when data profes-
sionals spent much of their time trying to reduce the amount of data stored in an
organisation is fast coming to an end. And Cloud will accelerate the trend as it pro-
vides an always available, infinitely flexible store of data.
</p>
<p>This super-availability of storage is probably one of the key drivers in the upsurge
in Big Data. It is certainly true that data production itself has also grown exponen-
tially over the past few years, but if the cost/Gbyte were currently the same as it was
in 1990 there can be little doubt that much of this generated data would be discarded
as &ldquo;not worth keeping&rdquo;.</p>
<p/>
<div class="annotation"><a href="http://www.mckinsey.com/insights/mgi/research/technology_and_innovation/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/mgi/research/technology_and_innovation/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/mgi/research/technology_and_innovation/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/mgi/research/technology_and_innovation/big_data_the_next_frontier_for_innovation</a></div>
</div>
<div class="page"><p/>
<p>138 6 Big Data
</p>
<p>As we shall see in the Analytics section, significant advances in analysis tech-
niques have allowed useful information to be retrieved from a seemingly mean-
ingless pile of raw data. This means that keeping data just in case it is useful is
becoming the norm.
</p>
<p>To put that into perspective, and remembering that we are talking about zettabytes
of data, IMC suggest that there is a massive gap between the total data stored and
that being analysed. They suggest that the global picture may be that 23 % of the data
stored would be of some use for analysis, but that currently only 0.5 % is actually
analysed (http://www.emc.com/collateral/analystreports/idc-the-digital-universe-
in-2020.pdf).
</p>
<p>6.2.1 The Four Vs
</p>
<p>There are many new data specialists currently helping us get an understanding of
big data, and they sometimes disagree. It isn&rsquo;t, of course, unusual for experts to
disagree, particularly when a topic is relatively new, but it can make it more difficult
for the newbie to come to their own understanding.
</p>
<p>An example in point is the number of &ldquo;V&rsquo;s&rdquo; that should be considered. Some
experts hold with three, others four. On the grounds only that it is easier to ignore
one, we have decided to go with the four &ldquo;V&rsquo;s&rdquo; (Fig. 6.1). Such acronyms are only
useful as an aide-memoir after all, so it is what they are describing that matters.
1. V is for Volume
</p>
<p>As we have seen, the most obvious characteristic of Big Data is that it is BIG.
It may even be so big that an organisation needs to look for new tools and tech-
niques to store and query it. This volume therefore is likely to be the biggest
challenge for data professionals in the near future. It may result in the need for
scalable distributed storage and querying, and many will turn to Cloud com-
puting to meet those needs.
Organisations will also recognise that they have historic data in archives that
may help provide analysts with longer time-lines of data to look for trends in.
These archives may be tape-based and hard to access and making potentially
valuable data available will also be an important part of the data professional&rsquo;s
job.
</p>
<p>2. V is for Velocity
The devices we carry with us everyday, like iPhones, have the ability to stream
vast quantities of data of different digital types including geolocation data.
A travelling salesman can use an App to report visit information back to head
office instantly. Only a couple of decades ago that return of information might
have been weekly. Not only is there more data being collected, but it is also
being collected instantly.
But it is the time taken by the decision making cycle that really matters. Gath-
ering the information quickly is of no benefit if we only analyse it once a week.
Real-time analytics is discussed later, but is about using very current data to
provide information that will help an organisation improve a service, or re-
spond to demand, in a much swifter way.</p>
<p/>
<div class="annotation"><a href="http://www.emc.com/collateral/analystreports/idc-the-digital-universe-in-2020.pdf">http://www.emc.com/collateral/analystreports/idc-the-digital-universe-in-2020.pdf</a></div>
<div class="annotation"><a href="http://www.emc.com/collateral/analystreports/idc-the-digital-universe-in-2020.pdf">http://www.emc.com/collateral/analystreports/idc-the-digital-universe-in-2020.pdf</a></div>
</div>
<div class="page"><p/>
<p>6.2 The Datacentric View of Big Data 139
</p>
<p>Fig. 6.1 The 4 &ldquo;V&rdquo;s
</p>
<p>3. V is for Variety
For those with a background in using relational databases the idea that data
should be anything other than structured may be difficult to grasp. For most
of the past three decades we would have models of all the data that a business
needed to keep. Whole methodologies grew around storing known specific data
items in the most space efficient way.
But one of the interesting aspects of Big Data is that we can use data from
a wide range of sources. Perhaps the most frequently quoted example is in
analysing the mass of readily available social media data to provide companies
with information about how their products are being perceived by the public,
without the need for focus groups or questionnaires. They collect and then mea-
sure the volume of comments and their sentiment. But the details about their
products will be needed too, and they may well be stored in a more traditional
RDBMS.
Many data sources are now readily available through open data government
portals, or sites like Infochimps. The data itself can be in many different for-
mats, and the ability to cope with CSV, Excel, XML, JSON and many other
formats is one of the skills the Data Scientist now needs.
</p>
<p>4. V is for Veracity
Data is just a series of ons-or-offs, usually on a magnetic medium, often
squirted around the globe in packets of ons-and-offs. Things go wrong! Or-
ganisations need to be able to verify the incoming data for accuracy and prove-
nance.
But as we begin to hear some astonishing helpful outputs from analytics we
also need to be aware that data analysis can get things wrong too. The fact is
that when you search for patterns in large data sets it is possible that the patterns
discovered are entirely caused by chance.
We also need to attempt to extract some meaning from the variety of inputs so
that we can be accurate about their content. When you have no control over the
provenance of data it can become very difficult to ensure its accuracy. Let&rsquo;s say
you have mined some social media and discovered the phrase: &ldquo;Life of Pi is the</p>
<p/>
</div>
<div class="page"><p/>
<p>140 6 Big Data
</p>
<p>best film showing in Washington this weekend.&rdquo; Does this mean Washington
state, Washington DC or Washington in County Durham in the UK?
</p>
<p>6.2.1.1 Non-V
The trouble with helpful terms like &ldquo;The 4 V&rsquo;s&rdquo; is that it prevents other equally help-
ful characteristics from being considered unless you can force them somehow into
a V-word. So, for example, there may be an argument that there should be a &ldquo;U&rdquo; in
there&mdash;Usefulness. This can be measured as Potentially Useful, Immediately Use-
ful, and so on. Some analytics experts would counter that all data is potentially
useful! Data storage costs have plummeted, but they aren&rsquo;t zero yet. There are occa-
sions when an organisation will decide it&rsquo;s just not worth hanging on to some sorts
of data.
</p>
<p>6.2.2 The Cloud Effect
</p>
<p>Cloud computing brought with it flexible approaches to data storage. Before cloud,
if you needed to capture 10 Terabytes of data, then filter it to remove unwanted
data, ending up with one Terabyte of data, you actually needed to buy 10 Terabytes
of storage. Once the filtering was done that would mean you had over-provisioned
by 9 Terabytes. Now, with cloud, you can simply rent ten Terabytes from a ser-
vice provider for a short period, carry out your filtering and then store only the one
Terabyte you need, releasing the unwanted disk space.
</p>
<p>If you add this flexibility to the relatively cheap cost of disk storage we can see
why the propensity to save data in case it might be useful has risen and the drive to
only store what is vital is reducing. If you then add the cloud&rsquo;s worldwide reach and
the ease with which datasets can be gathered and analysed we can begin to see how
Big Data is becoming so widely talked about.
</p>
<p>When organisations begin to use Facebook and Twitter data for data mining pur-
poses, they are, in effect, using the Cloud as part of their data storage strategy.
Just as with more traditional database centred approaches, the data professional
needs to ensure appropriate levels of availability to the data sources for the busi-
ness users.
</p>
<p>Guaranteed broadband speed connections to the internet are not always avail-
able, with availability rates depending upon geography as much as anything else.
The most obvious decision is that if the data you need is critical you should store
it where you have control and replicate it to ensure Disaster Recovery can hap-
pen.
</p>
<p>However, it may be the case that it does not really matter to a business that its
market intelligence gathered from Twitter does not have to be absolutely current.
In that sort of case, bothering with the effort and expense to store the twitter data
locally may not make sense, even in the era of cheap data storage. The decision
might be to gather the data directly from the internet on an as needed basis, and just
live with any gaps caused by lost connections.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Analytics View of Big Data 141
</p>
<p>In addition to externally sourced data, the Cloud now allows organisations to
rethink their backup and DR strategies. As Hill et al. (2013) say:
</p>
<p>&ldquo;Traditional backup and recovery methods have largely been based around
magnetic tape as a medium for storing data. The advantage of this medium
is that it is relative cheap. The disadvantage is that it is slow to access, and
can be quite labour intensive. Systems where the user simply clicks a button
to send their backup information to one of several servers somewhere in the
Cloud are fast pressuring the traditional approach.&rdquo;
</p>
<p>Because of the Pay-as-you-Go approach, Data Storage-as-a-Service has provided
organisations with the opportunity to store their data off-site and by storing multiple
copies of the data on several servers, these sites have built-in redundancy.
</p>
<p>Database-as-a-Service is also becoming an alternative to maintaining your own
database server. Microsoft&rsquo;s Azure is one example of this, but there are many oth-
ers. In terms of Big Data, what this provides is instant scalability and flexibility. As
we said above, should you need to store and analyse 1 Petabyte of data as a one-off
exercise, you merely buy the space and pay for it only whilst you need it. Tradition-
ally you would have had to ensure you had a free petabyte&rsquo;s worth of disk doing
nothing&mdash;not a frequent occurrence in even rich organisations!
</p>
<p>Cloud can be seen to be, in many ways, an enabler for Big Data. Arguably, we
could even say that Big Data would not exist without it.
</p>
<p>6.3 The Analytics View of Big Data
</p>
<p>Something that comes out clearly from the material written about Big Data is that
we are talking about making sense of the data we are storing, not just worrying
about the physical aspects of storage.
</p>
<p>Moreover, we are also examining data in new, innovative ways to discover new
information. Some of this innovation in analysis is to do with the flexibility the new
physical storage models allow. For the most part Business Analysts used to concen-
trate on asking questions of the data within their organisation&rsquo;s control; usually well
structured in format and usually stored in a relational database.
</p>
<p>Data warehousing as a discipline is not new. The core idea of capturing a com-
pany&rsquo;s OLTP data at certain points of time and storing them in a way that makes
querying more efficient has been with us since the late 1980s. The heavy CPU us-
age caused by the analysis of the data by Business Analysts could dramatically slow
down the database server and the core OLTP system would suffer as a result. Indexes
can speed up the return of query results, but they always slow the Insert, Delete or
Update process. The idea, then, was to extract the data, clean it and filter it, and then
store it in a query friendly way, with appropriate schemas and indexes, and store it
away from the &ldquo;live&rdquo; data.</p>
<p/>
</div>
<div class="page"><p/>
<p>142 6 Big Data
</p>
<p>This process is known by the acronym ETL; Extract, Transform and Load. As we
look at the techniques used in Big Data we will see that the process is also a large
part of the modern Analytics discipline and the Data Scientist needs to master it as
part of their standard toolset.
</p>
<p>Data warehousing is a large topic in its own right and outside of the scope of this
chapter, but the technologies and techniques we cover here are already beginning to
impact upon the area, and there will doubtless be a blurring of the edges between
traditional warehousing and Big Data.
</p>
<p>6.3.1 SoWhy Isn&rsquo;t Big Data Just Called DataWarehousing 2?
</p>
<p>Well, in some ways it could be, but there are differences. Probably the most signifi-
cant is to do with the analyst&rsquo;s ability to structure the data. As with most relational
tasks, a warehouse needs a schema to describe it. To create that schema the designer
has to know what data, and data-types, will be stored. They have to have some form
of foreknowledge to be able to plan a warehouse effectively.
</p>
<p>As we saw earlier, there are two &ldquo;V&rdquo; attributes of Big Data that make foreknowl-
edge less likely: Variety and Velocity. Data is now coming into the organisation&rsquo;s
purview from many different sources and presented in a variety of formats, such
as CSV, Excel, XML, JSON. And that data is coming at the organisation quickly.
When a new potentially useful data feed is found there is a need to instantly use it;
not to have to define and model it, and design a schema to manage it.
</p>
<p>The velocity and variety elements are going to be a big challenge. As global
research and advisory firm Forrester employee Mike Gualtieri predicts in his blog
(http://blogs.forrester.com/mike_gualtieri/13-01-02-big_data_predictions_for_
2013):
</p>
<p>Real-time architectures will swing to prominence. Firms that find predictive
models in big data must put them to use. Firms will seek out streaming, event
processing, and in-memory data technologies to provide real-time analytics
and run predictive models. Mobile is a key driver, because hyperconnected
consumers and employees will require architectures that can quickly process
incoming data from all digital channels to make business decisions and de-
liver engaging customer experiences in real time. The result: In 2013, en-
terprise architects will step out of their ivory towers to once again focus on
technology&mdash;real-time technology that is highly available, scalable, and per-
formant.
</p>
<p>But the Volume aspect will also impact upon the analyst. Many data warehouses
would archive out, or even delete, data that was beyond the standard reporting time
periods&mdash;often this year and last. This would be to prevent data storage running
out, and also to ensure good performance when querying the data. With these con-
straints, analysts tended to investigate trends within a relatively short time period&mdash;
how many tins of beans did we sell last month as compared to the previous month</p>
<p/>
<div class="annotation"><a href="http://blogs.forrester.com/mike_gualtieri/13-01-02-big_data_predictions_for_2013">http://blogs.forrester.com/mike_gualtieri/13-01-02-big_data_predictions_for_2013</a></div>
<div class="annotation"><a href="http://blogs.forrester.com/mike_gualtieri/13-01-02-big_data_predictions_for_2013">http://blogs.forrester.com/mike_gualtieri/13-01-02-big_data_predictions_for_2013</a></div>
</div>
<div class="page"><p/>
<p>6.3 The Analytics View of Big Data 143
</p>
<p>or this time last year&mdash;but we are now able to store much more data for longer term
analysis.
</p>
<p>The ability to store and analyse huge volumes of historic data is likely to make
new insights available to the savvy analyst. As always the hope when looking for
new information in a commercial environment is that you can provide some, al-
beit temporary, competitive advantage. In less commercially focused organisations,
such as those in the healthcare area, the hope is to be able to find trends that will
help in the identification of causal relationships between lifestyle factors and dis-
ease.
</p>
<p>As Samuel Arbesman put it in his wired article (http://www.wired.com/opinion/
2013/01/forget-bigdata-think-long-data/):
</p>
<p>Datasets of long timescales not only help us understand how the world is
changing, but how we, as humans, are changing it&mdash;without this awareness,
we fall victim to shifting baseline syndrome. This is the tendency to shift our
&ldquo;baseline,&rdquo; or what is considered &ldquo;normal&rdquo;&mdash;blinding us to shifts that occur
across generations (since the generation we are born into is taken to be the
norm).
</p>
<p>Probably the most high profile area using long time-scale data at the moment is
that of Climate Change, with historic data being used by proponents and doubters
alike. And the recent trend in making public data more openly available is also a
driver here. The UK Meteorological Office, for example has a selection of weather
details back to 1961 available for use by anyone (see Fig. 6.2).
</p>
<p>Data is also available from organisations like NOAA (National Oceanic and At-
mospheric Administration), the US federal agency whose mission is to understand
and predict changes in climate, weather, oceans, and coasts. They make public, for
example, data from ice cores and tree rings which provide palaeoclimatology ex-
perts with the ability to deduce how the climate has changed through the earth&rsquo;s
history.
</p>
<p>So, lots of interesting, detailed new sources of data have recently become avail-
able and analysable. Exciting though this may seem for data scientists, it does raise
another problem: just how do you select data which might produce useful results
after analysis? For data scientists employed by a commercial organisation this ques-
tion is likely to be framed as &ldquo;can we contribute to the bottom line as a result of this
research?&rdquo; That constraint will tend to restrict the datasets investigated by any but
the least risk-averse analyst as they try to make sure they keep their jobs. Nonethe-
less, the data scientists&rsquo; job most certainly includes exploring for new datasets. It
is just that it also must include a filtering process that ensures that the outcome is
suitably focused.
</p>
<p>This issue is well summarised by Vincent Granville on a blog (http://www.
analyticbridge.com/profiles/blogs/the-curse-of-big-data):</p>
<p/>
<div class="annotation"><a href="http://www.wired.com/opinion/2013/01/forget-bigdata-think-long-data/">http://www.wired.com/opinion/2013/01/forget-bigdata-think-long-data/</a></div>
<div class="annotation"><a href="http://www.wired.com/opinion/2013/01/forget-bigdata-think-long-data/">http://www.wired.com/opinion/2013/01/forget-bigdata-think-long-data/</a></div>
<div class="annotation"><a href="http://www.analyticbridge.com/profiles/blogs/the-curse-of-big-data">http://www.analyticbridge.com/profiles/blogs/the-curse-of-big-data</a></div>
<div class="annotation"><a href="http://www.analyticbridge.com/profiles/blogs/the-curse-of-big-data">http://www.analyticbridge.com/profiles/blogs/the-curse-of-big-data</a></div>
</div>
<div class="page"><p/>
<p>144 6 Big Data
</p>
<p>Fig. 6.2 UK Met Office data
</p>
<p>In short, the curse of big data is the fact that when you search for patterns in
very, very large data sets with billions or trillions of data points and thousands
of metrics, you are bound to identify coincidences that have no predictive
power&mdash;even worse, the strongest patterns might be:
&bull; entirely caused by chance (just like someone who wins at the lottery wins
</p>
<p>purely by chance) and
&bull; not replicable,
&bull; having no predictive power but obscuring weaker patterns that are ignored
</p>
<p>yet have a strong predictive power.
The question is: how do you discriminate between a real and an accidental
</p>
<p>signal in vast amounts of data?
</p>
<p>Two elements of his own answer to the question posed are:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.3 The Analytics View of Big Data 145
</p>
<p>Being a statistician helps, but you don&rsquo;t need to have advanced knowledge of
stats. Being a computer scientist also helps to scale your algorithms and make
them simple and efficient.
</p>
<p>Which leads us nicely on to the next section:
</p>
<p>6.3.2 What Is a Data Scientist?
</p>
<p>There are beginning to be many jobs advertised for organisations looking for Data
Scientists. The evidence is that this is a booming field and that the shortage of Data
Scientists means that those that exist can earn very good salaries. Interestingly there
are very few jobs advertised as looking for Big Data expertise, although the Data
Scientist is clearly the most obvious user of Big Data techniques in most organisa-
tions.
</p>
<p>The IBM Big Data website says that:
</p>
<p>Data scientists are inquisitive: exploring, asking questions, doing &ldquo;what if&rdquo;
analysis, questioning existing assumptions and processes. Armed with data
and analytical results, a top-tier data scientist will then communicate informed
conclusions and recommendations across an organization&rsquo;s leadership struc-
ture.
</p>
<p>Harvard Business Review published an article in October 2012 (http://hbr.org/
2012/10/datascientist-the-sexiest-job-of-the-21st-century/) with the headline:
</p>
<p>Data Scientist: The Sexiest Job of the 21st Century
</p>
<p>However, as a word of warning before we all go running off to become a data
scientists, they do go on to say that:
</p>
<p>There is also little consensus on where the role fits in an organization, how
data scientists can add the most value, and how their performance should be
measured.
</p>
<p>So this is a new field and if you are early to the market you might well be
able to earn significant rewards. In the UK, in one week (commencing 15th April
2013) there were newly listed 114 Data Science jobs on the single recruitment site
www.jobsite.co.uk.
</p>
<p>Many jobs for data scientists and data analysts are listed on the internet with
salaries in the midrange professional band of around &pound;40000 to &pound;80000 ($60000&ndash;
120000). The salary is usually dependent upon the expertise and experience of the</p>
<p/>
<div class="annotation"><a href="http://hbr.org/2012/10/datascientist-the-sexiest-job-of-the-21st-century/">http://hbr.org/2012/10/datascientist-the-sexiest-job-of-the-21st-century/</a></div>
<div class="annotation"><a href="http://hbr.org/2012/10/datascientist-the-sexiest-job-of-the-21st-century/">http://hbr.org/2012/10/datascientist-the-sexiest-job-of-the-21st-century/</a></div>
<div class="annotation"><a href="http://www.jobsite.co.uk">http://www.jobsite.co.uk</a></div>
</div>
<div class="page"><p/>
<p>146 6 Big Data
</p>
<p>individual, and the importance perceived by the employer of engaging high calibre
employees. The story is the same in the USA. The CIA recently advertised for a
data scientist with a salary range of $51,418&ndash;$136,771, for example.
</p>
<p>Quoted in the Guardian&rsquo;s DataBlog (http://www.guardian.co.uk/news/datablog/
2012/mar/02/datascientist) Monica Rogati, chief scientist at LinkedIn defines a data
scientist in this way:
</p>
<p>In my opinion, they are half hacker, half analyst, they use data to build prod-
ucts and find insights. It&rsquo;s Columbus meets Columbo&mdash;starry eyed explorers
and skeptical detectives.
</p>
<p>This is an example of what is often quoted as the ideal mix&mdash;that of statistical and
programming, or at least hacking, skills. However, the other part of the equation for
a good data scientist is that they should understand the business they are working in.
A hacking statistician with experience of working in the coal industry, for example,
is more likely to be able to ask appropriate questions for a coal mining corporation,
that than a hacking statistician who has experience in the clothes retailing industry.
They will know what to look for amongst the patterns they find in the data.
</p>
<p>Global consulting company McKinsey recently issued a Big Data report that pre-
dicted that:
</p>
<p>There will be a shortage of talent necessary for organizations to take advan-
tage of big data. By 2018, the United States alone could face a shortage of
140,000 to 190,000 people with deep analytical skills as well as 1.5 million
managers and analysts with the know-how to use the analysis of big data to
make effective decisions.
</p>
<p>http://www.mckinsey.com/insights/business_technology/big_data_the_next_
frontier_for_innovation.
</p>
<p>Similarly, in Great Britain, e-Skills UK published a report (e-Skills UK 2013)
called An assessment of demand for labour and skills, 2012&ndash;2017 which identified:
</p>
<p>. . . that there were approximately 3,790 advertised positions for big data staff
in the UK in the third quarter of 2012, 75 % of which were for permanent
posts.
</p>
<p>The report goes on to say:</p>
<p/>
<div class="annotation"><a href="http://www.guardian.co.uk/news/datablog/2012/mar/02/datascientist">http://www.guardian.co.uk/news/datablog/2012/mar/02/datascientist</a></div>
<div class="annotation"><a href="http://www.guardian.co.uk/news/datablog/2012/mar/02/datascientist">http://www.guardian.co.uk/news/datablog/2012/mar/02/datascientist</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
<div class="annotation"><a href="http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation">http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_innovation</a></div>
</div>
<div class="page"><p/>
<p>6.4 Big Data Tools 147
</p>
<p>Despite the currently unfavourable economic climate, demand for big data
staff has risen exponentially (912 %) over the past five years from less than
400 vacancies in the third quarter of 2007 to almost 4,000 in the third quarter
of 2012.
</p>
<p>6.3.3 What Is Data Analysis for Big Data?
</p>
<p>Probably the most important place to start is with questions, not data. Being data
driven can occasionally help find unexpected information, but given the need to
prioritise a data scientist&rsquo;s tasks, as mentioned above, some sort of identification of
what an organisation needs to know is essential. This is another reason why some
experience in the industry concerned can be a big advantage.
</p>
<p>Different industry sectors will tend to want to ask different types of questions.
Food retailers, for example, are often looking for buying patterns, both at the indi-
vidual, and at the market level. Put very simply, if you can use data to evidence the
fact that every summer month we sell twice as much ice cream as we do in other
months, we know that we should stock more to meet the expected demand.
</p>
<p>Many retail store managers would tell you that they make this sort of decision
intuitively anyway. And with years of experience in the job they may well be able to
&ldquo;sense&rdquo; trends rather than precisely discover them. However, they can not hope to be
able to identify trends for all products, especially when the trends are less obvious
than &ldquo;hot weather equals more ice cream&rdquo;.
</p>
<p>Production orientated organisations, such as in the coal mining industry, may
want to ask similar demand focused questions to help them regulate their supply
to the market. However, they may also want to ask questions about the production
itself. Historically, for example, can they say that the kilogrammes of coal dug per
man-shift is lower on any particular day of the week? Some senior mining engineers
might use their gut-feel and say that Friday is always the least productive day. But
does the data support that conjecture?
</p>
<p>In general, then, big data analytics is about exploring large volumes of data look-
ing for trends, anomalies, previously unknown correlations and obscure patterns
which can be used to an organisation&rsquo;s advantage by evidencing some sort of pre-
diction upon which better business decisions are taken.
</p>
<p>6.4 Big Data Tools
</p>
<p>If you were to doubt that Big Data is really a part of the corporate lexicon, just have
a look at some of the vendors who use the phrase prominently:
&bull; Oracle
&bull; SAS
&bull; SAP</p>
<p/>
</div>
<div class="page"><p/>
<p>148 6 Big Data
</p>
<p>&bull; IBM
&bull; EMC
</p>
<p>These are all leaders in the field of delivering enterprise scale solutions. All have
significant amounts of Web collateral which explain, propose, and sell Big Data
products. It is not likely that they have all called this wrong. And they are by no
means the only players in the field.
</p>
<p>The same e-Skills UK (2013) report referred to above identified the tools and
skills that employers are looking for:
</p>
<p>The technical skills most commonly required for big data positions as
a whole were: NoSQL, Oracle, Java and SQL, whilst the technical pro-
cess/methodological requirements most often cited by recruiters were in rela-
tion to: Agile Software Development, Test Driven Development (TDD), Ex-
tract, Transform and Load (ETL) and Cascading Style Sheets (CSS).
</p>
<p>Some of these tools, whilst highly relevant to Big Data, are not Big Data specific
and have been around for a while, but it is worth exploring a couple of the newer
tools. As with all new areas there are different understandings of what terms mean.
Hadoop, for example, is most certainly a frequently asked for skill, but does not
appear in the above list as it is presumably classified as NoSQL. In this book the
preceding chapter was all about NoSQL databases, but we have decided that Hadoop
is so Analytics focused that we would review it here, rather than in Chap. 5.
</p>
<p>6.4.1 MapReduce
</p>
<p>MapReduce has become a core technique for data scientists. Recognising that
analysing high volumes of potentially unstructured data that may be spread across
many data nodes is not something that the relational model would be good at han-
dling, people began to turn to a two stage process. At its simplest this entails:
&bull; looking through all the data and extracting a key identifier and a value, and then
</p>
<p>storing that in a list.
&bull; Then using grouping, reducing the data thus listed such that you can get infor-
</p>
<p>mation like group totals, averages, minimums and the like.
This process can be shown graphically by referring to Fig. 6.3.
Although the concept is not new, it is as a result of the Google innovators that we
</p>
<p>are currently seeing such heavy reliance on the technique to cope with large volumes
of data. Dean and Ghemawat (2008) tell us that in order to handle the complex and
large data analysis tasks they were daily encountering at Google:</p>
<p/>
</div>
<div class="page"><p/>
<p>6.4 Big Data Tools 149
</p>
<p>Fig. 6.3 The MapReduce
process
</p>
<p>. . . we designed a new abstraction that allows us to express the simple compu-
tations we were trying to perform but hides the messy details of paralleliza-
tion, fault-tolerance, data distribution and load balancing in a library. Our
abstraction is inspired by the map and reduce primitives present in Lisp and
many other functional languages.
</p>
<p>NoSQL tools, such as MongoDB have this open source method built into their
databases. For examples of using Map Reduce to analyse data, see the MongoDB
example in the tutorial section later in this chapter.
</p>
<p>6.4.2 Hadoop
</p>
<p>Just as MapReduce has become a big feature in the Big Data arena, one of the
most frequently used implementations of MapReduce is Hadoop. This is part of
the Apache open source suite, and so is freely downloadable from http://hadoop.
apache.org/. The website describes it as a:
</p>
<p>. . . a framework that allows for the distributed processing of large data sets
across clusters of computers using simple programming models.
</p>
<p>The two key elements of Hadoop are the distributed file management system
(HDFS) and Hadoop MapReduce. The MapReduce examples we provide later are
similar except that we use MongoDB to manage the raw data, as opposed to HDFS.</p>
<p/>
<div class="annotation"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></div>
<div class="annotation"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></div>
</div>
<div class="page"><p/>
<p>150 6 Big Data
</p>
<p>HDFS is distributed and expects to run over several, or many, nodes, although
it is possible to run it on a single PC and simulate multi-node operation. It uses
replication of data across nodes for both availability and performance reasons. Data
nodes are managed by a master node called the NameNode which keeps track of
all the files and rebalance data by moving copies around. Client applications, such
as MapReduce, talk to the NameNode to locate data. This makes the NameNode a
single point of failure and is the potential weakness in terms of High Availability
(see Chap. 10 for information on HA). MapReduce is a separate layer which uses
HDFS to source its data requests.
</p>
<p>6.4.2.1 If You ShouldWant to Explore Hadoop. . .
The examples later in this chapter, using MongoDB, show that MapReduce can be
a powerful tool. It is also at the core of Hadoop. At the time of writing there are sev-
eral example Hadoop installations available for downloading as Virtual Machines,
meaning you do not have to go through the pain of installation. That said, if you
have access to an Ubuntu (or other Linux) machine, then undertaking the installa-
tion journey for yourself will help you get a good understanding of the underpinning
architecture.
</p>
<p>Amazon&rsquo;s Elastic MapReduce A paid for service, but for the relatively low levels
of processing we are talking about for these tutorials, this is not expensive and is a
relatively hassle free way of getting started. As their website says:
</p>
<p>Amazon Elastic MapReduce (Amazon EMR) is a web service that enables
businesses, researchers, data analysts, and developers to easily and cost-
effectively process vast amounts of data. It utilizes a hosted Hadoop frame-
work running on the web-scale infrastructure of Amazon Elastic Compute
Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3).
</p>
<p>http://aws.amazon.com/elasticmapreduce/.
</p>
<p>Cloudera Cloudera sample downloadable VM image (https://ccp.cloudera.
com/display/SUPPORT/Cloudera&rsquo;s+Hadoop+Demo+VM+for+CDH4), also have a
downloadable VirtualBox Image.
</p>
<p>Hortonworks Their sandboxed version allows you to download for VM or Virtu-
alBox. For these tutorials the author used the VirtualBox version 4.2.10 running on
a Windows 7 host.
</p>
<p>6.4.3 Hive, Pig and Other Tools
</p>
<p>With computing tools there is often a trade-off between powerfulness and ease of
use. This is certainly true of Hadoop which is a very powerful and flexible tool, but
for which even its biggest fans do not claim ease of use as one of its attributes. In</p>
<p/>
<div class="annotation"><a href="http://aws.amazon.com/elasticmapreduce/">http://aws.amazon.com/elasticmapreduce/</a></div>
<div class="annotation"><a href="https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM+for+CDH4">https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM+for+CDH4</a></div>
<div class="annotation"><a href="https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM+for+CDH4">https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM+for+CDH4</a></div>
</div>
<div class="page"><p/>
<p>6.5 Getting Hands-on with MapReduce 151
</p>
<p>order to get the best from Hadoop the user should ideally have Java skills and be
fully conversant with the Linux operating environment and distributed systems.
</p>
<p>As Hadoop is open source there has been a sudden upsurge in Hadoop add-on
software to attempt to make the data scientist&rsquo;s job easier. Some, like JasperSoft
(http://www.jaspersoft.com/) provide an entire Business Intelligence stack that can
plumb into Hadoop (and other data sources). Other vendors, like Oracle, provide
their own tools but allow the use of &ldquo;connectors&rdquo; to use Hadoop when needed. Two
popular open source Hadoop running mates are Hive and Pig.
</p>
<p>Hive is, in effect, a data warehousing environment that uses Hadoop behind the
scenes. It aims to make data summaries and ad hoc queries against big data sets
simpler than it would be using raw Hadoop. One of the ways it does this is with a
SQL-like query language called QL. http://hive.apache.org/.
</p>
<p>Pig uses a programming language called Pig Latin which, when compiled, pro-
duces sequences of Map-Reduce programs using Hadoop. It provides a variety of
functions and shell and utility commands. Originally developed by Yahoo to provide
an ad-hoc way of creating and executing map-reduce jobs, it is now open source and
freely usable. http://pig.apache.org/.
</p>
<p>6.5 Getting Hands-on withMapReduce
</p>
<p>The tutorial material in this chapter demonstrates the use of MapReduce with Mon-
goDB on Linux, and assumes you have carried out the data inserts in the MongoDB
tutorial in Chap. 5.
</p>
<p>MapReduce as a means of analysing large amounts of data really came to the
fore as Google used it to analyse their BigTable data. In the open source world many
organisations have turned to Hadoop to perform the same sort of functionality.
</p>
<p>Before we move on to those examples, however, you do need to be aware that
if you are looking to gain employment as a Data Scientist you will probably have
to bite the bullet and become proficient with Hadoop, as it has become the de facto
standard in the Big Data area. Even big and powerful vendors like Oracle, SAS and
SAP have found no better solution than Hadoop and build it in to their own products.
Some examples of how you might get to grips with Hadoop are given at the end of
the chapter.
</p>
<p>Hadoop would doubtless take a few chapters in a specialist Data Science text-
book, and so we feel that its complications, and the need for reasonable Java pro-
gramming skills, make it unsuitable for this text. Hadoop installation is complex
and there are many potential pitfalls in the process, as the author learned to his cost!
As we are more interested in using MapReduce itself, and we already have some
MongoDB experience (Chap. 5) the easiest solution is for us to use the MapReduce
functionality built in to MongoDB to demonstrate its use.</p>
<p/>
<div class="annotation"><a href="http://www.jaspersoft.com/">http://www.jaspersoft.com/</a></div>
<div class="annotation"><a href="http://hive.apache.org/">http://hive.apache.org/</a></div>
<div class="annotation"><a href="http://pig.apache.org/">http://pig.apache.org/</a></div>
</div>
<div class="page"><p/>
<p>152 6 Big Data
</p>
<p>6.6 UsingMongoDB&rsquo;s db.collection.mapReduce() Method
</p>
<p>As we saw in Chap. 5, MongoDB provides a number of built-in methods in the shell.
We used the db.collection.find() method, for example, to search for records.
</p>
<p>Another shell method provided is the db.collection.mapReduce() method which
is a wrapper around the mapReduce command. We need to write a little bit of code
to make this work for us in our environment&mdash;in effect creating our own extension
to the Map-Reduce process.
</p>
<p>Start by re-opening the MongoDB database. Assuming you inserted the Airport
data in Chap. 5 you can jump straight to the Airports Database and we will be
working with the AllAirports collection. If you did not do this you need to go to the
end of the MongoDB tutorial in Chap. 5. The screen dump below reminds us how
to do this before starting to build the functions required.
</p>
<p>You would probably also need to remind yourself of what the data looks like
(again, see Chap. 5). Our first Map Reduce exercise will count the number of Air-
ports each country has.
</p>
<p>Remembering that MapReduce is actually a two stage process we need to cre-
ate first the map function and then the reduce function. In this case the Airport-
Count_map function adds the number 1 to the output against each instance of every
Country Code. The reduce function will use the 1 for its count. The &ldquo;emit&rdquo; line is
where we define what to output. The reduce function will then add up all the 1s for
each Country Code. Reduce takes the output from Map and creates an Array. The
word &ldquo;this&rdquo; is used to refer to &ldquo;the collection currently being operated on&rdquo;. When
we call this function you will see that the collection is part of the call.
</p>
<p>We have now created the functions (Fig. 6.4) required and the next step is to
actually call the functions, using the mapReduce method that MongoDB provides.
We have to tell the method which collection we are using, and this is what our map
function will use as &ldquo;this&rdquo;.
</p>
<p>The first two parameters to pass are the names of the map and reduce func-
tions that we have just created, and then a third parameter tells MongoDB which
collection to write the results to (Fig. 6.5).
</p>
<p>As you can see there is some reassurance returned when the method runs suc-
cessfully. The final step is to look at the results, which we have asked MongoDB to
store in the map_reduce_output collection. We need to be aware that if this collec-
tion already existed, it would get over-written by this. The opening section of the
content of the collection is shown in Fig. 6.6.
</p>
<p>So, given that we can discover that AU is the code for Australia we can see that
they have 610 airports, whereas Andorra (AD) has just one.
</p>
<p>As can be seen from this example the limit of what you can do with map reduce
is more to do with the user&rsquo;s programming ability than with the function itself! Just
to push this a little further, now let&rsquo;s see if we can use map reduce to tell us how
many airfields are on the same parallel.
</p>
<p>Firstly, we need to recognise that lines of parallel refer to latitude. Moreover, the
data we have is too exact and we will need to use only the units from the latitude
stored. So now all we need to do is alter the map method so that it outputs a parallel
and a 1. The reduce and output can remain the same.</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Using MongoDB&rsquo;s db.collection.mapReduce() Method 153
</p>
<p>Fig. 6.4 Creating the
MapReduce functions
</p>
<p>Fig. 6.5 MapReduce being
called
</p>
<p>Fig. 6.6 MapReduce output</p>
<p/>
</div>
<div class="page"><p/>
<p>154 6 Big Data
</p>
<p>Try changing the map code to read:
</p>
<p>var LatCount_map = function() {
var key = Math.floor(this.Lat);
emit(key, 1);
};
</p>
<p>But this isn&rsquo;t quite accurate enough since the floor function works the wrong
way for negative numbers&mdash;&minus;5.5 would become &minus;6 if we used floor. So we need
to use ceil with negatives. Here is the revised code, using the appropriate rounding
tool depending upon the Latitude passed:
</p>
<p>var LatCount_map = function() {
if (this.Lat &gt; 0)
{
var key = Math.floor(this.Lat);
}
else
{
var key = Math.ceil(this.Lat);
}
emit(key, 1);
};
</p>
<p>A screenshot of this being called is shown in Fig. 6.7.
</p>
<p>Exercise 1
</p>
<p>Now see if you can produce a map reduce output that tells us how many
airports there are in each of the earth&rsquo;s hemispheres. A worked example is at
the end of this chapter. HINT: the key in the last example was a number, but
it could have been text.
</p>
<p>We can also alter the functionality of the reduce function. Let us say we want to
discover the furthest north there was an airport. The map function could just send
out the list of latitudes and then the reduce could discover the maximum value:
</p>
<p>var MostNorth_map = function() {
emit(&lsquo;Lat&rsquo;, this.Lat);
</p>
<p>};</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Using MongoDB&rsquo;s db.collection.mapReduce() Method 155
</p>
<p>Fig. 6.7 Airfields on the same Parallel
</p>
<p>var MostNorth_reduce = function(key, values) {
var max = values[0];
values.forEach(function(val){
</p>
<p>if (val &gt; max) max = val;
})
return max;
</p>
<p>}
</p>
<p>db.AllAirports.mapReduce(
MostNorth_map,
MostNorth_reduce,
{ out: &ldquo;map_reduce_output&rdquo; }
)
</p>
<p>db.map_reduce_output.find()</p>
<p/>
</div>
<div class="page"><p/>
<p>156 6 Big Data
</p>
<p>Exercise 2
</p>
<p>Now see if you can produce a map reduce output that tells us what is the most
westerly airport from the Prime Meridian (0&deg;). A worked example is at the
end of this chapter.
</p>
<p>6.6.1 And If You Have Time to Test Your MongoDB and JS Skills
</p>
<p>Only try this if you have some Javascript knowledge! To help, you need to be aware
that what is returned when you use the collection find() method is an array, and the
output can be used programmatically by declaring a variable array to capture the
output values.
</p>
<p>Exercise 3
</p>
<p>Use the output from Exercise 2 to print the all airport details we hold for the
most westerly airport. A worked example is at the end of this chapter.
</p>
<p>6.6.2 Sample Solutions
</p>
<p>Ex 1 Hemispheres:
</p>
<p>var HemiCount_map = function() {
if (this.Lat &gt; 0)
{
</p>
<p>var key = &ldquo;North&rdquo;;
}
else
{
</p>
<p>var key = &ldquo;South&rdquo;;
}
emit(key, 1);
};
var HemiCount_reduce = function(Hemi, cownt) {
</p>
<p>return Array.sum(cownt);
};</p>
<p/>
</div>
<div class="page"><p/>
<p>6.6 Using MongoDB&rsquo;s db.collection.mapReduce() Method 157
</p>
<p>db.AllAirports.mapReduce(
HemiCount_map,
HemiCount_reduce,
{ out: &ldquo;map_reduce_output&rdquo; }
</p>
<p>)
db.map_reduce_output.find()
</p>
<p>Ex 2 Most Westerly:
</p>
<p>var MostWest_map = function() {
emit(&rsquo;Lon&rsquo;, this.Lon);
</p>
<p>};
var MostWest_reduce = function(key, values) {
</p>
<p>var min = values[0];
values.forEach(function(val){
</p>
<p>if (val &lt; min) min = val;
})
return min;
</p>
<p>}
db.AllAirports.mapReduce(
</p>
<p>MostWest_map,
MostWest_reduce,
{ out: &ldquo;map_reduce_output&rdquo; }
</p>
<p>)
db.map_reduce_output.find()
</p>
<p>Ex 3 Most Westerly details:
</p>
<p>var MRCursor = db.map_reduce_output.find();
var lon = myCursor[0].value
db.AllAirports.find( {&ldquo;Lon&rdquo;: lon})</p>
<p/>
</div>
<div class="page"><p/>
<p>158 6 Big Data
</p>
<p>6.7 Summary
</p>
<p>In this chapter we have seen that Big Data is not just about the number of bytes
of data we are storing, but about the complexity of the data, and the speed with
which it arrives. Organisations need to be able to make sense of more and more
data, and begin to look outside of their own data sources. Tools that have enabled
the change in the way we store and analyse data are also increasing in number and
maturity rapidly. At the time of writing Hadoop seems to have become the de facto
standard such tool with many leading commercial vendors adopting it and integrat-
ing with it. MapReduce is a key part of Hadoop, but is a general programming
approach and other tools allow MapReduce, as we saw with MongoDB in the tuto-
rial.
</p>
<p>6.8 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What has become the de facto standard approach for handling large datasets
</p>
<p>stored across many nodes? Your answer could be a framework or a program-
ming model&mdash;or both!
</p>
<p>&bull; What do the letters SPA stand for, as used by Forrester talking about big data?
&bull; What are the 4 &ldquo;V&rdquo;s?
&bull; What makes Big Data different from traditional data warehousing?
&bull; What is HDFS?
</p>
<p>6.9 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 As the CIO of a company you need to come to terms with
Big Data. Your board of directors are asking you what it might mean for them.
Review the key elements of Big Data, and how they may impact upon any or-
ganisation&rsquo;s information strategy, and attempt to report, in terms simple enough
for a non-technical executive, what changes to information management might
be worth exploring as a consequence of the new technologies becoming avail-
able.
</p>
<p>Discussion Topic 2 &ldquo;Big Data is nothing new&rdquo;. Discuss this assertion. You should
review the benefits and disadvantages of different approaches to storage and analysis
of large volumes of data.</p>
<p/>
</div>
<div class="page"><p/>
<p>References 159
</p>
<p>References
</p>
<p>Dean J, Ghemawat S (2008) MapReduce: simplified data processing on large clusters. Commun
ACM 51(1):107&ndash;113. 2008. doi:10.1145/1327452.1327492,
</p>
<p>e-Skills UK (Jan 2013) Big data analytics: an assessment of demand for labour and skills, 2012&ndash;
2017. http://www.e-skills.com/research/research-publications/big-data-analytics/
</p>
<p>Hill R, Hirsch L, Lake P, Moshiri S (2013) Guide to Cloud Computing: principles and practice.
Springer, London</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1145/1327452.1327492">http://dx.doi.org/10.1145/1327452.1327492</a></div>
<div class="annotation"><a href="http://www.e-skills.com/research/research-publications/big-data-analytics/">http://www.e-skills.com/research/research-publications/big-data-analytics/</a></div>
</div>
<div class="page"><p/>
<p>7Object and Object Relational Databases
</p>
<p>What the reader will learn:
</p>
<p>&bull; The problems with data structures and relational databases
&bull; What is an object
&bull; How are objects handled in a database
&bull; What is an object oriented database and its features
&bull; What is an object relational databases and how it is implemented
</p>
<p>7.1 Querying Data
</p>
<p>One of the issues with data is that it has become more and more complex. As we
saw in Chap. 2 there was a transition from early file based systems to more com-
plex database systems which could be accessed using a query language. The format
of data in these systems was alphabetic, numeric or alphanumeric with some spe-
cial types such as date. This was stored in independent files which required end to
end sequential processing. There was no need for a query language. The second
transition was to be able to search and select data directly using so called random
access files. These files were ultimately joined together to become databases which
could be manipulated with a powerful query language. As seen in Chap. 4, relational
databases became the de facto standard for this type of data. The next development
was the requirement to store complex objects and retrieve them using a query lan-
guage. Figure 7.1 gives a simple classification of data and queries. As will be seen
in the text below, considering object databases to only be associated with simple
queries is not correct.
</p>
<p>7.2 Problems with Relational Databases
</p>
<p>The first big problem with relational databases is that SQL only supports a restricted
number of built in types which deal with numbers and strings. Initially the only
complex object was BLOB (Binary Large OBject) but now vendors are including
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_7,
&copy; Springer-Verlag London 2013
</p>
<p>161</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_7">http://dx.doi.org/10.1007/978-1-4471-5601-7_7</a></div>
</div>
<div class="page"><p/>
<p>162 7 Object and Object Relational Databases
</p>
<p>Fig. 7.1 A simple
classification
</p>
<p>other objects such as CLOB (Character Large OBject) and XML_Type. XML will
be discussed later in this chapter. Increasingly there are requirements to deal with
different complex objects such as graphics, video, audio and complete documents,
all of which can be in a variety of different formats. The volume of this electronic
data is rapidly increasing and there is a tendency to store anything that can be stored.
Relational databases are not the best structures to store this type of data.
</p>
<p>The second problem is that relational tables are essentially flat files linked by
joins and do not easily support sets and arrays. Set theory means you can have data
which is grouped by some criteria (say people aged between 20 and 29) which can
be viewed in terms of its relationships with other groups of data (say smart phone
ownership). In this example the intersection of two sets would be the data common
to both sets (people aged between 20 and 29 who own a smart phone) while the
union would be all the data in both sets (all smart phone owners and all people aged
between 20 and 29). Arrays on the other hand bring us back to the first problem in
that they store lots of data, often images which have a pixel format.
</p>
<p>The third problem is there are certain types of relationships that cannot easily be
represented without some kind of work around. For example in Chap. 4 the concept
of a hierarchy was introduced where there was a superclass with one or more sub-
classes associated with it. Attributes and methods from the superclass were inherited
by the subclasses. This structure had to be converted into relational tables to make
it work in a relational database. Doing this always introduced some in efficiency
either in the form of NULL fields or excessive table joins which has an impact on
database performance. In the real world things are often organised into hierarchies.
For example in a staffing system different classifications of staff will have differ-
ent attributes, but some data, like staff number, name and address will be common
to classes of staff and would be at the top of the hierarchy, to be inherited by the
specialist subclass definitions.
</p>
<p>The fourth and probably most important issue is that there is often a mismatch
between the data access language (SQL) and the host language (for example java).
This is termed the impedance mismatch. For example in object oriented program-
ming one of the main concepts is encapsulation. Some encapsulated objects have
their representation hidden. These are called private objects and are at odds with
relational database representations where access is relative to need rather than an
absolute characteristic of the data. Issues such as these are often solved by a pro-
gramming work around.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.3 What Is an Object? 163
</p>
<p>Some vendors have solved at least some of these problems, but in a proprietary
way rather than an industry standard way. Oracle, for example, has an implementa-
tion for sets and operators to manipulate them.
</p>
<p>7.3 What Is an Object?
</p>
<p>It is probably best to start with a definition and discussion of objects, then move on
to object oriented databases, then look at the disadvantages of pure object oriented
databases when dealing with lots of &lsquo;traditional data&rsquo;. We will then look at the dis-
advantages of using relational databases when there are requirements to include and
merge with object oriented technologies.
</p>
<p>As we saw in Chap. 2, relational database theory is based in mathematics. Hu-
mans however tend to recognize &lsquo;objects&rsquo; immediately in terms of their totality
or &lsquo;wholeness&rsquo;. Therefore our original relational example of an invoice would be
viewed as a single object rather than being composed of a number of &lsquo;relations&rsquo; by
the average person.
</p>
<p>From a programming point of view, an object is an encapsulation of data and the
process which manipulate it. Only the data and methods which need to be seen on
an interface are &lsquo;visible&rsquo; to a user. For example, if we think of a television set as
an object there are a restricted number of controls and data entry you as a user can
interact with. The rest is &lsquo;hidden&rsquo; inside the device.
</p>
<p>In object oriented programming when an object instance ends, the data associated
with it is lost. The problem with this definition is that the data may &lsquo;outlive&rsquo; the
processes. With a digital television, it &lsquo;remembers&rsquo; the stations it has been tuned to,
even when it is switched off. This phenomenon of data outliving the object is known
as persistence.
</p>
<p>In fact in a pure object world a relational database is often represented as a single
persistent object. This fits with the concept of object orientation where the complex-
ity of the object is hidden and only its &lsquo;public&rsquo; data and processes can be &lsquo;seen&rsquo;.
</p>
<p>The discussion so far, that hasn&rsquo;t really defined an object in database terms. The
word &lsquo;object&rsquo; is really a shortened version of what we are talking about and there
are two interpretations: object class and object instance. Depending on the object
definition language you are using an object class is normally a description of the
classes attributes, the messages to which the object responds and the methods which
manipulate the object. The data itself is called an object instance. So if we have an
object class student, an instance of the class could contain the attributes for the
student &lsquo;Dorian&rsquo;.
</p>
<p>So far we are still looking at attributes which store simple data such as char-
acters and numbers, however a further complication has been the rise of graphics,
images, video and other large data items which require storing, linking and retriev-
ing. As already mentioned relational databases deal with these via a single data
type&mdash;BLOB (Binary Large OBject) although most vendors included other large</p>
<p/>
</div>
<div class="page"><p/>
<p>164 7 Object and Object Relational Databases
</p>
<p>object data types. Relational databases also include other, normally character based
attributes to assign keys. This was OK when dealing with something which fitted the
relational structure such as a single image of a stock item. It was not such a good
solution when dealing with the data required in a graphic based system of multi-
ple related images. In these systems using the traditional primary/foreign key joins
becomes highly complicated.
</p>
<p>A solution to this is to assign an object an identity that uniquely identifies the
object, but unlike a primary key is not stored as an attribute of the object.
</p>
<p>7.4 An Object Oriented Solution
</p>
<p>The Object Data Management Group (ODMG) which formed in 1991 and dis-
banded ten years later in 2001 developed a set of standards, the last of which was
ODMG 3.0 in 2000. This formed the basis of an industry standard giving guidelines
for a SQL like language to manipulate objects, Object Query Language (OQL).
</p>
<p>About the same time the Object Oriented Database System Manifesto was pro-
duced by Atkinson et al. (1992). This proposed thirteen mandatory features:
&bull; Complex Objects
&bull; Object Identity
&bull; Encapsulation
&bull; Types and Classes
&bull; Type and class hierarchies
&bull; Overriding, overloading and late binding
&bull; Computational completeness
&bull; Extensibility
&bull; Persistence
&bull; Efficiency
&bull; Concurrency
&bull; Reliability
&bull; Declarative query language
Taking each of these concepts in turn
Complex Objects: these are formed through constructor orthogonality. This
</p>
<p>means a small set of primitive constructs can be combined in a small number
of ways to form more complex structures. In simple terms this means complex
objects can be formed from other objects by a set of constructors.
</p>
<p>Object Identity: Each object has a unique identity assigned by the system. Ob-
jects can be shared through references to their identity. This corresponds to the
structure proposed by the ODMG.
</p>
<p>Encapsulation: In object orientation an object consists of an interface and imple-
mentation. The interface defines the way the object looks to the environment.
The implantation defines the object data and methods which are used for internal
manipulation. The state of the object can only be altered through its interface</p>
<p/>
</div>
<div class="page"><p/>
<p>7.4 An Object Oriented Solution 165
</p>
<p>although the data structure can have declarative queries applied to it.
Classes: Developers should be able to develop their own classes, for example an
</p>
<p>&lsquo;Employee&rsquo; class or &lsquo;Address&rsquo; class.
Hierarchies: Many data structures can be regarded as hierarchies. We already saw
</p>
<p>this in Fig. 4.15 of Chap. 4 where we had a hierarchy consisting of &lsquo;Motor Ve-
hicle&rsquo; as the super class at the top of the hierarchy and a number of subclasses.
Objects in the subclass automatically belong to and inherit all the attributes and
methods of the superclass although the subclass can have attributes and methods
in its own right.
</p>
<p>Overriding, overloading and late binding: This is related to hierarchies. In meth-
od overriding, a method in the superclass is redefined in the subclass. This allows
specialisation in the subclass while preserving the uniform interface defined in
the superclass. Overloading is the effect caused by method overriding. This is
because it is possible to have multiple methods in the same class that share the
same name but have different parameter lists. They must however have the same
number of return values. Late binding refers to the overloaded method that is
selected at run time and will depend on where you are in the hierarchy.
</p>
<p>Computational completeness is a requirement for the method implementation lan-
guage. Basically it should be possible to express any computable function.
Extensibility: The database has a set of predefined types which developers can use
</p>
<p>to define new types. This relates to the mandatory feature of complex objects and
constructor orthogonality.
</p>
<p>Persistence: The data has to survive the program execution. In all object oriented
applications a database is regarded as a persistent object.
</p>
<p>Efficiency: The database must have index management, data clustering and data
buffering to optimise queries.
</p>
<p>Reliability: The database must conform to the principles of ACID (see Chap. 2 for
details). In other words it must be resilient to user, software and hardware fail-
ures. Transactions must be all or nothing in terms of completeness and operations
must be logged.
</p>
<p>Declarative query language: Non-trivial queries must be able to be expressed
consistently through a text or graphical interface. The query language must be
vendor independent, in other words be able to work on any possible database.
</p>
<p>This of course was a manifesto and together with the ODMG served as guide to the
development of object oriented databases from the early 1990&rsquo;s. The resulting query
language, Object Query Language (OQL) met the requirements of the manifesto and
the ODMG. It looks very much like normal SQL but rather than naming tables in
the SELECT clause, object classes are named. The language also has a concept of
joins, but because the relationships are established with pointers rather than with a
primary/foreign key reference, only the key word JOIN is required. Other structures
such as hierarchies are also implemented.
</p>
<p>In the 1990&rsquo;s a number of vendor implementations of object-oriented databases
appeared. These included O2 (now owned by IBM), JADE and more recent open
source products such as db4o.
</p>
<p>As an example, in the O2 language (which follows ODMG OQL standard) a
simple select clause becomes:</p>
<p/>
</div>
<div class="page"><p/>
<p>166 7 Object and Object Relational Databases
</p>
<p>SELECT c FROM c IN BBB.customer
</p>
<p>WHERE date_added &lt; &lsquo;01 JAN 2013&rsquo;
</p>
<p>To retrieve data from two objects:
</p>
<p>SELECT DISTINCT inv.what FROM cl IN BBB.customer,
</p>
<p>inv IN cl.invoice
</p>
<p>WHERE cl.customer_name = &ldquo;Felsky&rdquo;
</p>
<p>For more details see the ODMG OQL User Manual Release 5.0&mdash;April 1998.
There have been a number of ways put forward to get around the impedance
</p>
<p>mismatch problem. Java Persistence Query Language (JP-QL) is an object query
language designed for use with Java and to be platform independent. This is not
designed to manipulate object oriented databases, but to allow an interface between
java and relational databases. For example:
</p>
<p>public List getStock() throws StockNotFoundException {
</p>
<p>try {
</p>
<p>return em.createQuery(
</p>
<p>&ldquo;SELECT st FROM Stock st ORDER BY st.code&rdquo;).
</p>
<p>getResultList();
</p>
<p>} catch(Exception ex){
</p>
<p>throw new StockNotFoundException(&ldquo;Could not find stock:&rdquo;
</p>
<p>+ ex.getMessage());
</p>
<p>}
</p>
<p>}
</p>
<p>which has a relational SELECT clause embedded in the middle.
</p>
<p>7.5 XML
</p>
<p>A final solution presented here is XML (eXtensible Markup Language) which can
be used to structure data. The central design goal for XML is to make it easy to
communicate information between applications by allowing the semantics of the
data to be described in the data itself. XML was designed to overcome the short-
comings of HTML by providing a way to have a domain specific markup language.
Many industry specific XML standards have been proposed. In the chemical indus-
try a standard called ChemML has been developed to represent chemicals and their
properties. For example in ChemML water (H2O) is described as:
</p>
<p>&lt;chem&gt;&lt;molecule n =&ldquo;2&rdquo;&gt;
</p>
<p>&lt;atom n =&ldquo;2&rdquo;&gt; H &lt; /atom&gt;
</p>
<p>&lt;atom&gt; O &lt; /atom&gt;
</p>
<p>&lt; /molecule&gt;&lt; /chem&gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>7.5 XML 167
</p>
<p>The primarily purpose of XML is to markup content, but it is claimed to have many
advantages as a data format because:
&bull; Utilizes unicode.
&bull; Platform independent
&bull; Human readable format makes which makes it easier for development and main-
</p>
<p>tenance (although this is sometimes contested).
&bull; Extensibility, so new information won&rsquo;t cause problems in applications that are
</p>
<p>based on older versions of the format.
&bull; There exists a large number of off the shelf XML tools
Therefore it is not primarily a database definition language. However code can then
be written to manipulate XML objects in the database. The are two main variants:
XML Data Reduced (XDR) and XML Schema Definition (XSD). XDR was an in-
terim standard adopted by Microsoft before the introduction of XSD which is the
World Wide Web Consortium (W3C) specification. However, because XDR was in-
troduced first (and by Microsoft) there are still a lot of applications which use it as
a base.
</p>
<p>The following XML Data Reduced (XDR) is in the a Microsoft based version
and defines a customer:
</p>
<p>&lt;Schema name+&ldquo;customer&rdquo;
</p>
<p>xmlns=&ldquo;urn:schemas-microsoft-com:xml-data&rdquo;
</p>
<p>xmlns=&ldquo;urn:schemas-microsoft-com:datatypes&rdquo;&gt;
</p>
<p>&lt;ElementType name=&ldquo;cutomer&rdquo; model=&ldquo;closed&rdquo;
</p>
<p>content=&ldquo;eltOny&rdquo; order=&ldquo;seq&rdquo;&gt;
</p>
<p>&lt;AttributeType name=&ldquo;CustomerDescription&rdquo; dt:type=&ldquo;string&rdquo;
</p>
<p>required+&ldquo;yes&rdquo;/ &gt;
</p>
<p>&lt;attribute type=&ldquo;CustomerDescription&rdquo;/ &gt;
</p>
<p>&lt;element type=&ldquo;Customer_Name&rdquo; minOccurs=&ldquo;1&rdquo;
</p>
<p>maxOccurs=&ldquo;&lowast;&rdquo;/ &gt;
</p>
<p>&lt; /ElementType&gt;
</p>
<p>&lt;ElementType name=&ldquo;Customer_Name&rdquo; model=&ldquo;closed&rdquo;
</p>
<p>content=&ldquo;textOnly&rdquo; dt:type=&ldquo;string&rdquo;&gt;
</p>
<p>&lt;AttributeType name=&ldquo;Customer_Address&rdquo; dt:type=&ldquo;string&rdquo;
</p>
<p>required=&ldquo;yes&rdquo;/ &gt;
</p>
<p>&lt;attribute type=&ldquo;Customer_Address&rdquo;/ &gt;
</p>
<p>&lt; /ElementType&gt;
</p>
<p>&lt; /Schema&gt;
</p>
<p>XML Schema Definition (XSD) looks very similar. Here we have a definition for
stock:
</p>
<p>&lt;xs:element name=&ldquo;stock&rdquo; maxOccurs=&ldquo;unbounded&rdquo;&gt;
</p>
<p>&lt;xs:complexType&gt;
</p>
<p>&lt;xs:sequence&gt;
</p>
<p>&lt;xs:element name=&ldquo;code&rdquo; type=&ldquo;xs:string&rdquo;/ &gt;</p>
<p/>
</div>
<div class="page"><p/>
<p>168 7 Object and Object Relational Databases
</p>
<p>&lt;xs:element name=&ldquo;description&rdquo; type=&ldquo;xs:string&rdquo; minOccurs=&ldquo;0&rdquo;/&gt;
</p>
<p>&lt;xs:element name=&ldquo;price&rdquo; type=&ldquo;xs:decimal&rdquo;/ &gt;
&lt; /xs:sequence&gt;
</p>
<p>&lt; /xs:complexType&gt;
</p>
<p>&lt; /xs:element&gt;
</p>
<p>To search for a record a typical code fragment would look like:
</p>
<p>FOR $b IN document(&ldquo;cust.xml&rdquo;)//customer
</p>
<p>WHERE $b/name = &ldquo;Warren Felsky&rdquo;
</p>
<p>AND $b/postcode = &ldquo;S11 4RT&rdquo;
</p>
<p>RETURN $b/customer_id
</p>
<p>which would return the customer_id&rsquo;s of any customer called Warren Felsky.
Major vendors have already embraced XML as part of their products. Oracle has
</p>
<p>developed Oracle XML DB, a native XML storage and retrieval technology which
is delivered as part of their standard database system. It provides support for all
of the key XML standards, including XML, XML Namespaces, DOM, XQuery,
SQL/XML and XSLT. This support enables XML-centric application development.
</p>
<p>Although not as advanced, SQL Server provides support for XML including sup-
port for the XML data type and the ability to specify an XQuery query against XML
data.
</p>
<p>There are also a number of free and open source XML database systems avail-
able such as Sedna which is a free native XML database. It provides a full
range of core database services including persistent storage, ACID transactions
and flexible XML processing facilities including W3C XQuery implementation (see
http://www.sedna.org/ last accessed 26/07/2013).
</p>
<p>Another is BaseX which is also free. This is a scalable light-weight XML
Database engine supporting XPath and XQuery Processing. It supports the latest
W3C Update and Full Text Recommendations (see http://basex.org/home/ last ac-
cessed 26/07/2013).
</p>
<p>7.6 Object Relational
</p>
<p>Object oriented databases were never really adopted by the vendor community de-
spite a flurry of products in the 1990&rsquo;s, but many of the principles ODMG defined
were incorporated into relational database management systems resulting in what
is often referred to as object relational databases. Oracle Corporation, for example
started incorporating object features in Oracle 9i and steadily expanded them and
associated development tools such as J developer. In the following examples Oracle
statements will be used. Microsoft&rsquo;s SQL Server also has object relational features
but these are not fully developed. For example constraints are not inherited in hier-
archy structures.</p>
<p/>
<div class="annotation"><a href="http://www.sedna.org/">http://www.sedna.org/</a></div>
<div class="annotation"><a href="http://basex.org/home/">http://basex.org/home/</a></div>
</div>
<div class="page"><p/>
<p>7.7 What Is Object Relational? 169
</p>
<p>PostgreSQL is another popular object relational database management system
which was first devised in the 1980&rsquo;s. The advantage of this system is it is cross
platform, free and open source. As a result it is popular with personal computer
users. For more information see Obe and Hsu (2012).
</p>
<p>It should be remembered that object relational is a work around to give some of
the functionality of object orientation by building on top of the existing relational
framework. As will be seen, there is always a relational table storing the objects.
</p>
<p>7.7 What Is Object Relational?
</p>
<p>Figure 7.2 will be used as the basis of the discussion and examples in the rest of
the chapter. The code in bold can be typed in as you work through this section so
it will be possible to create and experiment with object relational structures as you
proceed.
</p>
<p>Figure 7.2 shows a number of object classes. An object class consists of a num-
ber of attributes and the methods to manipulate them. In the example each of the
object classes has a number of methods or operations associated with them. Gener-
ally there is one operation for create, retrieve, update and delete (so called CRUD-
ing operations) but this is not a hard and fast rule. It should be noted that hierarchy
under shop_stock contains only one operation (in subclass magazine). This is be-
cause each of the subclasses inherits the methods of shop_stock. Magazine is an
exception because periodicals are not reordered based on stock levels instead being
ordered on expected sales so returns are kept to a minimum. This operation would
override the add new stock operation in the shop_stock class. This will be illus-
trated later.
</p>
<p>It is probably a good idea at this point to define an instance of an object. This is
roughly equivalent to a record in a relational database, so purchase order 45378 of
the 3rd December 2012 is an instance of the purchase_order class.
</p>
<p>The second thing to note about the class diagram is there are no primary or for-
eign keys. Classes are linked via associations implemented by pointers.
</p>
<p>7.8 Classes
</p>
<p>When a class is defined it can be used in any other definition where that class is
used. In most commercial databases we often define a table called address, but there
are many types of addresses for example: delivery address, home address, billing
address. Each of these has the same format, but we define them individually in a
relational database system. A much better approach would be to have a single data
item called &lsquo;address&rsquo; which we could then use whenever an address is needed.
</p>
<p>CREATE TYPE addr_ty AS OBJECT
</p>
<p>(street varchar2(60),
</p>
<p>city varchar2(30),
</p>
<p>postcode varchar(9));</p>
<p/>
</div>
<div class="page"><p/>
<p>170 7 Object and Object Relational Databases
</p>
<p>Fig. 7.2 A UML class diagram which will be implemented as an ORDB
</p>
<p>The address type could then be used in the definition of &lsquo;Shop Customer&rsquo;:
</p>
<p>CREATE TYPE shop_customer_ty AS OBJECT
</p>
<p>(customer_no varchar(9),
</p>
<p>name varchar2(25),
</p>
<p>address addr_ty);
</p>
<p>Obviously you must create something before you can use it in another creation state-
ment so in this example you must create the address type first before you can use
it in the shop customer type. Once you have created a type you can use it multiple
times, for example address is used in purchase order.
</p>
<p>Up until this stage the structure looks object oriented but actual data still needs
to be stored in a table, so now we need a table to hold the object structures:
</p>
<p>CREATE TABLE Shop_Customer OF Customer_objtyp
</p>
<p>(PRIMARY KEY (CustNo))
</p>
<p>OBJECT IDENTIFIER IS PRIMARY KEY;</p>
<p/>
</div>
<div class="page"><p/>
<p>7.8 Classes 171
</p>
<p>The Shop_Customer table contains Customer objects. Each of these has its own
object identifier or OID. You can either allow Oracle to generate this (OBJECT
IDENTIFIER IS SYSTEM GENERATED) or specify the objects primary key to
serve as it OID with OBJECT IDENTIFIER IS PRIMARY KEY which is what
we have done in this case. OBJECT IDENTIFIER IS PRIMARY KEY should
only be used if there is a naturally occurring attribute which lends itself to this role.
If there is no natural candidate for a primary key, you should not create one but use
OBJECT IDENTIFIER IS SYSTEM GENERATED.
</p>
<p>Once the table has been created it can be populated with object instances:
</p>
<p>INSERT INTO customer (shop_customer)
</p>
<p>VALUES
</p>
<p>(&lsquo;0032478FT&rsquo;,
</p>
<p>&lsquo;Warren Felsky&rsquo;,
</p>
<p>&lsquo;8 Mien Place, Sheffield&rsquo;,
</p>
<p>&lsquo;S1 6GH&rsquo; );
</p>
<p>The address information does not have to refer to the type (addr_ty) which was
created. It is just another data type, only one that has been created by the user.
</p>
<p>Once you have stored information of this object instance in this structure you
need to be able retrieve it again. In this case you must state what attributes you want
returned, but not their type:
</p>
<p>SELECT c.shop_customer.customer_no ID, c.shop_customer.name NAME,
</p>
<p>c.shop_customer.address ADDRESS
</p>
<p>FROM customer c
</p>
<p>WHERE c.shop_customer.customer_no = &lsquo;0032478FT&rsquo;
</p>
<p>We can use any of the selection constructs we saw in Chap. 4&rsquo;s discussion on SQL.
In this case we are looking for the details of customer &lsquo;0032478FT&rsquo; which will
result in the output:
</p>
<p>ID NAME ADDRESS
</p>
<p>--------- ------------------------- ------------
</p>
<p>0032478FT Warren Felsky 8 Mien Place, Sheffield.
</p>
<p>It is important to note that in Oracle, aliases (in this case &lsquo;c&rsquo;) must be used. This
allows the substitution of &lsquo;c&rsquo; for &lsquo;customer&rsquo; otherwise the statement would look
like:
</p>
<p>SELECT customer.shop_customer.customer_no ID,
</p>
<p>customer.shop_customer.name NAME,
</p>
<p>customer.shop_customer.address ADDRESS
</p>
<p>FROM customer
</p>
<p>WHERE customer.shop_customer.customer_no = &lsquo;0032478FT&rsquo;</p>
<p/>
</div>
<div class="page"><p/>
<p>172 7 Object and Object Relational Databases
</p>
<p>which is not only longer but messy and confusing. ID, NAME and ADDRESS are
also aliases for the column headings; otherwise the name of the selected attribute
would be used, for example c.shop_customer.customer_no would be the column
heading instead of the alias ID.
</p>
<p>7.9 Pointers
</p>
<p>In a traditional relational database, tables are linked via joins. The joins use a com-
mon primary key&mdash;foreign key attribute to link the two tables together. This is one
of the hardest concepts for those new to the database field to master. There are two
problems with this however. One is you may be creating an attribute for no other
reason than to give the data a unique identifier (the primary key). Second you may
include that data item in another table for no other reason than you want to join
them&mdash;it may not logically be an attribute of that table.
</p>
<p>In our example, the Purchase Order object contains a pointer to the associated
customer object. That is a customer can have many orders but the order can only
be associated with one customer&mdash;a classic one to many relationship. In a relational
database this would be implemented with a table join involving the primary key of
the customer table as the foreign key in the order table. In an object relational imple-
mentation the link must still be established but it is done with a pointer embedded
in one class pointing to another. So an instance of purchase order must include a
pointer to the instance of the customer class to which it belongs.
</p>
<p>Creating the purchase order object is done as follows. Note this object has
Cust_ref included to reference the customer object. A pointer is generated to the
appropriate object instance in the shop_customer table. This will be seen later
when we create the tables which hold the objects. Also note you could use the
syntax CREATE OR REPLACE in the creation of Address_objtyp and Cus-
tomer_objtyp. This means you do not have to first DROP an object before creating
its replacement:
</p>
<p>CREATE OR REPLACE TYPE Purchase_Order_objtyp AS OBJECT
</p>
<p>(PONo varchar2 (12),
</p>
<p>Cust_ref REF customer_objtyp,
</p>
<p>Orderdate DATE,
</p>
<p>shipdate DATE,
</p>
<p>shp_address_obj Address_objtyp);
</p>
<p>Just like the customer table holds the customer object, we need a table to hold the
purchase order object. There is a problem here as a foreign key constraint is re-
quired. This illustrates the issues of combining relational and object oriented struc-
tures. In this case the Customer Number needs to be added to Purchase Order as
a foreign key. When the purchase order object was created we already specified the
Cust_ref attribute was a pointer, now we specify which table it is pointing to.</p>
<p/>
</div>
<div class="page"><p/>
<p>7.9 Pointers 173
</p>
<p>CREATE TABLE purchase_order OF Purchase_Order_objtyp
</p>
<p>(PRIMARY KEY (PONo),
</p>
<p>FOREIGN KEY (Cust_ref) REFERENCES Shop_Customer)
</p>
<p>OBJECT IDENTIFIER IS PRIMARY KEY;
</p>
<p>Now we have created the basic structure for Customer, Address, Purchase Order
and Shipping Address. It also illustrates the trade off for having objects in what is
still essentially a relational database environment.
</p>
<p>The next stage is to add data:
</p>
<p>INSERT INTO Shop_Customer
</p>
<p>VALUES (1234, &lsquo;Warren Felsky&rsquo;,
</p>
<p>Address_Objtyp (&lsquo;2 Sherwood Place&rsquo;, &lsquo;Latrobe&rsquo;, &lsquo;L1 1AC&rsquo;));
</p>
<p>Adding data to a table with a REF construct to allow one object to reference another
is more complex than a relational INSERT statement and requires the use of a
SELECT as shown in the following example where a new purchase order is created.
This makes sure that each Purchase_Order object instance has a pointer (an object
ID or OID) to the associated Shop_Customer table object instance.
</p>
<p>Once again the order in which you create things is important. You must create a
Shop_Customer record before you can create a Purchase_Order record. In other
words you can&rsquo;t have a purchase order with no associated customer.
</p>
<p>INSERT INTO Purchase_Order
</p>
<p>SELECT &lsquo;1004&rsquo;, REF(c),
</p>
<p>SYSDATE, &lsquo;10-MAY-1999&rsquo;,
</p>
<p>address_objtyp( &lsquo;8 Mien Place&rsquo;,
</p>
<p>&lsquo;Sheffield&rsquo;,
</p>
<p>&lsquo;S1 6GH&rsquo; )
</p>
<p>FROM Shop_Customer c
</p>
<p>WHERE C.CustNo = 1234;
</p>
<p>In the above example:
&bull; The literal value &ldquo;1004&rdquo; is inserted into the Purchase_Order table.
&bull; The REF function returns the OID (object identifier) from the query on the
</p>
<p>selected Shop_Customer object.
&bull; The OID is now stored as a pointer to the row object in the Shop_Customer
</p>
<p>object table. This is the link from the purchase order object to the associated
customer.
</p>
<p>&bull; SYSDATE captures the current date from the system clock.
&bull; The delivery date is entered.
&bull; The address details are stored.
&bull; FROM Shop_Customer c WHERE C.CustNo = 1234 is the query associated
</p>
<p>with the REF function.</p>
<p/>
</div>
<div class="page"><p/>
<p>174 7 Object and Object Relational Databases
</p>
<p>The referenced value cannot be seen unless the DREF function is used. The DREF
function takes the OID and evaluates the reference to return a value. Any attempt to
select objects containing a REF, say with:
</p>
<p>SELECT &lowast; FROM purchase_order;
</p>
<p>would result in a long alphanumeric string representing the hexadecimal value of
the pointer rather than a real value.
</p>
<p>In the following example we want the customers name, purchase order numbers
and shipping address:
</p>
<p>SELECT (cust_ref).custno,
(cust_ref).custname,
p.pono,
p.shp_address_obj.city
</p>
<p>FROM Purchase_Order p
WHERE (cust_ref).custno = 1234;
</p>
<p>If you don&rsquo;t use the (cust_ref) and try to use both tables and not put in a table
join, we end up with a Cartesian join. Cust_ref is following the REF pointer to the
customer.
</p>
<p>Note that using p.cust_ref does not give the same value as c.custno as cust_ref
is a pointer to the appropriate record in Shop_Customer table, not an attribute that
stores &lsquo;real&rsquo; data (try it and see!). To see the real value you must use DEREF:
</p>
<p>SELECT DEREF(p.cust_ref)
FROM Purchase_Order p
WHERE p.pono = &lsquo;1001&rsquo;
</p>
<p>7.10 Hierarchies and Inheritance
</p>
<p>One of the problems in relational databases is you cannot directly implement inher-
itance. In our example we have items in our shop which share a lot of attributes, for
example barcode, price and description. However different types of stock then have
very specific attributes.
</p>
<p>In the design in Fig. 7.2 we have an object called shop_stock. We could populate
this with data, but when we consider different types of stock lower in the hierarchy,
each has its own unique attributes which need populating.
</p>
<p>In a relational structure, as we saw in Chap. 4, we would either include these in
description of shop stock which would mean a number of NULL fields or we would
create tables which duplicated shop stock but added a few extra attributes. Both of
these solutions create either inefficiencies or complexity.
</p>
<p>To create the objects involved in a hierarchy, you must start at the top grouping
the attributes which are common to all objects. This is the most generalised part of
the structure which is then inherited by lower level specialised objects. Therefore</p>
<p/>
</div>
<div class="page"><p/>
<p>7.10 Hierarchies and Inheritance 175
</p>
<p>the hierarchy involving the shop_stock object is created as follows:
</p>
<p>CREATE OR REPLACE TYPE shop_stock_objtyp AS OBJECT
</p>
<p>(barcode NUMBER (13),
</p>
<p>description VARCHAR2(50),
</p>
<p>price NUMBER(7,2),
</p>
<p>number_in_stock NUMBER(6) )
</p>
<p>NOT FINAL;
</p>
<p>The key words &lsquo;NOT FINAL&rsquo; means this object has subclasses associated with
it. Because the key words &lsquo;AS OBJECT&rsquo; also appear, it means this is the most
generalised type of superclass.
</p>
<p>The subclasses are created next. The order is not important and in this case we
will create a subclass to hold information about &lsquo;book&rsquo;. Note: do not use the key
words &lsquo;AS OBJECT&rsquo; here. You must however use the key word &lsquo;FINAL&rsquo; at the
bottom of any hierarchy. This specifies that there are no more subclasses below this
class.
</p>
<p>CREATE OR REPLACE TYPE book_type_objtyp
</p>
<p>UNDER shop_stock_objtyp
</p>
<p>(ISBN NUMBER(16),
</p>
<p>Author VARCHAR2(50),
</p>
<p>Title VARCHAR2(30))
</p>
<p>FINAL;
</p>
<p>The book object will inherit all the attributes of the shop stock. Just like all the
examples we have seen so far you must store data in tables, so you still have to
create a table to hold the shop_stock_object. This will contain the structure for the
complete hierarchy:
</p>
<p>CREATE TABLE shop_stock OF shop_stock_objtyp
</p>
<p>(PRIMARY KEY (barcode))
</p>
<p>OBJECT IDENTIFIER IS PRIMARY KEY
</p>
<p>You only need to worry about the object at the top of the hierarchy when creating
the table. However, all the subclasses need to be created before the table is created.
</p>
<p>Once you have created the table to hold the hierarchy you can start to populate
it. Here you do need to worry about which subclass you are populating, so to insert
a new book:
</p>
<p>INSERT INTO shop_stock (book)
</p>
<p>VALUES(&lsquo;5023765013141&rsquo;, &lsquo;hard back&rsquo;, 12.95, 14,
</p>
<p>book_type_object (9781444712247, &lsquo;Marco Vichi&rsquo;, &lsquo;Death and the Ol
</p>
<p>ive Grove&rsquo;));</p>
<p/>
</div>
<div class="page"><p/>
<p>176 7 Object and Object Relational Databases
</p>
<p>The following two examples show how to retrieve data from either the superclass
and the selected subclass, or individual attributes from a specified subclass. Is it not
possible to retrieve all data from all subclasses with a simple SQL statement
</p>
<p>SELECT s.barcode, s.price, s.number_in_stock,
TREAT(VALUE (s) AS book_type_objtyp)
FROM shop_stock s
WHERE VALUE (s) IS OF (ONLY book_type_objtyp)
</p>
<p>retrieves all the records which have an instance in the book subclass. The barcode,
price and quantity in stock along with all the subclass attributes will be displayed.
</p>
<p>SELECT s.barcode, TREAT(VALUE (s) AS book_type_objtyp).isbn
FROM shop_stock s
WHERE VALUE (s) IS OF (ONLY book_type_objtyp)
</p>
<p>retrieves the attribute barcode from the shop_stock superclass and the isbn from the
book subclass.
</p>
<p>7.11 Aggregation
</p>
<p>In object orientation, the structure &lsquo;is a part of&rsquo; is often required. If we go to our
example, a line item is a part of the purchase order. Effectively this is a one to many
relationship. In other words a purchase order is an aggregation of line items. In an
object relational system this is once again created by using REF&rsquo;s to link individual
line items to their purchase order. In this example there is also a reference to the
shop_stock object so line item details can be retrieved.
</p>
<p>Therefore, assuming we have both some purchase orders and shop stock we can
create line_item by first creating a line item object:
</p>
<p>CREATE OR REPLACE TYPE po_line_objtyp AS OBJECT
(PO_ref REF purchase_order_objtyp,
Qty NUMBER,
bar_ref REF shop_stock_objtyp)
</p>
<p>then creating a table to hold the object (and ultimately the data) by:
</p>
<p>CREATE TABLE line_item OF po_line_objtyp
(FOREIGN KEY (PO_ref) REFERENCES purchase_order,
FOREIGN KEY (bar_ref) REFERENCES shop_stock)
OBJECT IDENTIFIER IS SYSTEM GENERATED;
</p>
<p>You should note we haven&rsquo;t nominated an attribute as a PRIMARY KEY. We could
have introduced an attribute line_item_number, but its sole purpose would be allow
individual lines to be explicitly named. There is no requirement for this. However,</p>
<p/>
</div>
<div class="page"><p/>
<p>7.12 Encapsulation and Polymorphism 177
</p>
<p>because there is no PRIMARY KEY we must use the OBJECT IDENTIFIER IS
SYSTEM GENERATED clause.
</p>
<p>Once we have created the table, then data can be added:
</p>
<p>INSERT INTO line_item
</p>
<p>SELECT REF(p), &lsquo;2&rsquo;, REF(s)
</p>
<p>FROM purchase_order p, shop_stock s
</p>
<p>WHERE p.pono = &lsquo;1001&rsquo;
</p>
<p>AND s.barcode = &lsquo;5023765013141&rsquo;
</p>
<p>This code means insert a new line item which belongs to purchase order 1001 and
contains an order for some stock with bar code 5023765013141 and we want two of
them.
</p>
<p>7.12 Encapsulation and Polymorphism
</p>
<p>In a definition of an object oriented system, not only is data defined, but so are the
methods to manipulate that data. This is called encapsulation. The method can be
stored as part of the definition and then retrieved when required. In the following
example a method to calculate a person&rsquo;s age will be demonstrated. Normally only
date of birth is stored in a database as age can be calculated. Also, you are now older
than when you started reading this section so age is highly volatile and if it is stored,
it should be stored with a date/time stamp.
</p>
<p>In the example here, the code used is written in PL/SQL, Oracles proprietary
language. However in some of Oracles other products such as J Developer there has
been a move towards java.
</p>
<p>Creating the method is a two-step process with the first step being to create the
object which holds the method or function:
</p>
<p>CREATE OR REPLACE type newperson_ty as OBJECT
</p>
<p>(firstname varchar2(25),
</p>
<p>lastname varchar2(25),
</p>
<p>birthdate date,
</p>
<p>MEMBER FUNCTION age(birthdate IN DATE) RETURN NUMBER);
</p>
<p>We have created a member function called age and it uses the data stored in the
attribute birthdate. It outputs a value in NUMBER format.
</p>
<p>Once the object which is to hold the function has been created, then the function
itself can be defined. Note that in the following example the return value has to be
divided by 365 or it will return the persons age in days:</p>
<p/>
</div>
<div class="page"><p/>
<p>178 7 Object and Object Relational Databases
</p>
<p>CREATE OR REPLACE type body newperson_ty as
</p>
<p>MEMBER FUNCTION age(birthdate in DATE) RETURN
</p>
<p>NUMBER IS
</p>
<p>BEGIN
</p>
<p>RETURN ROUND(SYSDATE - BirthDate)/365;
</p>
<p>END;
</p>
<p>END;
</p>
<p>Assuming we have created a table called newperson and inserted one record with
a birthdate of 25th February 1983 and that todays date (SYSDATE) is 19 January
2013:
</p>
<p>SELECT p.person.age (p.person.birthdate) AGE IN YEARS
</p>
<p>FROM newperson p
</p>
<p>WHERE p.lastname = &lsquo;Felsky&rsquo;;
</p>
<p>would result in the output of:
</p>
<p>AGE IN YEARS
</p>
<p>-------------------------------------
</p>
<p>30
</p>
<p>Because age is a function it can be selected directly like an attribute, but the birth-
date must be specified as the input parameter.
</p>
<p>7.13 Polymorphism
</p>
<p>When using structures involving hierarchies and inheritance it is possible to imple-
ment ad-hoc polymorphism using function and method overloading. This allows a
method in a sub type to override a method in a super type. For example if we had
created the shop_stock_objtyp with functions:
</p>
<p>CREATE OR REPLACE TYPE shop_stock_objtyp AS OBJECT
</p>
<p>(Barcode NUMBER (13),
</p>
<p>Description VARCHAR2(50),
</p>
<p>Price NUMBER(7,2),
</p>
<p>Number_In_Stock NUMBER(6)
</p>
<p>MEMBER FUNCTION vat() RETURN NUMBER,
</p>
<p>MEMBER FUNCTION printme() return VARCHAR2) )
</p>
<p>NOT FINAL;
</p>
<p>and we create the card subclass:</p>
<p/>
</div>
<div class="page"><p/>
<p>7.14 Support for Object Oriented and Object Relational Database 179
</p>
<p>CREATE OR REPLACE TYPE card_type_objtyp
</p>
<p>UNDER shop_stock_objtyp
</p>
<p>(Printer VARCHAR2(30),
</p>
<p>Type VARCHAR2(50),
</p>
<p>Reorder_level NUMBER(16)
</p>
<p>MEMBER FUNCTION
</p>
<p>number_before_reorder(number_in _stock, re_order_level)
</p>
<p>RETURN NUMBER,
</p>
<p>OVERRIDING MEMBER FUNCTION printme() RETURN
</p>
<p>VARCHAR2)
</p>
<p>FINAL;
</p>
<p>You would have to write the printme() function first as you can&rsquo;t override some-
thing that does not exist. Assuming you have created the printme() function, the
number_before_reorder function would look like:
</p>
<p>CREATE OR REPLACE type body card_type_objtyp as
</p>
<p>MEMBER FUNCTION number_before_reorder(number_in _stock,
</p>
<p>re_order_level) RETURN NUMBER IS
</p>
<p>BEGIN
</p>
<p>RETURN (number_in _stock - re_order_level);
</p>
<p>END;
</p>
<p>END;
</p>
<p>On execution, whenever the printme function is called from the superclass the over-
riding function number_before_reorder will execute whenever a card type object
is retrieved.
</p>
<p>7.14 Support for Object Oriented and Object Relational
Database Development
</p>
<p>The most common support is in the form of UML (unified modeling language)
toolkits. Many of the available UML toolkits claim to have database modeling capa-
bilities. Generally these do not directly link to databases, an exception being Oracles
J Developer. They do however provide a useful modeling tools to help with the de-
sign of a database. Arguably the two most useful are:
Use Case: A use case diagram shows how a user will interact with s system. This
</p>
<p>tool can be used to model what the data requirements are for the system and can
even be used to define the requirements for the user interface (Fig. 7.3).
</p>
<p>Class Diagram: Figure 7.2 is a class diagram and shows the components of a
class and the links between classes.
</p>
<p>Oracle has introduced JDeveloper which includes UML like tools to develop
databases. These include a use case modeler to map user interactions and a class</p>
<p/>
</div>
<div class="page"><p/>
<p>180 7 Object and Object Relational Databases
</p>
<p>Fig. 7.3 Use Case diagram
</p>
<p>modeling like tool which can dynamically change the structure of underlying tables.
It is supports java application development and is offered as an alternative to Ora-
cles PL/SQL centric development tools. Oracle offer this tool as a free add on to its
database management system (see http://www.oracle.com/technetwork/developer-
tools/jdev/overview/index.html last accessed 26/07/2013).
</p>
<p>7.15 Will Object Technology Ever Become Predominant
in Database Systems?
</p>
<p>Object-oriented capabilities have in effect turned the traditional relational model and
the traditional view of normalization on its head in terms of design. Despite this, the
relational principles of having entities which are complete and encapsulated remain.
We are now adding methods to that encapsulation.
</p>
<p>One of the biggest issues is the complexity of the SQL needed to manipulate
object relational data despite the structure of the data being simplified. In object
relational systems it is made more complex by having to store all structures in tables
as ultimately data is still stored in tables rather than as true object instances.
</p>
<p>A further issue is the number of systems which have been developed using rela-
tional database management systems. The cost of converting these would be high as
would the retraining of database developers and administrators.
</p>
<p>The most likely scenario is there will be a gradual shift from the relational to
the object-oriented model. This is already happening in Oracle with object rela-
tional features becoming more common. It may be boosted by the need to store
non-alphanumeric objects such as images and sound files.</p>
<p/>
<div class="annotation"><a href="http://www.oracle.com/technetwork/developer-tools/jdev/overview/index.html">http://www.oracle.com/technetwork/developer-tools/jdev/overview/index.html</a></div>
<div class="annotation"><a href="http://www.oracle.com/technetwork/developer-tools/jdev/overview/index.html">http://www.oracle.com/technetwork/developer-tools/jdev/overview/index.html</a></div>
</div>
<div class="page"><p/>
<p>7.15 Will Object Technology Ever Become Predominant in Database 181
</p>
<p>7.15.1 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What is the difference between and object class and an object instance?
&bull; What does an object class define?
&bull; What is meant by persistent data?
&bull; What is an object identifier and what is it used for?
&bull; What is polymorphism?
</p>
<p>7.15.2 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Activity 1 If you have not already done so, create the objects and tables using the
examples in the text. Then populate and extend the example by reference to Fig. 7.2
and completing the following exercise:
</p>
<p>Add two more records to the Shop_Customer table and four more records to the
Purchase Order table. Don&rsquo;t forget that a Purchase Order must have an associated
customer first, but a customer can have more than one Purchase Order.
</p>
<p>The line_Item class in the original design is an association class. To serve its pur-
pose in an ORDBMS it needs to be an object with foreign key links to the Purchase
Order and the Shop Stock tables.
&bull; Create the user defined type Line_Item_objtyp.
&bull; Create a table to hold Line_Item_objtyp. This has two foreign keys, PONo to
</p>
<p>reference Purchase Order and barcode to reference the Barcode in Shop Stock.
Use the LineItemNo as the primary key as trying to use PONo and barcode as
a combined key will cause problems because they are defined with REF.)
</p>
<p>&bull; Populate line_Item_objtyp with at least 2 records for each purchase order.
&bull; List all stock items.
&bull; For purchase order 1001, list the shipping address and line items.
&bull; Modify the previous exercise by adding stock item description.
&bull; Create a new customer.
&bull; Create a purchase order for the customer with one line item.
&bull; Modify the customer&rsquo;s name.
&bull; List the customers name and associated Purchase Order number(s).
&bull; Delete the customer.
Define a function that calculates the number of items left in stock after a line item
has been added (Number in Stock &ndash; Qty). Store the function in Line_Item even
though you require data from Shop_Stock.</p>
<p/>
</div>
<div class="page"><p/>
<p>182 7 Object and Object Relational Databases
</p>
<p>Activity 2 Refer to the case study at the end of Chap. 4.
If you have not already done so, download one of the free UML tools available.
</p>
<p>1. Draw a class diagram for the scenario.
Include both attributes and operations.
Include relationships including hierarchies and aggregation.
</p>
<p>2. Create the database.
</p>
<p>References
</p>
<p>Atkinson M, Bancilhon F, DeWitt D, Dittrich K, Maier D, Zdonik S (1992) The object-oriented
database system manifesto. In: Building an object-oriented database system. Morgan Kauf-
mann, San Mateo
</p>
<p>Obe R, Hsu L (2012) PostgreSQL: up and running. O&rsquo;Reilly, Sebastopol. ISBN 1-4493-2633-1
ODMG OQL (1998) User manual release 5.0. Available at http://www.csd.uwo.ca/courses/
</p>
<p>CS4411b/pdfO2manuals/oql.pdf. Last accessed 29/04/2013.
</p>
<p>Further Reading
</p>
<p>Oracle Corporation (2008) A sample application using object-relational features. Available
at http://docs.oracle.com/cd/B28359_01/appdev.111/b28371/adobjxmp.htm#BABCCIBC. Last
accessed 07/12/2012
</p>
<p>Visual paradigm for UML community edition. http://www.visual-paradigm.com/solution/
freeumltool/. Last accessed 29/04/2013</p>
<p/>
<div class="annotation"><a href="http://www.csd.uwo.ca/courses/CS4411b/pdfO2manuals/oql.pdf">http://www.csd.uwo.ca/courses/CS4411b/pdfO2manuals/oql.pdf</a></div>
<div class="annotation"><a href="http://www.csd.uwo.ca/courses/CS4411b/pdfO2manuals/oql.pdf">http://www.csd.uwo.ca/courses/CS4411b/pdfO2manuals/oql.pdf</a></div>
<div class="annotation"><a href="http://docs.oracle.com/cd/B28359_01/appdev.111/b28371/adobjxmp.htm#BABCCIBC">http://docs.oracle.com/cd/B28359_01/appdev.111/b28371/adobjxmp.htm#BABCCIBC</a></div>
<div class="annotation"><a href="http://www.visual-paradigm.com/solution/freeumltool/">http://www.visual-paradigm.com/solution/freeumltool/</a></div>
<div class="annotation"><a href="http://www.visual-paradigm.com/solution/freeumltool/">http://www.visual-paradigm.com/solution/freeumltool/</a></div>
</div>
<div class="page"><p/>
<p>8In-Memory Databases
</p>
<p>What the reader will learn:
&bull; The origins of in-memory databases
&bull; The advantages and disadvantages of in-memory databases
&bull; Different implementations of in-memory databases
&bull; The type of applications suited to in-memory databases
&bull; The use of personal computers with in-memory databases
</p>
<p>8.1 Introduction
</p>
<p>Disk based database technology has influenced database design since the incep-
tion of electronic databases. One of the issues with disk based media is that the
physical design of systems tries to speed up processing by reducing disk access, in
other words disk input/output (I/O) is a limiting factor which needs to be optimised.
In-memory databases have been described as a disruptive technology or disruptive
tipping point because it provides a significant improvement in performance and use
of system resources.
</p>
<p>In-memory databases systems are database management systems where the data
is stored entirely in main memory. There are several competing technologies that
implement this. For example, Oracle&rsquo;s TimesTen system is effectively a relational
system loaded into memory. Another other big player in the field is SAP with its
HANA database which offers column-based storage. In contrast, Starcounter is a
OLTP (On Line Transaction Processing) transaction database using its own propri-
etary object oriented data manipulation language based around NewSQL. A com-
parison between these technologies will be made later in the chapter.
</p>
<p>8.2 Origins
</p>
<p>In early computers, memory was always the most expensive hardware component.
Until the mid-1970&rsquo;s magnetic core memory was the dominant memory technology.
It was made of magnetised rings (cores) that could be magnetised in one of two
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_8,
&copy; Springer-Verlag London 2013
</p>
<p>183</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_8">http://dx.doi.org/10.1007/978-1-4471-5601-7_8</a></div>
</div>
<div class="page"><p/>
<p>184 8 In-Memory Databases
</p>
<p>directions by four wires that passed through the centre of them forming a grid. Two
wires controlled the polarity and the others were sensors. This allowed binary repre-
sentation of data. It&rsquo;s one advantage was that it was non-volatile&mdash;when the power
went off the contents of memory was not lost.
</p>
<p>Core memory was however expensive and bulky. It also meant that programs
were written in such a way as to optimise memory usage. One way of doing this
was to use virtual memory where data was swapped in and out of memory and onto
disk storage. This optimised memory usage but degraded overall performance.
</p>
<p>From the mid-1970&rsquo;s memory started to get cheaper and faster. For example in
2001 the maximum capacity of memory was 256 megabytes. By 2012 that had risen
to 16 gigabytes, a 64 fold increase. Cost on the other hand dropped from 0.2 US
dollars a megabyte to just 0.009 US dollars per megabyte. But by far the biggest
change was in speed where in 2002 response time using hard disk drives was 5
milliseconds, by 2012 using in-memory technology, speed had increased to 100
nanoseconds, a 50,000 fold increase.
</p>
<p>The last hurdle to overcome was the D&mdash;the durability of ACID (atomicity, con-
sistency, isolation and durability) issue of memory. ACID is discussed in more de-
tail in Chap. 2. Early semi-conductor memory had been volatile so when power was
lost, so was all data stored in memory. NVDIMM (Non-Volatile Dual In-line Mem-
ory Module) solved that problem; initially by battery backup (BBUDIMM) then by
the use of super capacitors for power backup.
</p>
<p>This increase in speed along with a falling cost and non-volatility lead to a greater
interest in in-memory databases from the mid-2000&rsquo;s. Preimesberger (2013) coins
the term &lsquo;data half-life&rsquo; referring to data being more valuable in real time and dimin-
ishing in value over time. Therefore speed of processing giving real time insights
which can be used immediately is important. In-memory databases therefore allow
real-time online analytical processing (OLAP) analysis on data from online trans-
action processing (OLTP).
</p>
<p>It should be noted that an influential paper by Garcia-Molina and Salem (1992)
was already talking about the advantages of in-memory databases and the issues
in implementing them. The issues raised in this paper seem to have influenced the
direction of in-memory databases from that time.
</p>
<p>8.3 Online Transaction Processing Versus Online Analytical
Processing
</p>
<p>One of the issues with management information systems is the various components
are often at odds with each other. Business processing for example has different re-
quirements to analytics. As a result it is common for there to be at least two separate
systems. The first, usually based around a relational database was for operational
systems with lots of update transactions. The second was based around archived
data in a data warehouse. Transactions were added to a data warehouse, often with
data from other sources. There were inserts, but no updates of records. The database
part was usually associated with online transaction processing (OLTP) while the
data warehouse was associated with by online analytical processing (OLAP).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.4 Interim Solution&mdash;Create a RAM Disk 185
</p>
<p>In OLTP The data storage tends to be record or row based. Records are stored
in blocks which can be cached in main memory. Sophisticated indexing then gives
rapid access to individual records. The system slows if lots of records are required.
By contrast, OLAP tends to use a column based approach and often works on data
sets stored in a data warehouse. Optimisation, where attributes are converted to inte-
gers allows rapid processing, but the data is archival rather than operational. Chap-
ter 4 discusses column based approaches and NoSQL in more detail. Plattner (2009)
goes further and suggests data warehouses were a compromise with the flexibility
and speed paid for by the additional resources needed for loading data, extracting
data and controlling redundancy.
</p>
<p>Interestingly OLTP was a prerequisite for developing OLAP, however OLAP
provides the data necessary to understand the business and set strategic direction.
Plattner argues that although data warehouses allow integration of data from many
sources, integration of OLAP and OLTP into a single system has the capability of
making both components more valuable for real time decision making.
</p>
<p>8.4 Interim Solution&mdash;Create a RAMDisk
</p>
<p>Placing an entire on disk database into memory in the form of a RAM disk where the
database management software is treating memory as if it were a disk drive is at best
an interim solution to creating an in-memory database. This will speed up reads and
writes, but the systems still &lsquo;thinks&rsquo; the data is going to a disk drive and as such will
still operate caching and file input output services even though they are not required.
This is because the optimisation strategy of a disk based database is diametrically
opposed to an in-memory one. Disk based systems try and reduce the amount of
time consuming I/O operations. This is where the primary resource overhead is lo-
cated with data moved to numerous locations as it is used The trade-off is using
memory for cache which in turn uses CPU cycles to maintain. There is also a lot
of redundant data held in index structures which allow data to be directly retrieved
from the indexes rather than doing a disk I/O. The time saved therefore, is down to
eliminating the mechanical latency of the disk drive rather than any improvement in
data access strategies.
</p>
<p>Relational databases which are specifically designed to run in memory elimi-
nate the multiple data transfers and strip out redundant caching and buffering. This
reduces memory consumption and simplifies processing resulting in reduced CPU
demands.
</p>
<p>An experiment run by McObject, admittedly using their own in-memory database
management systems, showed the dramatic differences in speed between a disk
based, a RAM based and an in-memory database system:
</p>
<p>&lsquo;We tested an application performing the same tasks with three storage scenar-
ios: using an on-disk DBMS with a hard drive; the same on-disk DBMS with a
RAM-disk; and an IMDS (McObject&rsquo;s eXtremeDB). Moving the on-disk database
to a RAM drive resulted in nearly 4&times; improvement in database reads, and more
than 3&times; improvement in writes. But the IMDS (using main memory for storage)</p>
<p/>
</div>
<div class="page"><p/>
<p>186 8 In-Memory Databases
</p>
<p>outperformed the RAM-disk database by 4&times; for reads and 420x for writes.&rsquo; (Steve
Graves, co-founder and CEO of McObject available on line at ODBMS Industry
Watch (2012) http://www.odbms.org/blog/2012/03/in-memory-database-systems-
interview-with-steve-graves-mcobject/ last accessed 29/06/2013).
</p>
<p>8.5 Interim Solution&mdash;Solid State Drive (SSD)
</p>
<p>In many ways this is another type of RAM disk but the technology is different and
there are reliability issues after a period of operation. Removable flash memory
drives became available in 1995 and after an initially slow take up rate, these de-
vices have become the main type of removal storage media for personal computing.
They are fast, reliable and have a high capacity which means you can store an entire
database on them. Their advantage over traditional disk drives is they have no mov-
ing parts so the issues of seek and latency times are removed. They are not without
limitations as the following section will illustrate.
</p>
<p>SSD&rsquo;s have a number of components: a processor, cache and NAND chips.
NAND chips are named after NAND logic gates (the other type is NOR) and are
random access memory chips (NOR chips are used for read only memory applica-
tions). In a flash drives the NAND chips are multi-layer chips (MLC) which give a
greater capacity for the same area as a single layer NAND.
</p>
<p>One disadvantage of SSDs, specifically the NAND chips is they do wear out.
This is because they have individually erasable segments, each of which can be
put through a limited number of erase cycles before becoming unreliable. This is
usually around 3,000 to 5,000 cycles. To reduce the risk of uneven wear where
some segments are used more heavily than others a process known as wear levelling
is used. This involves arranging data so that erasures and re-writes are distributed
evenly across the whole chip. It also means that everything wears out at the same
time.
</p>
<p>Georglev (2013) gives a summary of the issues involved in using SSDs compared
with traditional HDD technology:
</p>
<p>Fragmentation This is still an issues with SSD&rsquo;s despite access to all cells being
equally fast. This is because if indexes are fragmented they require more reads than
for one which is not. Fragmentation also reduces available disk capacity.
</p>
<p>Access Speed There is still a difference between random and sequential reads
and writes which also depend on block sizes. They are still faster than HDD&rsquo;s but
performance diminishes with writes and particularly updates.
</p>
<p>Speed vs. Capacity Fragmentation and wear (segments become unreliable) lower
capacity, but speed is still greater than with a HDD. The biggest issue is when does
the SSD become unusable. Wear levelling means once one segment becomes unreli-
able, the rest of the segments will soon become unreliable as well. The time it takes
this to happen will depend on the number of update operations being performed&mdash;
the more operations, the sooner the device will fail.</p>
<p/>
<div class="annotation"><a href="http://www.odbms.org/blog/2012/03/in-memory-database-systems-interview-with-steve-graves-mcobject/">http://www.odbms.org/blog/2012/03/in-memory-database-systems-interview-with-steve-graves-mcobject/</a></div>
<div class="annotation"><a href="http://www.odbms.org/blog/2012/03/in-memory-database-systems-interview-with-steve-graves-mcobject/">http://www.odbms.org/blog/2012/03/in-memory-database-systems-interview-with-steve-graves-mcobject/</a></div>
</div>
<div class="page"><p/>
<p>8.6 In-Memory Databases&mdash;Some Misconceptions 187
</p>
<p>Caching data is held in cache as long as possible to reduce wear, in other words
attempts are made to do as many updates as possible before writing to the NAND.
The danger here is reliability may be compromised in the event of a power failure
as cache tends to be volatile.
</p>
<p>Data Most Suitable for SSDs Updates are the biggest issue for SSDs because
when data is updated the entire block where the data resides has to be moved to a
new block with the update, then the old block has to be erased. This causes both
wear and gives a performance overhead. Given this situation SSD are best for static
data where there many reads but few updates.
</p>
<p>Data Recovery When a HDD fails there are many ways to extract most of the
data depending on the nature of the failure. With an SSD once its dead, its dead&mdash;it
is almost impossible to recover any data. It should also be noted that an SSD has a
shelf life of about seven years. This means if it is not powered up for a significant
period of time data loss should be expected.
</p>
<p>The bottom line is that SSDs are useful as a backup and data transportation
medium (as long as you take into account the data recovery issues). They are not an
ideal replacement for disk based databases when lots of transactions involving up-
dates are expected as their life cycle will be curtailed. If you are planning on using
this technology you should read the technical specifications relating to the device
and ask how many erase cycles can a segment sustain.
</p>
<p>8.6 In-Memory Databases&mdash;SomeMisconceptions
</p>
<p>Populating the Database The first misconception is populating a very large in-
memory database is a slower process than populating an equivalent database on
disk. On disk systems use memory caches to speed up the process, but ultimately
the cache becomes full and must be written to disk. This requires moving data from
cache to I/O buffers. From there data is physically written to disk and this is rel-
atively slow compared with other processes. As well as writing data to disk in-
dexes mapping the data&rsquo;s storage location are developed. These grow bigger and
more complex as more data is added to the system. Also as the size of the database
grows the amount of data that can be held in cache memory becomes a smaller and
smaller percentage of the total database further reducing performance. Finally as
the database grows the physical size on the disk gets bigger resulting in higher seek
times. Basically as the size of the database grows there is a continual degradation of
performance.
</p>
<p>By contrast the performance of an in-memory database remains roughly constant
as more and more data is added because none of the on disk issues apply.
</p>
<p>Single User, Single System In memory databases are not restricted to being single
user. The database can be held in shared memory with the database management sys-
tem handling concurrent access requests. They can also operate with remote servers</p>
<p/>
</div>
<div class="page"><p/>
<p>188 8 In-Memory Databases
</p>
<p>(nodes). In other words in-memory databases can be shared by multiple threads,
processes and users.
</p>
<p>An In-Memory Database Is the Same as an Embedded Database This is
not a total misconception, some in-memory databases are the same as embedded
databases. These are databases which are part of an application with the database
itself being invisible to the end user. However, as seen with the previous point, in-
memory databases can also employ the client server model with the use of shared
memory.
</p>
<p>8.7 In-Memory Relational Database&mdash;The Oracle TimesTen
Approach
</p>
<p>No one technology appears dominant in the in-memory database sphere. There are
however several contrasting approaches. The first to be discussed is implementing
what is essentially a relational database in-memory. Oracles TimesTen is an exam-
ple of this. Simply put TimesTen is a relational database where all data resides in
memory at runtime.
</p>
<p>In a cached relational database management system a lot of processing power
is used to manage memory buffers and control multiple data locations on disk and
in memory. The in-memory approach only uses disks for persistence and recovery
rather than primary storage. This gets over the D-durability issue of ACID. Changes
from committed transactions are logged to disk and periodically a disk image of the
database is updated (in Oracle&rsquo;s terms, a checkpoint). The timing of this checkpoint
is configurable and many applications favour higher throughput over synchronous
logging, in other words the period between checkpoints is maximised to improve
performance. The trade-off is that there is a small risk that data could be lost al-
though this is unlikely to be due to power failure if NVDIMM is used and more
likely to be to a catastrophic hardware failure. Transactions are also logged asyn-
chronously which means if the period between checkpoints is too long, there is a
danger of running out of disk space. When a checkpoint is executed, the logs are
cleared (For more information see Oracle 2006).
</p>
<p>There are five system&rsquo;s components of TimesTen: Shared: libraries, memory
resident data structures, system processes, administrative programs and check-
point files and log files on disk (Fig. 8.1). The memory requirements for this
are given by Oracle as PermSize + TempSize + LogBufMB + 20 MB over-
head although discussion boards suggest an overhead of 64 MB is more realistic
(see https://forums.oracle.com/thread/2547114 accessed 29/07/2013 for example).
PermSize is the size of the permanent data and includes the tables and indexes.
The permanent data partition is written to disk during checkpoint operations. Temp-
Size is the size of the temporary data and includes locks, cursors, compiled com-
mands, and other structures needed for command execution and query evaluation.
LogBufMB is The size of the internal log buffer in MB and has a default of 64 MB
(Oracle 2011).</p>
<p/>
<div class="annotation"><a href="https://forums.oracle.com/thread/2547114">https://forums.oracle.com/thread/2547114</a></div>
</div>
<div class="page"><p/>
<p>8.7 In-Memory Relational Database&mdash;The Oracle TimesTen Approach 189
</p>
<p>Fig. 8.1 Oracle TimesTen in-memory database
</p>
<p>Although TimesTen is a relational database, this refers to the data structure. Tra-
ditionally a relational database management system (RDBMS) assumes the data it
is managing is on disk at runtime. However with all data in memory the system op-
timises its management for that environment. One of the main differences is query
optimisation.
</p>
<p>In a disk based system data might be on disk or cached in main memory at any
given moment. The balance is crucial in reducing disk input/output which from a
resource point of view is expensive. However this problem goes away if all the data
is in memory. The thing to be optimised now is processing and this can be influenced
in a number of ways:
</p>
<p>Indexing The primary function of an index in a disk based system is to locate a
record on disk as quickly as possible based on an index key and record identifier. In
an in-memory system index keys do not need to be stored. Indexes are implemented
as record pointers which point to the corresponding record containing the key. In
effect the record identifier is implemented as the record pointer. Because an index
key is not stored in the index there is no duplication of key values in the index
structure which reduces it size. Also pointers are all the same size so the need to
manage variable length keys goes away making index implementation simpler.</p>
<p/>
</div>
<div class="page"><p/>
<p>190 8 In-Memory Databases
</p>
<p>Query Processing A common clause in SQL is ORDER BY. In a disk based sys-
tem this is fast if the data in the table being targeted by the query is stored in the
order requested. Often, in fact usually, this is not the case. The next best option is to
use an index scan. This carries the overhead of random access to records which may
result in a high input/output cost. The problem is exasperated if sorting is required
on multiple columns. In an in-memory database, as seen in the indexing, the only
entries in the index are the record identifiers which are implemented as record point-
ers. In other words index entries point directly to the memory address where data
resides, so a random scan is no more expensive than a sequential scan, and of course
there is no disk I/O. Buffer pool management is also unnecessary because you don&rsquo;t
need to cache data in main memory because you are not doing any disk I/O.
</p>
<p>Backup and Recovery as already mentioned the primary means of backing up a
TimesTen database is by checkpoints where a snapshot is taken of the permanent
data that includes the tables and indexes. These are used with the on disk log files to
recreate the database in the event of a failure.
</p>
<p>8.8 In-Memory Column Based Storage&mdash;The SAP HANA
Approach
</p>
<p>Like TimesTen, Hana incorporates a full database management system with a stan-
dard SQL interface and ACID. It is geared towards existing SAP users in that appli-
cations using its proprietary version of SQL (Open SQL) can run on the SAP HANA
platform.
</p>
<p>SAP HANA exploits parallel multicore processors, so if a native SQL command
is received it is optimised to allow parallel execution by partitioning the data into
sections. This scales with the number of available core processors. For &lsquo;Big Data&rsquo;,
this can be partitioned across multiple hosts.
</p>
<p>The libraries shown in Fig. 8.2 contain business applications, for example cur-
rency conversion and converting business calendars from different countries. These
can be used for processing directly in main memory rather than the traditional way
for utilising plain SQL.
</p>
<p>HANA optimises memory usage by data compression and by adapting the data
store for the task. For applications that are processed row by row, records are placed
in sequence for the best performance. If the application is calculation intensive exe-
cuting on a restricted number of columns, then these are aggregated into a column
store. Finally, graphical objects are created by specifying minimal database schema
information, such as an edge store name, a vertex store name, and a vertex identifier
description (where a vertex is a description of a set of attributes describing a point
on a graph). The object body is stored in sequence and the graph navigation based
on the schema information is stored as another sequence to support unstructured
and semi structured data storage and retrieval. For more information on graphical
objects in SAP HANA see Rudolf et al. (2013).</p>
<p/>
</div>
<div class="page"><p/>
<p>8.8 In-Memory Column Based Storage&mdash;The SAP HANA Approach 191
</p>
<p>Fig. 8.2 SAP HANA In-memory database
</p>
<p>Parallel Execution In SAP HANA sequential processing is avoided. Key to this
is aggregation where values such as maximum, minimum, average and mode are
required. Aggregation is performed by creating a number of threads that act in par-
allel. This is done by creating a aggregation function which creates threads that fetch
a small partition of the input table which is then processed (for example to find the
maximum). The process continues until the whole table is aggregated. Each thread
has a hash table where it writes its aggregation results and when the threads are
finished the buffered hash tables are merged.
</p>
<p>Column Versus Row-Based Storage As mentioned you can specify whether to
use column or row storage. Row based storage is recommended when:
&bull; The table has a small number of rows
&bull; Records are processed one at a time
&bull; The complete record is needed
&bull; Column values are mainly distinct, so compression rates would be low
&bull; Aggregations and fast searching are not required
Column based storage is recommended when:
&bull; Calculations are executed on a restricted number of columns
&bull; Only a few columns are used as the basis of searches
&bull; The table has a large number of columns
&bull; There are a large number of records and columnar operations are required
&bull; The majority of columns contain only a few distinct values (compared to the
</p>
<p>number of rows) which means a higher compression rate is possible.
Row based tables can be joined with column based tables although master data that
is often joined with transaction data (the database versus data warehouse issue) is
put in a column store.</p>
<p/>
</div>
<div class="page"><p/>
<p>192 8 In-Memory Databases
</p>
<p>The advantages and operation of column based databases are discussed in detail
in Chap. 5 and will not be repeated here.
</p>
<p>SAP claim increased benefits for using in-memory databases:
</p>
<p>&ldquo;In financial applications, different kinds of totals and balances are typically persisted as
materialized aggregates for the different ledgers: general ledger, accounts payable, accounts
receivable, cash ledger, material ledger, and so on. With an in-memory column store, these
materialized aggregates can be eliminated as all totals and balances can be computed on the
fly with high performance from accounting document items.&rdquo; (SAP 2013b)
</p>
<p>8.9 In-Memory On-line Transaction Processing&mdash;The
Starcounter Approach
</p>
<p>So far we have looked at two of the biggest players in the in-memory database field
both of which have taken different philosophies to their implementation. Here we
will look at one of the smaller players which have taken a very different approach
to either of them.
</p>
<p>Starcounter is an in-memory object oriented database management system which
is focussed on OLTP (Online Transaction Processing). It is designed for &ldquo;highly
transactional large-scale and real-time systems, systems supporting thousands or
</p>
<p>millions of simultaneous users, such as retail systems, adserve applications, online
</p>
<p>stores and finance applications&rdquo; (http://www.crunchbase.com/product/starcounter-
in-memory-database accessed 29/06/2013).
</p>
<p>A key feature of Starcounter is it integrates an application run time virtual ma-
chine with the database, a product they call VMDBMS. Effectively this allows the
database data to reside in a single place in RAM and not get copied back and forth
between the application and the database. This has been likened to effectively in-
tegrating the application with the database management system where you transfer
code to the database image instead of data from the database image to the code. The
downside of this is the database needs to understand the code of the application so
is not universally language compatible.
</p>
<p>Like Oracle&rsquo;s TimesTen product Starcounter only writes logs to disk. Reads
only happen if and when there is a need to recover the database. Users can query
the database using SQL, however Starcounter have created their own proprietary
object-oriented data manipulation language called NewSQL (not to be confused
with NoSQL discussed in Chap. 4). This is SQL like (in fact it is claimed to adhere
to SQL 92 standard) and is embedded in java code, for example:
</p>
<p>string query = &ldquo;SELECT e FROM
Employee e WHERE e.FirstName = ?&rdquo;;
Employee emp = Db.SQL(query, &ldquo;Warren&rdquo;).First
emp.PrintCV();
</p>
<p>At the time of writing the biggest user of Starcounter is Gekas, a superstore in Swe-
den with 100,000 different product lines visited by 4.5 million customers each year
(Reuters, May 15, 2013, http://www.reuters.com/article/2013/05/15/starcounter-
idUSnBw155295a+100+BSW20130515 accessed 29/06/2013).</p>
<p/>
<div class="annotation"><a href="http://www.crunchbase.com/product/starcounter-in-memory-database">http://www.crunchbase.com/product/starcounter-in-memory-database</a></div>
<div class="annotation"><a href="http://www.crunchbase.com/product/starcounter-in-memory-database">http://www.crunchbase.com/product/starcounter-in-memory-database</a></div>
<div class="annotation"><a href="http://www.reuters.com/article/2013/05/15/starcounter-idUSnBw155295a+100+BSW20130515">http://www.reuters.com/article/2013/05/15/starcounter-idUSnBw155295a+100+BSW20130515</a></div>
<div class="annotation"><a href="http://www.reuters.com/article/2013/05/15/starcounter-idUSnBw155295a+100+BSW20130515">http://www.reuters.com/article/2013/05/15/starcounter-idUSnBw155295a+100+BSW20130515</a></div>
</div>
<div class="page"><p/>
<p>8.10 Applications Suited to In-Memory Databases 193
</p>
<p>8.10 Applications Suited to In-Memory Databases
</p>
<p>From the previous discussion it will come as no surprise that systems that need to
do OLAP and OLTP simultaneously are those most suited to in-memory database
deployment. As an example Savvis, an IT hosting and colocation service provider
uses another of Oracles in-memory products, Exalytics. Here a broad range of users
in the organisation need to view a variety of complex data sets to extract data for
their specific requirements. This ranges from customer service staff who need to
know what products a customer has purchased, service requests in progress and sat-
isfaction survey feedback through to senior management who need financial data
for strategic planning. The data all groups use has to be current from the operational
database. The analytics are provided by a combination of visualisation and filtering
of real-time data. The most effective way to provide this without building parallel
systems is with an in-memory database. This particular application also allows var-
ious scenarios to be tested (so called what-if scenarios) using the operational data.
</p>
<p>As a contrasting example Bigpoint GmbH, an on-line game developer uses SAP
HANA to analyse the behaviour of players using its Battlestar Galactica online
game. They use the in-memory database for analytics for targeted marketing of add-
ons to the game and to sell virtual items to players in real time.
</p>
<p>If we look at different types of applications, a study by King (2011) found in-
memory database usage by organisations were as follows:
&bull; Business Analytics 42 %
&bull; Web-based transactions 38 %
&bull; Reporting 35 %
&bull; Finance (trading, market data, etc.) 32 %
&bull; Billing and Provisioning 25 %
&bull; Embedded/Mobile applications 18 %
Obviously most companies are implementing more than one as the survey asked
respondents to &lsquo;tick all that apply&rsquo;. Other applications included supply chain man-
agement, sensor data management, geo-spatial applications, network information
management, product information management, online advertising and materials
management. This is a wide range of applications with the common thread of need-
ing a database capable of processing data in real-time and where a traditional disk
based system was not fast enough.
</p>
<p>In terms of growth, 77 % of organisations said their data requirements were ex-
pected to grow, with only 2 % expecting shrinkage. Likewise 68 % of surveyed or-
ganisations were expecting their use of in-memory databases to increase with only
9 % expecting a decrease. The remaining 23 % did not know. It is unfortunate that
King did not get the organisations to say which sector they belonged to.
</p>
<p>8.11 In Memory Databases and Personal Computers
</p>
<p>The problem with in-memory databases for personal computers is not so much the
fact it can&rsquo;t be done (it can) but rather should it be done? The applications which
have been described so far have been for corporate systems which want to be able</p>
<p/>
</div>
<div class="page"><p/>
<p>194 8 In-Memory Databases
</p>
<p>to do analytics on real-time data with multiple simultaneous users. From this stand-
point the main usage of a personal computer would be to access an in-memory
database located on a server rather than having its own in-memory database.
</p>
<p>The place where in-memory database technology is most likely to be found on
personal computers is in the form of embedded databases. You will recall from the
definition of an embedded database that these are databases you don&rsquo;t see, because
they are hidden inside another application managing that applications data. They
are typically single application databases that do not share their data with other
applications. Some examples are as follows:
</p>
<p>Games Systems The technical challenge with persistent games, particular mas-
sively multiplayer online (MMO) games is transaction management. Every move
and interaction is a transaction. In single user games data corruption can be solved
by restarting the session. Persistent games, on the other hand, must react by rolling
back incomplete or failed transactions. These manage their view of the games world
within a database. Early versions attempted this via a disk based solution, however
disk resident databases have difficulties handling the transaction load required in
MMO&rsquo;s. Games vendors also exploit the data streams being generated by MMO&rsquo;s
as our earlier example of Bigpoint GmbH showed.
</p>
<p>Route Delivery Management The database in this application is used to provide
delivery drivers with the best route to the next delivery. Basically it is the database
behind most satnav systems. As an example MJC2 (see http://www.mjc2.com/real-
time-vehicle-scheduling.htm last accessed 29/07/2013) produces planning and
scheduling software, part of which is real-time routing and scheduling for logistics.
Their dynamic route planner software is able to schedule and reschedule very large
distribution and transport operations in real-time, responding automatically to live
data feeds from order databases, vehicle tracking systems, telematics and fleet man-
agement systems. The system requires an in-memory solution to allow managers
to respond dynamically to a continuously changing operations including optimal
responses to variable demand, last minute orders, cancellations and redirections.
</p>
<p>Gas and Electricity Consumption Reading This is a sector specific application
where the database is used to capture meter readings, often remotely. For exam-
ple, Australian energy retailer, AGL has adopted an in-memory computing to help it
cope with the vast amount of data coming from smart meters. Smart meters measure
energy usage at regular intervals, for example every 30 minutes, and can tell how
much electricity is consumed during on and off-peak periods. Instead of one simple
reading every three months the meters now provide continuous data streams gener-
ating around 4400 readings per quarter instead of the traditional 1. As well as still
generating customer bills, the data streams are analysed to separate out base meter
data from other data for use in forecasting. The system used is SAP HANA.
</p>
<p>Mobile Customer Resource Management (CRM) Marketing and sales need
customer resource management applications without wanting to worry about the
database driving it. This often relies on replication of a segment of the master</p>
<p/>
<div class="annotation"><a href="http://www.mjc2.com/real-time-vehicle-scheduling.htm">http://www.mjc2.com/real-time-vehicle-scheduling.htm</a></div>
<div class="annotation"><a href="http://www.mjc2.com/real-time-vehicle-scheduling.htm">http://www.mjc2.com/real-time-vehicle-scheduling.htm</a></div>
</div>
<div class="page"><p/>
<p>8.12 Summary 195
</p>
<p>database. It may also be integrated with a satnav application similar to the route
delivery management system described above. SAP have developed their CRM sys-
tem to be deployed on SAP HANA to provide sales people with real-time customer
information throughout the sales cycle on a mobile device.
</p>
<p>8.12 Summary
</p>
<p>Attaining the fastest speed possible has always been the goal of database applica-
tions. Unfortunately optimisation efforts in the past have been targeted at disk based
systems which have led to duplicating data. One version of data has been placed
in a database which is used for operational OLTP systems and the other is placed
in data warehouses for applications associated with OLAP. This is because OLTP
is I/O heavy while OLAP is computationally heavy and requires diametrically op-
posite optimisation routines. In this chapter we have seen that in-memory database
applications have the capability of combing both requirements in a single system.
</p>
<p>Common misconceptions of slow population of in-memory databases and them
only being available for single user systems were discussed and dismissed. However
there is no one standard approach to their implementation which ranged from Ora-
cles&rsquo; TimesTen which was an in memory relational system, SAP HANA which used
column based storage through to new players exemplified by Starcounter which used
an object oriented approach effectively integrating an application with the database
management system.
</p>
<p>8.13 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What is NVDIMM and why is it important in the context of in-memory
</p>
<p>databases?
&bull; What are three misconceptions about in-memory databases?
&bull; What are the performance increases an in-memory database can give compared
</p>
<p>to a on disk database?
&bull; What approach to implementation of an in-memory database has Oracle taken
</p>
<p>with its TimesTen product?
&bull; How do SAP HANA and Starcounter differ from Oracle TimesTen approach to
</p>
<p>in-memory database implementation?
</p>
<p>8.14 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.</p>
<p/>
</div>
<div class="page"><p/>
<p>196 8 In-Memory Databases
</p>
<p>Activity 1 Wikipedia in its definition of &lsquo;embedded databases&rsquo; seems to have fallen
into the trap of considering in-memory databases as being the same thing. Go to
http://en.wikipedia.org/wiki/Embedded_database where you will see a list headed
&lsquo;Comparisons of database storage engines&rsquo;. Examine the list and determine which
are purely embedded (single user) and those which have client/server capabilities.
</p>
<p>Activity 2 Look in your wallet or purse and see how many plastic cards you have.
What are each of them for? (you may have bank cash cards, phone cards, loyalty
cards, membership cards, student card, a driving licence among others). Which of
these, when you use them, could be providing data for an OLTP or a OLAP. Make
a list of 5 columns headed Card, OLTP, OLAP, Both and Neither and fill it in with
an X in the appropriate part of the grid. For those with an X in OLTP, OLAP and
both, work out what is happening to your data. For those labelled both&mdash;do you
think there is a in-memory database behind the processing? (One pointer is do you
get targeted advertising before you complete your transaction)
</p>
<p>References
</p>
<p>Garcia-Molina H, Salem K (1992) Main memory database systems: an overview. IEEE Trans
Knowl Data Eng 4(6):509&ndash;516
</p>
<p>Georglev F (2013) HDDs, SSDs and database considerations. https://www.simple-talk.com/sql/
database-administration/hdds,-ssds-and-database-considerations/. Accessed 29/07/2013
</p>
<p>King E (2011) The growth and expanding application of in-memory databases. Available at
http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;
ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%
2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=
AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw. Last ac-
cessed 30/6/2013
</p>
<p>Oracle Corp (2006) TimesTen in-memory database recommended programming prac-
tices release 6.0, p 28. http://download.oracle.com/otn_hosted_doc/timesten/603/TimesTen-
Documentation/goodpractices.pdf. Accessed 24/07/2013
</p>
<p>Oracle Corp (2011) Oracle TimesTen in-memory database operations guide release 11.2.1, Part
1 Managing TimesTen databases. http://download.oracle.com/otn_hosted_doc/timesten/1121/
doc/timesten.1121/e13065/using.htm#BCGIDAJG. Accessed 29/07/2013
</p>
<p>Plattner H (2009) A common database approach for OLTP and OLAP using an in-memory
column database. In: Proceedings of SIGMOD 09 (special interest group on manage-
ment of data), Providence, Rhode Island, USA. Available at www.sigmod09.org/images/
sigmod1ktp-plattner.pdf. Last accessed 29/06/2013
</p>
<p>Preimesberger C (2013) In-memory databases driving big data efficiency: 10 reasons why. e-week,
available at http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-
data-efficiency-10-reasons-why/, last accessed 14/11/2013.
</p>
<p>Rudolf M, Paradies M, Bornh&ouml;vd C, Lehner W (2013) The graph story of the SAP HANA
database. In: Proceedings of 15th GI-symposium database systems for business, technol-
ogy and web, Madeburg, Germany. Available at http://www.btw-2013.de/proceedings/The%
20Graph%20Story%20of%20the%20SAP%20HANA%20Database.pdf. Last accessed 29/07/
2013
</p>
<p>SAP (2013b) SAP white paper: SAP HANA&reg; database for next-generation business appli-
cations and real-time analytics explore and analyze vast quantities of data from virtually
any source at the speed of thought. Available at http://download.sap.com/download.epd?</p>
<p/>
<div class="annotation"><a href="http://en.wikipedia.org/wiki/Embedded_database">http://en.wikipedia.org/wiki/Embedded_database</a></div>
<div class="annotation"><a href="https://www.simple-talk.com/sql/database-administration/hdds,-ssds-and-database-considerations/">https://www.simple-talk.com/sql/database-administration/hdds,-ssds-and-database-considerations/</a></div>
<div class="annotation"><a href="https://www.simple-talk.com/sql/database-administration/hdds,-ssds-and-database-considerations/">https://www.simple-talk.com/sql/database-administration/hdds,-ssds-and-database-considerations/</a></div>
<div class="annotation"><a href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw">http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw</a></div>
<div class="annotation"><a href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw">http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw</a></div>
<div class="annotation"><a href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw">http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw</a></div>
<div class="annotation"><a href="http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw">http://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;frm=1&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fwww.loyola.edu%2F~%2Fmedia%2Fdepartment%2Flattanze%2Fdocuments%2FWP0611-107.ashx&amp;ei=mjX2UcehMajI0AXkgIEI&amp;usg=AFQjCNH18VJL5brvz-otsfSIeIhEj1ToTA&amp;sig2=y6EKZgsDtgss7MRHyjqyXw</a></div>
<div class="annotation"><a href="http://download.oracle.com/otn_hosted_doc/timesten/603/TimesTen-Documentation/goodpractices.pdf">http://download.oracle.com/otn_hosted_doc/timesten/603/TimesTen-Documentation/goodpractices.pdf</a></div>
<div class="annotation"><a href="http://download.oracle.com/otn_hosted_doc/timesten/603/TimesTen-Documentation/goodpractices.pdf">http://download.oracle.com/otn_hosted_doc/timesten/603/TimesTen-Documentation/goodpractices.pdf</a></div>
<div class="annotation"><a href="http://download.oracle.com/otn_hosted_doc/timesten/1121/doc/timesten.1121/e13065/using.htm#BCGIDAJG">http://download.oracle.com/otn_hosted_doc/timesten/1121/doc/timesten.1121/e13065/using.htm#BCGIDAJG</a></div>
<div class="annotation"><a href="http://download.oracle.com/otn_hosted_doc/timesten/1121/doc/timesten.1121/e13065/using.htm#BCGIDAJG">http://download.oracle.com/otn_hosted_doc/timesten/1121/doc/timesten.1121/e13065/using.htm#BCGIDAJG</a></div>
<div class="annotation"><a href="http://www.sigmod09.org/images/sigmod1ktp-plattner.pdf">http://www.sigmod09.org/images/sigmod1ktp-plattner.pdf</a></div>
<div class="annotation"><a href="http://www.sigmod09.org/images/sigmod1ktp-plattner.pdf">http://www.sigmod09.org/images/sigmod1ktp-plattner.pdf</a></div>
<div class="annotation"><a href="http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-data-efficiency-10-reasons-why/">http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-data-efficiency-10-reasons-why/</a></div>
<div class="annotation"><a href="http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-data-efficiency-10-reasons-why/">http://www.eweek.com/database/slideshows/in-memory-databases-driving-big-data-efficiency-10-reasons-why/</a></div>
<div class="annotation"><a href="http://www.btw-2013.de/proceedings/The%20Graph%20Story%20of%20the%20SAP%20HANA%20Database.pdf">http://www.btw-2013.de/proceedings/The%20Graph%20Story%20of%20the%20SAP%20HANA%20Database.pdf</a></div>
<div class="annotation"><a href="http://www.btw-2013.de/proceedings/The%20Graph%20Story%20of%20the%20SAP%20HANA%20Database.pdf">http://www.btw-2013.de/proceedings/The%20Graph%20Story%20of%20the%20SAP%20HANA%20Database.pdf</a></div>
<div class="annotation"><a href="http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E">http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E</a></div>
</div>
<div class="page"><p/>
<p>References 197
</p>
<p>context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB35
0EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E. Last accessed 29/06/
2013
</p>
<p>Further Reading
</p>
<p>McObject (2013) In-memory database systems&mdash;questions and answers. Available at http://www.
mcobject.com/in_memory_database. Last accessed 29/06/2013
</p>
<p>Oracle Corp (2013) Oracle TimesTen in-memory database and Oracle in-memory database cache.
Available at http://www.oracle.com/technetwork/products/timesten/overview/index.html. Last
accessed 29/06/2013
</p>
<p>SAP (2013a) Consolidate transactional and analytical workloads onto a single, real-
time database&mdash;with SAP HANA. Available at http://www54.sap.com/pc/tech/in-memory-
computing-hana/software/platform/database.html. Last accessed 29/06/2013</p>
<p/>
<div class="annotation"><a href="http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E">http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E</a></div>
<div class="annotation"><a href="http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E">http://download.sap.com/download.epd?context=E67AFF9FD4CFC6693FC443EB965A7B4FE599BA88ED6C9F622486D0E5BEB350EB7DAD751393D16A2D916A3C9EA834D1CB2A8138467760590E</a></div>
<div class="annotation"><a href="http://www.mcobject.com/in_memory_database">http://www.mcobject.com/in_memory_database</a></div>
<div class="annotation"><a href="http://www.mcobject.com/in_memory_database">http://www.mcobject.com/in_memory_database</a></div>
<div class="annotation"><a href="http://www.oracle.com/technetwork/products/timesten/overview/index.html">http://www.oracle.com/technetwork/products/timesten/overview/index.html</a></div>
<div class="annotation"><a href="http://www54.sap.com/pc/tech/in-memory-computing-hana/software/platform/database.html">http://www54.sap.com/pc/tech/in-memory-computing-hana/software/platform/database.html</a></div>
<div class="annotation"><a href="http://www54.sap.com/pc/tech/in-memory-computing-hana/software/platform/database.html">http://www54.sap.com/pc/tech/in-memory-computing-hana/software/platform/database.html</a></div>
</div>
<div class="page"><p/>
<p>Part III
</p>
<p>What Database Professionals Worry About</p>
<p/>
</div>
<div class="page"><p/>
<p>9Database Scalability
</p>
<p>What the reader will learn:
&bull; that the number of concurrent users is one important aspect of scalability
&bull; and that volumes of data stored and accessed is also important, particularly in
</p>
<p>the era of Big Data
&bull; that scalability brings its own issues for DBAs, such as the cost and performance
</p>
<p>implications
&bull; that cloud computing&rsquo;s &ldquo;use when needed&rdquo; model can be helpful in handling
</p>
<p>scaling issues
&bull; that there are many approaches to tackling scalability issues and that each has
</p>
<p>strengths and weaknesses
</p>
<p>9.1 What DoWeMean by Scalability?
</p>
<p>As with most chapters in this book we will start by defining what we mean by
scalability. And, as with some other chapters there are several possible answers.
</p>
<p>Dictionary.com defines it as:
</p>
<p>the ability of something, especially a computer system, to adapt to increased demands.
</p>
<p>But even this is debatable. In its literal sense scale can go up (increased demands),
but it can go down.
</p>
<p>It is far from only computing that uses the term. Here is a definition from a paper
in the healthcare area:
</p>
<p>The ability of a health intervention shown to be efficacious on a small scale
and or under controlled conditions to be expanded under real world conditions
to reach a greater proportion of the eligible population.
</p>
<p>But this is a database book, so let us narrow this down to what it means for
database professionals. Even here, however, there are possible areas of confusion.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_9,
&copy; Springer-Verlag London 2013
</p>
<p>201</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_9">http://dx.doi.org/10.1007/978-1-4471-5601-7_9</a></div>
</div>
<div class="page"><p/>
<p>202 9 Database Scalability
</p>
<p>Hoskins and Frank (2002) define scalability in terms of processing power:
</p>
<p>&ldquo;the ability to retain performance levels when adding additional processors&rdquo;
</p>
<p>In this MSDN Dunmall and Clarke (2003) article about load testing, scalability
is described in terms of:
</p>
<p>&ldquo;. . . the number of concurrent users anticipated at peak load in production&rdquo;
</p>
<p>In the era of Big Data the focus is often more to do with the volumes of data
that an organisation needs to store. This is 10-Gen&rsquo;s take on scalability from their
MongoDB website:
</p>
<p>Auto-sharding allows MongoDB to scale from single server deployments to large, complex
multi-data center architectures.
</p>
<p>In actual fact these three elements are often closely related: concurrent user
count; processing loads; data volume. It is often the case that more data or users
will cause more processing, for example. So, for this book, we will describe scala-
bility as:
</p>
<p>The ability of a database system to continue to function well when more users or data are
added.
</p>
<p>We will split this chapter into two main parts as we first examine strategies for
coping with increases in the number of users of a system, and then look at the
alternative approaches to handling the growth of data being stored and queried.
</p>
<p>9.2 Coping with Growing Numbers of Users
</p>
<p>As with many Microsoft products, ease of use is a real plus-point for Access. Some
relational database concepts are difficult for non-technical users, and yet people
are able to use the Wizards to help them create complex databases with apparent
ease. And, compared to licenses for products like SQL Server and Oracle, Access is
cheap.
</p>
<p>So why doesn&rsquo;t everyone just use Access? Well we can get into a war between
vendor&rsquo;s claims and counter-claims if we are not careful, but the more general point
in a chapter about scalability is that even Microsoft only claim 255 as the maximum
number of users it can cope with. Anecdotal evidence, and the author&rsquo;s experience,
leads to the belief that considerably fewer than 255 may be the sensible maximum
number of concurrent users.
</p>
<p>If you are storing information about your immediate family&rsquo;s addresses, or you
are creating an application for the administrators of a small company, then this limit
may not matter. However, if you are hoping to be an Amazon, with thousands of
concurrent users, Access will clearly not do the job! According to Complete.com
(http://www.compete.com/us/) Amazon.com had 129 million users in the month be-
fore this chapter was written. Even if every user logged on at different times of day
and was logged on for less than a minute, this is roughly 3000 concurrent users
at any point in the day. Naturally their systems designers will have to build in re-
sources to cope with peak times, so this figure will actually be much, much greater
than 3000.</p>
<p/>
<div class="annotation"><a href="http://www.compete.com/us/">http://www.compete.com/us/</a></div>
</div>
<div class="page"><p/>
<p>9.2 Coping with Growing Numbers of Users 203
</p>
<p>Fig. 9.1 Peer-to-peer
database system
</p>
<p>9.2.1 SoWhy Can&rsquo;t Access Cope with Lots of Users?
</p>
<p>Well, because it was never designed to be truly multi-user. Access is a file-server
solution. It relies heavily on the operating system of a single file-server (usually
a PC) on which it runs and stores its data. Databases like SQLServer and Oracle
are Client-Server databases. Architecturally they were designed from the start to
be capable of allowing many users to access the system. Client-server solutions
can manage their own processes and data placement, including the use of multiple
servers to share the load. This is not to say that Access&rsquo;s architecture is second
best&mdash;it is just different; designed to meet different needs.
</p>
<p>Typically the file-server database is designed with a single user in mind. Many
Access users will use the PC they sit at to install and run Access. In that unconnected
mode, multiple users are never going to be an issue.
</p>
<p>Before Access became the success it currently is there was an trend for comput-
ers to become connected to one another in a peer-to-peer way. This meant that the
Access-like databases on the PC mentioned above could now be available to other
PCs in a workgroup. But what actually happens is that Access is installed on all the
PCs that are permitted to use the database, and a copy of the data is taken to work on
the second PC. This scenario, and the stand-alone PC are both referred to as one-tier
architecture (Fig. 9.1).
</p>
<p>Allowing multiple users to read data is a relatively trivial task for a database.
However, one of the most significant issues for multi-user databases is for the man-
agement system to control the write processing that is needed when users want to
insert, delete or update data. And one of the control mechanisms used is that of
locking rows, or tables&mdash;preventing other users from having access to a row whilst
it is being changed. We cover locking in more detail in Chap. 3. Locks are often a
problem because, to the users who do not own the lock, this just looks like they are
having to wait for the data. They tend to see it as a system performance problem.
</p>
<p>As with any system that relies exclusively on the resources of one PC, things
can go wrong. Information about Access locks is stored in a.ldb file. These files are
cleaned up as the system looses users, but occasionally they are left behind and need
to be cleared up by the systems administrator.
</p>
<p>These problems may not be an issue for smaller systems with few users. How-
ever, the more users there are, the more susceptible the system becomes to things
going wrong. And this means we can say that Access does not scale well.</p>
<p/>
</div>
<div class="page"><p/>
<p>204 9 Database Scalability
</p>
<p>Fig. 9.2 A client/server
database system
</p>
<p>9.2.2 Client/Server
</p>
<p>Largely in response to the scalability problems associated with the peer-to-peer ap-
proach, computer science began to look to a different model. What became appar-
ent was that connecting computers together and allowing them to share processing
in some way was a good use of resource. It was also clear, particular at a time
when disk, cpu and ram were very expensive, that having some sort of speciali-
sation for particular machines was the best way of maximizing the usage of these
resources.
</p>
<p>So evolved powerful PC-like computers that became known as Servers. They
&ldquo;served&rdquo; services to PCs that connected and requested such services. The latter be-
came known as the Client of the Server. The model of high performing machine
serving one or many client machines the became known as Client/Server (or Client-
Server) (Fig. 9.2).
</p>
<p>There are all sorts of variations on this theme. The World Wide Web is a form of
client/server, for example. Even mainframes, it could be argued, are Client/Server
systems with central processes carrying out the work and then returning the results
to dumb terminals.
</p>
<p>One of the decisions for designers of client/server systems is where to do the
processing. In the mainframe example all the processing is carried out at the server
end. What made the client/server revolution different to mainframes was that the
clients tended to be relatively intelligent PCs. In database terms, the decision then
becomes&mdash;do you use the server to serve-up the data and then do the manipulation
(in the form of sorting, filtering, etc) at the client end? Or do you do everything at
the server end and just send the finished dataset, but use the PC&rsquo;s graphical abilities
to present it to the user?</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Coping with Growing Numbers of Users 205
</p>
<p>Another pair of oft-used terms here are &ldquo;thin client&rdquo; or &ldquo;fat client&rdquo;. The former
can almost be thought of as a more graphically pleasing version the dumb terminal.
All the processing is carried out at the server end. At the opposite end we have all
the processing carried out by high performance client PCs, and the server merely
&ldquo;serving&rdquo; requests for data.
</p>
<p>There are advantages and disadvantages to both the fat and thin approaches. Per-
haps the biggest reason for the client to be a fat client PC is that of user expectation.
Users now expect to be able to have some control over their own computing. In
addition thick clients allow sometimes lengthy data transfer to happen in the back-
ground whilst the user works on other tasks. Simple tasks like screen refreshes and
paging through data can be processed at the PC, whereas, with a thin client, network
traffic would be increased significantly as the keystrokes and results pass back and
forth.
</p>
<p>Amongst the advantages of thin client, perhaps the two most significant are those
of cost savings and greenness. Without the need for powerful computing power,
thin clients can be better than half the price of a PC to purchase, and can use five-
or six-times more electricity in use.
</p>
<p>In addition thin client supporters claim it to be easier be easier to manage (soft-
ware upgrades are easy since they just apply to one machine, for example) and more
secure (since the applications are entirely housed in the data center, where strict
rules and policies can be applied).
</p>
<p>Whichever approach is taken it was clear that the client/server model allowed for
much better scaling than did peer-to-peer. Handling more users could be as simple
as adding more RAM or upgrading the CPU on the server.
</p>
<p>However, as pressure rose for more and more users to be able to access an or-
ganisation&rsquo;s data, the single server solution (known as two-tier) showed weaknesses
and the next step was to split server-side processing over two servers. Typically
this would be a Database Server and an Application Server. This became known as
Three-tier architecture. Many organisations now use this approach. Extra scalabil-
ity can come in this environment by having users connect to the application server
which then manages connections to the database server, pooling connections rather
than requiring the server to have a process for every user. The most recent move
has been to n-tier architecture, in which many servers each cope with aspects of
servicing the clients (see Fig. 9.4).
</p>
<p>The advent of Cloud computing has now brought us the ability to scale either
way (up and down) very easily, and this is seen by many as one of the great benefits
of Cloud. If you had to buy and maintain a client/server system adequate to support
an estimated 200 concurrent users, and then you discovered that the estimate was
wildly wrong and you needed only to cope with 20 concurrent users, you would
be grossly over resourced (and over spent!). Having bought hardware it is virtually
impossible to downsize. However, Cloud just needs you to change your contract
with your service provider. And of course virtually unlimited and very rapid upward
scaling is also available on the Cloud.</p>
<p/>
</div>
<div class="page"><p/>
<p>206 9 Database Scalability
</p>
<p>Fig. 9.3 Scalability eras
</p>
<p>9.2.3 Scalability Eras
</p>
<p>The different approaches to the scalability issue outline above have all taken place
since the PC became a viable business tool and each approach has evolved. A sum-
mary of this evolution in shown in Fig. 9.3.
</p>
<p>Some of the aspects discussed above also impinge heavily upon the networking
domain. Having a server do all of the processing and just sending the final dataset
to the client means that network traffic is minimised. But the fat client solution may
mean a need for network upgrades when more users begin to use the system. As
always, database and networking professionals should ideally be liaising to provide
the organisation with an optimal solution.
</p>
<p>9.2.3.1 Separating Users from Sessions
In the traditional client/server database system the user connects to the database and
the database will then manage a bit of server RAM and some server-based processes
on behalf of that user, called a database &ldquo;session&rdquo;.
</p>
<p>Managing sessions is an important and resource heavy task for the RDBMS. Idle
sessions need to be continually checked and killed after a certain time since they
are using up valuable RAM without doing anything. The activity of every session is
recorded in great detail. This information is useful for performance tuning activity,
allowing the DBA to ascertain which sessions used most resources. However, its</p>
<p/>
</div>
<div class="page"><p/>
<p>9.2 Coping with Growing Numbers of Users 207
</p>
<p>primary purpose is to act to allow audits of activity. But why this activity happens
need not concern us here&mdash;what we need to know is that the more sessions we have
the less well the database will perform.
</p>
<p>However, especially in a e-commerce type environment it may well be that users
who are logged on to the server for as long as an hour only actually make calls to
the database that take less than one minute of database process time. Consider a
simplified version of the shopper who asks the site to show him all the gentlemen&rsquo;s
trousers available:
1. The SQL is formed which says something like Select &lowast; from catalog where
</p>
<p>item_type = Trouser; The SQL is sent to the server which returns the dataset
asked for.
</p>
<p>2. The shopping basket may or may not be stored on the server
3. The user then browses the dataset for 50 minutes, decided to make a cup of tea
</p>
<p>half way through the process.
4. The user completes the purchasing process.
</p>
<p>If we were to kill this session for inactivity whilst the user made their tea we
would loose our customer&mdash;not a sensible approach to take! On the other hand,
tying up resources, especially when multiplied thousands of time over with the po-
tential number of concurrent users accessing the site, could be very costly in terms
of having RAM and CPU in the server &ldquo;just in case&rdquo; all the users need to access the
database at the same time.
</p>
<p>The solution is to have an extra layer between the client and the database server
that manages the connection process. This extra layer, known as the Application
Server, will own the database connection and will manage requests for data access
on behalf of clients. This is often called Connection Pooling. In effect, the problem
with the fifty minutes of inactivity identified above is lost since the application server
will simply service other requests from other clients with the open session.
</p>
<p>The application server can do much more than manage sessions. It can, for ex-
ample, carry out the query processing, such as sorting and filtering, removing pro-
cessing load from both database server and client.
</p>
<p>9.2.3.2 Adding Tiers
As we saw above, there is a trend towards multiple tier server solutions. In our ex-
ample above, we are divorcing database server from the need to worry about what
happens to the data once it is served to the requesting agent. This means that physical
aspects of the server (RAM and CPU) can be optimised for data-intensive process-
ing, whilst the application server can also be optimised.
</p>
<p>Three tier architecture is now the norm in enterprise scale architecture. The three
tiers are identified as:
1. The Data tier: responsible for writing to and reading from the data store or file
</p>
<p>system
2. Business Logic: responsible for liaising between the other tiers, including shar-
</p>
<p>ing connections. But this layer can also contain rules, business logic and other
processes that can work with any extracted data before passing it on to the
client</p>
<p/>
</div>
<div class="page"><p/>
<p>208 9 Database Scalability
</p>
<p>Fig. 9.4 Multi-tier systems
</p>
<p>3. Presentation: responsible for turning the information from the second tier into
something the user can read.
</p>
<p>If the clients are thin clients then the middle tier tends to have to carry out much
more work and that tier itself needs to be broken down into different layers, each
sitting on a different server. A diagram of such an n-tier architecture is shown in
Fig. 9.4.
</p>
<p>9.3 Coping with Growing Volumes of Data
</p>
<p>As we saw in Chap. 6 we are now working in an era where volumes of data are
increasing at an enormous speed. Storing that data load as it grows, in a way
that allows it to be accessed efficiently, is what scalability means in terms of
data.
</p>
<p>One of the reasons to use normalisation was to reduce the amount of data
being stored. And one of the key reasons for that was that storage media were
very expensive until quite recently. They were also very unwieldy (see Fig. 9.5).
The continuing reduction in relative cost and enhanced functionality of com-
puting technologies are key in driving new expectations and requirements. The
fact that we can now store 1 terabyte of data for around $100 (under &pound;70)
means that the business that wants to store that amount of data can easily do so.
Only twenty years ago the cost would have been hundreds of times higher and
would therefore have been far more likely to prevent the storage from happen-
ing.
</p>
<p>The physical performance of data stored on disk, too, has increased significantly.
In terms of data transfer rates, today&rsquo;s 6 gigabits per second SATA drives are 4&times;
faster than when SATA drives started to become available in 2003. There are some</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Coping with Growing Volumes of Data 209
</p>
<p>Fig. 9.5 What HDD used to
look like! Courtesy of
International Business
Machines Corporation,
&copy;International Business
Machines Corporation
</p>
<p>who argue that disk speeds are beginning reach the best they can possibly be as a re-
sult of the constraints set by the physical elements that make up the HDD (spinning
platters on a spindle and a moving arm).
</p>
<p>SSD technology on the other hand has no moving parts and can therefore be
much faster. This may well be the way that data storage moves in the future, but,
though the cost/Gb of SSD is dropping, it may be a few years before the cost and
reliability matches HDD technology.
</p>
<p>9.3.1 E-Commerce and Cloud Applications Need Scalable Solutions
</p>
<p>The rise of companies whose core business is selling to consumers by using the
internet has brought a whole new requirement in terms of scalability. When starting
new businesses in this area the entrepreneur does not know how many customers
they will have using its site. They can only guess. And guesses can be considerably
out (either way!).
</p>
<p>But the hope is usually that customers will come flocking in to your new e-
commerce site. And once you have your reputation built, even more will come,
month on month increasing the number of hits your site receives.</p>
<p/>
</div>
<div class="page"><p/>
<p>210 9 Database Scalability
</p>
<p>But the worry is that your website is only as good as its ability to cope with peak
traffic. As the hits increase you need to be able to be sure that your website, and
its underlying database will be able to scale upwards. Failure to scale will result in
disgruntled customers who could even ruin the business if it gets that bad!
</p>
<p>If your solution is cloud-based then you can afford to worry a little less since you
can quickly buy your way out of scaling problems by renting more CPU and disk
from your provider.
</p>
<p>9.3.2 Vertical and Horizontal Scaling
</p>
<p>These two terms have come to the fore recently to describe two completely different
approaches to allowing database applications to scale. Vertical is also referred to as
Scaling UP and Horizontal as Scaling OUT.
</p>
<p>Unfortunately these terms are amongst many in computing that suffer from defi-
nition creep. For ease we will here suggest that Vertical Scaling is the act of adding
resource, such as CPU or RAM to a database server, and/or using database archi-
tectural techniques to assist in allow more data to be added to the system without
degrading performance. Horizontal is the act of connecting several cheap (usually
referred to as &ldquo;commodity servers&rdquo;) computers to share out the storage and process-
ing load.
</p>
<p>Both of these scaling techniques have their strengths and weaknesses. There
can be little doubt that the complexity involved in managing multiple nodes in an
Hadoop cluster might be enough to put even technical experts off from using that
approach, especially if their expertise lies in the relational arena. But if the business
need is to store and analyse masses of data that can be spread across multiple nodes,
it may be that the complexity is worth it. Google, for example, would not have gone
to the trouble of creating their own file system if they had thought scaling up an
standard RDBMS could have coped.
</p>
<p>9.3.3 Database Scaling Issues
</p>
<p>So why are there problems in adding more and more data to a database? Surely a
table can simply keep growing? Well, yes it can keep growing&mdash;provided the under-
pinning disk system can keep expanding. There are physical limits to the amount of
storage that can be addressed, but the main limit is still likely to be cost. However,
even if you have a disk array that will house all of your data, other problems occur
as a consequence of constantly adding data.
</p>
<p>The most significant problem is the time it takes to access the data in response to
SQL queries. There is a pre-retrieval overhead for all queries that will not get slower
because there is more data, but the actual retrieval phase could take twice as long if
you are reading twice as many rows.
</p>
<p>One of the most frequent processes carried out by an SQL query execution plan is
a full table scan. In Oracle, for example, if the optimiser calculates that your query
will return more than around 15 % of the total rows, the execution plan will not</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Coping with Growing Volumes of Data 211
</p>
<p>include the use of indexes (even if they exist), but will read every row in the table,
discarding unwanted ones in memory.
</p>
<p>This is quicker because the use of indexes will frequently lead to several read
operations per row fetched, and each read may require the disk read head to have to
jump to different physical addresses. A full table scan only requires one move&mdash;to
the address of the start of the table&mdash;and then to keep reading until it gets to the
end of the table (identified by a High Water Mark). It can also use much larger I/O
calls, biting more off with each read. Making fewer large I/O calls is less costly than
making many small calls.
</p>
<p>Any query that uses a full table scan, then, will take longer the more rows you
add to the table concerned. Queries that use index retrieval will also probably take
longer as rows are added for a couple of reasons. Firstly, more rows means more
leaf blocks to the index and possibly more branches (each requiring a read) to get to
those nodes. Secondly, having retrieved the rowid (which contains the hex-address
of the row on the disk) it is likely that the disk head will need to move further to
access the data concerned.
</p>
<p>Bigger datasets also probably mean more data maintenance&mdash;things like updates
to, or deletions of, rows. Both updates and deletions have effects on any indexes
concerned too. Inserts to tables with Foreign Key constraints will take longer if the
referenced table is growing significantly.
</p>
<p>All this extra work as a result of the volume of data stored will slow the system
down. The need to protect against database failure means that Redo Logs activity
may well increase significantly with increased usage and may result in the need for
more disk space to be devoted to Redo. More data changes also means increased
Undo activity as the need to be able to Rollback and provide read consistency ex-
pands. Again this means more disk space and slower access to the contents as it
increases in size.
</p>
<p>If you have a Disaster Recover policy that relies on a mirrored server in another
physical location you will have double the issues of disk space to worry about, as
both servers need to be in step. Moreover the network traffic will increase as more
information needs to be passed to the mirror.
</p>
<p>Back-up will take longer as you have more data. There may be only slight in-
creases in any incremental backing up carried out, but full, cold back ups will take
longer. More worrying, the fact that there is more data involved means that the time
to recovery will be much slower should you need to use those back ups.
</p>
<p>All of these are just natural bi-products of a growth in data. Just as it is true that it
takes longer to read War and Peace than Of Mice and Men, so it is true that reading
lots of data takes longer than reading small amounts.
</p>
<p>Even if it the data involved made it appropriate, the database architect may well
not be able to make sweeping changes to the infrastructure in response to problems
caused by growing data. If your organisation has invested heavily in large, powerful
servers running Oracle or SQLServer, it would be a very brave DBA indeed that
suggested throwing that all on the waste heap and replacing it with many commodity
servers running Hadoop. As always, the constraints on the DBA will not always be
technical ones.</p>
<p/>
</div>
<div class="page"><p/>
<p>212 9 Database Scalability
</p>
<p>For this reason, in the next few sections we will examine possible scalability-
related solutions for different environments, recognising as we do, that the current
environment is often the given that has to be worked around.
</p>
<p>9.3.4 Single Server Solutions
</p>
<p>In this section we consider the example of a single server running a Oracle or
SQLServer database which is growing and which, as a consequence, beginning to
suffer from poor performance. It is taken for granted that the extra data is being
handled by extending the storage disk-farm.
</p>
<p>It is true that there may be an immediate cures in terms of simply upgrading the
CPU or providing more RAM, but they can be costly, and are not always the right so-
lution. There is also the possibility that improvements can be found by changing the
underlying operating system, though this latter may have far too many organisation-
wide repercussions to be considered realistic.
</p>
<p>Extra RAM&mdash;lots of it&mdash;might seem like the obvious solution to our problem.
After all we will discover in the performance chapter later, reading a row from a
HDD is far slower than reading from memory. So more RAM means less HDD
reading, which means we can scale our database up and ensure our users do not
notice any change in performance.
</p>
<p>There is some truth in this. As we saw in the In-Memory chapter, there are ex-
amples (Oracle Times-Ten, VoltDB) of databases running entirely in memory. How-
ever, just having picked at random a cheap 1 Tb HDD and 1 Gb RAM on Amazon,
the cost/Gb difference is very significant; it is around 300 times more expensive/Gb
for RAM than it is for HDD. Except for some very time critical applications it is
probably that we will need to continue working with HDD technology as the means
of making our data permanent.
</p>
<p>The other important factor is just what you are doing with the data. If this is
a very volatile many user OLTP system, it may well be the case that much of the
data that is cached into memory as a result of one users SQL statement will simply
time-out and disappear as it isn&rsquo;t needed again before the RAM is needed for other
data. In this circumstance performance will not improve at all by adding RAM. As
usual we need to understand what the application needs from the database before
we make our decisions about how to tackle scalability.
</p>
<p>Memory management is a very important part of any RDBMS system. Newer
databases will often manage the available RAM automatically, but if the RDBMS is
not the only software running on the server, some manual intervention and decision
making will need to be carried out.
</p>
<p>What we do know is that there will come a time when the benefit accrued from
adding RAM, or CPUs, will not seem worth the expense. So are there other, software
related, ways of improving performance when our data increases?</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Coping with Growing Volumes of Data 213
</p>
<p>9.3.4.1 Partitioning
Partitioning data, also known as vertical scaling, is one approach to extending data
storage within the existing schema. At its core the concept is relatively simple: if
you are struggling to access large volumes of data, simply break the storage object
(usually a table) into smaller objects.
</p>
<p>There are several ways of doing this. One is to take a table and attempt to break
it into two or more tables, with data that is regularly accessed in one table, and
the other tables holding the less active columns. It is sometimes referred to as row-
splitting.
</p>
<p>The original, unsplit table can be recreated with a view which joins the tables
this split. Care needs to be taken however since this activity obviously increases
(resource intensive) join activity so, as usual, it will depend upon what activity your
database is supporting as to whether the split is worthwhile.
</p>
<p>Another way of subdividing the data is by breaking the table into logical portions
based upon a key value. For example, in an OLTP system you could partition the
data such that data for the current month is stored on one tablespace, whilst data
for previous months which can&rsquo;t be updated, is stored on faster to access read-only
disks.
</p>
<p>As partitioning requires the DBA to identify criteria for breaking up the structures
it is not useful in situations where the data distribution changes often. Predictability
and some regularity in the distribution of data between the new structures makes
partitioning more likely to succeed.
</p>
<p>9.3.5 Distributed RDBMS: Shared Nothing vs. Shared Disk
</p>
<p>One of the problems with scaling up data storage on a traditional RDBMS sitting on
a single server is that the server itself is expensive.
</p>
<p>RAID technologies (see Chap. 3) began the process of utilising cheap technology
(in that case HDD technology) in enterprise systems that had hitherto feared that
reliability would suffer if cheaper hardware was used.
</p>
<p>In 2006 Oracle released Version 10g of its database. The &ldquo;g&rdquo; stands for grid.
Grids are just pools of computers (cheap, off-the-shelf) that can share the process-
ing required to run the database between themselves. Scalability is achieved by per-
mitting extra computers to be added to (or removed from) the grid. Naturally, in
order for this to work, there has to be some sort of Grid Control mechanism which
manages the flow of tasks and data.
</p>
<p>Whilst the term &ldquo;grid&rdquo; was the cause of some disagreement in the industry be-
cause grid computing was a term already in use by Computer science, the concept
is still seen as a sensible approach to the problem of scalability and cost-effective
performance.
</p>
<p>In the Oracle grid solution, whilst additional nodes can be added to the grid, what
they actually provide is extra only places for processing to take place. The data is
not owned by any individual node. Rather it accesses a single shared database. This
may, in actual fact, reside on many different physical disks in a SAN, but, as far as</p>
<p/>
</div>
<div class="page"><p/>
<p>214 9 Database Scalability
</p>
<p>Fig. 9.6 Shared Everything
(or Shared Disk)
</p>
<p>the RDBMS is concerned, it is treated as a single database which is attached to by
the nodes when they need data. All the nodes share the database, hence one of the
names for this approach: Shared Disk (see Fig. 9.6).
</p>
<p>Confusingly, different manufacturers use different terms to describe concepts.
The shared disk approach is also referred to as Clustering. Oracle calls this approach
Real Application Clustering (RAC). Its real strength is in enhancing availability
since, should one of the nodes fail, the other nodes are available for users to attach
to, and thence gain access to the shared data.
</p>
<p>Query performance can also improved since each node can, if appropriate, use
its computing resources to return parts of an overall query at the same time; in effect
allowing for parallel processing.
</p>
<p>Unfortunately, although there are some performance advantages with this ap-
proach the fact that each node is accessing the same data means that there will be
many more locks in play than on a single server, and those locks need synchronis-
ing across all the node. It can be argued that this makes this approach less than truly
scalable as a result of this.
</p>
<p>Shared disk is not the only approach to distribution. A shared nothing approach
means that each node also looks after its own data and it has its own memory and
disk resource. Examples of this approach are IBM&rsquo;s DB2 and Teradata.
</p>
<p>Here the whole database is actually a logical construct made up of the individual
databases on each node. This can be its performance weakness since joins across
multiple nodes can be very slow. Moreover, if a node fails you loose far more than
you do in the shared disk approach&mdash;you loose part of your whole database, so
availability is an issue which needs to be thought carefully about. However, it is
very much easier to scale. You just add another commodity server when you need
more resource. This approach is popular for e-commerce sites for which the degree
of scaling is unsure, or certain to be high.
</p>
<p>There are, as usual, pluses and misuses to each approach. The business needs
will need to be the driver in the decision making.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Coping with Growing Volumes of Data 215
</p>
<p>Fig. 9.7 Shared Nothing
</p>
<p>We are now moving into the grey area between shared nothing and sharding.
Indeed, it could be argued that Sharding is the same as Shared Nothing is the same
as horizontal scaling (see Fig. 9.7).
</p>
<p>9.3.6 Horizontal Scaling Solutions
</p>
<p>As we have seen, there are various names for the basic idea of splitting your data
across two or more databases (often referred to as nodes) such that all nodes have a
subset of the data. This works well in cases when a query only needs to connect to
one node. It can be more cumbersome if many nodes need to be accessed to return
data from a query. This is sometimes referred to as &ldquo;scaling out&rdquo; as opposed to the
traditional approach of &ldquo;scaling up&rdquo;.
</p>
<p>As the data stored for web usage expanded dramatically early in 2000s Google
realised that the traditional RDBMS solutions would not cope with work load be-
ing placed on them by the various applications like Google Search, Web-indexing,
Google Earth, and others. They set about inventing their own file system (GFS) and
sat their own database (BigTable) on this platform. The need to scale easily was
paramount. They achieved this in a cost-effective way by using cheap, commodity
servers which can be easily slotted in and out of the data centre infrastructure.
</p>
<p>Google started a trend. GFS and BigTable are proprietary products, but very sim-
ilar tools began to appear in the open source environment. At the time of writing the
industry standard horizontal scaling tool seems to be becoming Apache Hadoop.
Other NoSQL databases (see Chap. 5) also adopt this approach. The examples we
looked at earlier, Cassandra and MongoDB both shard their data to allow scaling.
But it is not just the open source arena that is turning to sharding. Microsoft&rsquo;s Azure
allows sharding too.
</p>
<p>One of the first problems when designing sharded systems is in identifying a key
on which to split the data. Let review a simple example:</p>
<p/>
</div>
<div class="page"><p/>
<p>216 9 Database Scalability
</p>
<p>An application starts by looking up information about a client based upon
their surname.
</p>
<p>You have two servers available. Perhaps it may be sensible to have two
shards, one with all the names beginning with A to L, and the other M to Z.
</p>
<p>And whilst the only query asked of the data is about one individual, then this split
will reduce the searching that needs to be carried out. But if you were to want to
discover all clients who lived in a particular town, this query may well result in
joins needing to be carried out and the management of that process may take more
time than if all the data were in one single table. It is clear that selection of the
sharding key is key to the success, or otherwise of this approach.
</p>
<p>The management of the process includes the need to maintain an centrally con-
trolled index of the keys used in the sharding in order to make sure that the request
for data is made to the correct database. All this means there is an extra overhead
involved in managing the database connections.
</p>
<p>There can be little doubt that the use of cheap servers to allow easy scalability is
very attractive and this is a significant factor in the recent rise in the use of horizontal
scaling. As always in IT, however, we need to be careful not to just jump on the
current band-wagon. We should assess every situation individually since Sharding
is not a silver bullet.
</p>
<p>It may be that Sharding the whole system is overkill. Perhaps only the part that
comes under most load&mdash;the web front-end, for example&mdash;needs sharding whilst
the traditional back-office and transactional systems remain on single servers.
</p>
<p>As we have seen above, sharding is not the only way of coping with the need to
scale. It could be that when or if the price of RAM falls in the future we will look
back at this fundamentally disjointed approach to database architecture and laugh.
However, for the foreseeable future the scaling out approach is likely to be the de
facto standard for large datasets.
</p>
<p>9.3.6.1 Cloud Computing
</p>
<p>Many texts about Cloud site flexibility and scalability as inherent benefits from
adopting a cloud-based strategy. Hill et al. (2013) say:
</p>
<p>It is probably true that a need for scalability is a significant driver towards
adopting cloud. If an organisation understands its business well and it is rela-
tively stable, it can plan what capacity is required and purchase as and when
required. Many organisations, however, go through unexpected sharp up- and
downturns in their OLTP traffic in step with the business performance. Not
having to purchase extra capacity &lsquo;just in case&rsquo; in such circumstances can
make public cloud more appealing.</p>
<p/>
</div>
<div class="page"><p/>
<p>9.3 Coping with Growing Volumes of Data 217
</p>
<p>It is true that the cost of the physical resources needed to cope with the maximum
processing load a database might face can be expensive. Being able to temporarily
gain extra resource and pay for it &ldquo;as used&rdquo; can be a good way to keep capital
expenditure down on a large scale database project. The costs, however, will not
disappear, but rather will appear as rental fees in revue costs.
</p>
<p>That caveat aside, it is clear that Cloud computing does offer virtually unlimited
scalability, provided the costs do not become prohibitive. Some vendors make rela-
tional databases available in the cloud, allowing the single (albeit virtualised) server
solution that can, almost instantly, be turned into a distributed solution. The NoSQL
tools are also used heavily in the cloud. Having a system with multiple data nodes
across many geographic regions becomes a relatively simple task for the system
designer, if such horizontal scaling is required.
</p>
<p>9.3.7 Scaling Down for Mobile
</p>
<p>A recent phenomenon that has begin to impact upon the world of database design
is that of mobile computing in general, and more specifically the need to integrate
mobile devices into enterprise-wide systems. The term being used is BOYD (Bring
Your Own Device). It recognises that employees are often willing to work in places
other than at their desk and that they want to access data from all sorts of place, at
all sorts of times, from all sorts of devices.
</p>
<p>The problem with this move is that, despite the continuing evolution and im-
provement of mobile devices, they suffer from resource poverty (RAM and Stor-
age) compared to desktop PCs. This can be a major obstacle for many applications
and means that computation on mobile devices will always involve some form of
compromise.
</p>
<p>Perhaps the easiest solution to this problem is to treat mobile devices just the
same as a very thin client and allow a server to carry the processing and data storage
load. Unfortunately, however, this solution requires ubiquitous broadband to be of
any use, and even in some developed nations there are places where communications
are unavailable, meaning the system is, in effect, unusable for the mobile client.
</p>
<p>At the other end of the spectrum, another solution is to have the system run
entirely on the mobile device and have a database there to store the data locally.
In these systems data responsibility for data persistence is handed to the device in
question. There are databases designed to operate with a very small footprint, such
as the public domain SQLite. But this also has it&rsquo;s own problem; enterprise systems
called for sharing data, not lots of independent data stores.
</p>
<p>Synchronising with a Cloud-based server whilst communications allow it is one
solution used. This can also be a means for ensure vital corporate data is regularly
backed-up. Management of availability when there any many different client system
types is far more complicated, but if all devices are connecting to the Cloud this can
be made easier and automated.</p>
<p/>
</div>
<div class="page"><p/>
<p>218 9 Database Scalability
</p>
<p>9.4 Summary
</p>
<p>This chapter has reviewed scalability in terms of growing data and growing user-
bases. We have seen that small, PC based databases have their strengths, but they do
not allow scalable systems to be created. We have seen that there are many possible
approaches to database design which may help with scalability. We saw that the
systems designer needs to consider where the processing should happen (thin/thick
client) in a client-Server design. If more scalability is required there is the option to
distribute the processing and/or the data and that many of today&rsquo;s Big Data focused
databases using Sharding to allow them to easily scale.
</p>
<p>9.5 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
</p>
<p>&bull; What is the maximum number of concurrent users that can log on to Access
databases?
</p>
<p>&bull; Why is the separation of a database session from a user logging-on to an appli-
cation an important factor in scalability?
</p>
<p>&bull; What does Sharding mean?
&bull; Describe the elements of a three-tier architecture
&bull; How does partitioning help when data in a table grows to extent that perfor-
</p>
<p>mance worsens?
</p>
<p>9.6 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 You are asked to discuss appropriate database designs for
an e-commerce site your organisation is wanting to create. As the application be-
comes available they expect only a few users, but they believe that, once the word
gets round about how good the application is, that the number of concurrent users
will dramatically increase in the coming months. Your aim is to architect a scalable
back-end to the application being created. What alternative design options can you
suggest, and what are their respective strengths and weaknesses?
</p>
<p>Discussion Topic 2 What are the arguments for and against thin client solutions
for an office-based environment? Review some of the sales material from market
leaders in the area, such as Citrix and see what they say.
</p>
<p>References
</p>
<p>Dunmall J, Clarke K (2003) Real-world load testing tips to avoid bottlenecks when your web app
goes live. http://msdn.microsoft.com/en-us/magazine/cc188783.aspx</p>
<p/>
<div class="annotation"><a href="http://msdn.microsoft.com/en-us/magazine/cc188783.aspx">http://msdn.microsoft.com/en-us/magazine/cc188783.aspx</a></div>
</div>
<div class="page"><p/>
<p>References 219
</p>
<p>Hill R, Hirsch L, Lake P, Moshiri S (2013) Guide to cloud computing: principles and practice.
Springer, London
</p>
<p>Hoskins J, Frank B (2002) Exploring IBM EServer ZSeries and S/390 servers: see why IBM&rsquo;s
redesigned mainframe computer family has become more popular than ever! Maximum Press,
464 pages</p>
<p/>
</div>
<div class="page"><p/>
<p>10Database Availability
</p>
<p>What the reader will learn:
</p>
<p>&bull; that we should not expect our databases to be always available
&bull; that the more available a system is, generally the more expensive it is
&bull; that there are several approaches to making a system as available as is appropri-
</p>
<p>ate to suit the business need
&bull; that Cloud computing changes the way we think about availability
&bull; In some circumstances the expense of a Disaster Recovery strategy is irrelevant
</p>
<p>as client expectation will drive the decisions
</p>
<p>10.1 What DoWeMean by Availability?
</p>
<p>As with most chapters in this book we will start by defining what we mean by
availability. And, as with some other chapters there are several possible answers.
</p>
<p>For an online business, availability is measured mostly by uptime. Because every
second the database is unavailable (or &ldquo;down&rdquo;) to potential customers could cost
thousands of dollars in lost income, having the database usable for the magic five
nines (99.999 % up-time) is a sensible, if difficult to achieve, target. This is around
5 minutes downtime in a year.
</p>
<p>As we shall see, the move from three nines (99.9 %) to five nines can be very
expensive. Some organisations will not need that level of availability. Three nines
is under nine hours per year. If you need to spend an extra $100000 to get to five
nines, those will be a very costly few hours of computing you have bought!
</p>
<p>To be more accurate, database up time is not the same as availability. It is possible
for a database to be open, but for network problems to prevent access to the database.
</p>
<p>But there is more to availability than merely keeping a database open and acces-
sible. What happens if your data centre is struck by a meteor? Or flood? If you are a
bank, for example, your customers will probably expect you to have an alternative
server somewhere else that can kick in to (almost) seamlessly replace the destroyed
one. Disaster recovery (DR) is part of the availability mix. A DR strategy like that
of the bank would be very, very expensive and there would really need to be a good
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_10,
&copy; Springer-Verlag London 2013
</p>
<p>221</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_10">http://dx.doi.org/10.1007/978-1-4471-5601-7_10</a></div>
</div>
<div class="page"><p/>
<p>222 10 Database Availability
</p>
<p>business case for having an expensive server (or servers) sitting doing nothing &ldquo;just
in case&rdquo;.
</p>
<p>And the other important element to availability is recovering from smaller prob-
lems cleanly. If a user mistakenly deletes some data and wants to retrieve it, the
DBA will have a number of tools available to address the problem. This could even
be something the user can effect themselves, like Oracle&rsquo;s Flashback technology.
Part of the standard toolkit here is Backup and Recovery. Taking a full copy of the
database once every now and then (full backup), and then recording the changes
made on a regular basis is one very common strategy that we will examine in this
chapter.
</p>
<p>In a traditional Client/Server environment a fail-over strategy may also include
the use of redundant disks (RAID). Again the downside is the expense is the extra
disk and processing required to make the system more robust, and a potential effect
on performance.
</p>
<p>Cloud has allowed us to change the way we think about availability. Disk Storage
as a Service is now commonplace, meaning that you do not need to worry about
maintaining an array of expensive disks, but rather you rent the space you need,
when you need it. But what Cloud changes is more to do with costs and flexibility
than the overall principles involved ensuring optimum availability.
</p>
<p>So, what is our definition of Database Availability? How about this:
</p>
<p>&ldquo;Ensuring that the database is open and accessible for as long as possible, by planning for
as many eventualities as is reasonable, given the constraint of cost.&rdquo;
</p>
<p>In this chapter we will explore three key areas in database availability: Keeping the
system running&mdash;immediate solutions to short term problems; back-up and recov-
ery; Disaster Recovery (DR).
</p>
<p>10.2 Keeping the System Running&mdash;Immediate Solutions to
Short Term Problems
</p>
<p>High Availability (HA) is not a new concept and has been part of every leading
database vendor&rsquo;s sales pitches for many years. But before we look at some ap-
proaches to ensuring high availability, we need to explore some key concepts.
</p>
<p>As we discussed in Chap. 3, databases are primarily about making data perma-
nent. From the earliest days of relational databases that has meant storing ons and
offs on some form of magnetic medium, such as a floppy, or hard disk.
</p>
<p>Although, in the author&rsquo;s experience, it is true that computers have become gen-
erally more reliable over the decades, it is also true that they are far from perfect.
Disks can fail, as can CPUs, motherboard elements and a whole host of communica-
tions interfaces. Availability is about recognising the less than perfect environment
and putting in place processes for coping with such disturbances with minimum
effect on the users.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Keeping the System Running&mdash;Immediate Solutions to Short Term 223
</p>
<p>10.2.1 What Can GoWrong?
</p>
<p>There is an apocryphal story that does the rounds with seasoned database profes-
sionals. It happens that the DBAs would regularly come in on a Monday morning
and find that the database had been down for about half an hour at 4pm on Sunday
afternoon. None of them came in to work on a Sunday, and there was no remote
access. The server brought itself up, but nonetheless, this was a worrying event.
</p>
<p>One week the lead DBA decided to go in and watch the server for himself, and
couldn&rsquo;t believe his eyes when at 4pm the server room door was opened by a cleaner
who, after smiling and waving to our DBA, proceeded to unplug the server and plug
in the vacuum cleaner.
</p>
<p>What this story highlights is that one of the major points of failure in any database
system is the humans who interact with it in any way. These events may never have
happened, but the database professional should attempt to think about all possible
occurrences that might in any way affect the database and work out ways of pre-
venting the event, or at least limiting the damage caused by it.
</p>
<p>The other significant problem caused by humans is in the writing of the code
which makes up the database management system, and any database systems using
that database. Any database management system is itself a piece of software that
has been written by software engineers. A product like Oracle will have had many
software engineers produce many thousands of lines of code to bring us the product
we now use. And they still beaver away, generating new releases with improvements
(or corrections) to previous versions.
</p>
<p>10.2.1.1 Database Software Related Problems
The problem is, humans make mistakes. And software engineers are no different,
regardless of the quality assurance mechanisms that are put in place. When products
evolve, as Oracle has from Version 6 in 1988 to its current Version 12c, there is
potential for mistakes to be built on mistakes. This isn&rsquo;t to say Oracle is an unsafe
product&mdash;far from it, Oracle&rsquo;s reputation is very strong in the area of reliability.
However, if you count the patches that are released every year you can&rsquo;t help but
notice that somebody, somewhere, gets things wrong!
</p>
<p>Often such problems are caused by software engineers making assumptions
about customer&rsquo;s operating environments and data distributions, and so it isn&rsquo;t nec-
essarily an error that is being corrected so much as an omission. But this nicety of
definition doesn&rsquo;t help the DBA affected by it.
</p>
<p>So potential fault number one is a problem introduced by a change to database
software release, or its configuration. As a DBA looking to keep the database avail-
able at all times, how can we mitigate the effect of these potential problems?
</p>
<p>Most of the approaches we cover in this chapter start with the age-old dilemma:
how much are we willing to spend to make the system more robust? One solution to
the dangers of errors introduced by upgrades is to run two parallel systems for a pe-
riod, with the upgraded version running besides the older version. Should anything
bad happen the new version can be switched off and the old version can continue.</p>
<p/>
</div>
<div class="page"><p/>
<p>224 10 Database Availability
</p>
<p>This approach sounds like a sensible one. But then the question is, how long do
we run in parallel? A week; a month; a year? Much will depend upon the type of
system this is (OLTP or Decision Support, for example) and the volatility of the data.
It will also depend upon how critical not losing the database is to the organisation.
Whatever the decision, an accountant may see this as a very expensive safety net.
</p>
<p>If you are a bank, however, you will probably be willing to pay very considerable
amounts to ensure that there is no outage as a result of an upgrade or change to a
system. Your customers would, quite rightly, be very angry if they could not access
their cash whenever they wanted to. In these circumstances the servers involved
would probably already be mirrored (see later in the chapter about mirroring) with
DR stand-by servers involved. These will all probably need to be upgraded at the
same time too, and will need protecting as they are upgraded. This makes it a very
complicated and expensive process with the risk of significant financial damage if it
were to go wrong.
</p>
<p>10.2.1.2 Systems Design Problems and User Mistakes
RDBMSs are at the heart of many applications and e-commerce systems. Vendors
design their applications to interact with the database in many different ways and it
is possible for there to be mistakes in the coding. Whilst we should not get involved
in the wider discussion of systems testing, DBAs may need to be able to change
the values stored because a badly designed system element has caused some values
to be mistakenly altered. It might even be as simple as users misunderstanding the
input screens and typing wrong numbers into a dialogue box.
</p>
<p>DBAs themselves are not infallible. There may be issues like there not being
enough disk storage space available to write some inserts, or inappropriate settings
on the UNDO which cause processes to fail due to the &ldquo;snapshot too old&rdquo; error.
</p>
<p>Whatever the cause, it is not unusual for the DBA to be asked to return the row
or rows affected to the way they were at a point in time, or a point in a transaction
process to allow the correct values or operations to be applied.
</p>
<p>This process of reverting to previous values was made easier in Oracle with the
development of flashback technology. This was first released in Version 9i and has
been built on in subsequent versions. Oracle are not forthcoming about how this
technology works, but it is evident that the use of UNDO data, stored in the UNDO
TABLESPACE is important. Undo was around before 9i. It was sometime referred
to as Rollback segments as it serves as a place to store the values before any change
is made so that, should the ROLLBACK command be issued, changes can be un-
done. It is also used to allow read consistency (see Chap. 4).
</p>
<p>Let us take an example of a user who has just accidentally deleted a customer,
ID201213, from the CUSTOMER table. They contact the DBA and they can retrieve
the missing row by issuing the following command:
</p>
<p>INSERT INTO CUSTOMER
(SELECT &lowast; FROM CUSTOMER AS OF TIMESTAMP
</p>
<p>TO_TIMESTAMP(&lsquo;2013-04-04 09:30:00&rsquo;, &lsquo;YYYY-MM-DD HH:MI:SS&rsquo;)
WHERE cust_id = &lsquo;ID201213&rsquo;);</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Keeping the System Running&mdash;Immediate Solutions to Short Term 225
</p>
<p>The subquery (within the brackets) retrieves the row for ID201213 as it was at
a point of time, as described by the timestamp. This row is then inserted to the
CUSTOMER table as it is now.
</p>
<p>Even careless dropping of tables can be undone using flashback. Here we retrieve
a table called PERSONS that has been accidentally dropped:
</p>
<p>FLASHBACK TABLE persons TO BEFORE DROP;
</p>
<p>Perhaps the most powerful is the FLASHBACK DATABASE command which
allows you to revert the database to the way it was at a particular point in time,
or at a particular System Change Number (SCN). Naturally the database has to be
down for this to happen, and you will lose all the data between the point in time
and shutting the database down. Replaying data from the Redo logs or archive files
would allow some recovery.
</p>
<p>10.2.1.3 Technology Failures
As well as human-initiated problems, databases exist in a world of fallible technol-
ogy. The Hard disks (HDD) that many database management systems use to provide
permanency to their data are, at their core, very old technology. They record their
data by magnetising a very thin film of ferromagnetic material and this can become
corrupted, losing or altering the data as it does. That would result in the partial loss
of some data. The mechanisms which allow the disk to be spun and the disk head
reader moved to the appropriate place are also vulnerable to failure, and any such
failure would result in the loss of all the disk content.
</p>
<p>The measure used to describe disk reliability is Mean Time Between Failure
(MTBF). The fact that there is even a measurement for this should worry us! Some
HDD manufacturers claim 300000 hours between failures. But since this is a mean,
many devices could fail well before that time has elapsed. The point is they DO fail,
however occasionally. Even the most expensive disks fail; you pay the extra because
they fail less frequently. And if they fail in the middle of your company payroll run,
you can bet you will have many unhappy employees!
</p>
<p>The HDD is the weakest link in a database management system. As Sam Alapati
(2009) says in the Expert Oracle Database 11g Administration:
</p>
<p>The most common errors on a day-to-day basis are hardware related. Disks
or the controllers that manage them fail on a regular basis. The larger the
database (and the larger the number of disk drives), the greater the likelihood
that on any given day a service person is fixing or replacing a faulty part.
Because it is well known that the entire disk system is a vulnerable point,
you must provide redundancy in your database. Either a mirrored system or a
RAID 5 disk system, or a combination of both, will give you the redundancy
you need.
</p>
<p>We will review RAID later in this chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>226 10 Database Availability
</p>
<p>If you refer back to Chap. 3 you will see that Oracle keeps its data in Tablespaces
which are themselves supported by one or more Operating System files. If the area
of disk which stores the data gets corrupted, the DBA will need to recover that
file, potentially taking the tablespace offline (making it unusable) whilst the copy is
restored from a back-up. Naturally, whilst recovery can be relatively trivial in terms
of tasks to undertake, the loss of a part of the database can have expensive business
repercussions.
</p>
<p>Once the backed up replacement file is in place the DBA will need to &ldquo;replay&rdquo;
the changes made between the time of back-up and the failure. They do that using
the Redo Log files. More on this process follows in the Recovery section of this
chapter. But the fact that Redo is required for any recovery, and that Redo is also
stored on vulnerable HDDs means that we have to be extra careful with Redo files.
Critical elements of the database architecture, such as Redo and Control Files, are
stored multiple times across multiple disks to ensure that recovery for a disk failure
can be both rapid and, where possible, automatic.
</p>
<p>Redo Logs which consist of two or more files that store all the changes made to
the database as they occur, are so critical to the recovery task that they are multi-
plexed. This means at least two identical copies of the redo log are automatically
maintained, but on separate locations, ideally on separate disks.
</p>
<p>10.2.1.4 CPU, RAM andMotherboards
</p>
<p>Other parts of the server architecture are also vulnerable to failure. CPUs are made
up from many transistors and failure of one can cause the failure of the processor
itself. The failure can be triggered by excesses in temperature, for example.
</p>
<p>If your database is running in a single server, as it will be for most smaller scale
applications, then failure of CPU or motherboard can mean your database is down.
As we keep saying, that outage can be expensive to the company.
</p>
<p>Larger systems may well consist of more than one physical server. Oracle Ver-
sion 10 gained a &ldquo;g&rdquo; suffix, becoming Oracle 10g. The &ldquo;g&rdquo; stands for grid. In essence
grid, in Oracle terms, is about using multiple, relative cheap &ldquo;commodity servers&rdquo;,
all connected together working on the same database to meet an organisation&rsquo;s per-
formance and availability requirements. Oracle have two types of grid, as explained
in their online references:
</p>
<p>&bull; A Database Server Grid is a collection of commodity servers connected
to run one or more databases.
</p>
<p>&bull; A Database Storage Grid is a collection of low-cost modular storage ar-
rays combined together and accessed by the computers in the Database
Server Grid.
</p>
<p>They go on to say:</p>
<p/>
</div>
<div class="page"><p/>
<p>10.2 Keeping the System Running&mdash;Immediate Solutions to Short Term 227
</p>
<p>The same grid computing concepts can be used to create a standby database
hub that provides data protection, minimizes planned downtime, and provides
ideal test systems for quality assurance testing and all for multiple primary
databases
</p>
<p>Having multiple servers and redundant disks available was, and still can be, an
ideal way of managing availability. Some forms of Cloud computing offer this sort
of advantage without having to pay for and maintain multiple servers yourself, thus
replacing the high capital expenditure costs involved with hardware purchase with a
longer term spread of revenue costs as processing is &ldquo;rented&rdquo; from cloud providers.
</p>
<p>Storage as a Service is fast becoming a popular offering by providers. As this
allows an organisation to have redundant data spread across the world the service
offers potentially more security than having servers in one geographical location.
</p>
<p>10.2.1.5 Using RAID to Improve Availability
Some organisations will not be looking to the Cloud for some time to come. Those
that worry about security, or which have recently invested heavily in internal infras-
tructure, will still want to ensure their databases are as available as possible.
</p>
<p>A Redundant Array of Independent Disks (RAID) is one way of enabling higher
availability. Most external storage devices provide support for RAID. There are sev-
eral types of RAID types, known as levels. RAID can either improve performance or
provide higher availability, dependant upon the level used. As with many aspects of
database management there are trade-offs between the different desired outcomes.
Replicating data to more than one disk will add redundancy and thus improve avail-
ability, but, even if the writes are made in parallel, there will be an overhead involved
in ensuring the success of the write that will slow the write process down.
</p>
<p>Here we are concentrating on availability, and the RAID levels most often used
are RAID 1, also known as Disk Mirroring, and RAID 5. RAID 1 is the simplest
form of high availability implementation, and needs two disks. If the pair of disks
are available for parallel reading, queries can perform more quickly than against a
single disk. However, since it needs double the amount of disk space to store the
data it is not the most efficient mechanism.
</p>
<p>RAID 5 is a popular RAID level for enterprise systems since it offers better
performance than RAID 1 as well as high availability, and although it needs the use
of at least 3 disks, it is potentially less space hungry since not all data is replicated. It
manages this by striping both data and parity information across the three (or more)
disks. (See Chap. 3 for a discussion of Striping and Parity.)
</p>
<p>Parity arrays work well because they are built on the assumption that a HDD will
rarely fail entirely, but may on occasion fail in a sector of the disk. A recovery of
that lost sector is made possible by using the parity value, with the data you do have,
applying an exclusive or (XOR) and regenerating the missing data.
</p>
<p>If you have a system with n disks, you will need n &times; 2 disks for RAID 1, and
n + 1 for RAID 5. In addition to using less disk resource, RAID 5 can be better
performing than RAID 1 since your data is striped across more disks and therefore,</p>
<p/>
</div>
<div class="page"><p/>
<p>228 10 Database Availability
</p>
<p>with parallel reading enabled, potentially able to return data faster. The downside
however is that writes will be slower than RAID 1 since the parity calculation will
need to occur for all data being written.
</p>
<p>As usual there is no simple answer for the DBA. They need to know the sys-
tems they are supporting well to be able to select the most appropriate availability
mechanisms. Write-heavy OLTP systems, for example, might find RAID 5 too slow,
whereas it may be very suitable for a non-volatile Decision Support System.
</p>
<p>10.2.1.6 Recovery at Startup
Failures to write data can happen at anytime. They can even be as a result of a DBA&rsquo;s
actions if they issue the SHUTDOWN ABORT command, for example. This might
be the only way they can get the database to close, but it has to be a last resort since
any live SQL statements are terminated immediately.
</p>
<p>If the database has terminated uncleanly, either because of an ABORT, or some
system generated fault, Oracle&rsquo;s STARTUP command will try and carry out an au-
tomatic recovery. The process which looks after this is called SMON (System Mon-
itor) and this is a core process, which will cause the database to terminate if it were
to fail.
</p>
<p>The DBA may be lucky and the system may be able to get back to the last con-
sistent state, or they may have to recover files manually before opening the database
for wider access. They would do the latter by issuing the STARTUP NOMOUNT
command. This starts the database and allocates memory structures, but does not
connect to the disks, allowing changes to be made at the operating system level.
Once the recovery has happened the DBA can make the database attached to disks
and available to all users by issuing the ALTER DATABASE OPEN command;
</p>
<p>10.2.1.7 Proactive Management
You could, of course, greatly reduce the risk of disk failure on your system by simply
having a rolling replacement program that replaces your HDD resources every three
months, or perhaps six. Using the MTBF measure, at least statistically, you will have
a more robust system. However, against that possible improvement in availability
you have to recognise the actual cost of excess HDD purchasing.
</p>
<p>Silly though the above example may seem, if your data is core to your busi-
ness&rsquo;s success, you may well be willing to pay good money to reduce the risks of
failure. System maintenance should indeed be an important aspect of keeping a sys-
tem available. Replacement of servers and disks should be part of the strategy, with
replacements being timed to cause least disruption.
</p>
<p>10.2.1.8 Using Enterprise Manager
Most of the main RDBMS vendors have some form of management console. Ora-
cle&rsquo;s is called Enterprise Manager. It has a dashboard home screen on which appears
warnings of various types, some of which the DBAs will have set up for themselves.
You can configure it, for example, to warn you when a particular tablespace is get-
ting to 75 % full. That may give you time to go and buy some more disks to allow
for more expansion later. You can also have warnings emailed to the DBA team to
ensure that they are not overlooked.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Back-up and Recovery 229
</p>
<p>Since the database will, at the very least, reject inserted rows if the file supporting
the tablespace is full, and at worst, abort, taking a proactive approach to disk usage
is very much a key part of availability management.
</p>
<p>10.3 Back-up and Recovery
</p>
<p>As we have seen, we can&rsquo;t rely on the database platform being error free. To safe-
guard against unexpected data loss we need to take copies of the data known as
Back-ups. If you should lose any data, then you use the copies stored as a backup to
replace the lost data.
</p>
<p>Before we begin looking at backing up, we should just remind ourselves that the
logical data we see in our RDBMS is made permanent using physical media, such
as HDDs. There are, therefore, two types of back-up: Physical, which is the primary
concern of this chapter and which will be dealing with operating system level files,
and Logical which is more often to do with taking temporary copies of data, often
of tables, as a precaution when applying changes to the original data.
</p>
<p>Dealing with Logical first, let us follow an example. The user wishes to apply
a 5 % payrise to all employees with a Grade of &lsquo;B&rsquo;. Because things can go wrong
our cautious user wants to take a copy of the Employee table before applying the
changes. They are, in effect, backing up the table. This code might be what they use:
</p>
<p>Create Table oldEmployee as select &lowast; from Employee;
</p>
<p>. . . . do the processing
</p>
<p>. . . . When happy that you no longer need the Table back-up:
Drop Table oldEmplyee;
</p>
<p>As we have seen with the Flashback command, there is more than one way of
getting to the way the data was at a point in time, but this could be one solution.
</p>
<p>All the logical application data in the database, and all of the data dictionary data,
will actually be stored in operating system files. These files are precious and regular
copying will allow the database to be restored in the event of corruption to the files
used by the database. In Oracle these files can be either copied (backed up) directly
from the operating system, known as User-Managed back-up, or through an Oracle
process called RMAN which helps with the management of the back-up process by
collecting meta-data about what is being copied.
</p>
<p>Some important back-up concepts and terms used need exploring first:
1. Consistent Back-up
</p>
<p>A consistent backup is one where datafiles and control files are in agree-
ment with the system change number (SCN). The only way to ensure a con-
sistent Back-up is to issue a SHUTDOWN command other than SHUTDOWN
ABORT, and then take the back-up. An open database may well have important
data in memory rather than written to disk, so all files have to be closed. This</p>
<p/>
</div>
<div class="page"><p/>
<p>230 10 Database Availability
</p>
<p>of course means the database is unusable whilst it in down and the back-ups
are being taken. This is also referred to as a cold back-up.
</p>
<p>2. Hot Back-up
Shutting down the database is clearly not a good thing to do if users are
</p>
<p>being refused access as a result. The irony is that you are, albeit temporarily,
making the database unavailable to help with future availability.
</p>
<p>Hot back-ups are taken when the database is still open. If it is supporting
a 24/7 operation you may have very little choice but to use online backups.
However, the risk is that a datafile is being written to whilst the back-up is
being taken and this will result in an inconsistent back-up. This means that
using a Hot Back-up to restore from always runs the risk of needing to apply
redo data to remove the inconsistency.
</p>
<p>If you are in ARCHIVELOGS mode (that is, storing the redo logs so they
do not get overwritten) then Oracle can help manage the problem by placing
a tablespace being hot-backed up in a special status whilst the data is being
copied, in which no users can write data to the tablespace. Assuming the copy is
quite rapid users will see this as little more than a glitch in system performance.
</p>
<p>However, there is always a slight risk with hot backups that you do not
account for changes being made at the moment of back-up. These risks can
be limited, and so many DBAs will take the risk rather than bring the whole
database down to enable a consistent cold back-up. Backing up tablespaces one
at a time, rather than backing up all datafiles at once also spreads the overhead
of back-up processing, often allowing the user to see no adverse affect from the
back-up process.
</p>
<p>3. Whole Database Back-up or Partial Back-up and Incremental Back-ups
If we take a backup of the whole database whilst it is down we will have a
</p>
<p>consistent and immediately usable copy of the data. If the back-up finishes and
then one second later, once the database is restarted, it collapses, the DBA will
simply need to restore the back-up they have just taken.
</p>
<p>Unfortunately that sort of luck rarely happens! Full back-ups can take hours,
depending upon how much data is being stored. And unless you are fortunate
enough to work in an environment which doesn&rsquo;t mind the database being down
for a day, full back-ups are not going to be a popular move.
</p>
<p>So, one option is that you take a full back-up on one day, and then every day
thereafter you store only the changes that have been made to the data. These are
called incremental back-ups and they save only data blocks that have changed
since the previous back-up.
</p>
<p>The good thing about incremental is they are so much faster to store the
back-up. The bad thing is that, should you need to recover the database, it will
be so much slower since you will need to restore the last full back-up and then
apply the changes as stored in the incremental back-ups.
</p>
<p>The trade off between time to back-up and time to restore is again typical of the
DBAs balancing act. For different organisations, different back-up strategies will be
appropriate. In terms of the lifespan of the backed up data, the final decision is to
have a retention policy which specifies which backups need to be retained to meet</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Back-up and Recovery 231
</p>
<p>Fig. 10.1 Generalised view
of back-up process
</p>
<p>the organisation&rsquo;s recovery requirements. Probably the minimum would be the last
full back-up plus subsequent incremental ones, but there may be legal obligations
on the company to keep data much longer than this would enable.
</p>
<p>A generalised view of the back-up process can be seen in Fig. 10.1.
</p>
<p>10.3.1 MTBF andMTTR
</p>
<p>Two key metrics when deciding on availability strategies are:
&bull; Mean Time Between Failures (MTBF) which is a measurement of how long
</p>
<p>elapses between failure. This will be influenced by both the hardware in use
(which may fail) and the software systems and human interaction with it and
so will be different for every organisation. Historic failure data is often used to
estimate this by dividing a given period (say a month) by the number of times
the system was unavailable.
</p>
<p>&bull; Mean Time To Recover (MTTR) which is a measure of how quickly your
database will be available in the event of a failure.
</p>
<p>Oracle allows you to set a target time for MTTR and will automatically cal-
culate what it needs to do to ensure that this happens. The FAST_START_MTTR_
TARGET initialization parameter controls the duration of recovery and startup after
instance failure.
</p>
<p>However, as always, there is a trade off. Oracle speeds up the MTTR by forcing
the system to checkpoint more often. This ensures that the consistent data (which
can therefore be used immediately) is as big a proportion of the total data as possi-
ble, and that there is very little Redo data (with SCN greater than that at the check-
point) to apply to bring the database back to its position at failure. The increased
checkpoint activity is, however, detrimental to performance as it forces more write
activity, and if your system is an active OLTP system this could be quite a problem.</p>
<p/>
</div>
<div class="page"><p/>
<p>232 10 Database Availability
</p>
<p>10.3.1.1 Recovery
The other half of Back-up and Recovery is just as important as taking the back-ups.
A DBA needs to know where the most current back-ups are and how to apply them.
Typically this will mean the use of tape storage devices and the retrieval of tape
cartridges from a fire-proof safe. The cloud now offers a cleaner alternative, with
the back-up being retrieved from a Storage-as-a-Service provider.
</p>
<p>Two key terms are used in bringing a database back to as close to its state at
failure as possible;
&bull; Restore: Copying datafiles from a back-up to the server
&bull; Recovery: &ldquo;replaying&rdquo; the changes since the last back-up by applying the redo
</p>
<p>logs.
The first, and most critical part of any recovery is the notification of failure. Mod-
</p>
<p>ern dashboard systems like Oracle&rsquo;s Enterprise Manager will automatically notify
the DBA, sometimes by sending emails, when there is some error or warning from
the database. If the DBA is out on a picnic in an area with no telecoms available, the
recovery may take longer than it could, so having policies in place to ensure DBA
access between team members is an important starting point!
</p>
<p>Assuming the DBA gains actual, or remote, access to the database terminal the
next task is to establish the type of problem. SQL errors may require no intervention
other than talking to the user responsible. An error with one of the key processes
may require the database to be &ldquo;bumped&rdquo;&mdash;shutdown and restarted.
</p>
<p>But in terms of restoring data it is more likely that the DBA will be responding
to a disk failure or similar media related problem. If Oracle tries unsuccessfully to
read a data file on a disk it will flag the problem and the DBA can restore from a
back-up and replay any redo information to bring the file up-to-date. The database
will continue to run and be accessible to any processes not accessing that file.
</p>
<p>If Oracle can&rsquo;t write to a data file it takes it off line It is the datafile that is taken
offline&mdash;the tablespace remains online and can be used. If Oracle can&rsquo;t write to the
SYSTEM Tablespace (which contains the data dictionary) however, the database
will shut down automatically and a restore will be required from the last available
back-up.
</p>
<p>10.3.1.2 RMAN and Enterprise Manager
As usual we will use an Oracle database to provide examples of the tasks that make
up back-up and recovery. And again, as usual, Oracle provides two key approaches:
the GUI Enterprise Manager with built-in wizards; and the command line utility
called RMAN (Recovery Manager). You will need special privileges to issue these
commands, either granted to a specially created Backup Manager user, or using SYS
or SYSTEM DBA privileges.
</p>
<p>Newcomers may wonder why DBAs don&rsquo;t just use the far simpler interface. But
the fact is that many Oracle systems have grown from earlier versions of Oracle, and
many DBAs have years of experience of writing scripts to do the important mainte-
nance tasks in the database, and so the command line tool remains very popular.
</p>
<p>Let us start by comparing the approaches to resetting the MTTR. The Enterprise
Manager approach is shown below, and is followed by the command line equivalent
(Fig. 10.2).</p>
<p/>
</div>
<div class="page"><p/>
<p>10.3 Back-up and Recovery 233
</p>
<p>Fig. 10.2 MTTR setting in Enterprise Manager
</p>
<p>At the SQL prompt the equivalent is:
</p>
<p>SQL&gt; ALTER SYSTEM SET fast_start_mttr_target = 25 SCOPE=BOTH
</p>
<p>The Availability management page of Enterprise manager (11g) is shown in
Fig. 10.3, showing the sorts of tasks that can be controlled.
</p>
<p>And these are the commands required to do a full backup of a database called
orcl using RMAN (Fig. 10.4).
</p>
<p>10.3.1.3 Storing the Back-Ups
Whatever your back-up strategy you will need to decide on which medium you will
select to store the data. As usual there are good and bad points about most of the
alternatives.
</p>
<p>Solid State Drives (SSD) are becoming relatively cheap and can store large vol-
umes of data and because back-ups write few times, it could be argued this makes
SSD a good medium for back-ups. HDD are also dropping significantly in price/Gb
and are therefore a reasonable suggestion for storing back-up.
</p>
<p>Both of these media would be far quicker at the restore stage than would the more
traditional medium of magnetic tape, but, even at today&rsquo;s prices, they would also be
more expensive. As tape is a sequential read medium access time to any specific
section of a back-up is likely to be far slower to the HDD which allows random
access.
</p>
<p>Tape drives are relatively simple and not over-used and so they tend to last a long
time. Many organisations who invested in databases more than a few years ago will
therefore already have invested in a (tape oriented) back-up system which they will
not need to replace for a long time. The cost of the alternatives, therefore, is only
likely to be borne should there be a good business case for speeding recovery times.</p>
<p/>
</div>
<div class="page"><p/>
<p>234 10 Database Availability
</p>
<p>F
ig
.1
</p>
<p>0
.3
</p>
<p>U
si
</p>
<p>ng
E
</p>
<p>nt
er
</p>
<p>pr
is
</p>
<p>e
M
</p>
<p>an
ag
</p>
<p>er
to
</p>
<p>co
nt
</p>
<p>ro
l
</p>
<p>B
ac
</p>
<p>k-
up
</p>
<p>ta
sk
</p>
<p>s</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Disaster Recovery (DR) 235
</p>
<p>Fig. 10.4 Using RMAN from the command prompt
</p>
<p>In the last couple of years Storage as a Service has meant that HDD storage has
become more popular by default as Cloud users access disk farms made available by
the service providers. As the service is rented there is no up-front capital investment
needed to provide back-up facilities, but the revenue cost will eventually accumulate
over time and may well overtake any capital cost that might have been incurred
instead.
</p>
<p>10.4 Disaster Recovery (DR)
</p>
<p>Whilst Back-ups ensure the longevity of an organisation&rsquo;s data, they are not really
a means of keeping a database readily available. Should a major event, such as an
earthquake or tsunami destroy your company&rsquo;s data-centre, the presence of a set of
back-up tapes stored off-site will allow you to get your database back&mdash;but only
eventually, and only to the point of the last back-up!
</p>
<p>Finding and procuring replacement IT infrastructure will take time, maybe even
weeks. And then the process of rebuilding the database from the back-ups could be
lengthy too, especially on a large database which has been backed-up using incre-
mental back-ups. The back-up medium can also slow things down as tape is not the
fastest (but is among cheapest) storage method.
</p>
<p>In our often quoted example of a bank, having your system unavailable for weeks
is going to do you a lot of damage&mdash;maybe even enough to put you out of business.
When the disaster happens you need to be able to, as seamlessly as possible, get the
system up and working again.
</p>
<p>An example of the sort of damage that happens to businesses the Bank of New
York (BONY) immediately post 9/11. The oldest bank in the US had a DR strategy
but found that because most operations were heavily concentrated around Manhat-
tan, and because of the scale of the disaster, they were unable to keep in reliable
contact with customers.
</p>
<p>As the Wall Street and Technology online journal (2001) said (Guerra 2001):</p>
<p/>
</div>
<div class="page"><p/>
<p>236 10 Database Availability
</p>
<p>But the temporary loss of an entire data center and a damaged telecommu-
nications network left BONY struggling to connect with its customers and
industry utilities like the DTC.
</p>
<p>BONY&rsquo;s difficulties during the hours after the attack gain gravity when its
vital role in world of securities processing is understood, making the fact that
it could regroup from the disaster&mdash;and do it quickly&mdash;of intense interest to
more than just the firm&rsquo;s shareholders.
</p>
<p>Demonstrating how short-lived can be feelings of sympathy where money is con-
cerned, the article goes on to say:
</p>
<p>Starting just days after the attack, BONY began to be pilloried in the press for
systems malfunctions and other technological failures which had purportedly
left billions of dollars in securities transactions unsettled. Articles cited angry
customers who were misled about the status of BONY systems, demanding
the facts on unsettled orders.
</p>
<p>And yet it is clear that the Bank did have a disaster recovery strategy and that it
worked. However, it did not work well enough to meet expectations of some cus-
tomers and commentators.
</p>
<p>10.4.1 Business Impact
</p>
<p>It is assumed that some level of difficulty would follow the loss of an organisation&rsquo;s
database. After all, why incur the expense of having one if it isn&rsquo;t that vital? How-
ever, it may be that your business will not suffer unduly if you lose access to your
database for a few days. In other words, not everyone needs to spend lots of money
on HA.
</p>
<p>So one of the first steps required when considering your DR strategy is to measure
the potential impact of the loss of your database systems. This will involve tangible
(for example, lost production) and intangible costs (for example, loss of goodwill)
and discovering just which records are vital and time-critical. As usual, the more
broadly robust you make your systems, the more expense you will incur, so you will
need to balance the potential losses against real costs.
</p>
<p>One key variable in the size of the costs is the time you are willing to be without
your database. This is referred to as the Recovery Time Objective (RTO). In general,
the shorter the RTO the more expensive the solution. So the decisions about DR tend
to be business driven, not technology led, although there are alternative approaches
available as we shall see in the next section.</p>
<p/>
</div>
<div class="page"><p/>
<p>10.4 Disaster Recovery (DR) 237
</p>
<p>10.4.2 High Availability for Critical Systems
</p>
<p>We have already discussed a couple of key methods of maintaining availability. In
some circumstances&mdash;let&rsquo;s stick with banks as a good example&mdash;Availability is of
critical importance to the business and expense is much less of a barrier.
</p>
<p>These are the circumstances in which the term High Availability (HA) tends to
get used. There are several approaches the database architect can take:
&bull; Stand-by databases which are copies of the Master database and which kick-in
</p>
<p>in the event of failure of the Master
&bull; Cloud-based stand-by services
&bull; Clustering the database, using several servers to provide some redundancy. Dif-
</p>
<p>ferent vendors use different approaches to clustering:
&ndash; Shared Disk (for example, Oracle) where separate nodes each run an in-
</p>
<p>stance of Oracle but share the same database (that is data, redo logs, control
files). The database must sit on a Storage Area Network (SAN), which pro-
vides the capability to connect to many servers, usually through high-speed
fibre connections. Having multiple servers allows for automatic fail-over to
happen and connections seem to persist, even if, in the background, another
instance has taken over. Disk redundancy is managed by the SAN.
</p>
<p>&ndash; Shared nothing (for example MySQL) where multiple servers and their
disks save copies of the same database. Data within each node is copied
to at least one other data node. If a node fails, the same information is al-
ways to be found in at least one other server.
</p>
<p>&bull; Distributed databases, providing the copies of the same database on multiple
sites, allowing continuous operations even if a catastrophic event happens in
one geographic location.
</p>
<p>A standby system is the traditional form of providing DR. Having one, or several
identical servers running an exact copy of your live system allows the organisation
to switch between servers in the event of a failure to one.
</p>
<p>The most significant issue with stand-by servers is when and how you do the
copy process. If money is no object and you can manage your connections in-house,
the speediest way to bring a standby server up is to use an approach similar to RAID
1, with two servers mirrored. This solution requires good network connectivity and
the required infrastructure can be expensive to provide. The standby server is on and
running all required applications. This is often referred to as Hot Standby.
</p>
<p>A less costly solution is to send packets, for example a day&rsquo;s worth, of archived
redo logs to the standbys and then apply the changes. The downside to this solution
is that its can take time to replay the redo information so fail-over, whilst it can be
automated, is not instant. This is known as Warm Standby.
</p>
<p>A Cold Standby is the least complex solution which differs little from a Back-up
and Recovery strategy except that the recovery is to a different server in a different
location. The standby server, when needed, is started. A valid backup may need to
be applied and as much redo history as can be replayed is used to bring the database
to as close to current as possible. This approach will be much slower that the other
two approaches.</p>
<p/>
</div>
<div class="page"><p/>
<p>238 10 Database Availability
</p>
<p>Having a standby system available is not a new idea by any means. But us-
ing the cloud as a DR solution is. Specialist services providers are beginning
to appear in this area. OpSource, for example (http://www.opsource.net/Services/
Cloud-Hosting/Managed-Services/Cloud-Based-Disaster-Recovery) are suggesting
a 4 hours restore time for a standby server is much cheaper in the cloud than on
in-house owned hardware. The basis of the saving is that the database server is built
and then taken off-line. As with most cloud solutions, the highest rental cost is for
CPU and RAM usage, which are clearly zero whilst the database is offline. Should
a disaster happen the server can be restarted and restored and connected to by users.
</p>
<p>Since Storage As A Service is becoming relatively affordable and convenient
even smaller organisations can provision their own DR using a provider&rsquo;s disk space
and just start-up a server and restore the data for themselves.
</p>
<p>10.4.3 Trade-offs and Balances
</p>
<p>As we saw in Chap. 5, CAP theory has it that we can&rsquo;t have everything: out of
Consistency, Availability, and Partition Tolerance, we can only have two. NoSQL
databases, and Big Data systems like Hadoop, are built to be massively distributed.
Availability comes as a result of the built-in redundancy from replicating the data
across many nodes.
</p>
<p>If always being able to access the data, regardless of network breakages is the
most important thing to your organisation the theory tells us we can&rsquo;t guarantee
anything other than eventual consistency. If both availability and consistency is crit-
ical (that is all the data across nodes agrees) then you have to live with the fact that
your systems may be vulnerable to network problems to the degree that a network
outage means your database is unavailable. This approach is what happens in many
traditional RDBMS systems. Replication is used for availability and protocols such
as the two-phase commit are used for consistency.
</p>
<p>10.4.4 The Challenge or Opportunity of Mobile
</p>
<p>As more organisations encourage employees to use their own devices such as smart
phones and tablets, there is a danger of critical information being spread out in an
unmanageable way. Data collected on a tablet but not synchronised with a server is
exceptionally vulnerable, and yet could be business critical in nature.
</p>
<p>This is a new challenge for information professionals. Adopting appropriate
Bring Your Own Device policies which include synchronisation and back-up is
clearly one useful step.
</p>
<p>But the fact that some of these devices have relatively large storage available
on-board, and the very fact that they are mobile means they are likely to be in a dif-
ferent geographical location to the central servers, or even other devices. A natural
distributed system which perhaps could be used as part of a DR strategy? At the
time of writing this technology is still new and this may end up being the section of
this book that future readers laugh at. . . but who knows!</p>
<p/>
<div class="annotation"><a href="http://www.opsource.net/Services/Cloud-Hosting/Managed-Services/Cloud-Based-Disaster-Recovery">http://www.opsource.net/Services/Cloud-Hosting/Managed-Services/Cloud-Based-Disaster-Recovery</a></div>
<div class="annotation"><a href="http://www.opsource.net/Services/Cloud-Hosting/Managed-Services/Cloud-Based-Disaster-Recovery">http://www.opsource.net/Services/Cloud-Hosting/Managed-Services/Cloud-Based-Disaster-Recovery</a></div>
</div>
<div class="page"><p/>
<p>10.5 Summary 239
</p>
<p>10.5 Summary
</p>
<p>This chapter has reviewed the need to keep databases available for as long as possi-
ble. We discussed the trade-off between seeking Five Nines availability and the ex-
pense that that sort of robustness can incur. We then went on to talk about Back-up
and Recovery types, and finished by looking at alternatives for limiting the down-
time a database may suffer because of some sort of disaster.
</p>
<p>10.6 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What is meant by &ldquo;Five Nines&rdquo; in terms of database availability?
&bull; What element in a typical database server is most vulnerable to failure?
&bull; What is meant by MTTR? And MTBF?
&bull; Describe how RAID 1 and RAID 5 can be part of an availability strategy
&bull; How does Shared Nothing differ from Shared Disk?
</p>
<p>10.7 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 You are asked to consider what can be done to mitigate against
a potential disaster, such as an earthquake or flood, hitting your organisation&rsquo;s data
centre.. Discuss some different approaches there are to Disaster Recovery whilst
relating your discussion to the business requirements being addressed by each ap-
proach. Consider drivers like stakeholder expectation, cost, technical capability.
</p>
<p>Discussion Topic 2 Keeping copies of the organisation&rsquo;s database is obviously a
sensible precaution. There are, however, pluses and minuses to each potential ap-
proach. Imagine you are a DBA faced with implementing a back-up strategy. What
options will you have before you, and what factors would impact your final strategy
selection?
</p>
<p>References
</p>
<p>Alapati S (2009) Expert Oracle database 11g database. Apress, New York
Guerra A (Nov 2001) The buck stopped here: BONY&rsquo;s disaster recovery comes under at-
</p>
<p>tack. http://www.wallstreetandtech.com/operations/the-buck-stopped-here-bonys-disaster-rec/
14703629</p>
<p/>
<div class="annotation"><a href="http://www.wallstreetandtech.com/operations/the-buck-stopped-here-bonys-disaster-rec/14703629">http://www.wallstreetandtech.com/operations/the-buck-stopped-here-bonys-disaster-rec/14703629</a></div>
<div class="annotation"><a href="http://www.wallstreetandtech.com/operations/the-buck-stopped-here-bonys-disaster-rec/14703629">http://www.wallstreetandtech.com/operations/the-buck-stopped-here-bonys-disaster-rec/14703629</a></div>
</div>
<div class="page"><p/>
<p>11Database Performance
</p>
<p>What the reader will learn:
&bull; that the different physical and logical aspects of database performance make
</p>
<p>tuning a complex task
&bull; that optimising for read and write performance is not the same thing and the
</p>
<p>this can cause conflicts when tuning
&bull; that database tuning is an ongoing requirement in an active OLTP system
&bull; that there are several types of index, each aimed at returning specific kinds of
</p>
<p>data more rapidly
&bull; that disk operations are amongst the slowest element of any database operation
</p>
<p>The examples we deal with in this chapter are primarily based on Client/Server
RDBMS technology. Of course, as we saw in Chap. 5, other types of database tech-
nology do exist, and do claim to bring high performance in certain scenarios.
</p>
<p>The worked examples and diagrams rely heavily on Oracle. We have used Oracle
11g. However, much of what is covered holds true for versions back to 8i. Many
architectural diagrams will hold true in principle for SqlServer, MySQL and most
other true Client/Server databases.
</p>
<p>11.1 What DoWeMean by Performance?
</p>
<p>For users&mdash;very important stakeholders in any system&mdash;database performance is
often simply measured in terms of how quickly data returns to their application.
And they will often intuitively know when that is &ldquo;too slow&rdquo; despite not having
timed the process.
</p>
<p>Returning data can be a very important element of a database system&rsquo;s require-
ments. In a Decision Support System (DSS), for example, the system may well
spend more than 90 % of its processing time serving out results sets and only ever
have data loads occasionally. OLTP systems, on the other hand are often writing new
rows, or updating existing ones, whilst relatively less frequently answering queries.
</p>
<p>Heavy use of indexing to speed query output will be very likely to slow inserts
and updates (since the system has more information to store). The need, therefore,
whether your system is read- or write-intensive is a very good starting point.
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_11,
&copy; Springer-Verlag London 2013
</p>
<p>241</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_11">http://dx.doi.org/10.1007/978-1-4471-5601-7_11</a></div>
</div>
<div class="page"><p/>
<p>242 11 Database Performance
</p>
<p>Fig. 11.1 Elements which
influence database
performance
</p>
<p>So performance is not only about the speed with which queries are answered.
As we look further at the building blocks of RDBMS we will need to come to a
more rounded view. Performance is about ensuring you use all the system resources
optimally in the process of meeting the application&rsquo;s requirements, at the least cost.
</p>
<p>Modern database systems are typically built on several layers of technologies.
When running a SQL query, the physical parts of the host server can be just as
important in determining how quickly we get the result report runs as any of the
processes within the RDBMS itself (see Fig. 11.1).
</p>
<p>We can divide the elements that might impact database performance into the
following broad, interrelated categories:
</p>
<p>Physical layer, such as disks and RAM
Operating System (OS)
Database Server processes
Schema level: Data types, location and volumes
SQL optimisation
Network
Application
</p>
<p>Typical production databases are dynamic, growing entities, which means that
yesterday&rsquo;s perfectly tuned database can become today&rsquo;s performance dog just be-
cause it is being used. We will review the influence the data itself can make on
performance later in the chapter.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.2 A Simplified RDBMS Architecture 243
</p>
<p>A good database professional needs to understand all the factors that impact upon
the performance of their databases. Proactive performance tuning can save a DBA
time in the long run as reactive problem chasing can be both time consuming and
detrimental to the business.
</p>
<p>Before we start, we should clarify some of the important aspects of database
performance:
</p>
<p>Workload is the combination of all the online transactions, ad hoc queries, data
warehousing and mining, batch jobs, and system-generated commands being ac-
tioned by the database at any one time. Workload can vary dramatically. Some-
times workload can be predicted, such as peaks with heavy month-end process-
ing, or lulls after the workforce has gone home. Unfortunately, however, DBAs
do not always have the luxury of such predictability. This is especially so in
a 24/7 environment, such as an e-commerce system. Naturally, workload has a
major influence upon performance.
</p>
<p>Throughput is a measure of the database&rsquo;s capability to carry out the process-
ing involved with the workload. There are many factors that can impact upon
throughput, including: Disk I/O speed, CPU speed, size of available RAM, the
effectiveness of any workload distribution such as parallel processing, and the
efficiency of the database server operating system.
</p>
<p>Contention is the condition in which two or more components of the workload
are attempting to use a single resource. Multiple users wanting to update a sin-
gle row, for example. As contention increases, throughput decreases. Locks are
placed on resources to ensure that tasks are completed before the resource is
made available again, resulting in waits. The user notices this and calls it poor
performance.
</p>
<p>11.2 A Simplified RDBMS Architecture
</p>
<p>Before we look at the individual performance factors, it would be good to have some
idea of what happens when a user interacts with a database, so that we know where
to look for improvements.
</p>
<p>Review Fig. 11.1 and, with this diagram in mind, let us walk through one inter-
action, which is a simple fetch of data:
</p>
<p>1. The user starts their client-side application process. This could be a simple
command-line process, like SQLPLUS, or a full blown application.
</p>
<p>2. The client process connects to the rdbms server process.
3. An area of rdbms managed RAM is set aside to hold inputs and outputs, and
</p>
<p>to allow server-side processes on behalf of the client, such as sorting
4. The request for the rdbms to process some SQL is passed over the connection
</p>
<p>from client to server.
5. The SQL string is then compared with the SQL which has been run by the
</p>
<p>server before, which is stored in the library cache. If this query has been run
before the execution plan can be recalled from cache and reused, saving hefty
processing on parsing and execution plan generation.</p>
<p/>
</div>
<div class="page"><p/>
<p>244 11 Database Performance
</p>
<p>Fig. 11.2 Phases in
processing an SQL query
</p>
<p>6. If this is a new piece of SQL then the string needs to be parsed. This
checks that the objects (tables, indexes etc.) referred to exist and that the
user has access rights. It checks that the statement is syntactically correct.
It will also gather run-time values if there are bind variables involved in the
query.
</p>
<p>7. So now we have a valid, new query that we have sufficient privileges to run,
we then pass over to the optimiser (see section on optimisation below) to
determine the best way to gather the information.
</p>
<p>8. The optimiser will generate an execution plan
9. The SQL processor will then carry out the instruction in the plan
</p>
<p>10. As this is a query which requires data to be fetched, then the RDBMS will
have to maintain read consistency to ensure that data it returns is from a cer-
tain point in time and not affected by any changes made by other users during
the fetch.
</p>
<p>11. If the data has been fetched by another user recently enough for it not to have
dropped out of the buffer cache, and it and is clean (it has not been changed),
the RDBMS will gather the data from the data buffer rather than use a time
consuming disk read.
</p>
<p>12. If this data isn&rsquo;t in the cache, it will be read from disk and placed into the
buffer cache.
</p>
<p>13. If there are any post fetch tasks, such as sorting, these can be carried out at
the server or at the client.
</p>
<p>14. Once all gathered the requested dataset will be returned to the client for use
in the user application.
</p>
<p>With this set of core processes in mind, we will now examine each of the layers
we outlined above.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 245
</p>
<p>11.3 Physical Storage
</p>
<p>Unlike RAM, the medium used to store permanent data in a database has to be non-
volatile&mdash;the data stored must remain the same regardless of whether the device has
power to it or not. Most database servers are dependant upon a disk, or array of disks,
to enable the permanency of the data. This is a weak point in terms of performance
since disk operations are amongst the slowest elements of any database process. The
DBA&rsquo;s task, therefore, is more about accepting that there will be some delays as a
result of I/O latency, and finding ways to minimise it.
</p>
<p>Much of the database&rsquo;s inbuilt processing will already be automatically attempt-
ing to minimise the disk I/o. Where possible data that has been fetched from disk
will be kept in the server&rsquo;s RAM for as long as possible in case other users may want
the same data. Naturally, in a very busy production system this can mean either a
huge (and therefore expensive) bank of RAM, or a volatile data pool which has to
swap out data from memory frequently, thus causing more disk reads.
</p>
<p>11.3.1 Block Size
</p>
<p>Of course we should never loose sight of the fact that we are storing ons and offs on
a magnetic material when we store our data. The operating system is given the job of
managing the disk handling required to seek a piece of information, but what it will
return will be a &ldquo;block&rdquo; of ons and offs which the RDBMS will have to manipulate.
It may have to extract a small part of what is returned. Or it might have to send the
disk head reader to several places, extract data from each read, and then glue the
portions together.
</p>
<p>In an Oracle database objects are stored in a Tablespace. This is a container for
tables, indexes Large Objects, and is a continuous area of physical disk that is un-
der Oracle&rsquo;s control and can&rsquo;t be used by other applications. The Tablespace is the
Logical-Physical boundary since one of the parameters passed when issuing the Cre-
ate Tablespace command is the physical o/s file(s) being used to store the contents.
</p>
<p>What the operating system will deal in is a block. Block size is set at the OS
level. Db block, which is what the RDBMS deal in, needs to be a multiple of that
figure in order that we do not waste time retrieving or writing O/S blocks which are
only part full of the data we want to collect.
</p>
<p>The Oracle Tuning Guide offers this advice:
</p>
<p>A block size of 8 KB is optimal for most systems. However, OLTP systems occasionally
use smaller block sizes and DSS systems occasionally use larger block sizes.
</p>
<p>Now we have another circumstance when the DBA needs to understand the type
of database he is managing. In two extremes, for example, we could have a Data
Warehouse which rarely has data added and is always causing the SQL engine to
read large datasets from disk. On the other hand we might have an OLTP system
recording only small amounts of data from a sales line, and nearly always serving
small datasets back to queries.</p>
<p/>
</div>
<div class="page"><p/>
<p>246 11 Database Performance
</p>
<p>The task we want to help the HDD avoid is spinning the disk and moving the
read-head to a new position, since this is a really slow part of the overall process.
If, in the data warehouse example, all the data is physically all together in adjacent
disk blocks it would be good if we could &ldquo;suck up&rdquo; all those with only one head-
positioning movement. This is why Oracle controls an area of disk in a Tablespace.
It attempts to manage that portion of disk such that data is most likely to be adjacent
to similar data.flash
</p>
<p>11.3.2 Disk Arrays and RAID
</p>
<p>The main database vendors all support reading and writing to multiple disks. There
can be different reasons for doing this (such as robustness and performance), but in
recent years the driving force has probably been the low cost nature of HDD, and
the ease with which they can be slotted into a rack.
</p>
<p>RAID technology is aimed to either improve the I/O performance of the disk
subsystem or to make the data more available. There is usually a trade-off to be had,
but some RAID configurations attempt to do both. The other variable to consider
is cost, since, generally speaking, the more complex the RAID management system
needs to be, the more it will cost.
</p>
<p>These storage options can impact on a system&rsquo;s performance. Striping your data
such that subsequent data writes are made to different disks in a round-robin, can
have a large effect on read speed if parallel processing is enabled since data can be
read from all disks at once and then glued together in memory. More information
about RAID and arrays can be found in Chap. 3.
</p>
<p>11.3.3 Alternatives to the HDD
</p>
<p>All of this section so far has been written with the standard HDD as the perma-
nent storage device in mind. There are alternatives. In more recent times mobile
computing has come to the fore, and the shock-resistant, lightweight and power ef-
ficient nature of flash memory makes it a sensible addition to embedded systems or
portable computing devices. RAM is still faster than flash, but in read operations
flash can outperform a HDD by factors of several hundreds. This is primarily be-
cause there is no spinning platter to add the mechanical resistance the HDD suffers
from. One drawback however is that erasing, and therefore effectively managing the
disk content, is considerably slower for flash. At the time of writing, flash is also
considerably more expensive/Mb than a HDD.
</p>
<p>Whilst the need to be mobile can excuse the weaknesses of flash in some ap-
plications, it is unlikely that many large scale commercial applications will rely on
flash just yet, and research papers are being published which identify hybrid solu-
tions which attempt to take advantage of flash to speed certain operations, whilst
maintaining the traditional virtues of the HDD.
</p>
<p>Perhaps the most extreme form of performance enhancement is to have a totally
in-memory database.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 247
</p>
<p>Fig. 11.3 Operating Systems supported by Oracle
</p>
<p>One example is SAP HANA, used for hefty analytics, business processes, and
predictive analysis this combines columnar data storage (see Chap. 5) with parallel
processing and in-memory computing. Another example is Oracle&rsquo;s Times Ten in-
memory database which can be used for any time-critical database processes as well
as analytics.
</p>
<p>The similarity for both is that performance comes as a result of processing en-
tirely in memory. In order to do this they are likely to need large amounts of RAM.
The downside to this approach is that RAM is nowhere near as cheap as HDD stor-
age, so these are expensive systems for specialist use.
</p>
<p>They have to provide durability and this is achieved by some form of logging
transactions to disk, so they are not truly disk-free solutions. However, persistence is
probably the most important attribute of a database system, so this can&rsquo;t be avoided.
</p>
<p>11.3.4 Operating System (OS)
</p>
<p>Modern commercial databases run on a variety of OS platforms. The OS platforms
supported by Oracle, for example, can be seen from this screen shot of their docu-
mentation page (Fig. 11.3).
</p>
<p>The major player which bucks the trend of having multiple versions is Mi-
crosoft&rsquo;s SQL Server which runs only on a Windows platform. However, although
overall they are behind Oracle on sales, on the sale of databases to run on Windows
platforms, Microsoft performs much better.
</p>
<p>The perception amongst professionals can to be that the Unix OS was always
built for true multitasking and is therefore a more robust starting point for any criti-
cal production system. Microsoft, naturally, would disagree with this. In actual fact,
all OSs have there own strengths and weaknesses. A database professional may in-
deed have to work in an environment where there is a mix of server OS systems. To a</p>
<p/>
</div>
<div class="page"><p/>
<p>248 11 Database Performance
</p>
<p>degree modern RDBMS systems, once installed and running, make the OS invisible.
This is especially so since GUI-based management consoles have come along to re-
place the script-based maintenance. However, even with GUI management consoles,
in order to be more employable, a mixture of OS experience improves any CV.
</p>
<p>11.3.5 Database Server Processes
</p>
<p>Whatever the OS, the RDBMS has to perform a number of processes in order to en-
able all the rich functionality available in most modern client/server systems. Some
of these may not seem likely to have an impact upon performance, but they can
cause problems if ignored or misunderstood by the DBA.
</p>
<p>The connection process and how that works for example, may not seem immedi-
ately likely to be a cause for performance problems. However, taking Oracle as an
example, there are a number of parameters which can be altered, depending upon
the connection workload and the types of tasks being undertaken at the client end.
</p>
<p>The default is for a client session to be given their own server-side process once
the connection is made. This, in effect, reduces the server&rsquo;s available RAM, since
a portion is now given over to the client. When you have thousands of concurrent
users attaching to your system, this can mean that RAM gets used up and swapping
to (slow) disk will occur more regularly.
</p>
<p>This one-to-one relationship between client and server is the standard connection
type in Oracle. If you create such a connection and then go and make a cup of tea
the area of RAM is needlessly occupied when it could be used elsewhere. You can,
however, opt for a shared server setting in your start-up parameters. This will mean
that a large area of RAM is set aside for shared use by all users, with the expectation
that pauses and delays that are a natural part in any SQL process will even out the
load across the users.
</p>
<p>Another RDBMS background process is a process manager. This constantly
checks to make sure that all open connections, with their associated server sessions,
are active. Any timed-out sessions are killed by this monitor, since every connection
takes some resource, even if it isn&rsquo;t active, thus affecting overall performance.
</p>
<p>11.3.6 ArchiveManager
</p>
<p>As we have seen elsewhere there are often conflicting needs that a DBA needs to
balance. Availability vs Performance is one area of regular tension. Writing data to
the Redo Log is an essential part of making sure the database is not compromised
during any outage and ensuring that the service, when it comes back up, needs
minimum manual correction.
</p>
<p>In the default mode, a number of redo logs, which are preallocated files, are filled
in a round-robin fashion such that when the last disk is filled, the Log Writer writes
the next value to the first disk, overwriting what is there.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 249
</p>
<p>All of this is fine in a development environment, but in a production database you
would normally want to be able to recover a database to any point in time, and to do
that you need to store all the redo information. In Oracle you do this by issuing this
SQL command:
</p>
<p>Alter database Archivelog;
</p>
<p>You need to be aware, however, that this will also slow your system down. Usu-
ally, large redo log files provide better performance, but obviously they take up more
disk space.
</p>
<p>11.3.7 Schema Level: Data Types, Location and Volumes
</p>
<p>The whole relational thing of relating connected tables, of normalising to reduce
data duplication, was largely a response to the expense of disk storage. At the time
when Codd&rsquo;s model was beginning to be used in earnest a &ldquo;big&rdquo; HDD was around
1 Mb to 5 Mb and would cost about 1/10th of the average annual wage. Today the
average weekly wage would allow you to buy a large HDD and have lots of change!
The Per Gb cost of HDD storage has dropped in the same period from thousands of
dollars to fractions of a dollar. In short, it made sense to reduce the amount of data
being stored.
</p>
<p>As HDD have become much less expensive, and able to hold so much more data,
this driver has become less important. Whereas most database courses stress the im-
portance of normalisation, practitioners began to realise that the join which enabled
related data to be reconstructed in user queries was one of the most time consuming
tasks undertaken by the database. De-normalisation&mdash;purposefully allowing data to
be repeated to avoid joins&mdash;became a useful performance tool.
</p>
<p>To an extent the new wave of NoSQL databases (see Chap. 5) recognises this.
MongoDb, for example, is completely schemaless and allows any old mix of data
and datatypes and does not have a mechanism equivalent to normalisation.
</p>
<p>With the advent of XML as the de facto medium for data exchange between het-
erogeneous databases DBA&rsquo;s have had to find ways to store and manipulate XML.
Oracle created a new datatype to cope with this. XMLTYPE is an extension to the
existing Character Large Object (CLOB) type and it provides functionality to the
user, such as XQuery if the data is stored in that datatype.
</p>
<p>However, yet again, our DBA needs to know what actually is going to happen
to the data when it is stored. If there will be many Xqueries to search and manip-
ulate the data then the performance overhead of using the XMLTYPE might well
be justified. However, data loads can be many times slower for the same data being
stored as XMLType as compared to CLOB. And Xquery has been tested as slower
than SQL in a number of circumstances. Another alternative is to map the incoming
XML into relational tables, allowing users to access the data using SQL.
</p>
<p>As we can see the decisions taken at design time can have a considerable effect
upon performance and these decisions need care.</p>
<p/>
</div>
<div class="page"><p/>
<p>250 11 Database Performance
</p>
<p>11.3.8 SQL Optimisation
</p>
<p>Imagine you need to get this information from your rdbms:
</p>
<p>Select a.X, a.Y , b.Z from employee a, department b where a.deptid = b.departmentno and
b.departmentno = 22;
</p>
<p>Could you have written this SQL any better? Will this bring back the data you
want in the quickest way possible?
</p>
<p>The good news is that in most cases you don&rsquo;t need to worry about the perfor-
mance aspects of your query. The parser will pass the requirements over to some-
thing called an Optimiser. This inbuilt process is responsible for, in effect, re-writing
any query into the most efficient possible. And it often surprises newcomers to op-
timisation that there are likely to be hundreds of ways to return the data you are
looking for.
</p>
<p>In actual fact the optimiser will not re-write your query. It will simply try and
work out which access paths are the most efficient in terms of returning the dataset.
Naturally it will depend upon the design of your database and the data distribution
within it. Some of the most straightforward options before it might include:
&bull; Using an index on table a
&bull; Using an index on table b
&bull; Doing a full table scan on b followed by using an index on a
&bull; Using a full table scan on both
</p>
<p>This would get more complicated depending upon:
&bull; Whether there is more than one index to choose from
&bull; Whilst b.departmentno = 22 may be likely to return less than 15 % of the rows
</p>
<p>and therefore be a good candidate for the use of an index, the optimiser needs
to estimate the proportion of all rows that will be returned from table a, before
it can decide on whether or not to use an index
</p>
<p>Once these decisions are taken the optimiser will create a Query Plan. You can
usually examine the choices made (in Oracle there is Explain Plan&mdash;see Tools sec-
tion below). And you can force the database to do something different if you know
better (see Hints in the Tools section).
</p>
<p>In order to make sensible decisions, the optimiser needs to do better than just
guess. In earlier versions of Oracle the process was one of following rules (Rule-
based Optimisation&mdash;RBO). At their most basic the rules say things like:
&bull; Only do a full table scan if there is no option
&bull; Use a single row fetch using a Rowid rather than a Primary Key if possible
</p>
<p>These rules are simple to implement and therefore the processing involved is
relatively trivial. However Oracle moved away from RBO and it has been obsoleted
since 10g. The default optimiser is now a cost based one (CBO). The CBO approach
is a response to the fact that the RBO does not pay any regard to the type and
distribution of data being queried, but rather blindly applies a single set of rules.
</p>
<p>CBO works by estimating the resources (primarily disk I/o and cpu usage) that
will have to be used for each of the possible access paths and then selects the least
&ldquo;expensive&rdquo; access method. In order to do this estimation the optimiser must have</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 251
</p>
<p>an understanding of the data stored. It does this by gathering statistics for every table
and storing these in the data dictionary.
</p>
<p>The use of statistics is, in its own right, a balancing act for the DBA. The statistics
stored are about the size, cardinality and distribution of the data stored. But the
statistics are only useful if they are reasonably current, allowing accurate estimates
to follow. However, the process of gathering the statistics is, itself, an intensive one
and slows the database down. A DBA needs to know how regularly to update the
statistics. Of course, if the database isn&rsquo;t used at certain periods, such as weekends,
this is a job that can be scheduled to run then. But in a 24/7 environment this is more
tricky.
</p>
<p>11.3.9 Indexes
</p>
<p>Perhaps the most well known, and often over-used performance tool used by
database creators is the index. In its most usual form the B-Tree index is funda-
mentally just the same as the key/value pair that we saw in Chap. 5. It&rsquo;s just that
the value being stored against the key is the hex address of the row containing the
column or columns being referenced (see Fig. 11.4).
</p>
<p>11.3.9.1 B-Tree Indexes
There are many different possible ways to locate a series of rows. If an index is
available, and we are searching for a small percentage of the total number of rows in
the table, then the RDBMS may choose to look up the key from the index, and then
pass the required hex address to the disk reader so that it can retrieve the required
row.
</p>
<p>But this needs at least two extra reads (to get to the right leaf node of the in-
dex) and more processing than would be required to just read the entire table and
disregard unwanted rows (known as a full table scan). In Oracle, if there you are
retrieving over 15 % of the rows Oracle will ignore an index, preferring instead the
relative simplicity of a full table scan.
</p>
<p>Which columns are best suited for an index? The most obvious is the primary
key of a table. Most RDBMS will automatically create an index to support your
primary key constraint. Oracle does not, however, index a foreign key. Generally
foreign keys should be indexed provided the child table is big enough to warrant
this. This is because there is an inferred likelihood of some joining going on in
queries between tables related in this way.
</p>
<p>Small tables should not be indexed. A full table scan is much the speedier way to
gather a row from a table containing just a few rows. Indexing would be overkill&mdash;
even if you were only ever returning one row.
</p>
<p>When looking for suitable candidates for secondary indexes, a DBA should be
looking for columns that are often involved in selection or join criteria. Yet again,
the DBA needs to understand the application the database is supporting. If users
are frequently accessing data with a WHERE clause of a non-primary key column,
and that query returns fewer than 15 % of the total rows stored, then a secondary
index may well help with speeding up reads. However, if that query we have just</p>
<p/>
</div>
<div class="page"><p/>
<p>252 11 Database Performance
</p>
<p>Fig. 11.4 B-Tree index
</p>
<p>described runs just once a year, whilst the table itself gets written to many times
a minute, it may be that the slowing effect during the writes (caused by having
to create and update rows in the index and the consequential need to maintain the
index&rsquo;s structure) far outweighs the advantage of having an index. Of course, it
could be that the once a year report is run by the Managing Director, and therefore
the political dynamic changes the decision making process yet again! The DBA&rsquo;s
balancing act can be a difficult one!
</p>
<p>There are workarounds to the tensions between write- and read-performance ef-
fects available to DBAs if they know their system well enough. For example, updat-
ing secondary indexes during write operations can slow the entire system due to the
extra locking it generates. To avoid this the DBA could store the inserts and updates
into a temporary table during the working day, and then apply them all overnight in
a single batch job. This is would be unreasonable in a 24/7 operation, but may well
work in some circumstances.
</p>
<p>Loading large amounts of data can slow the database down. Oracle&rsquo;s bulk loader,
SQLLDR, for example, if used in default mode, will generate an insert statement for
each row to be loaded. This will generate Redo and Undo and might significantly
increase the amount of locking. The workaround may be to delete any indexes before
loading, then load the data, then recreate any indexes&mdash;preferably at a time when the
database is less active anyway. This is another balancing act for the DBA: will the
decrease in query response times against the tables when they are without indexes
cause sufficiently little overall disruption to justify this approach?
</p>
<p>11.3.9.2 Non B-Tree Indexes&mdash;Bitmap Indexes
B-tree indexes have been around for decades. Many databases only have this type
of index available. Sometimes, however, this form of physical address lookup is not
the best. This is particularly true of column data which has low cardinality (very few
possible values). Columns containing only values such as Yes/No, for example, or
Male/Female, or even colours of the rainbow, are not well suited to B-tree indexes.
This is because many valid keys will be stored across potentially many leaf blocks,</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 253
</p>
<p>which will mean many reads per row returned, and the index leaf blocks may well
be well distributed across the tablespace, slowing the read process.
</p>
<p>To get around this problem the Bitmap Index type was created. The SQL syntax
is the same as for a B-tree standard index, but with the addition of the keyword;
Bitmap
</p>
<p>In effect the index for each possible key value for this index looks like this:
</p>
<p>Male, Start ROWID, End ROWID, 1000110100010100010010
Female, Start ROWID, End ROWID, 01110010111101011101101
</p>
<p>Bitmap indexes use arrays of bits to record whether or not a particular value is
present in a row, and answers SQL queries by performing bitwise logic against these
arrays. Because these indexes use fast, low level bitwise logic they can significantly
reduce query response times and may also reduce storage requirements.
</p>
<p>Bitmap indexes can substantially improve the performance of queries in which
the individual predicates on low cardinality columns return a large number of rows.
It can be especially useful when queries need to use two or more bitmaps to select
rows since bitwise logic can rapidly select matches. The downside however, is that
the mapping mechanism can be slow during updates and inserts. For this reason
bitmap indexes are often used in decision support environments. These environ-
ments usually have large amounts of data but few writes.
</p>
<p>11.3.9.3 Non B-Tree Indexes&mdash;Reverse Key Index
A physical problem that Indexes can cause is that of &ldquo;hot disk&rdquo;. This is exactly what
it sounds like. If the disk head is constantly accessing a particular sector of the disk,
perhaps because lots of similar key values are being written into the same index leaf
block, then the HDD&rsquo;s capacity to execute I/O will be exceeded which can cause
performance problems as users wait for their turn to get to that area of disk.
</p>
<p>As an example, think of a relatively straightforward concept of creating a com-
posite index on three columns: date, branch_id and saleperson_id. The scenario is
that this is a chain of stores recording its sales information and to speed the monthly
reports this index is created.
</p>
<p>On Thursday 15th Sept 2013, Store 44, during a busy period, sends this series of
rows to the server.
</p>
<p>Transaction_ID 143434
Date 15092013
Store 44
Salesperson 11
Item_id 732
Qty 4
</p>
<p>Transaction_ID 143435
Date 15092013
Store 44</p>
<p/>
</div>
<div class="page"><p/>
<p>254 11 Database Performance
</p>
<p>Salesperson 11
Item_id 861
Qty 1
</p>
<p>and so on.
Perhaps during a very busy period all their sales assistants are sending very sim-
</p>
<p>ilar information.
In the space of a few seconds the server may have to write the following leaf key
</p>
<p>value pairs:
</p>
<p>150920134411 hex address of the row with this composite value
150920134412 hex address of the row with this composite value
150920134421 hex address of the row with this composite value
150920134411 hex address of the row with this composite value
150920134405 hex address of the row with this composite value
150920134411 hex address of the row with this composite value
150920134411 hex address of the row with this composite value
150920134407 hex address of the row with this composite value
150920134412 hex address of the row with this composite value
</p>
<p>The point here is that all these keys are so similar that they will end up being writ-
ten to the same leaf block. This may mean some waiting occurs as each key/value
takes its turn to take a lock on the leaf so they can write to it, and, even worse, be-
cause the head is virtually constantly over the same physical spot on the disk, we
may end up with hot disk.
</p>
<p>Oracle came up with a solution to this problem called the Reverse key index.
A simple algorithm works to transpose what the application wants to write to the
system (using the first row in the above example: 150920134411) and reverses it
at the point of storage, in this case to: 114431029051. The benefit becomes clear
when we look at what happens to the next key value pair: 214431029051. This
is significantly different and will very probably need to be written to a different
branch/leaf, reducing contention for blocks and minimising the likelihood of hot
disk.
</p>
<p>This is clearly a specialist solution. The reverse algorithm has to apply when
applications try and retrieve data, and there will have to be an overhead in terms
of performance for applying this process to every read and write. However, it is
probably much better than destroying your HDD!
</p>
<p>11.3.9.4 Non B-Tree Indexes&mdash;Function-Based Indexes
Sometimes we have occasions when we are not sure what a user is going to input&mdash;
all we know is that they are always right! Think of an employee table containing a
column called last_name which USER A inserts to:
</p>
<p>Insert into Employee (id, last_name) values (12, &lsquo;Jones&rsquo;);
</p>
<p>Meanwhile User B inserts:
</p>
<p>Insert into Employee (id, last_name) values (12, &lsquo;JONES&rsquo;);</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 255
</p>
<p>What would USER C expect to see if he issues this:
</p>
<p>Select &lowast; from Employee where last_name = &lsquo;Jones&rsquo;;
</p>
<p>The answer is that they would not see the row entered by User B.
One solution would be to put a CHECK constraint on the column to ensure all
</p>
<p>names are inserted in uppercase. However, this would force McEwan to be written
as MCEWAN, and this may upset the name&rsquo;s owner.
</p>
<p>However, if we do not have a CHECK constraint any mix of case will be allowed,
and as the Jones example shows, that leaves us with a problem when attempting to
find all people called Jones.
</p>
<p>Oracle provide a solution to this, and other problems, by allowing us to create
an index which is made from altered source data, whilst not actually changing the
source itself. The syntax is not dissimilar to creating an ordinary index. In this ex-
ample we turn the value in last_name to uppercase before it is stored in the index.
</p>
<p>CREATE INDEX upperlastname_idx ON employees (UPPER(last_name));
</p>
<p>This will sort the &lsquo;Jones&rsquo; problem for us without us having to change the column
data.
</p>
<p>11.3.9.5 Indexes&mdash;Final Thoughts
</p>
<p>As with other elements of tuning, index use needs ongoing review. Reports are avail-
able which show how well used indexes are. If you discover an index is rarely used
that index is a candidate for dropping since the overhead cost of keeping it active is
generating no reward.
</p>
<p>The distribution of the actual data which is being indexed may also change sig-
nificantly over time. A small table, initial deemed unworthy of an index, may, for
example, grow into a large table, which would benefit from an index. Or a column
which the designers thought would contain many different values may, it turns out,
contain very few, making it worth considering for a bitmap index.
</p>
<p>11.3.10 Network
</p>
<p>There is often little the DBA can do about network speed. Database clients are often
distant, or sometimes very distant from the server and this means there are many
places where bottlenecks on the network layer can happen. A DBA needs to work
closely with his networking colleagues.
</p>
<p>There are, however, a few things a DBA can affect in terms of networking per-
formance. Oracle, for example, encapsulates data into buffers sized according to the
DEFAULT_SDU_SIZE parameter. (SDU is session data unit.) The default is for this
size to be 8192 bytes or less. However, if your application is typically bringing back
bigger chunks of data you can consider making this larger so that fewer packages
need to be sent on the network for each query.</p>
<p/>
</div>
<div class="page"><p/>
<p>256 11 Database Performance
</p>
<p>11.3.11 Application
</p>
<p>11.3.11.1 Design with Performance in Mind
Without pointing an accusatory finger at developers, it is true that many database
systems are written by developers who know SQL well but don&rsquo;t understand the
physical aspects of database design. Many IDEs have SQL generators allowing com-
plex joins to be written by merely drag-and-dropping. Worse, the developer can fall
into the trap of only worrying about system outputs. This can result in excessive use
of indexes, with the consequent hit on write performance.
</p>
<p>One sensible solution, if the organisation can afford it, is to include DBAs in
a development team. They can ensure that the design process is implementation
focused, and not merely &ldquo;well designed&rdquo;. There are a number of less well known
structures available for specialist use which can speed query responses. But because
they are non-standard they often get overlooked.
</p>
<p>Indexed Organised Table (IOT) If a table is very frequently, or solely searched
via the primary key it is a candidate for being stored as an IOT. In effect the table
information gets stored in an B-tree-like index structure, which means that searches
for a particular primary key will be faster than a standard table+index structure
since there is no need to look up the rowid from the index to gather the row data
as it is stored in the leaf block itself. Unlike an ordinary table where data is stored
as an unordered collection, data for an index-organized table is stored in a primary
key order so range access by the primary key involves minimum block accesses.
Because of this storage mechanism secondary indexes become difficult to maintain
and are probably best avoided.
</p>
<p>Wide tables are not ideal candidates, although there is an ability to overflow data
that is seldom required into a separate, non-b-tree area. The syntax for creating an
IOT is the same as for an ordinary table with the exception of the ORGANISATION
INDEX clause. There has to be a Primary Key. Here is an example:
</p>
<p>CREATE TABLE WardReqIOT
</p>
<p>(
WardID NUMBER,
RequestDate DATE,
Grade NUMBER,
QtyReq NUMBER,
AuthorisedBy VARCHAR2(30),
CONSTRAINT PK_WardReqIOT PRIMARY KEY
</p>
<p>(WardID, RequestDate, Grade)
)
ORGANIZATION INDEX INCLUDING QtyReq OVERFLOW Tablespace Users
</p>
<p>Here all the columns up to and including the one named in the INCLUDING
option of the OVERFLOW clause (QtyReq) are stored in primary key order in a
B-tree structure. The other data is stored in a table structure in a tablespace called
Users.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.3 Physical Storage 257
</p>
<p>Fig. 11.5 Partitioning data
across disks
</p>
<p>Partitioning Oracle can manage very large tables comprising many millions of
rows. However, performing maintenance operations on such large tables is difficult,
particularly in a 24/7 environment. There can be performance issues associated with
inserting into, or reading from tables this large. Indexes can help, of course, in re-
ducing the number of full table scans required, but another solution might be to
partition the data (see Fig. 11.5).
</p>
<p>A Partition will be assigned according to defined criteria for a key column(s).
Two examples are:
</p>
<p>Range Slices the data according to which range a partitioning key is in. For ex-
ample you could partition rows where the Transaction_Date column contains data
from different months, with partitions for January, February and so on.
</p>
<p>List Instead of a range you can partition by creating a list of values. For example
all rows where the column Location is any of: Edinburgh, Glasgow, Montrose, or
Inverness could be in a partition called Scotland.
</p>
<p>Depending upon the type of data being kept rows may be divided into a series of
key ranges, such as ones based on date. Although each partition will have the same
logical structure as the others, they may have different physical storage properties.</p>
<p/>
</div>
<div class="page"><p/>
<p>258 11 Database Performance
</p>
<p>Data may be stored in different tablespaces, or even disks, enabling a sort of striping
potentially speeding up data access by allowing parallel reading to occur.
</p>
<p>Furthermore, if the historic data is unlikely to be updated, the majority of the
partitions in a history table can be stored in a read-only tablespace (good for perfor-
mance). This approach can also reduce back-up times.
</p>
<p>11.4 Tuning
</p>
<p>11.4.1 What Is Database Tuning?
</p>
<p>There are several steps in the process of ensuring a database is performing ade-
quately. This is often called tuning. Different professionals have different thoughts
about what the process should be. Is it an Art or a Science?
</p>
<p>At its most abstract, however, the steps will usually be:
1. Monitor
2. Analyse
3. Select the appropriate type of action, which could be any or all of:
</p>
<p>(a) Make changes to the hardware, usually by adding RAM or disk
(b) Alter the RDBMS parameters to allocate resources more efficiently
(c) Modify the SQL to run more efficiently.
(d) Change the application and the way it interacts with the RDBMS
</p>
<p>4. Go back to step 1 to assess the success of the actions taken
Ideally this should be an ongoing process. As we have already highlighted, the
</p>
<p>data that is loaded today may well have made the database less well tuned. Some-
times production systems start off performing well. Any slowing that happens may
happen slowly over a number of months. This can make the analysis phase much
lengthier. However, in a busy DBA section, when your users aren&rsquo;t complaining, it
is very easy to loose sight of the need to constantly monitor.
</p>
<p>Modern RDBMS can help (see Tools section below). They have inbuilt perfor-
mance monitoring and can flash alert on the DBA dashboards when thing begin to
go wrong. There may well be situations, however, where there is no forewarning.
The DBA who receives a call from some irate user saying their report is taking ages
to complete needs to be able to review many possible causes very quickly:
</p>
<p>System-Wide What else is happening? Is there a large batch job running now
which doesn&rsquo;t usually run? Is a tablespace offline because it is being backed-up or
moved to a new device? Is the network functioning normally?
</p>
<p>Application-Specific Can we see which SQL is causing the problem? It may not
be that belonging to the report in question. . . it could be that the report is waiting
because some rows have been locked by another user. Is the report SQL suitably
tuned? Does the client process any of the returned dataset, and if so, what else is
happening at the client end?</p>
<p/>
</div>
<div class="page"><p/>
<p>11.4 Tuning 259
</p>
<p>11.4.2 Benchmarking
</p>
<p>Especially when it comes time to decide upon which RDBMS to purchase, people
begin to ask questions like: which is the best database for performance? But we
have seen throughout this chapter that database performance is a slippery thing. The
simple act of inserting data to the most finely tuned database tends to worsen its
performance.
</p>
<p>The truth of the matter is that all the databases currently on the market have their
own strengths and weaknesses and some perform better in some ways, whilst others
perform better in other ways. That isn&rsquo;t, however, to say that we simply use a pin to
decide&mdash;nor allow ourselves to be beguiled by the smoothest sales pitch!
</p>
<p>A series of realistic tests is a sensible approach, using examples of each possible
alternative database to run some realistic processes against the sort of data your
application will be collecting. This is a sensible approach if you have the time,
and the skills available to set up suitable test scenarios. However, this isn&rsquo;t always
possible and you may need to rely on data gathered by others.
</p>
<p>It almost goes without saying that test results provided by vendors are to be
treated with a degree of scepticism. What is really needed is a fair, standard set of
tests against a known dataset. This is what is used in benchmarking.
</p>
<p>Probably the most well known impartial agency for this sort of thing is the Trans-
action Processing Council (TPC). On their website (http://www.tpc.org/ ) there are
the results of a number of different tests against different types of data. Their mis-
sion is declared as:
</p>
<p>The TPC is a non-profit corporation founded to define transaction processing and database
benchmarks and to disseminate objective, verifiable TPC performance data to the industry.
</p>
<p>An output from one of their tests at the time of writing is given in Fig. 11.6.
</p>
<p>11.4.3 Another Perspective
</p>
<p>As we saw in Chap. 5 we have, in more recent times, been re-examining some of
the assumptions we have worked with since the advent of the client/server RDBMS.
Indeed, it is sensible for any IT professional to constantly review the current appro-
priateness of a technology. This is true of database tuning too.
</p>
<p>In their paper, Rethinking Cost and Performance of Database Systems, Florescu
and Kossmann (2009) argue that the traditional question, which they saw as:
</p>
<p>&ldquo;Given a set of machines, try to minimize the response time of each request.&rdquo;
</p>
<p>should be reshaped to be:
</p>
<p>&ldquo;Given a response time goal for each request, try to minimize the number of machines (i.e.,
cost in $).&rdquo;
</p>
<p>Of course minimising cost has always been a major objective for any DBA, but
recasting the tuning question to provide this focus helps us recognise what our pri-
orities need to be. This is especially useful in the era of cheap computing grids and</p>
<p/>
<div class="annotation"><a href="http://www.tpc.org/">http://www.tpc.org/</a></div>
</div>
<div class="page"><p/>
<p>260 11 Database Performance
</p>
<p>F
ig
.1
</p>
<p>1
.6
</p>
<p>T
C
</p>
<p>P
te
</p>
<p>st
re
</p>
<p>su
lt
</p>
<p>s</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 261
</p>
<p>the ultimate flexibility offered by Cloud provisioning. Indeed, in the era of cloud
database perhaps their question could be reworked again to:
</p>
<p>&ldquo;Given a response time goal for each request, try to minimize the cost (in $) of provision.&rdquo;
</p>
<p>11.5 Tools
</p>
<p>11.5.1 Tuning and Performance Tools
</p>
<p>In this section we have a tutorial exploring some of the tools that are available to
help tuning the database. We are using Oracle as an example. In order for this to
work you need to have admin rights on an Oracle 10g or 11g instance with the
sample HR schema installed (which is there by default unless you ask not to have
it).
</p>
<p>In order to give ourselves some meaningful data to experiment with we will also
use a further table called EmpHours which should have about 15 Mb of data in it
and will therefore make timings improvements more noticeable than with smaller
tables. Details about how to create the table and generate these rows are in at the
end of the chapter.
</p>
<p>11.5.1.1 SQLPLUS Tools
For years Oracle has shipped with a command line tool called SQLPLUS. It is part
of Oracle whatever the Operating System which means that a DBA who masters it
can ply his or her trade on any platform.
</p>
<p>It isn&rsquo;t a friendly environment, being invented before the GUI environments be-
came the norm, but it is very functional, and has some built-in tools that can assist
with basic performance tuning.
</p>
<p>Timing When users say things like; &ldquo;that sql report took a long time to run&rdquo; there
are a number of responses a DBA could give&mdash;some of which are not polite! But the
good DBA will know that what they need is hard, cold measurement. In this case,
measurement of the time it takes to get the data back to the user.
</p>
<p>From your Oracle working directory, log in as HR and select all the rows from
the Departments table where Department_id is less than 100 (Fig. 11.7).
</p>
<p>How quickly did that answer come back? Chances are it will have been so quick
you might say it responded instantaneously. We can find out how long the query
took by using the built-in facility within SQLPLUS to time events. If we issue the
set timing on command and then re-run the query we now get some information.
Don&rsquo;t worry that you don&rsquo;t get any message back after issuing the command. You
will get an error if you issue a bad command (Fig. 11.8).
</p>
<p>Yes, almost instantaneous: 0.01 of a second! But, we didn&rsquo;t really stretch the
optimiser with that query, so let&rsquo;s try something with a join in it that runs against
our big table: EmpHours. The set timing on will remain the default action now
whilst the session lasts, or you issue the set timing off command.</p>
<p/>
</div>
<div class="page"><p/>
<p>262 11 Database Performance
</p>
<p>Fig. 11.7 SQL query where department_id &lt; 100
</p>
<p>Try running this query:
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours
from employees a, emphours b
where b.emp_id &lt; 105 AND b.emp_id = a.employee_id AND b.fee_earning
= &lsquo;Y&rsquo;;
</p>
<p>Use a text editor to type this in, or copy it in the save it in your working folder
and call it something like q1.sql
</p>
<p>On the author&rsquo;s server this takes over 3 seconds (Fig. 11.9).
Now run the same query immediately again (Fig. 11.10).
It may not have looked any quicker, but it is almost half a second quicker. Was
</p>
<p>yours the same? I would guess it would be. If you remember we said reading from
disk was a bottleneck. The first time the query ran Oracle will have had to gather
the data from the disk. The results would be kept in RAM until they were flushed
out by other data. So the second time we asked the question Oracle could get the
answer from memory, saving disk access time. The rule then must be to always</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 263
</p>
<p>Fig. 11.8 Turning timing on
</p>
<p>Fig. 11.9 Test output
showing elapsed time
</p>
<p>Fig. 11.10 Second
run&mdash;usually will be quicker
</p>
<p>run tests several times to get an average, and always exclude the first time from
</p>
<p>that calculation.
</p>
<p>Autotrace Statistics Let&rsquo;s now use another SQLPLUS tool: set autotrace on.
In order to use this as HR we will need to create the PLUSTRACE role as SYS-
</p>
<p>DBA and then grant its privileges to HR. Here are those steps to be carried out as
SYSDBA (Fig. 11.11).
</p>
<p>Now, after moving back to your working directory and logging on to SQLPLUS
as HR, we can issue the command; set autotrace on. As this generates a report
that is wider than the default line width we also need to issue the set linesize 200
command to allow for the output to format nicely.
</p>
<p>Now re-run the q1 query and see what we get. There is a lot of extra information
now appended to the end of the output. We will examine what the key items are, but
first, just to emphasise the point about disk reads being slow, just note the statistics
section below which was run after restarting Oracle (and therefore emptying the
buffers) (Fig. 11.12).</p>
<p/>
</div>
<div class="page"><p/>
<p>264 11 Database Performance
</p>
<p>Fig. 11.11 Setting up autotrace
</p>
<p>Now, unless you restarted your instance, what you would be more likely to get is
shown in Fig. 11.13.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 265
</p>
<p>Fig. 11.12 Autotrace first
time through
</p>
<p>Fig. 11.13 Autotrace with
no physical reads
</p>
<p>Note that one line of the Statistics report shows how many physical reads there
have been. This tells you how many blocks Oracle has had to read from disk in order
to be able to answer the query. We can see that the first time we needed 175 block to
be read from disk, but the second time we needed none, since all rows were already
in the buffer (i.e. in RAM).
</p>
<p>When looking for tuning problems, looking to reduce disk reads is an obvious
place to start. The point here, however, is that the first time you run a query is not a
sensible time to look at tuning data since, in the normal course of events, an active
production system is likely to be able to service some, or all, data requests from
memory. To reiterate the rule:
</p>
<p>always run tests several times to get an average, and always exclude the first time from that
calculation.
</p>
<p>Now we will review the other useful information presented when we turn on
Autotrace. Firstly, continuing in the Statistics area we have, we can particularly
note:
&bull; recursive calls: these are internal SQL statements that Oracle needs to issue
</p>
<p>to service the request. There isn&rsquo;t much you can do about this. Note, in the
example above, that the first time through this number was a lot higher. This is
probably because of the extra parsing work that Oracle will have had to carry
out</p>
<p/>
</div>
<div class="page"><p/>
<p>266 11 Database Performance
</p>
<p>&bull; consistent gets: this is probably the most useful item to keep an eye on when
tuning. It is the total number of blocks Oracle needed to read from disk and
memory to service the query. The bigger the number, in general, the slower the
query.
</p>
<p>&bull; sorts: Sometimes you can&rsquo;t avoid having data sorted by Oracle, but sorts can
slow the query considerably. You could try rewriting queries to reduce sorts if
you know the client will be processing the data. In any case we should aim for
memory sorts rather than disk sorts.
</p>
<p>We are now going to be playing around with the same query to demonstrate the
effect of changes. However, we need to remember our rule about never trusting the
first run of a query when we make changes to queries since the second time the
revised query runs is likely to be faster than the first time.
</p>
<p>In this case let us add an ORDER BY to the query so that our output is sorted
by hours worked and then by date worked. Try saving this as q2.sql and then run it
twice:
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours
from employees a, emphours b
where b.emp_id &lt; 105 AND b.emp_id = a.employee_id AND b.fee_earning
</p>
<p>= &lsquo;Y&rsquo;
ORDER BY b.hours, b.work_date;
</p>
<p>The second time, this ran on the author&rsquo;s server the run time returned to just
a little longer at 2.82 seconds as compared with 2.22 for q1. The thing that has
changed in the Statistics section is the Sorts (memory) which is now 1. Because the
rows were already in RAM from the previous query, Oracle could carry out the sort
in one operation, all in memory&mdash;hence the only slight performance hit of 0.6 of a
second.
</p>
<p>The query generates many rows which makes comparing Statistics difficult so,
since we know what the query output looks like, we can turn off query row output
by issuing set autotrace traceonly. Once you have issued that command, before
running the next query, run q2 one more time so we have the information we need
to compare with the next query. But now look at the time to run. In the author&rsquo;s
case it has shrunk from 2.22 secs to a mere 0.22. Why is this? Because writing the
results to the screen takes time. The traceonly results are the times for the answers
to be generated rather than delivered and this is fairer since writing output will take
different times on different platforms.
</p>
<p>To give SQL more rows to work with we can now remove the b.emp_id predi-
cate to see what effect that has. Try saving this as q3.sql and then run it twice (see
Fig. 11.14).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 267
</p>
<p>Fig. 11.14 Screenshots of the output for q3.sql
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours
from employees a, emphours b
where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;Y&rsquo;
ORDER BY b.hours, b.work_date;
</p>
<p>As usual, the first run through would probably be slower. On the author&rsquo;s server
the output from the second run looked like in Fig. 11.14.
</p>
<p>We can note that we now have 313015 rows returned as compared to 21464 in q2.
So the report is returning about 14 times more rows. And then if we compare bytes
sent via SQL*Net to client 10134177 for q3 and 604542 for q2 we see that we are
sending nearly 17 times more data. However, look at the comparison in runtime:
2.02 for q3 and 0.22 for q2. The response is only 9 times slower.</p>
<p/>
</div>
<div class="page"><p/>
<p>268 11 Database Performance
</p>
<p>Fig. 11.15 Output from q4.sql demonstrating overheads
</p>
<p>So another useful thing to remember is that the volume of data is not the only
factor in the speed of a query, although it will plainly have an effect. There are
some processes that will need to happen only once, or a few times only, in the
overall task, regardless of the number of rows being returned. Parsing, for example,
is not dependant upon likely row count, and it must happen for every query. These
&ldquo;overheads&rdquo; can be demonstrated by creating q4 as shown in Fig. 11.15. Note, there
are no rows with a fee-earning value of &lsquo;Q&rsquo;.
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours
from employees a, emphours b
where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;Q&rsquo;
ORDER BY b.hours, b.work_date;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 269
</p>
<p>Fig. 11.16 Execution plan for q2.sql
</p>
<p>Despite not returning a single row this process took 0.05 of a second and needed
to read 1332 blocks.
</p>
<p>Autotrace Execution Plan The other significant section in the autotrace outputs
is the execution plan. We will use q2 and q3 again so either find the output, or run
them again.
</p>
<p>Execution Plan for Q2 is shown in Fig. 11.16.
The first thing to mention about these plans is that you need to read them from the
</p>
<p>inside out. The Operation column tells you what Oracle will be doing to deliver the
required dataset. The name tells you any objects Oracle will be using. It will help to
note that SYS_C0010205 is the system generated index to support the primary key
in EMPHOURS, whilst EMP_EMP_ID_PK is the index used to support the primary
key in EMPLOYEES.
</p>
<p>The columns to the right of Name are all estimates that Oracle generates, based
upon the statistics in the Data Dictionary. Cost is how Oracle Cost-Based Optimiser
decides which plan to use since lowest cost will typically also be most efficient in
terms of resource usage. The cost estimates the IO, CPU, and network resources that
will be used to answer the query.
</p>
<p>Start reading this plan with the one that has the most indentation as this will
probably be the first one to be executed. (If two statements have the same level of
indentation read top down.)
</p>
<p>In this case we have an Index Range Scan using the EMP_EMP_ID_PK index.
This means that Oracle will read that index to get the ROWIDS for what it estimates
is 5 rows. (It is right, isn&rsquo;t it? Ids 100&ndash;104 meet the query WHERE b.emp_id &lt; 105).
</p>
<p>Operation ID 4 and 6 are equally nested, so next comes the earlier operation;
Table Access by Index Rowid. Oracle uses the ROWID to read the block(s) that
contain the Employee data it needs for the query. It estimates that will be around
60 bytes.
</p>
<p>Next comes the Index Range Scan on the EMPHOURS primary key index. It
will use the emp_id number discovered in operation 4 to lookup the ROWID from
the index of all the rows in EMPHOURS that have this emp_id. Finally Oracle can
access EMPHOURS and return the rows selected by Operation 6.</p>
<p/>
</div>
<div class="page"><p/>
<p>270 11 Database Performance
</p>
<p>Fig. 11.17 Revised execution plan output
</p>
<p>These operations are in nested loops. First all the rows from Employees are re-
turned and then EMPHOURS index information each row is returned from Employ-
ees. Once completed Oracle can sort the data in memory.
</p>
<p>Operation 0 is the finished output. Note how poor the estimated number of rows
is. Oracle doesn&rsquo;t know how many EMPHOURS rows are likely to be returned for
each EMPLOYEE row.
</p>
<p>To see the optimiser use an alternative approach to getting similar data, let&rsquo;s now
run q3&mdash;which removes the emp_id &lt; 105 predicate and generates many more rows
(Fig. 11.17).
</p>
<p>Because we are returning more than 15 % of the rows from both tables Oracle
now decides that it will not use indexes. Instead it will read all the rows from the two
tables straight into memory in an operation called a Full Table Scan (here described
as TABLE ACCESS FULL). Thus two sets of rows are then joined on the join key.
</p>
<p>Two common means of joining are:
&bull; Merge Join: Outputs from both scans are sorted by the join key and then merged
</p>
<p>together
&bull; Hash Join: A hash join iterates through the rows of the smaller table and per-
</p>
<p>forms a hash algorithm on the columns for the joined columns and then stores
the result. It then iterates through the rows of the other table performing the
same hashing algorithm on the joined columns. It then compares with the first
result and if they match it returns the row.
</p>
<p>Once the rows are output Oracle sorts. Because it has estimated how many rows
will be output it also suggests it will need 15 Mb of Temporary Space to perform
the sort. Note that the estimated row count is a lot more accurate this time.
</p>
<p>What we sometimes need to do when testing is to run several scripts and then
want to compare the outputs, saving them for future reference. We can use the
SQLPLUS Spool command to send output to a file. We also need to close the file
when we have finished recording. Before we run this test script it is a good idea to
make sure that the data dictionary has up-to-date statistics. To force this you can
issue this command:
</p>
<p>analyze table emphours compute statistics;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 271
</p>
<p>We shall now create such a script, run an SQL query twice, then create an index
and then run the same query twice again to see what difference it makes to the
Execution plan. The query returns employees who have recorded more than 40 hours
which was non-fee-paying in the previous 30 days.
</p>
<p>Save the follow to q6.sql and then run it, after making sure you understand what
the script is doing.
</p>
<p>set timing on
set autotrace traceonly
set linesize 200
spool test.txt
</p>
<p>drop index hrs_idx;
select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours &gt; 40 AND work_date &gt; (SYSDATE -30);
select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours &gt; 40 AND work_date &gt; (SYSDATE -30);
</p>
<p>create index hrs_idx on EMPHOURS(hours);
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours &gt; 40 AND work_date &gt; (SYSDATE -30);
select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours &gt; 40 AND work_date &gt; (SYSDATE -30);
</p>
<p>spool off
</p>
<p>You can now read the results by opening the file test.txt in your working direc-
tory.
</p>
<p>Our developers have said that they think that there will be several queries, like
this one, which use the hours field to select and filter on. So, they have suggested,
we should have an index on that column. Sounds reasonable?
</p>
<p>Well if we look at the result we can see this isn&rsquo;t such a good idea. With or without
an index on the Hours column Oracle uses the same Execution Plan (Fig. 11.18).
</p>
<p>In other words, the Primary Key is used (at Id3) to filter on date first and there is
no need for Oracle to use the new index.
</p>
<p>The bad news is that having that index sitting there doing nothing is costing
the system performance for every insert and update since it needlessly continues to
maintain the index. So not a good idea after all.
</p>
<p>Since we are filtering on the Y/N field you could try the same script again, but
this time create a bitmap index on the fee_earning column:</p>
<p/>
</div>
<div class="page"><p/>
<p>272 11 Database Performance
</p>
<p>Fig. 11.18 Execution plan showing the index is not used
</p>
<p>create bitmap index fee_bit_idx on EMPHOURS(fee_earning);
</p>
<p>What did this last experiment tell you?
We now know indexes aren&rsquo;t always a good thing. So when would we use them?
</p>
<p>Here is advice from the Oracle Tuning Guide:
</p>
<p>(You should. . . ) index keys that have high selectivity. The selectivity of an in-
dex is the percentage of rows in a table having the same value for the indexed
key. An index&rsquo;s selectivity is optimal if few rows have the same value.
</p>
<p>The problem with indexing hours is that it has low selectivity. Prove this by
running the following query:
</p>
<p>select hours, count(hours) from emphours group by hours order by hours;
</p>
<p>Actually, if we had bothered to understand our data better, we could have guessed
that 30 different values which were randomly allocated for 400000+ rows are likely
to generate many rows with the same value.
</p>
<p>The reverse case, when we have low cardinality data, calls out for a bitmap index,
but only really if the data is non-volatile.
</p>
<p>To see when Oracle might use an index, have a look at this slightly reworked
query from q6.sql above:
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours &gt; 40 AND work_date &gt; (SYSDATE -30) AND a.last_name =
&lsquo;Kumar&rsquo;;</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 273
</p>
<p>Fig. 11.19 Execution plan using an index to get at a single row
</p>
<p>We are now asking for the same information for only one particular employee.
The Execution plan now uses indexes only&mdash;no table scans (Fig. 11.19).
</p>
<p>To prove how useful bitmap indexes can be, let us assume the emphours table
is a read-only table within a data warehouse. You have to write a report that lists all
rows where no fee has been earned and yet the employee has claimed 50 hours.
</p>
<p>The query might now look like this:
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours = 50;
</p>
<p>We can note that two columns which are in the WHERE clause are low cardinal-
ity, and the data is not volatile, so bitmap indexes might help. Placing this within
our test template in the same way that we did with q6.sql, we get:
</p>
<p>set timing on
set autotrace traceonly
set linesize 200
spool test2.txt
</p>
<p>drop index hrs_bit_idx;
drop index fee_bit_idx;
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours = 50;
select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours = 50;</p>
<p/>
</div>
<div class="page"><p/>
<p>274 11 Database Performance
</p>
<p>Fig. 11.20 Without Bitmap
</p>
<p>Fig. 11.21 With Bitmap indexes
</p>
<p>create bitmap index hrs_bit_idx on EMPHOURS(hours);
create bitmap index fee_bit_idx on EMPHOURS(fee_earning);
</p>
<p>select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours = 50;
select a.employee_id, a.last_name, b.work_date, b.hours from employees a,
emphours b where b.emp_id = a.employee_id AND b.fee_earning = &lsquo;N&rsquo;
AND hours = 50;
</p>
<p>spool off
</p>
<p>Try this and see what the effect is. On the author&rsquo;s server the difference was
striking in that the query time was halved (Figs. 11.20, 11.21).</p>
<p/>
</div>
<div class="page"><p/>
<p>11.5 Tools 275
</p>
<p>Fig. 11.22 Example Enterprise Manager tuning recommendation
</p>
<p>11.5.2 Using the Built-in Advisers
</p>
<p>The testing suggested above is fine during system builds, especially if we have re-
alistic data to test against. However, once an optimised system is in place. It may
well cause problems in the future due to changes in data. Assuming we want to be
proactive and not just wait for complaints from users, how does a DBA establish
which SQL might be worth investigating? Logged on as a DBA we can see what
SQL has run recently using a DBA-only view called V$SQL.
</p>
<p>select sql_text from v$sql;
</p>
<p>Unfortunately this can list all sorts of system generated SQL and in an operational
production system there may be thousands of different SQL statements being run at
any time. It is possible to join other v$ views to restrict the rows returned to those
that might be of interest, but the easiest solution is to use the Enterprise Manager&rsquo;s
advisors.
</p>
<p>If you have Enterprise Manager open, move to Advisor Central and then select
Automated Maintenance Tasks, and then run the Automatic SQL Tuning and finally
review the Automatic SQL Tuning Result Details. On the author&rsquo;s server there was
a recommendation for an index to support one of the queries that has recently run
(Fig. 11.22).</p>
<p/>
</div>
<div class="page"><p/>
<p>276 11 Database Performance
</p>
<p>11.5.3 Over to You!
</p>
<p>11.5.3.1 Scenario One
We have been asked to create a report that counts the number of entries in EM-
PHOURS where no fee was earned. It should have a count for Hours &lt;30, one for
30&ndash;40, and one for &gt;40.
</p>
<p>This could be tackled by having two UNIONS join three queries, or by using the
CASE statement. You need to establish which is quicker.
</p>
<p>NOTE: You should still have your bitmap index on the two columns from earlier
for this to work well.
</p>
<p>11.5.3.2 Scenario Two
Your developers are suggesting that some reports would benefit from there being
a bitmap index on the fee_earning column. You need to get an understanding of
how much slower your database would become in writing new records in a write-
intensive period if you were to implement a bitmap index on the fee_earning table.
</p>
<p>As a suggestion, slightly rewrite the emphours creation and insertion script to:
1. Create an identical table, called emphours2
2. Time the INSERTEMPHOURS process
3. drop the table
4. create it again, this time with an index
5. time INSERTEMPHOURS again
</p>
<p>If you put this all in one script you might find it useful to name your timers so
they are easy to read when you are looking at your spooled outputs. You do this in
sqlplus like this:
</p>
<p>Timing start InsertsNoIdx
do some processing . . . .
Timing stop InsertsNoIdx
</p>
<p>Is the insertion process any longer with an index?
</p>
<p>11.6 Summary
</p>
<p>In this chapter we have reviewed the impact each of the tiers in any database system
can have on overall performance. We have seen that CPU usage and Disk I/o are
important, but that so are some of the less physical aspects of a system, such as
its design, how the indexes are used, and which types of indexes and tables are
used. We have seen that performance problems can be hard to diagnose because of
the complexity of the tiers. Moreover, even if we do find the cause and resolve the
problem, there is no guarantee that it will remain solved because, as we add data to
our system, the data distribution may make some of our previous design assumptions
invalid.</p>
<p/>
</div>
<div class="page"><p/>
<p>11.7 Review Questions 277
</p>
<p>11.7 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
&bull; What is the difference between Throughput and Workload?
&bull; Which physical element of a database system is generally the slowest
&bull; Why might a DBA decide not to update the CBO statistics often?
&bull; What effect does a B-tree index have on inserts and updates?
&bull; Why will a query which returns 20 % of a table&rsquo;s rows probably not use an
</p>
<p>index, and what will it use?
&bull; Name at least two alternative types of index, other than B-Tree, and explain
</p>
<p>when they might best be used
</p>
<p>11.8 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Discussion Topic 1 Indexes are a nuisance in a volatile OLTP system which man-
</p>
<p>ages many writes per second. Discuss this assertion. You should review the benefits
and disadvantages of different types of indexes in doing so.
</p>
<p>Discussion Topic 2 Normalisation is often seen as a required process in the design
of a database. Explain why this is so, and then go on to discuss scenarios when no
normalisation is required, or indeed, when data should be denormalised.
</p>
<p>Appendix: Creation Scripts and Hints
</p>
<p>To create the EMPHOURS table and add the rows, save this code to a file in your
Oracle working directory and call it something like CreateEmpHours.sql. Then Log
in as hr/hr and run the script. This generates 400000+ rows and so it will take up to
a minute or so to complete.
</p>
<p>drop table emphours;
</p>
<p>create table emphours ( emp_id number,
</p>
<p>work_date date,
</p>
<p>hours number,
</p>
<p>fee_earning varchar(1),
</p>
<p>FOREIGN KEY (emp_id) REFERENCES HR.EMPLOYEES
</p>
<p>(EMPLOYEE_ID),
</p>
<p>PRIMARY KEY (emp_id, work_date) );
</p>
<p>create or replace</p>
<p/>
</div>
<div class="page"><p/>
<p>278 11 Database Performance
</p>
<p>PROCEDURE INSERTEMPHOURS AS
</p>
<p>BEGIN
</p>
<p>DECLARE
</p>
<p>eid employees.employee_id%TYPE;
</p>
<p>hdate employees.hire_date%TYPE;
</p>
<p>fee emphours.fee_earning%TYPE;
</p>
<p>howrs emphours.hours%TYPE;
</p>
<p>rnd Integer;
</p>
<p>CURSOR c1 IS
</p>
<p>SELECT EMPLOYEE_ID, hire_date FROM EMPLOYEES;
</p>
<p>BEGIN
</p>
<p>OPEN c1;
</p>
<p>-- loop through each of Employee rows
</p>
<p>LOOP
</p>
<p>FETCH c1 INTO eid, hdate;
</p>
<p>EXIT WHEN c1%NOTFOUND;
</p>
<p>-- loop through every day since they started to sysdate
</p>
<p>LOOP
</p>
<p>EXIT WHEN hdate &gt; sysdate;
</p>
<p>hdate := hdate +1;
-- check if weekend
</p>
<p>IF to_char(hdate, &lsquo;D&rsquo;) &gt; 1 AND to_char(hdate, &lsquo;D&rsquo;) &lt; 7
THEN
</p>
<p>-- make fee_earning a random Y or N but with more Y
</p>
<p>values
</p>
<p>rnd := DBMS_RANDOM.value(low =&gt; 1, high =&gt; 10);
IF rnd &gt; 7 THEN
fee := &lsquo;N&rsquo;;
</p>
<p>ELSE
</p>
<p>fee := &lsquo;Y&rsquo;;
</p>
<p>END IF;
</p>
<p>-- generate a random number of hours works between 20
</p>
<p>and 50 howrs := round(DBMS_RANDOM.value(low=&gt; 20, high
</p>
<p>=&gt; 50));
</p>
<p>INSERT INTO emphours (emp_id, work_date, hours,
</p>
<p>fee_earning)
</p>
<p>values (eid, hdate, howrs, fee );
</p>
<p>END IF;
</p>
<p>END LOOP;
</p>
<p>END LOOP;
</p>
<p>CLOSE c1;
</p>
<p>END;
</p>
<p>END INSERTEMPHOURS;
</p>
<p>/
</p>
<p>begin
</p>
<p>INSERTEMPHOURS;
</p>
<p>end;
</p>
<p>/</p>
<p/>
</div>
<div class="page"><p/>
<p>Creation Scripts and Hints 279
</p>
<p>SQL&gt; @createEmpHours
</p>
<p>Table dropped.
</p>
<p>Table created.
</p>
<p>Procedure created.
</p>
<p>PL/SQL procedure successfully completed.
</p>
<p>SQL&gt; select count(&lowast;) from emphours;
</p>
<p>COUNT(&lowast;)
</p>
<p>--------
</p>
<p>434068
</p>
<p>A.1: Hints on the Over to You Section
</p>
<p>A.1.1: Scenario One
</p>
<p>set timing on
set autotrace on
set linesize 200
spool test19.txt
</p>
<p>set timing on
SELECT COUNT (&lowast;)
FROM emphours
WHERE fee_earning = &lsquo;N&rsquo; AND hours &lt; 30
UNION
SELECT COUNT (&lowast;)
FROM emphours
WHERE fee_earning = &lsquo;N&rsquo; AND hours BETWEEN 30 AND 40
UNION
SELECT COUNT (&lowast;)
FROM emphours
WHERE fee_earning = &lsquo;N&rsquo; AND hours &gt; 40;
</p>
<p>SELECT COUNT (case when fee_earning = &lsquo;N&rsquo; AND hours &lt; 30 THEN 1 ELSE
null END) lessthan30,
COUNT (case when fee_earning = &lsquo;N&rsquo; AND hours BETWEEN 30 AND 40
THEN 1
ELSE null END) betwix3040,
COUNT (case when fee_earning = &lsquo;N&rsquo; AND hours &gt; 40 THEN 1 ELSE null END)
gt40</p>
<p/>
</div>
<div class="page"><p/>
<p>280 11 Database Performance
</p>
<p>From EMPHOURS;
</p>
<p>spool off
</p>
<p>The CASE fails to use the index and so is much slower.
</p>
<p>A.1.2: Scenario Two
</p>
<p>drop table emphours2;
</p>
<p>create table emphours2 (emp_id number,
work_date date,
hours number,
fee_earning varchar(1),
FOREIGN KEY (emp_id) REFERENCES HR.EMPLOYEES
(EMPLOYEE_ID),
PRIMARY KEY (emp_id, work_date));
</p>
<p>create or replace
PROCEDURE INSERTEMPHOURS AS
BEGIN
</p>
<p>DECLARE
eid employees.employee_id%TYPE;
hdate employees.hire_date%TYPE;
fee emphours2.fee_earning%TYPE;
howrs emphours2.hours%TYPE;
rnd Integer;
</p>
<p>CURSOR c1 IS
SELECT EMPLOYEE_ID, hire_date FROM EMPLOYEES;
</p>
<p>BEGIN
OPEN c1;
&ndash; loop through each of Employee rows
LOOP
</p>
<p>FETCH c1 INTO eid, hdate;
EXIT WHEN c1%NOTFOUND;
</p>
<p>&ndash; loop through every day since they started to sysdate
LOOP
</p>
<p>EXIT WHEN hdate &gt; sysdate;
hdate := hdate + 1;
&ndash; check if weekend
IF to_char(hdate, &lsquo;D&rsquo;) &gt; 1 AND to_char(hdate, &lsquo;D&rsquo;) &lt; 7
</p>
<p>THEN
&ndash; make fee_earning a random Y or N but with more Y values
rnd := DBMS_RANDOM.value(low =&gt; 1, high =&gt; 10);</p>
<p/>
</div>
<div class="page"><p/>
<p>Creation Scripts and Hints 281
</p>
<p>IF rnd &gt; 7 THEN
fee := &lsquo;N&rsquo;;
</p>
<p>ELSE
fee := &lsquo;Y&rsquo;;
</p>
<p>END IF;
&ndash; generate a random number of hours works between 20 and 50
howrs := round(DBMS_RANDOM.value(low =&gt; 20, high =&gt; 50));
INSERT INTO emphours2 (emp_id, work_date, hours, fee_earning)
</p>
<p>values (eid, hdate, howrs, fee );
END IF;
</p>
<p>END LOOP;
END LOOP;
CLOSE c1;
</p>
<p>END;
END INSERTEMPHOURS;
/
</p>
<p>Timing start InsertsNoIdx
begin
</p>
<p>INSERTEMPHOURS;
end;
/
</p>
<p>Timing stop InsertsNoIdx
</p>
<p>&ndash; now drop the table and recreate with an index
</p>
<p>drop table emphours2;
</p>
<p>create table emphours2 (emp_id number,
work_date date,
hours number,
fee_earning varchar(1),
FOREIGN KEY (emp_id) REFERENCES HR.EMPLOYEES
(EMPLOYEE_ID),
PRIMARY KEY (emp_id, work_date));
</p>
<p>create bitmap index fee_bit_idx on Emphours2 (fee_earning);
</p>
<p>Timing start InsertsWithIdx
begin
</p>
<p>INSERTEMPHOURS;
end;
/
Timing stop InsertsWithIdx
</p>
<p>spool off</p>
<p/>
</div>
<div class="page"><p/>
<p>282 11 Database Performance
</p>
<p>References
</p>
<p>Florescu D, Kossmann D (2009) Rethinking cost and performance of database systems. SIGMOD
Rec 38(1):43&ndash;48. 2009</p>
<p/>
</div>
<div class="page"><p/>
<p>12Security
</p>
<p>What the reader will learn:
&bull; The importance of database security
&bull; Physical security considerations
&bull; Database security risks and how to mitigate them
&bull; Security issues in the cloud
&bull; How to develop a Security Policy
</p>
<p>12.1 Introduction
</p>
<p>As mentioned in Chap. 2, data is one of organisations most important assets, there-
fore steps need to be taken to protect it. Security generally has three aspects to it:
physical security, software security and procedures. In each case precautions need
to be in place along with a risk assessment and a recovery plan. Security is also
closely linked with availability which was discussed in Chap. 11. The simple differ-
ence is that availability refers to making sure the systems don&rsquo;t shut down, whereas
security is keeping the &lsquo;bad guys&rsquo; out. However if you have a security breach it may
mean your database is not available, or your data has been corrupted.
</p>
<p>In the case of physical security, the main question to ask is how to protect the
servers and infrastructure from damage or loss. This may range from a catastrophic
event in the data centre through to someone stealing the actual server. Physical se-
curity is not only an issue of restricting access to a computer centre, but how to
recover operations if there is a major incident. In other words it is closely linked to
the question of availability.
</p>
<p>A more recent physical security threat has emerged from the increase in use of
mobile devices. These include smartphones, laptop and tablet computers as well as
removal storage devices such as pen drives. Loss and theft of these devices which
may contain sensitive information or have the ability to automatically connect to
sensitive systems has been a growing problem for some time.
</p>
<p>Although physical security is important, software security is the more important
security consideration on a day to day basis. Many databases have internet access
to them, although this is usually through another server. It is however where most
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7_12,
&copy; Springer-Verlag London 2013
</p>
<p>283</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7_12">http://dx.doi.org/10.1007/978-1-4471-5601-7_12</a></div>
</div>
<div class="page"><p/>
<p>284 12 Security
</p>
<p>threats come from. The nature of these threats and what can be done to mitigate
them will be discussed in this chapter.
</p>
<p>The final aspect is the procedures which are in place. These will also have an im-
pact on how physical and software security is implemented and maintained. Ques-
tions to ask are what procedures are in place, are they adequate and how are these
audited. A major component of this is a risk register which needs to be regularly
updated and reviewed.
</p>
<p>12.2 Physical Security
</p>
<p>Physical security is probably the easiest aspect of security to deal with, but it is
potentially the most catastrophic when things go wrong.
</p>
<p>A review by Scalet (2009) gives a comprehensive 19 point guide to physical
security including building a secure centre capable of surviving explosions. A lot
of this is in response to the aftermath of the terrorist attacks of 9/11 and much
of it is beyond the capability of an average company. However, as the following
Buncefield example shows, there needs to be a contingency plan as to what to do
if the processing centre is destroyed. On 11th December 2005 as a result of an
industrial accident there was an explosion at the Buncefield oil storage terminal
at Hemel Hempstead just north of London in the UK. The resultant fire was still
burning on the 14th December. There were a number of businesses in the adjacent
business park which were also damaged or destroyed. One of these was Northgate
Information Solutions whose headquarters were badly damaged by a wall being
blown out, it was rendered completely unusable.
</p>
<p>Northfield hosted systems for other organisations including the payroll system
for the author&rsquo;s university. As a result of the explosion it was not possible to process
the payroll for the next payday (18 December and just before Christmas). Although
the company had other processing sites it was impossible to completely set the sys-
tem up in time for the next payday. As a result a stop gap measure where staff were
paid the same as they were paid the previous month was instituted. The system was
backed up at another site in time for the January 2006 payday.
</p>
<p>This illustrates the need of having a contingency plan for when something does
go wrong. It is extremely rare that a data centre gets completely destroyed, but
floods and fire do happen as the example shows. One question that needs to be
asked is how much an organisation is willing to spend on disaster proofing physical
infrastructure&mdash;it is probably impossible to make a building totally indestructible.
A better question is how will operations continue if it does happen and how will the
database be recovered.
</p>
<p>The other component of physical security is access control where physical access
to sensitive hardware and parts of a building are restricted. A 3 line security system
consisting of something you are, something you know and something you have gives
the best level of security for access to both building space and access to systems:
</p>
<p>Something you are: iris recognition, fingerprint recognition
Something you know: password or code</p>
<p/>
</div>
<div class="page"><p/>
<p>12.2 Physical Security 285
</p>
<p>Something you have: passkey or other device
(see Federal Financial Institutions Examination Council Authentication in
an Internet Banking Environment http://www.ffiec.gov/pdf/authentication_
guidance.pdf accessed 28/07/2013).
</p>
<p>Oracle&rsquo;s Security Guide for Oracle 10g (Oracle Corp 2012a, 2012b) gives a phys-
ical and personnel control checklist. At its most basic, the policy should be that no
one should be able to walk into a data centre without proper authorisation and identi-
fication (which is checked). It should be noted that although this list is not included
in the Oracle 11g security guide it is still valid. Scalet (2009) suggests that there
should be a minimum of 3 separate checks of personnel&mdash;on entering the building,
on entering the employee area and on entering the data centre &lsquo;core&rsquo;. She further
points out that access to the core should be strictly controlled and on a needs only
basis.
</p>
<p>Oracle adds that the elaborate measures documented by Scalet are probably not
needed by the majority of organisations. Factors such as organisation size, risk of
loss, access controls and frequency of external visitors will determine the level of
measures needed. They also add that the visibility of security measures such as
video surveillance will act as a deterrent. Oracle say: &lsquo;Make it difficult to get in,
difficult to remain or leave unobserved or unidentified, difficult to get at sensitive or
secure areas inside, and difficult not to leave a trace.&rsquo; (Oracle&reg; Database Security
Guide 10g Release 2 (10.2) B14266-09 p 2-2). Even a small company should restrict
access to their server, even if that means keeping it locked in a cupboard.
</p>
<p>Shinder (2007) in her article lists 10 physical security measures every organiza-
tion should take which add a few more concerns to the list we have already seen.
Workstations, particularly any which are left logged on in an unattended are a po-
tential security risk. The machine at the front receptionist&rsquo;s desk can be a particular
risk. Related to this is the physical removal of workstation components such as the
hard disk drive. A case lock is a low cost solution to this problem.
</p>
<p>A classic television and cinema scenario is the good (or bad) guy breaks into an
office and downloads the information on a workstations hard drive to a memory sick
or pen drive. In practice it is more likely an employee who is going to do this&mdash;often
not for malicious intent (it may be they want to work on data at home). It is how-
ever a security risk and many organisations take measures to prevent unauthorised
downloading by disabling USB ports and any device capable or writing to removal
media.
</p>
<p>A final, often forgotten risk in physical security is disposal of equipment. It is
essential hard drives are properly erased. This is more than just deleting files be-
cause in most cases only directories which point to data are deleted. The actual
data often remains and there have been cases of machines being bought at auction
and their hard drives being reconstructed. In some cases even a simple delete had
not been carried out. Free utilities such as Active@Kill Disk&mdash;Hard Drive Eraser
from LSoft Technologies (see http://www.killdisk.com accessed 30/07/2013) exist
for overwriting disks. Disks can also be cleared magnetically&mdash;a process known as
degaussing.</p>
<p/>
<div class="annotation"><a href="http://www.ffiec.gov/pdf/authentication_guidance.pdf">http://www.ffiec.gov/pdf/authentication_guidance.pdf</a></div>
<div class="annotation"><a href="http://www.ffiec.gov/pdf/authentication_guidance.pdf">http://www.ffiec.gov/pdf/authentication_guidance.pdf</a></div>
<div class="annotation"><a href="http://www.killdisk.com">http://www.killdisk.com</a></div>
</div>
<div class="page"><p/>
<p>286 12 Security
</p>
<p>12.3 Software Security&mdash;Threats
</p>
<p>On line access security is the area which is arguably of greater concern to busi-
nesses and potentially a much more of a serious risk than the physical securing of
a database. Once a building is physically secure, assuming there are no lapses in
protocol, the issue is solved. Problems associated with software tend to be more
on going and continually evolving, particularly unauthorised access and changes to
data.
</p>
<p>Therefore this type of security includes access to software systems (rather than
physical access to hardware discussed earlier) and data integrity measures. Data
integrity is ensuring the accuracy, reliability and consistency of data in a database.
There also needs to be a decision as to where security protocols are enforced. It is
not normal for an internet user to directly interact with a database, there is usually
some middleware handling security and checking transactions before a database
is updated. For example, some banks require up to three security questions to be
answered before a user is allowed access to on-line banking. The Yorkshire Bank
requires a user ID, then a password followed by a security question. The password
itself is not entered but randomly selected characters from it are requested. This is
also done when purchases are made on-line. Only when these security measures
have been passed is the user (in this case a customer) able to carry out on-line
financial transactions.
</p>
<p>There are number of types of threats to databases: privilege abuse, platform
weaknesses, SQL injection, weak audit, protocol vulnerabilities, authentication vul-
nerabilities, backup data exposure and mobile device based threats, each of which
we will review below.
</p>
<p>12.4 Privilege Abuse
</p>
<p>Users of a database will require different levels of access known as privileges. For
example, a database administrator will need to be able to modify the database defini-
tion using data definition language and perform database management tasks such as
brining the server down or starting it. At the other end of the spectrum there will be
users who should only be able to view a restricted subset of the data in the database.
Between these extremes there will be users who will need various levels of access
to manipulate data to fulfil their roles.
</p>
<p>Database Privileges can be arranged in the following way with progressive capa-
bilities to make changes to the database. They are arranged by user: those who are
concerned with data and administrator: those who are concerned with management
of the databases structure:
</p>
<p>User
</p>
<p>Read&mdash;the basic level most users require to access data. Modification is not
allowed
Insert&mdash;add new data, but no modification of existing data. This may not auto-
matically give read privilege</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Privilege Abuse 287
</p>
<p>Update&mdash;modify, but not delete data
Delete&mdash;ability to delete a complete record
</p>
<p>Administrator
</p>
<p>References&mdash;an extension to update where foreign keys can be created
Index&mdash;create or drop indexes
Create&mdash;ability to create new tables
Create a new database
Modify&mdash;add or delete attributes
Drop&mdash;delete a table
Grant&mdash;ability to grant privileges including create new users Start and stop a
database
</p>
<p>The way these privileges are maintained depends on the system. For example,
on IBM systems these privileges are held in an access control list (ACL). Privilege
abuse occurs when users or administrators are given privileges which exceed their
requirements and then proceed to exploit these &lsquo;excess&rsquo; privileges.
</p>
<p>To mitigate against this type of problem the ACL or its equivalent needs to be
developed and maintained. A precursor to this is that the role of the user must be
considered and only the minimum rights should be granted to enable the user to
carry out their roll. This is not a trivial task in terms of its creation and maintenance.
IBM divides the ACL into three parts:
&bull; Access
&bull; User
&bull; Privilege
</p>
<p>Basically the first two determine the third. In terms of access, IBM visualises the
hierarchy as an inverted pyramid with Manager at the top and &lsquo;No Access&rsquo; at the
bottom.
</p>
<p>MANAGER Can access everything
DESIGNER Can modify all design elements
EDITOR Can create documents and edit all documents
AUTHOR Can create documents and edit those documents they have created
READER Can only read documents, but not create or modify documents
DEPOSITOR Can only create documents, but not read those documents
NO ACCESS No access to anything
</p>
<p>Adapted form &lsquo;The ABC&rsquo;s of using the ACL&rsquo; at http://www.ibm.com/
developerworks/lotus/library/ls-Using_the_ACL/#N10085 accessed 04/06/2013.
</p>
<p>No Access, Reader and Depositor access is fairly straight forward, but when
you get to Author there are a number of options available. For example, you might
be designated an author of a document, but you might not have a &lsquo;delete&rsquo; right
for the document (even though you created it). The same is true for Editor ac-
cess.</p>
<p/>
<div class="annotation"><a href="http://www.ibm.com/developerworks/lotus/library/ls-Using_the_ACL/#N10085">http://www.ibm.com/developerworks/lotus/library/ls-Using_the_ACL/#N10085</a></div>
<div class="annotation"><a href="http://www.ibm.com/developerworks/lotus/library/ls-Using_the_ACL/#N10085">http://www.ibm.com/developerworks/lotus/library/ls-Using_the_ACL/#N10085</a></div>
</div>
<div class="page"><p/>
<p>288 12 Security
</p>
<p>As well as access, IBM considers five user types.
</p>
<p>PERSON You as an individual
PERSON GROUP List of names belonging to a group
SERVER Individual server
SERVER GROUP Servers in a group
MIXED GROUP Combination of server and person groups.
</p>
<p>The user types are a means of assigning an ID for accessing the database and all
vendors have an equivalent system although they vary widely in implementation.
An individual ID will not give a user access to a database that requires a group ID.
Likewise if a server or server group is specified, a user will not be able to access the
database unless they use specified server. The final option only requires you to be a
member of a group and you can be either a server or a client.
</p>
<p>These settings allow a database administrator to have a structured approach to
privileges. It should also be noted that a user may be another server rather than
a human. Once created the ACL needs to be maintained. Users roles change and
when this happens their privileges must be reviewed.
</p>
<p>Even when you have established privileges and keep the ACL up to date, there
can still be an issue with legitimate privilege abuse where legitimate access privi-
leges are used for unauthorised purposes. Examples of this kind of abuse include:
&bull; Accessing confidential data unnecessarily. There was a case where a tax official
</p>
<p>was viewing a celebrities records for no other reason than they were a fan.
&bull; Retrieving large quantities of data for no legitimate reason causing performance
</p>
<p>degradation.
&bull; Downloading data and storing it locally. This often happens when someone sets
</p>
<p>up their own bespoke system&mdash;typically on a spreadsheet. The issue is com-
pounded if it is sensitive data and it gets lost. How many times have you read
about laptops and pen drives with sensitive information on them getting lost or
stolen?
</p>
<p>This kind of abuse is more difficult to control but could be aided by more granular
ACL policies and firm personnel management.
</p>
<p>The final type of privilege abuse is privilege elevation where a user who has
legitimate access exploits a vulnerability to gain administrative privileges. This will
be discussed further in the next section on platform vulnerabilities.
</p>
<p>As already discussed, the primary way of dealing with privilege abuse is through
a ACL or an equivalent and we have seen the philosophy behind IBM&rsquo;s approach.
However different vendors set this up in different ways. For example Oracle does not
have an ACL but has privileges grouped into roles which are placed in a hierarchy.
There are 52 predefined user roles in Oracle 11g and new roles can be created,
assuming you have the privileges to do it. Some roles like sys also need to have root
privileges for the operating system.
</p>
<p>SQL Server on the other hand does not have a dedicated ACL but instead has
a series of operations which can be applied to define privileges and give the same
resource as an ACL. This is held as a hierarchy with the top level user being iden-</p>
<p/>
</div>
<div class="page"><p/>
<p>12.4 Privilege Abuse 289
</p>
<p>tified as the principle. Below that there are the operating system, SQL Server and
database level permissions.
</p>
<p>The other way to restrict access to data is via views. A view is in effect a virtual
table which may allow only certain attributes to be viewed and manipulate. Views
can include joins so the virtual table a user is working with is made up of columns
from two or more tables. As well as restricting access to data it makes complex
queries easy by hiding the original (complex) SQL query. It also allows the same
data to be presented in different ways to meet user requirements.
</p>
<p>In ORACLE and SQL Server a simple view can be created by:
</p>
<p>CREATE VIEW phone_contact_vu
</p>
<p>AS SELECT customer_name, customer_phone, customer_mobile
</p>
<p>FROM customer;
</p>
<p>This would create a view to retrieve telephone details ignoring other columns
such as address details. A WHERE clause can be added to further restrict data:
</p>
<p>CREATE VIEW phone_contact_vu
</p>
<p>AS SELECT customer_name, customer_phone, customer_mobile
</p>
<p>FROM customer
</p>
<p>WHERE customer_city = &lsquo;Sheffield&rsquo;;
</p>
<p>Once the view is created it can be used to retrieve data as if it was a table, for
example:
</p>
<p>SELECT &lowast; FROM phone_contact_vu;
</p>
<p>You can add restrictions, for example if this was to be used only for reference
and the user did not have permission to change details:
</p>
<p>CREATE VIEW phone_contact_vu
</p>
<p>AS SELECT customer_name, customer_phone, customer_mobile
</p>
<p>FROM customer
</p>
<p>WHERE customer_city = &lsquo;Sheffield&rsquo;
</p>
<p>WITH READ ONLY;
</p>
<p>Any attempt to alter the table with either a DELETE FROM, INSERT or UP-
DATE statement would result in an error. An alternative to the READ ONLY clause
is WITH CHECK OPTION where any DELETE, INSERT or UPDATE must con-
form to the definition of the view. So if you created the view with:
</p>
<p>CREATE VIEW phone_contact_vu
</p>
<p>AS SELECT customer_name, customer_phone, customer_mobile
</p>
<p>FROM customer
</p>
<p>WHERE customer_city = &lsquo;Sheffield&rsquo;
</p>
<p>WITH CHECK OPTION;
</p>
<p>and then tried an UPDATE of the form:
</p>
<p>UPDATE phone_contact_vu SET customer_city = &lsquo;Leeds&rsquo;;</p>
<p/>
</div>
<div class="page"><p/>
<p>290 12 Security
</p>
<p>you would get an error because the view is defined such that the customer city can
only be Sheffield.
</p>
<p>A complex view has more than one table in its definition, for example:
</p>
<p>CREATE VIEW outstading_invoice_vu
</p>
<p>AS SELECT customer_name, invoice_id, invoice_date
</p>
<p>FROM customer
</p>
<p>INNER JOIN invoice
</p>
<p>ON customer.customer_id = invoice.customer_id
</p>
<p>WHERE invoice_status &lt;&gt; &lsquo;paid&rsquo;
</p>
<p>ORDER BY invoice_date;
</p>
<p>which can be used to retrieve customers who have an outstanding invoice. One prob-
lem with complex views in ORACLE is that you cannot perform data manipulation
with them. To achieve this requires a procedure or trigger needs to be written.
</p>
<p>Not all mistakes in data entry are deliberate, so integrity checks should be set
up which are enforced by the database management system. These include check
constraints to limit the chances of wrong data being entered. For example to add a
constraint:
</p>
<p>ALTER TABLE customer
</p>
<p>ADD CONSTRAINT customer_city_ck
</p>
<p>CHECK (customer_city IN (&lsquo;Sheffield&rsquo;, &lsquo;Rotherham&rsquo;, &lsquo;Doncaster&rsquo;));
</p>
<p>In the above example it should be noted that each of the city names has to be in
the format described. &lsquo;SHEFFIELD&rsquo;, for example would generate an error. A better
way to limit invalid data entry is to use lookup tables so the user has to choose
from a list rather than entering raw data. This would also reduce the possibility of
mistyping data on entry. To implement this would require a simple program known
as a procedure or trigger to be written.
</p>
<p>Where possible, rules to trap transcription and transposition errors should always
be implemented. A typical example of a transcription error is entering a zero in place
of the character &lsquo;O&rsquo;. Transposition errors are harder to trap, for example entering
547619 instead of 546719.
</p>
<p>Some database management systems such as those following CODASYL con-
ventions use a concept of subschemas instead of views. A subschema is part of
the database that a user can access and manipulate. This was quite common in early
mainframe databases with hierarchical and network structures which were described
in Chap. 2.
</p>
<p>Ultimately the guiding principle for privileges is to only give a user the bare
minimum they require to be able to complete their tasks. When a new user is added
to the system unnecessary privileges (often added as defaults) should be removed.
If a user&rsquo;s responsibilities change, their privileges should be reviewed.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.5 PlatformWeaknesses 291
</p>
<p>12.5 PlatformWeaknesses
</p>
<p>Unfortunately most products and their underlying operating systems and services
have weaknesses and unresolved issues. That is why regular patches are issued by
vendors. These have two roles. The first and of most concern here are to fix reported
security weaknesses and bugs in a product. The second is to improve usability and
performance. It is always a good idea to install these as soon as possible otherwise
your system could be vulnerable.
</p>
<p>An example of what can happen was posted on 14th January 2013 in Yahoo news
(http://news.yahoo.com/oracle-says-java-fixed-feds-maintain-warning-230702904&ndash;
finance.html)
</p>
<p>&ldquo;Oracle Corp. said Monday it has released a fix for the flaw in its Java software that raised
an alarm from the U.S. Department of Homeland Security last week. Even after the patch
was issued, the federal agency continued to recommend that users disable Java in their Web
browsers.
&ldquo;This and previous Java vulnerabilities have been widely targeted by attackers, and new Java
vulnerabilities are likely to be discovered,&rdquo; DHS said Monday in an updated alert published
on the website of its Computer Emergency Readiness Team. &ldquo;To defend against this and
future Java vulnerabilities, consider disabling Java in Web browsers until adequate updates
are available.&rdquo;
The alert follows on the department&rsquo;s warning late Thursday. Java allows programs to run
within websites and powers some advertising networks. Users who disable Java may not be
able to see portions of websites that display real-time data such as stock prices, graphical
menus, weather updates and ads. Vulnerability in the latest version, Java 7, was &ldquo;being
actively exploited,&rdquo; the department said. Java 7 was released in 2011. Oracle said installing
its &ldquo;Update 11&rdquo; will fix the problem.
Security experts said that special code to take advantage of the weakness is being sold on
the black market through so-called &ldquo;Web exploit packs&rdquo; to Internet abusers who can use
it to steal credit card data, personal information or cause other harm. The packs, sold for
upwards of $1,500 apiece, make complex hacker codes available to relative amateurs. This
particular flaw even enables hackers to compromise legitimate websites by taking over ad
networks. The result: users are redirected to malicious sites where damaging software can
be loaded onto their computers.&rdquo;
</p>
<p>Oracle issue regular security alerts (http://www.oracle.com/technetwork/topics/
security/) which includes critical patch updates. Likewise Microsoft issues SQL
Server alerts (see http://support.microsoft.com/kb/959420).
</p>
<p>It is beyond the scope of this book to discuss intrusion and virus threats in de-
tail. New viruses are released constantly so updates to anti-virus software are re-
leased almost daily and sometimes more often. Likewise attempts to gain unautho-
rized access to systems are getting more sophisticated. It goes without saying you
should have firewall and virus detection software in place that is regularly updated.
There are both free (for example Avast, www.avast.com, last accessed 30/07/2013)
and commercial (for example McAfee, www.mcafeeprotection.com, last accessed
30/07/2013) solutions readily available.</p>
<p/>
<div class="annotation"><a href="http://news.yahoo.com/oracle-says-java-fixed-feds-maintain-warning-230702904--finance.html">http://news.yahoo.com/oracle-says-java-fixed-feds-maintain-warning-230702904--finance.html</a></div>
<div class="annotation"><a href="http://news.yahoo.com/oracle-says-java-fixed-feds-maintain-warning-230702904--finance.html">http://news.yahoo.com/oracle-says-java-fixed-feds-maintain-warning-230702904--finance.html</a></div>
<div class="annotation"><a href="http://www.oracle.com/technetwork/topics/security/">http://www.oracle.com/technetwork/topics/security/</a></div>
<div class="annotation"><a href="http://www.oracle.com/technetwork/topics/security/">http://www.oracle.com/technetwork/topics/security/</a></div>
<div class="annotation"><a href="http://support.microsoft.com/kb/959420">http://support.microsoft.com/kb/959420</a></div>
<div class="annotation"><a href="http://www.avast.com">http://www.avast.com</a></div>
<div class="annotation"><a href="http://www.mcafeeprotection.com">http://www.mcafeeprotection.com</a></div>
</div>
<div class="page"><p/>
<p>292 12 Security
</p>
<p>12.6 SQL Injection
</p>
<p>In some ways this is an extension to the previous topic on system vulnerabilities,
but it is a common way of compromising databases. It refers to the insertion of
SQL statements in a data entry field. It is the most common type of attack on web
connected databases. If the attack is successful it can result in access to unauthorized
data and manipulation of that data or privilege elevation which we have already
discussed under privilege abuse. SQL injection comes in four main forms: SQL
manipulation, code injection, function call injection and buffer overflow.
</p>
<p>SQL manipulation is the most common form of SQL injection and involves an
attacker attempting to modify existing SQL by adding elements to the WHERE
clause and seeing if any records were returned. If the original statement was:
</p>
<p>SELECT &lowast; FROM users
</p>
<p>WHERE username = &lsquo;Dorian&rsquo; AND password = &lsquo;&amp;mypassword&rsquo;;
</p>
<p>and the attacker added an OR clause so it became:
</p>
<p>SELECT &lowast; FROM users
</p>
<p>WHERE username= &lsquo;Dorian&rsquo; AND password= &lsquo;&amp;mypassword&rsquo; OR &lsquo;a&rsquo;= &lsquo;a&rsquo;;
</p>
<p>then the WHERE clause would always be true because of operator precedence and
the attacker would have gained access to the application.
</p>
<p>Another variation is to use the UNION operator to try and return rows from
another table, so if the original query was:
</p>
<p>SELECT item_description FROM stock_item
</p>
<p>WHERE price &lt;10;
</p>
<p>the attacker might attempt:
</p>
<p>SELECT item_description FROM stock_item
</p>
<p>WHERE price &lt;10
</p>
<p>UNION
</p>
<p>SELECT username FROM admin_user
</p>
<p>WHERE username like &lsquo;%&rsquo;;
</p>
<p>which returns not only a list of items but also all database administration users.
Code injection tends to be more of a problem with Microsoft SQL Server and
</p>
<p>PostgreSQL (Kost 2007) because it involves adding an extra SQL statement to an
existing statement. Oracle does not support multiple SQL statements per database
request so it is afforded some protection. However, underlying platform weaknesses
as already discussed may allow a java program in a web application to execute
multiple requests.
</p>
<p>Function call injection is a variation on SQL manipulation where a database
function (either a built in one or a custom written one) is added to vulnerable SQL
code. Oracle supplies some functions which perform network communication which
could be exploited as well as over a thousand other functions which would have no
use in an attack.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.7 Weak Audit 293
</p>
<p>Buffer overflow occurs when a program writes data to an area of memory and
overwrites adjacent memory. A buffer is a temporary area in memory storage space
where data is kept prior to writing to another device. In the case of databases, that
device is disk storage. Some Oracle functions are susceptible to buffer overflows
and this can be exploited through a SQL injection attack.
</p>
<p>It is easy to protect against SQL injection attacks so it ultimately comes down
to enforcing a series of methods to every web accessible procedure and function
as even one unprotected SQL statement could cause problems. The following are a
minimum:
&bull; Always use bind variables (substitution variables that are used in place of liter-
</p>
<p>als). In any SQL statement where a variable is required, for example in a select
statement, the variable should have been declared as a bind variable. It also im-
proves system performance because statements are reused rather than reparsed
at every execution.
</p>
<p>&bull; Always validate every passed string parameter.
&bull; Restrict all functions that are not absolutely necessary
</p>
<p>12.7 Weak Audit
</p>
<p>Database auditing is at the core of any database security policy. Database systems
generate an audit log that captures a record of all data access and alternations. This
log is managed internally by the database management system and is the most ac-
curate source of monitoring activity. However, there is a possibility of issues arising
if an organisation bases all of its audit policies on the built in database mechanisms.
The main problem here is that the system will generate a large amount of data as
each transaction is logged. The sheer volume of data requires knowledge of what to
look for to make sure there have been no security violations, for example privilege
elevation. There are five things that should be monitored for suspicious activity:
&bull; Who accessed which systems when and how. If there is evidence of failed logins
</p>
<p>there is a high probability someone is trying to break into your system. This is
particularly important when you consider most users do not directly log in to a
database but go through other applications which automatically connect.
</p>
<p>&bull; User and administrator activity should be examined. Failed queries should be
taken seriously as they may be an indication of an attacker probing known vul-
nerabilities in a system including assuming specific features or structures will
be present. The error details which are returned when a query fails should be
logged rather than displayed to the user as they may use the details to refine
their attack. Even if the failed query is not a result of an attack it should still be
checked because it may be an indication of a flaw in a script which in turn could
lead to application failure.
</p>
<p>&bull; Abnormal activity such as unusual access to sensitive data should be logged and
investigated. This may require identifying what is sensitive data then applying
filters to audit logs to isolate activity on such data.</p>
<p/>
</div>
<div class="page"><p/>
<p>294 12 Security
</p>
<p>&bull; All databases should be audited for vulnerabilities and threats. This relates back
to platform weaknesses. If you know about weaknesses, you can monitor activ-
ity attempting to exploit them.
</p>
<p>&bull; Changes to metadata should be monitored. Changes to the databases structure
can lead to new vulnerabilities. It is essential to establish a baseline policy for
monitoring changes to database configuration. If there are any deviations from
that baseline, they should be tracked back to establish their source.
</p>
<p>Recording everything in an audit log provides lots of useful information, but
there is an impact on performance. Therefore it is necessary to understand the audit
settings on your system and implement them in a way that will capture critical infor-
mation without overly degrading performance. Oracle allows generation of &lsquo;. . . audit
records that include information about the operation that was audited, the user per-
forming the operation, and the date and time of the operation. Audit records can be
stored in the database audit trail or in files on the operating system. Standard au-
diting includes operations on privileges, schemas, objects, and statements.&rsquo; (Oracle,
&lsquo;12c Traditional Database Auditing&rsquo;, http://www.oracle.com/technetwork/database/
security/index-085803.html, accessed 28/07/2013). Oracle suggests that the audit
records should be stored on the operating system as this has the least overhead com-
pared to storing it on the host database system. It should also be noted that every
write to the audit files is a system overhead that will reduced system performance.
This has to be balance with the need to recover a system or look for evidence of
security breaches.
</p>
<p>12.8 Protocol Vulnerabilities
</p>
<p>A communication protocol defines the methods of exchanging messages between
computer systems and there can be weaknesses. As an example, in 2003 the SQL
Slammer Worm exploited a weakness in Microsoft SQL Server and Desktop Server
protocol. It was small and resident in memory. Its effect was to generate random
IP addresses and send itself to those addresses. It should be noted the worm did
not use SQL code, but exploited a buffer overflow problem. It was used to execute
denial of service attacks severely compromising performance. Interestingly a patch
was available to guard against the problem six months before the worms launch, but
many systems did not have it installed.
</p>
<p>Although this example features SQL Server, communication protocol vulnerabil-
ities have been identified for every vendor&rsquo;s products. Basically a communication
protocol is how components of a system communicate and if a component can be
impersonated, then there is a security issue.
</p>
<p>It is therefore essential that protocol validation is enabled. This technology breaks
down database traffic, a process known as parsing, and looks for anomalies. Only
normal client generated messages should be allowed through with requests using
hidden features blocked. Obviously know attacks (like the SQL Slammer Worm)
should be checked for which means implementing security patches as they become
available.</p>
<p/>
<div class="annotation"><a href="http://www.oracle.com/technetwork/database/security/index-085803.html">http://www.oracle.com/technetwork/database/security/index-085803.html</a></div>
<div class="annotation"><a href="http://www.oracle.com/technetwork/database/security/index-085803.html">http://www.oracle.com/technetwork/database/security/index-085803.html</a></div>
</div>
<div class="page"><p/>
<p>12.9 Authentication Vulnerabilities 295
</p>
<p>12.9 Authentication Vulnerabilities
</p>
<p>What you are trying to defend against here is someone guessing account names and
passwords. In a large English language western organisation the chances of there
being a &lsquo;John Smith&rsquo; with legitimate access rights is quite large. Some organisa-
tions have a standard which is used for both e-mail addresses and userid&rsquo;s for ex-
ample, J.Smith@somecompany.org and then J.Smith as a userid. Combine this with
a predictable choice of password (&lsquo;password&rsquo; is the most common password) and
you have access. Unfortunately many people use the same password for multiple
systems, so once you have access to one system, you may have access to multiple
systems. This type of knowledge is used in so called &lsquo;brute force&rsquo; attacks were at-
tacking systems use common names then a list of common passwords in an attempt
to break in.
</p>
<p>Measures to mitigate these problems include using a different standard for e-
mail addresses and userids with the userids avoiding real names. There should be a
strong password policy. The authors&rsquo; institution requires passwords to be changed
on a regular basis, the password to be at least eight characters long, the password
to contain a mixture of alphabetic and other characters and the password not to
have been used recently. The authors&rsquo; bank doesn&rsquo;t enforce a password change but
requires random characters from it to be entered along with a &lsquo;secret&rsquo; word randomly
selected from a list provided by the user. Most systems now disable an account if
three unsuccessful attempts are made to log in.
</p>
<p>Even with these authentication measures in place, unsuccessful access attempts
should be logged and investigated, particularly if there is a pattern of related attacks.
</p>
<p>12.10 Backup Data Exposure
</p>
<p>Backup and recovery are topics dealt with in the chapter on availability. However it
is worth emphasising that databases and in fact any data should be regularly backed
up. Most database management systems supply utilities to do this. However there
have been many incidents where backup media has been lost or stolen. This has
become an escalating problem with the proliferation of mobile devices and memory
sticks. Often the data on these is not encrypted meaning there is a potential for large
amounts of sensitive data to be compromised.
</p>
<p>Encryption of data won&rsquo;t stop devices being lost, but it will keep the data se-
cure. There are however costs, the most noteworthy being performance degradation.
Encryption is also often application dependent sometimes with complex key man-
agement.
</p>
<p>To mitigate backup data exposure, some organisations have banned the use of
none encrypted devices to transport data. The authors organisation requires staff
transporting sensitive data on memory sticks to use IronKey devices. IronKey is a
proprietary device with built in encryption. It can be configured to delete all data
stored on it if the maximum number of failed password entry attempts is exceeded.</p>
<p/>
<div class="annotation"><a href="mailto:J.Smith@somecompany.org">mailto:J.Smith@somecompany.org</a></div>
</div>
<div class="page"><p/>
<p>296 12 Security
</p>
<p>12.11 Mobile Device Based Threats
</p>
<p>Most of the threats mentioned above also apply to mobile devices, especially backup
data exposure. Loss or theft of an unsecured mobile device is a major concern. For
example, loss and theft of mobile devices cost the BBC over &pound;750,000 between
2010 and 2012 (ComputerWorld UK 2013). This was just the value of the hardware
and did not include the cost of data loss and security exposure related to stored
passwords. Many mobile devices require a user to activate password security, the
default being it is off meaning if this was the case with the BBC devices there was
a major security breach. As already mentioned many systems including Microsoft
Windows will offer to remember passwords. That should always be declined and
password security to the device should be enabled.
</p>
<p>There is another aspect which relates to distributed databases. Most mobile
databases are distributed among wired components which are accessed by mobile
devices rather than being held on the devices themselves. This is the thin client ap-
proach where the mobile device is the thin client and all the processing is done on
the host system. Following the rules of never let your device store the passwords
and always activate the password security on the device, this should be secure. But
in a second scenario data is distributed among both wired and wireless components
with active data being distributed to the mobile device. Data management involves
data fragmented between mobile devices and fixed servers or base stations. In this
scenario base stations and mobile devices must all be secure.
</p>
<p>Something that can be regarded either as a threat or a security tool is a wireless
sniffer. This is a form of packet analyser. Data is transmitted in packets and this tool
decodes the packet&rsquo;s raw data, showing the values of various fields in the packet, and
analyses its content. On the positive side it can be used to analyse network problems
and detect network intrusion attempts and network misuse. However wireless snif-
fers can also be used to gain information for executing a network intrusion and to
spy on network users and gather sensitive information. To guard against these threats
router security should be activated and data should be encrypted.
</p>
<p>12.12 Security Issues in Cloud Based Databases
</p>
<p>Cloud computing is a technology that gained prominence in 2006 when Amazon
introduced the Elastic Compute Cloud (EC2) and was at the peak of Gartner&rsquo;s hype
cycle for emerging technologies from 2009 onwards (see Chap. 2 for a description
of the Gartner Hype Cycle). Since 2012, Gartner published an annual snap shot of
the hype cycle specifically for cloud computing. One aspect of this is using the cloud
as a database service platform.
</p>
<p>The idea of cloud computing is that an organisation does not have to store its
own data, or even applications. Instead they are stored on a provider&rsquo;s servers and
accessed via the internet. This has many advantages for a user but introduces some
new security issues and threats.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.12 Security Issues in Cloud Based Databases 297
</p>
<p>Because a user is no longer directly responsible for database security, there is
the possibility of failure to provide adequate security by the provider. The provider
controls the servers, network and security regime therefore the user must trust the
provider&rsquo;s security. It follows that when choosing a cloud provider an organisation
should verify the providers security measures before signing a contract with them
and to monitor the provider&rsquo;s security performance. It should be noted however that
a cloud provider often has security measures that far exceed those of most small to
medium sized enterprises.
</p>
<p>A second issue is attacks by other users. This process was documented by Ris-
tenpart et al. (2009) where a method of stealing data from Amazons cloud was
demonstrated. It was based on the fact that if you hired multiple virtual machines
from Amazons cloud, they had similar IP addresses. If you knew an application was
cloud based, an attacker could bombard a victim site with requests forcing the target
organisation to hire more virtual machines. At the same time the attacker would hire
more virtual machines themselves. It was found that 40 % of the time the attack-
ers and victims ended up on the same server where the attacker could monitor the
victim&rsquo;s virtual machine. Although they didn&rsquo;t actually steal any data, the authors
said outright data theft was possible. It comes about because provider resources
are shared with other parties, therefore the provider must take measures to separate
users data and applications. Amazon has since closed this security loophole.
</p>
<p>A related issue is data sanitization. Cloud computing is dynamic with data con-
stantly being moved as demand on servers fluctuates. When data is moved, the phys-
ical location where the data is moved from needs to be overwritten rather than just
being flagged as &lsquo;free&rsquo;. This differs from an in-house application where pointers to
data are removed and the disk space is flagged as being available rather than data be-
ing physically removed. In a cloud environment this is not sufficient as other clients
of the cloud provider may (and probably will) use the space and could potentially see
residual data. On in-house systems not physically deleting data has the advantage
that if the &lsquo;accidental&rsquo; delete is recognised soon enough the data can be &lsquo;un-deleted&rsquo;.
In a cloud environment with multiple organisational users, it is a security risk.
</p>
<p>An area of vulnerability for data in the cloud is during data transfer. Standard
communication protocols and procedures such as Hypertext Transfer Protocol Se-
cure (HTTPS) and Secure Shell (SSH) should be implemented. The data should be
encrypted to guard against data sanitisation failure.
</p>
<p>Although not directly a security threat, there is an issue with regulatory compli-
ance. For example if an organisation in the United Kingdom is using cloud resources
and the servers supplying that resource are in the United States, then the resource is
subject to US law and regulation&mdash;in other words location matters. This has resulted
in many organisations adopting a policy of only non-critical systems with data that
is not sensitive being deployed in the cloud.
</p>
<p>A lot of the security issues associated with databases in the cloud come down to
the service provider. Before embarking on a project to move data to the cloud the
provider should be thoroughly vetted. Questions to ask related to the issues raised
in this section to provide an idea of risk should include:
&bull; What is the providers security policy including segregation of users?</p>
<p/>
</div>
<div class="page"><p/>
<p>298 12 Security
</p>
<p>&bull; What is the providers&rsquo; backup and recovery policy?
&bull; What are the providers&rsquo; encryption procedures?
&bull; Where is the physical location of the data (although this could be several loca-
</p>
<p>tions)?
&bull; Conduct a risk assessment of the providers viability (how likely are they to go
</p>
<p>bust or get taken over?)
</p>
<p>12.13 Policies and Procedures
</p>
<p>Unfortunately the greatest online security threat comes from within an organisation
from personnel who have legitimate access to a database. We have already looked
at some of the issues here in the section on privilege abuse. To reduce the internal
threat to an organisations database system there needs to be policies and procedures
in place which are enforced and monitored:
</p>
<p>Personnel This comes down to good personnel management and monitoring. All
personnel should have a profile detailing what access level they need to carry out
their duties. This should be set at a minimum and relates to both physical access to
parts of the building and system access. Any attempts, whether deliberate or acci-
dental to circumvent this must be followed up. In the event of an employee leaving,
their privileges and access must be immediately revoked. This often means paying
out an employee&rsquo;s notice period rather than letting them remain on the organisations
premises. If an employee is fired or disgruntled, this becomes even more critical.
</p>
<p>Software Upgrades These should be carried out as soon as practical, particularly
security upgrades relating to firewalls, viruses and system vulnerabilities. This needs
to be monitored to make sure it is happening. If you have deployed a database in the
cloud you also need to check the provider has those policies in place and is adhering
to them.
</p>
<p>Building Maintenance There is little point having CCTV surveillance if the cam-
eras or recording devices are not working. The same goes for access controls, for
example secure doors should never be wedged open, even during cleaning opera-
tions. If any part of the physical security of the organisations infrastructure develops
a fault it should be fixed immediately.
</p>
<p>12.14 A Security Checklist
</p>
<p>The following is based on Oracle&rsquo;s checklist, but is generalised to include other
platforms. It details software and configuration security rather than physical security
of a system.
&bull; Disable default user accounts. Most database management systems come with
</p>
<p>preconfigured user accounts&mdash;Oracle has some 20 of these. They are a com-
mon first place an attempt to break into a system will occur. Other proprietary
systems may have a &lsquo;guest&rsquo; user or other default accounts.</p>
<p/>
</div>
<div class="page"><p/>
<p>12.15 Review Questions 299
</p>
<p>&bull; Change default passwords. This is a trivial issue, but one that can leave a system
vulnerable if not implemented. It is crucial for administrative accounts. This is
important for any system&mdash;not just databases (how many of you still have a de-
fault password set on your broadband hub?). Passwords should also be changed
regularly. If your system does not have a feature to enforce this, develop and
enforce a procedure for your users.
</p>
<p>&bull; Protect the important resource of your database data dictionary. This stores the
name, meaning, relationships to other data, usage, and format of all components
of your database. Protection must therefore be enabled and access restricted to
those with DBA roles.
</p>
<p>&bull; Give users the lowest level of privilege possible. This means granting only
necessary privileges, but also revoking unnecessary privileges if a user&rsquo;s role
changes. Monitor employees roles and review and update privileges if their roles
change.
</p>
<p>&bull; Create and maintain a Access Control List or its equivalent for your system.
&bull; Authenticate client systems properly&mdash;in other words there needs to be stringent
</p>
<p>authentication of any client system trying to access the database.
&bull; Protect any database gateway to the network. Particularly do not let any gate-
</p>
<p>way software read or write files in the database or sever address space. This is
another favoured way of attacking systems via sql injection.
</p>
<p>&bull; Monitor who accesses your system. This should be done at the user rather than
server level as servers may be compromised.
</p>
<p>&bull; Apply all security patches as soon as they are available. In most cases there is
an automatic update, but it is worth regularly checking your vendors web site to
check for security alerts and patches. Related to this is to inform the vendor if
you become aware of a vulnerability so a patch can be developed.
</p>
<p>&bull; If you are using a web interface to your database consider including a
CAPTCHA (Completely Automated Public Turing test to tell Computers and
Humans Apart) test where a string of characters are presented in a deformed
way and the user is asked to enter them. This stops attacks where a computer
program attempts to access a website. Recent reports suggest that ways are
being found to negate this test.
</p>
<p>&bull; Enforce a minimum access security regime of a userid and a password. Make
sure there is a password policy which vets passwords to make sure they are not
easily guessable. In a database environment there needs to be several levels of
security so you may wish to include further security checks.
</p>
<p>&bull; If you are using a cloud service provider for your database verify that they have
a security checklist and enforce it.
</p>
<p>12.15 Review Questions
</p>
<p>The answers to these questions can be found in the text of this chapter.
</p>
<p>&bull; What are 8 types of threats to database security?
&bull; What are the three basic things you should have for physical access to a secure
</p>
<p>part of a building?</p>
<p/>
</div>
<div class="page"><p/>
<p>300 12 Security
</p>
<p>&bull; What is privilege abuse and what measures can be taken to mitigate it?
&bull; Why are mobile devices a particular security threat and what measures can be
</p>
<p>taken to reduce that threat?
&bull; What is an SQL injection attack?
</p>
<p>12.16 GroupWork Research Activities
</p>
<p>These activities require you to research beyond the contents of the book and can be
</p>
<p>tackled individually or as a discussion group.
</p>
<p>Activity 1 Create a table of four columns. In the first list the devices you own
or have administrative rights to. Don&rsquo;t forget mobile devices such as phones and
tablets. In the second column place a tick if you have changed the default password
(and remember, the default may be no password). In the third column place an X
for devices you have the same password for. If you have several passwords you use
regularly, use other letters of the alphabet. Leave a blank if the password is unique.
In the fourth column write the date of the last time you changed the password. If
you have never changed it, put an X there.
</p>
<p>You have a security issue if there is no tick in column two, anything in column
three and an X or a date older than 30 days in column four.
</p>
<p>If you have no security risks identified congratulations!
If you have any security issues identified take steps to rectify them including a
</p>
<p>schedule to change passwords. You should also ask yourself if the password is easily
guessable (like &lsquo;password&rsquo;)
</p>
<p>You might consider repeating this exercise for online services (including bank
accounts) and fix them as well.
</p>
<p>Did you remember to include your wireless hub?
</p>
<p>Activity 2 This exercise deals with security software.
What virus checking and firewall software do you run?
Does it automatically download and install updates?
When did it last install an update?
When was the last time you did a full system check? It is possible that you had
</p>
<p>a virus last time you did a full scan that wasn&rsquo;t detected by what was then the most
up-to-date version.
</p>
<p>Is your product licence current?
Do you subscribe to a service that regularly gives news of security threats?
If any of your answers are &lsquo;none&rsquo;, &lsquo;don&rsquo;t know&rsquo; or &lsquo;no&rsquo; you need to take urgent
</p>
<p>action as your system may be exposed.
</p>
<p>References
</p>
<p>ComputerWorld UK (2013) Loss and theft of mobile devices costs the BBC over &pound;750,000 in three
years. http://www.computerworlduk.com/news/mobile-wireless/3441452/loss-and-theft-of-
mobile-devices-costs-bbc-over-750000-in-three-years/. Accessed 28/07/2013</p>
<p/>
<div class="annotation"><a href="http://www.computerworlduk.com/news/mobile-wireless/3441452/loss-and-theft-of-mobile-devices-costs-bbc-over-750000-in-three-years/">http://www.computerworlduk.com/news/mobile-wireless/3441452/loss-and-theft-of-mobile-devices-costs-bbc-over-750000-in-three-years/</a></div>
<div class="annotation"><a href="http://www.computerworlduk.com/news/mobile-wireless/3441452/loss-and-theft-of-mobile-devices-costs-bbc-over-750000-in-three-years/">http://www.computerworlduk.com/news/mobile-wireless/3441452/loss-and-theft-of-mobile-devices-costs-bbc-over-750000-in-three-years/</a></div>
</div>
<div class="page"><p/>
<p>References 301
</p>
<p>Kost S (2007) An introduction to SQL injection attacks for Oracle developers. Integrity white
paper. http://www.integrigy.com/files/Integrigy_Oracle_SQL_Injection_Attacks.pdf. Accessed
30/07/2013
</p>
<p>Oracle Corp (2012a) Oracle&reg; database security guide 10g Release 2 (10.2) B14266-09. Available
on line at docs.oracle.com/cd/B19306_01/network.102/b14266.pdf. Accessed 02/05/2013
</p>
<p>Oracle Corp (2012b) Oracle&reg; database security guide 11g Release 1 (11.1) B28531-19.
Available on line at http://docs.oracle.com/cd/B28359_01/network.111/b28531.pdf. Accessed
28/07/2013
</p>
<p>Ristenpart T, Tromer E, Shacham H, Savage S (2009) Hey, you, get off of my cloud: exploring in-
formation leakage in third-party compute clouds. In: Proceedings of the 16th ACM conference
on computer and communications security, pp 199&ndash;212. Available at http://www.cs.cornell.
edu/courses/cs6460/2011sp/papers/cloudsec-ccs09.pdf. Accessed 15/05/2013
</p>
<p>Scalet SD (2009) 19 ways to build physical security into a data center. http://www.
csoonline.com/article/220665/19-ways-to-build-physical-security-into-a-data-center. Accessed
09/04/2013
</p>
<p>Shinder D (2007) 10 physical security measures every organization should take. http://www.
techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-
take/. Accessed 28/07/2013
</p>
<p>Further Reading
</p>
<p>IBM (2013) Database ACL settings. Available online at http://publib.boulder.ibm.com/infocenter/
sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.
html. Accessed 02/05/2013.
</p>
<p>LSoft Technologies (2013) Active@ Kill Disk&mdash;Hard Drive Eraser. http://download.cnet.com/
Active-Kill-Disk-Hard-Drive-Eraser/3000-2092_4-10073508.html?tag=mncol;2. Accessed
28/07/2013
</p>
<p>SQL Server (2012) Security and protection (database engine). Available on-line at http://msdn.
microsoft.com/en-us/library/bb510589.aspx. Accessed 28/07/2013</p>
<p/>
<div class="annotation"><a href="http://www.integrigy.com/files/Integrigy_Oracle_SQL_Injection_Attacks.pdf">http://www.integrigy.com/files/Integrigy_Oracle_SQL_Injection_Attacks.pdf</a></div>
<div class="annotation"><a href="http://docs.oracle.com/cd/B19306_01/network.102/b14266.pdf">http://docs.oracle.com/cd/B19306_01/network.102/b14266.pdf</a></div>
<div class="annotation"><a href="http://docs.oracle.com/cd/B28359_01/network.111/b28531.pdf">http://docs.oracle.com/cd/B28359_01/network.111/b28531.pdf</a></div>
<div class="annotation"><a href="http://www.cs.cornell.edu/courses/cs6460/2011sp/papers/cloudsec-ccs09.pdf">http://www.cs.cornell.edu/courses/cs6460/2011sp/papers/cloudsec-ccs09.pdf</a></div>
<div class="annotation"><a href="http://www.cs.cornell.edu/courses/cs6460/2011sp/papers/cloudsec-ccs09.pdf">http://www.cs.cornell.edu/courses/cs6460/2011sp/papers/cloudsec-ccs09.pdf</a></div>
<div class="annotation"><a href="http://www.csoonline.com/article/220665/19-ways-to-build-physical-security-into-a-data-center">http://www.csoonline.com/article/220665/19-ways-to-build-physical-security-into-a-data-center</a></div>
<div class="annotation"><a href="http://www.csoonline.com/article/220665/19-ways-to-build-physical-security-into-a-data-center">http://www.csoonline.com/article/220665/19-ways-to-build-physical-security-into-a-data-center</a></div>
<div class="annotation"><a href="http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/">http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/</a></div>
<div class="annotation"><a href="http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/">http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/</a></div>
<div class="annotation"><a href="http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/">http://www.techrepublic.com/blog/10-things/10-physical-security-measures-every-organization-should-take/</a></div>
<div class="annotation"><a href="http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html">http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html</a></div>
<div class="annotation"><a href="http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html">http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html</a></div>
<div class="annotation"><a href="http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html">http://publib.boulder.ibm.com/infocenter/sametime/v7r5m1/topic/com.ibm.help.sametime.imlu.doc/st_adm_security_usertypeacl_c.html</a></div>
<div class="annotation"><a href="http://download.cnet.com/Active-Kill-Disk-Hard-Drive-Eraser/3000-2092_4-10073508.html?tag=mncol;2">http://download.cnet.com/Active-Kill-Disk-Hard-Drive-Eraser/3000-2092_4-10073508.html?tag=mncol;2</a></div>
<div class="annotation"><a href="http://download.cnet.com/Active-Kill-Disk-Hard-Drive-Eraser/3000-2092_4-10073508.html?tag=mncol;2">http://download.cnet.com/Active-Kill-Disk-Hard-Drive-Eraser/3000-2092_4-10073508.html?tag=mncol;2</a></div>
<div class="annotation"><a href="http://msdn.microsoft.com/en-us/library/bb510589.aspx">http://msdn.microsoft.com/en-us/library/bb510589.aspx</a></div>
<div class="annotation"><a href="http://msdn.microsoft.com/en-us/library/bb510589.aspx">http://msdn.microsoft.com/en-us/library/bb510589.aspx</a></div>
</div>
<div class="page"><p/>
<p>Index
</p>
<p>A
Access, 23, 29, 36, 46, 61, 136, 186, 202, 203,
</p>
<p>218, 269, 287, 299
ACID, 25, 26, 39, 95, 98, 99, 101&ndash;103, 132,
</p>
<p>165, 168, 184, 188, 190
Actor, 77
Aggregation, 176, 191
Alert Log, 54
Amazon, 3, 11, 16, 33, 150, 202, 212, 296, 297
Analytics, 137, 138, 141, 142, 148, 159, 193,
</p>
<p>196
Application server, 205, 207
Architecture, 36, 41, 47, 97, 98, 117, 121, 150,
</p>
<p>203, 205, 207, 208, 216, 218, 226
Atomicity, 25, 102
Attribute, 28, 38, 39, 69, 72, 74, 77&ndash;79, 82, 83,
</p>
<p>88, 93, 94, 104, 106, 164, 167, 171,
172, 174, 176&ndash;178, 247
</p>
<p>Audit, 286, 293, 294
Availability, 7, 10, 17, 25, 41, 61, 63, 64, 96,
</p>
<p>99, 117, 140, 150, 214, 217, 221, 222,
226&ndash;231, 237&ndash;239, 283, 295
</p>
<p>Azure, 141, 215
</p>
<p>B
B-tree indexes, 252, 253, 254
Backed up, 284
Backup, 5, 15, 16, 34, 46, 54, 141, 184, 187,
</p>
<p>222, 226, 229&ndash;233, 235, 237&ndash;239, 248,
258, 286, 295, 296, 298
</p>
<p>BASE, 103, 132
Batch jobs, 243
Benchmarking, 259
Big Data, 15, 16, 32, 33, 39, 63, 98, 135&ndash;142,
</p>
<p>145&ndash;149, 151, 158, 159, 190, 201, 202,
218, 238
</p>
<p>Big data analytics, 147
Binary XML, 59
Bind variables, 244, 293
Bitmap Index, 253
BLOB, 42, 86, 161, 163
</p>
<p>Block, 13, 41, 50&ndash;52, 57, 186, 187, 245, 253,
254, 256, 265, 269
</p>
<p>Boolean, 42, 85
Bring Your Own Device, 217, 238
Browser, 59, 99
Buffer, 13, 43&ndash;46, 53, 188, 190, 244, 265,
</p>
<p>292&ndash;294
Buffer cache, 44, 244
</p>
<p>C
C++, 30
Cache, 43, 44, 185&ndash;187, 190, 197, 243, 244
CAP theorem, 101, 102
Cartesian join, 93
Cassandra, 100, 104, 106, 107, 109&ndash;112,
</p>
<p>115&ndash;117, 119, 120, 124, 132, 215
CBO, 250, 277
Check constraints, 86, 290
Checkpoint, 46, 53
Chen, 83, 85, 96
Cisco, 135
Client-server, 203, 204
Client/server, 204, 222, 241
CLOB, 86, 162, 249
Cloud, 16, 32, 33, 40, 63, 97, 98, 132, 134,
</p>
<p>137, 138, 140, 141, 150, 159, 205, 209,
216, 217, 219, 221, 222, 227, 235, 237,
238, 261, 296, 297
</p>
<p>Cloud computing, 16, 33, 201, 296
Cloud providers, 227
COBOL, 13, 24, 27, 28
CODASYL, 24, 290
Codd, 28, 39, 40, 69, 70, 75, 96, 249
Code injection, 292
Column-based, 104, 132, 183
Column-based database, 100
Comma Separated Variable, 58
Command-line, 59, 99, 100, 243
Commit, 25&ndash;27, 37, 39, 46, 53, 57, 58, 238
Compression, 104, 190, 191
Computer memory, 15, 39
</p>
<p>P. Lake, P. Crowther, Concise Guide to Databases,
Undergraduate Topics in Computer Science, DOI 10.1007/978-1-4471-5601-7,
&copy; Springer-Verlag London 2013
</p>
<p>303</p>
<p/>
<div class="annotation"><a href="http://dx.doi.org/10.1007/978-1-4471-5601-7">http://dx.doi.org/10.1007/978-1-4471-5601-7</a></div>
</div>
<div class="page"><p/>
<p>304 Index
</p>
<p>Concurrent users, 201, 202, 205, 207, 218, 248
Consistency, 25, 26, 57, 63, 101&ndash;103, 184,
</p>
<p>211, 224, 238, 244, 286
Constraint, 86&ndash;90, 94, 95, 143, 172, 222, 251,
</p>
<p>255, 290
Constructor orthogonality, 164, 165
Control file, 53
Control files, 46, 229, 237
Cost based one, 250
CRUD, 42
CRUDing, 25, 89, 95, 169
CSV, 58, 59, 61, 116, 117, 130, 133, 134, 139,
</p>
<p>142
</p>
<p>D
Dashboard, 228, 232
Data, 3, 11&ndash;13, 15&ndash;17, 22, 28, 30&ndash;34, 40, 44,
</p>
<p>47, 50, 52, 58, 61, 69, 90, 93, 96, 99,
101, 107, 112, 122, 125, 127, 130, 134,
137, 139&ndash;143, 145, 147, 148, 150, 151,
158, 161, 164, 167, 187, 196, 207, 237,
238, 242, 245, 249, 258, 269, 286, 296,
301
</p>
<p>Data storage, 3, 12, 15, 16, 28, 33, 42, 75, 98,
103, 132, 140, 142, 185, 190, 209, 213,
247
</p>
<p>Data warehouse, 16, 30&ndash;32, 184, 185, 191,
246, 273
</p>
<p>Data warehousing, 33, 151, 158, 243
Data-block, 50&ndash;53
Database management system, 26, 28, 29,
</p>
<p>34&ndash;37, 42, 47, 87, 169, 180, 187&ndash;190,
192, 195, 223, 225, 290, 293
</p>
<p>Database privileges, 286
Database server, 45, 123, 141, 205, 207, 210,
</p>
<p>238, 243
Datafile, 46, 230, 232
Datasets, 33, 99, 140, 143, 158, 211, 216, 245
DBA, 45, 55&ndash;57, 61, 99, 101, 206, 211, 213,
</p>
<p>222&ndash;224, 226, 228, 230, 232, 239, 243,
245, 248, 249, 251, 252, 255, 258, 259,
261, 275, 277, 299
</p>
<p>DBMS, 43, 49, 62, 103, 104, 106, 185, 278,
280, 281
</p>
<p>De-normalised, 31
Deadlock, 57, 64
Decision Support, 103, 104, 224, 228, 241
Direct access, 23, 34
Disaster recover, 211
Disaster recovery, 63, 140, 221, 222, 235, 239
Distributed, 11, 21, 36, 40, 61&ndash;63, 213, 237
Distributed database, 5, 26, 62, 63, 131
Distributed DBMS, 62
Document type definition, 37
</p>
<p>Document-based, 100, 120, 122
Document-based database, 100
Document-centric, 121
Durability, 25, 26, 184, 188, 247
</p>
<p>E
</p>
<p>E-commerce, 207, 209, 214, 218, 224, 243
Embedded systems, 246
Encapsulation, 162&ndash;164, 177, 180
Encryption, 295
Entity modelling, 69, 76, 81, 86, 96
Equijoin, 91
ER diagram, 80, 86
Export, 60
Extensibility, 164, 165, 167
</p>
<p>F
</p>
<p>Fat client, 205, 206
Fifth Normal Form, 76
First Normal Form, 71
Flashback, 222, 229
Flashback technology, 224
Foreign key, 29, 71, 72, 74, 78, 79, 82, 86,
</p>
<p>88&ndash;91, 94, 95, 164, 165, 172, 181, 211,
251
</p>
<p>Forget phase, 26
Fourth Normal Form, 75
Full Outer Join, 92
</p>
<p>G
</p>
<p>G-Cloud, 16
Gartner Hype Cycle, 32, 33, 40, 296
GFS, 63, 98, 215
Google, 3, 41, 63, 65, 98, 148, 151, 210, 215
</p>
<p>H
</p>
<p>Hadoop, 63, 64, 135, 148&ndash;151, 158, 210, 211,
215, 238
</p>
<p>HANA, 183, 190, 191, 193&ndash;197, 247
HBase, 98
HDD, 42, 44, 47, 51, 53, 137, 186, 187, 209,
</p>
<p>212, 213, 225, 227, 228, 233, 235, 246,
247, 249, 253, 254
</p>
<p>HDFS, 63, 149, 150, 158
Header, 46, 50, 130
&lsquo;Heavy&rsquo; entities, 78, 81
Hierarchical database, 27
Hierarchy, 24, 52, 78, 79, 82, 83, 162, 165,
</p>
<p>168, 169, 174, 175, 287, 288
High availability, 150, 222, 237
Horizontal scaling, 100, 106, 210, 215&ndash;217
HTML, 38, 166</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 305
</p>
<p>I
I/O, 183, 185, 187, 190, 195, 211, 243, 245,
</p>
<p>246, 253
IBM, 13, 15, 16, 22&ndash;24, 27, 36, 136, 145, 148,
</p>
<p>165, 214, 287, 288, 301
Impedance mismatch, 162, 166
Import, 60, 61
IMS, 24, 27
In-memory, 15, 33, 34, 35, 39, 43, 76, 142,
</p>
<p>183&ndash;190, 192&ndash;197, 212, 246, 247
In-memory databases, 15, 33, 35, 76, 183, 184,
</p>
<p>188, 192, 193, 195, 196
Index, 12, 44, 52, 70, 101, 111&ndash;113, 115, 118,
</p>
<p>119, 122, 129, 133, 165, 180, 185, 190,
211, 216, 241, 250&ndash;256, 269&ndash;277, 280,
281, 294
</p>
<p>Indexing, 129, 189, 251
Inheritance, 174, 178
Inner Join, 91
Instance, 17, 39, 45&ndash;47, 53, 54, 56, 121, 131,
</p>
<p>152, 163, 169, 172, 173, 176, 231, 237,
261, 264
</p>
<p>Isolation, 25
</p>
<p>J
Java, 30, 100, 116, 148, 151, 162, 166, 177,
</p>
<p>180, 192, 291, 292
JavaScript, 122, 123, 125
Joins, 28, 30, 36, 44, 70, 76, 91, 92, 94, 99,
</p>
<p>103, 162, 164, 165, 172, 213, 214, 216,
249, 256, 289
</p>
<p>L
Late binding, 164, 165
Left Outer Join, 91
LINUX, 36
Locking, 57
Locks, 203, 243
Log, 26, 46, 53, 54, 56, 102, 188, 190, 218,
</p>
<p>226, 249, 261, 293&ndash;295
</p>
<p>M
Magnetic tape, 13, 15, 23, 141, 233
Mainframe, 13, 27, 36, 99, 204, 290
Maintenance, 4, 28, 55, 56, 167, 211, 228, 232,
</p>
<p>248, 257, 287, 298
Map-reduce, 151, 152
Mean Time Between Failure (MTBF), 225,
</p>
<p>228, 231, 239
Mean Time To Recover (MTTR), 231, 232,
</p>
<p>239
Memory management, 212
Memory structure, 43, 44
Method overriding, 165
</p>
<p>Microsoft, 29, 36, 74, 87, 96, 141, 167, 168,
202, 215, 247, 291, 292, 294, 296
</p>
<p>Mobile, 17, 195, 217, 238, 246, 283, 286, 289,
295, 296, 300
</p>
<p>MongoDB, 52, 120&ndash;122, 124, 125, 127,
129&ndash;132, 134, 149&ndash;152, 156, 158, 202,
215
</p>
<p>Monitor, 45, 228, 258, 299
Multi-user databases, 57, 203
MySQL, 36, 40, 97, 120, 131, 237, 241
</p>
<p>N
</p>
<p>Network, 7, 24, 27, 33, 37, 62, 101, 102, 193,
205, 206, 211, 221, 236&ndash;238, 255, 258,
269, 290, 292, 296, 297, 299, 301
</p>
<p>Network database, 24, 27, 28
No access, 287
Node, 26, 63, 101&ndash;103, 117, 121, 150,
</p>
<p>213&ndash;215, 237, 251
Nodes, 26, 37, 63, 98, 99, 101, 103, 106, 117,
</p>
<p>121, 148, 150, 158, 188, 210, 211,
213&ndash;215, 217, 237, 238
</p>
<p>Normalisation, 69, 70, 76, 86, 93, 96, 103,
249, 277
</p>
<p>Normalised data, 120
NoSQL, 35, 36, 40, 42, 76, 97&ndash;103, 116, 132,
</p>
<p>134, 148, 149, 185, 192, 215, 217, 238,
249
</p>
<p>Not null, 86
</p>
<p>O
Object class, 163, 169, 181
Object identity, 164
Object instance, 163, 171&ndash;173, 181
Object notation, 122
Object oriented, 30, 35, 36, 39, 76, 82,
</p>
<p>161&ndash;166, 170, 172, 177, 179, 183, 195
Object oriented database, 30, 36, 161, 168, 192
Object Query Language, 35, 100, 164, 165
Object relational, 30, 82, 168, 172, 180
OLAP, 184, 185, 193, 196
OLTP, 58, 121, 141, 183&ndash;185, 192, 193, 195,
</p>
<p>196, 212, 213, 216, 228, 231, 241, 245,
277
</p>
<p>One to many relationship, 29, 38, 77, 172, 176
Open source, 97
Operating system, 36, 47, 115, 203, 212, 228,
</p>
<p>229, 243, 245, 288, 289, 294
Optimisation, 104, 185, 189, 195, 242, 244,
</p>
<p>250
Oracle, 26, 28, 29, 34, 35, 38, 42&ndash;47, 49&ndash;54,
</p>
<p>56, 57, 59, 60, 63, 64, 70, 79, 85, 87,
89&ndash;91, 93, 96, 97, 115, 147, 148, 151,
163, 168, 171, 180, 182, 183, 188, 189,</p>
<p/>
</div>
<div class="page"><p/>
<p>306 Index
</p>
<p>192, 195&ndash;197, 202, 203, 210&ndash;214,
222&ndash;226, 228&ndash;232, 237, 239, 241,
245&ndash;252, 254, 255, 257, 261&ndash;263, 265,
266, 269&ndash;272, 277, 285, 288, 291&ndash;294,
298, 301
</p>
<p>Organisational asset, 3, 17, 21, 30
OS, 50&ndash;52, 242, 245, 247, 248
Overloading, 164, 165, 178
</p>
<p>P
</p>
<p>Parallel processing, 62, 63, 121, 214, 243, 246,
247
</p>
<p>Parameter, 47, 49, 50, 54, 59, 60, 128, 130,
152, 165, 178, 231, 255, 293
</p>
<p>Parameters, 56, 125, 152, 245, 248, 258
Partitioning data, 213
Password, 123, 284, 286, 292, 295, 296, 299,
</p>
<p>300
Peer-to-peer, 203&ndash;205
Performance, 34, 37, 41, 44, 62&ndash;64, 70, 76, 98,
</p>
<p>99, 101&ndash;104, 106, 120, 121, 129, 131,
132, 145, 150, 162, 183, 184, 186&ndash;188,
190, 192, 195, 201&ndash;203, 205, 206, 208,
210, 212&ndash;214, 216, 218, 222, 226, 227,
230, 231, 241&ndash;243, 245&ndash;259, 261, 266,
271, 276, 282, 288, 291, 293&ndash;295, 297
</p>
<p>Permanency, 97, 225, 245
Persistent, 30, 163, 165, 168, 181, 194
Physical resources, 217
Physical security, 283&ndash;285, 298, 301
Pointer, 27, 44, 52, 101, 119, 131, 172&ndash;174,
</p>
<p>189, 196
Pointers, 24, 27, 30, 104, 165, 169, 189, 190,
</p>
<p>297
Polymorphism, 177, 178
PostgreSQL, 169, 182, 292
Primary key, 28, 57, 69, 71&ndash;74, 78, 79, 86&ndash;89,
</p>
<p>91, 95, 122, 164, 171, 172, 181, 250,
251, 256, 269, 271
</p>
<p>Privacy, 3, 16&ndash;19
Privilege, 286, 287
Privilege abuse, 286, 288, 292, 298, 300
Protocol, 294, 297
Pseudocolumn, 44, 49
</p>
<p>Q
</p>
<p>Quantum computers, 41
Query, 26, 29, 30, 35, 43, 44, 63, 70, 89, 96,
</p>
<p>100, 101, 107, 112, 129, 130, 138, 141,
151, 161, 164&ndash;166, 168, 173, 188&ndash;190,
192, 207, 210, 211, 214&ndash;216, 241&ndash;244,
250&ndash;253, 255, 256, 261&ndash;263, 265, 266,
268, 269, 271&ndash;274, 277, 289, 292, 293
</p>
<p>R
RAID, 213, 222, 225, 227, 228, 237, 239, 246
RAM, 42, 43, 47, 99, 100, 185, 186, 192,
</p>
<p>205&ndash;207, 210, 212, 216, 217, 226, 238,
242, 243, 245&ndash;248, 258, 262, 265, 266
</p>
<p>Random access, 15, 39, 161, 186, 190, 233
RDBMS, 42, 43, 45, 52, 97&ndash;99, 101&ndash;104, 106,
</p>
<p>110&ndash;112, 120&ndash;122, 124, 127, 132, 135,
137, 139, 189, 206, 210, 212&ndash;215, 228,
229, 238, 241&ndash;245, 248, 251, 258, 259
</p>
<p>Record, 4, 6, 7, 10, 12, 15, 22&ndash;24, 27&ndash;29, 31,
34, 58, 60, 61, 69&ndash;71, 74, 82, 89, 90,
92&ndash;95, 104, 168, 169, 173, 174, 178,
185, 189&ndash;191, 225, 253, 287, 293
</p>
<p>Recovery, 15, 16, 34, 45, 53, 141, 187, 188,
211, 221, 222, 225&ndash;228, 231&ndash;233, 236,
237, 283, 295, 298
</p>
<p>Redo log, 46, 54, 226, 248
Redo logs, 46, 53, 54, 64, 211, 226
Relation, 35, 39, 148
Relational, 21, 25, 28&ndash;31, 34&ndash;36, 39, 42, 52,
</p>
<p>62, 69, 76, 82, 90, 94, 97, 98, 100, 103,
104, 106, 111, 124, 127, 132, 139, 141,
142, 148, 161&ndash;163, 166, 168, 169,
172&ndash;174, 176, 179, 180, 183, 184, 188,
189, 195, 202, 210, 217, 222, 249
</p>
<p>Relational algebra, 29, 69
Relational databases, 28, 30, 69, 161
Right Outer Join, 92
RMAN, 229, 232, 233
Rollback, 25, 26, 54, 57, 211, 224
</p>
<p>S
SAP, 63, 147, 151, 183, 190&ndash;197, 247
Scalability, 100, 121, 141, 201, 202, 204&ndash;206,
</p>
<p>208, 209, 212, 213, 216&ndash;218
Scaling out, 215, 216
Scaling up, 210, 213, 215
Schema, 31, 32, 38, 167, 242, 249
SCN, 46, 53, 225, 229, 231
Second Normal Form, 72
Security, 5, 16, 25, 34, 37, 63, 87, 102, 227,
</p>
<p>283&ndash;286, 291, 293, 294, 296&ndash;301
Segment, 52
Self join, 80, 92
Semantics, 166
Sequential, 15, 22
Server, 11, 16, 26, 29, 43, 45&ndash;47, 58, 62, 64,
</p>
<p>96, 99, 106, 109, 116, 123, 188, 194,
196, 202&ndash;208, 211&ndash;214, 217, 221, 223,
226, 232, 237, 238, 242&ndash;245, 247, 248,
253&ndash;255, 259, 262, 266, 267, 274, 275,
283, 285, 286, 288, 297, 299
</p>
<p>Sharding, 121, 132, 215, 216, 218</p>
<p/>
</div>
<div class="page"><p/>
<p>Index 307
</p>
<p>Shared memory, 57, 187, 188
SHUTDOWN, 56, 57, 228, 229
Sixth Normal Form, 76
Snowflake schemas, 31
Software engineers, 223
Software upgrades, 205
Solid state drives, 233
Spatial databases, 35
Spreadsheet, 36, 116, 133, 134, 288
SQL, 29, 30, 35, 43, 44, 47, 49, 51, 56, 59, 62,
</p>
<p>70, 87, 89, 90, 96, 99, 100, 110,
115&ndash;119, 134, 148, 151, 161, 162, 164,
165, 168, 171, 176, 177, 180, 190, 192,
202, 207, 210, 212, 228, 232, 233,
242&ndash;245, 247&ndash;250, 253, 256, 258,
265&ndash;267, 271, 275, 279, 286, 288, 289,
291&ndash;294, 300, 301
</p>
<p>SQL injection, 292
SQLServer, 27, 203, 212
SSD, 186, 187, 209, 233
Star schema, 31
Starcounter, 183, 192, 195
Starflake schema, 31
Startup, 228
Storage, 3, 15, 18, 23, 24, 33, 35, 41, 42, 46,
</p>
<p>47, 52, 62, 70, 72, 97, 103, 104, 106,
110, 116, 121, 124, 137, 138, 140, 141,
158, 168, 183&ndash;188, 190, 191, 195, 196,
208, 210, 212, 213, 217, 224, 226, 227,
232, 235, 238, 246, 247, 249, 253, 254,
256, 257, 283, 284, 293
</p>
<p>Storage as a service, 222, 227, 235
Structured Query Language, 69
Subclass, 162, 165, 169, 175, 176, 178
Superclass, 82, 162, 165, 175, 176, 179
System change number, 46, 53, 225
Systems testing, 224
</p>
<p>T
Table, 28&ndash;31, 35&ndash;37, 44, 47&ndash;49, 51, 52, 57&ndash;61,
</p>
<p>69, 71&ndash;77, 79, 82, 86&ndash;94, 106, 110,
116, 121, 122, 124, 162, 169&ndash;178, 181,
190, 191, 210, 211, 213, 216, 218, 224,
</p>
<p>225, 229, 250&ndash;252, 254&ndash;258, 261, 270,
272, 273, 276, 277, 280, 281, 287, 289,
290, 292, 300
</p>
<p>Tablespace, 46, 47, 52, 213, 226, 228&ndash;230,
232, 253, 256, 258
</p>
<p>Thin client, 205, 217, 218, 296
Third Normal Form, 73, 75
Timestamp, 106, 110, 225
TimesTen, 34, 183, 188&ndash;190, 192, 195&ndash;197
Transaction, 3, 4, 10, 11, 21, 23&ndash;26, 31, 34, 37,
</p>
<p>38, 53, 95, 106, 121, 136, 183, 184,
191, 192, 194, 196, 224, 259, 293
</p>
<p>Transaction Processing Council, 259
Transcription error, 290
Transposition errors, 290
Tuning, 206, 241, 243, 255, 258, 259, 261,
</p>
<p>265, 266
Tuple, 39, 69, 95
Two phase, 27, 37, 39
Two-phase commit, 26
</p>
<p>U
</p>
<p>UML, 76, 82&ndash;84, 170, 179, 182
UNDO, 54, 224
Unix, 106, 107, 122, 247
Use case, 76, 77, 179, 180
Use case analysis, 81
</p>
<p>V
</p>
<p>Vertical scaling, 210
View, 30, 37, 76, 90, 106, 137, 141, 163, 180,
</p>
<p>189, 193, 194, 213, 231, 242, 275, 286,
289, 290
</p>
<p>Virtual machines, 150
Virus, 291, 300
Volatile data, 104, 245
</p>
<p>X
</p>
<p>XML, 37, 38, 59, 139, 142, 162, 166&ndash;168, 249
XPath, 38, 168
XQuery, 38, 168, 249
XSLT, 38, 168</p>
<p/>
</div>
<ul>	<li>Concise Guide to Databases</li>
<ul>	<li>Foreword</li>
	<li>Preface</li>
<ul>	<li>Overview and Goals</li>
	<li>Organisation and Features</li>
	<li>Target Audiences</li>
	<li>Suggested Uses</li>
	<li>Review Questions</li>
	<li>Hands-on Exercises</li>
	<li>Chapter Summary</li>
</ul>
	<li>Contents</li>
</ul>
	<li>Part I: Databases in Context</li>
<ul>	<li>Chapter 1: Data, an Organisational Asset</li>
<ul>	<li>1.1 Introduction</li>
	<li>1.2 In the Beginning</li>
	<li>1.3 The Rise of Organisations</li>
	<li>1.4 The Challenges of Multi-site Operation</li>
	<li>1.5 Internationalisation</li>
	<li>1.6 Industrialisation</li>
	<li>1.7 Mass Transport</li>
	<li>1.8 Communication</li>
	<li>1.9 Stocks and Shares</li>
	<li>1.10 Corporate Takeovers</li>
	<li>1.11 The Challenges of Multi National Operations</li>
	<li>1.12 The Data Asset</li>
	<li>1.13 Electronic Storage</li>
	<li>1.14 Big Data</li>
	<li>1.15 Assets in the Cloud</li>
	<li>1.16 Data, Data Everywhere</li>
	<li>1.17 Summary</li>
	<li>1.18 Exercises</li>
<ul>	<li>1.18.1 Review Questions</li>
	<li>1.18.2 Group Work Research Activities</li>
</ul>
	<li>References</li>
</ul>
	<li>Chapter 2: A History of Databases</li>
<ul>	<li>2.1 Introduction</li>
	<li>2.2 The Digital Age</li>
	<li>2.3 Sequential Systems</li>
	<li>2.4 Random Access</li>
	<li>2.5 Origins of Modern Databases</li>
	<li>2.6 Transaction Processing and ACID</li>
	<li>2.7 Two-Phase Commit</li>
	<li>2.8 Hierarchical Databases</li>
	<li>2.9 Network Databases</li>
	<li>2.10 Relational Databases</li>
	<li>2.11 Object Oriented Databases</li>
	<li>2.12 Data Warehouse</li>
	<li>2.13 The Gartner Hype Cycle</li>
	<li>2.14 Big Data</li>
	<li>2.15 Data in the Cloud</li>
	<li>2.16 The Need for Speed</li>
	<li>2.17 In-Memory Database</li>
	<li>2.18 NoSQL</li>
	<li>2.19 Spatial Databases</li>
	<li>2.20 Databases on Personal Computers</li>
	<li>2.21 Distributed Databases</li>
	<li>2.22 XML</li>
	<li>2.23 Temporal Databases</li>
	<li>2.24 Summary</li>
	<li>2.25 Exercises</li>
<ul>	<li>2.25.1 Review Questions</li>
	<li>2.25.2 Group Work Research Activities</li>
</ul>
	<li>References</li>
</ul>
	<li>Chapter 3: Physical Storage and Distribution</li>
<ul>	<li>3.1 The Fundamental Building Block</li>
	<li>3.2 Overall Database Architecture</li>
<ul>	<li>3.2.1 In-Memory Structures</li>
	<li>3.2.2 Walking Through a Straightforward Read</li>
	<li>3.2.3 Server Processes</li>
	<li>3.2.4 Permanent Structures</li>
</ul>
	<li>3.3 Data Storage</li>
<ul>	<li>3.3.1 Row Chaining and Migration</li>
	<li>3.3.2 Non-relational Databases</li>
</ul>
	<li>3.4 How Logical Data Structures Map to Physical</li>
	<li>3.5 Control, Redo and Undo</li>
	<li>3.6 Log and Trace Files</li>
	<li>3.7 Stages of Start-up and Shutdown</li>
	<li>3.8 Locking</li>
	<li>3.9 Moving Data</li>
	<li>3.10 Import and Export</li>
<ul>	<li>3.10.1 Data Is Important</li>
</ul>
	<li>3.11 Distributed Databases</li>
	<li>3.12 Summary</li>
	<li>3.13 Review Questions</li>
	<li>3.14 Group Work Research Activities</li>
	<li>References</li>
</ul>
</ul>
	<li>Part II: Database Types</li>
<ul>	<li>Chapter 4: Relational Databases</li>
<ul>	<li>4.1 Origins</li>
	<li>4.2 Normalisation</li>
<ul>	<li>4.2.1 First Normal Form (1NF)</li>
</ul>
	<li>4.3 Second Normal Form (2NF)</li>
	<li>4.4 Third Normal Form (3NF)</li>
	<li>4.5 Beyond Third Normal Form</li>
	<li>4.6 Entity Modelling</li>
	<li>4.7 Use Case Modelling</li>
	<li>4.8 Further Modelling Techniques</li>
	<li>4.9 Notation</li>
	<li>4.10 Converting a Design into a Relational Database</li>
	<li>4.11 Worked Example</li>
	<li>4.12 Create the Tables</li>
	<li>4.13 CRUDing</li>
	<li>4.14 Populate the Tables</li>
	<li>4.15 Retrieve Data</li>
	<li>4.16 Joins</li>
	<li>4.17 More Complex Data Retrieval</li>
	<li>4.18 UPDATE and DELETE</li>
	<li>4.19 Review Questions</li>
	<li>4.20 Group Work Research Activity</li>
	<li>References</li>
</ul>
	<li>Chapter 5: NoSQL Databases</li>
<ul>	<li>5.1 Databases and the Web</li>
	<li>5.2 The NoSQL Movement</li>
<ul>	<li>5.2.1 What Is Meant by NoSQL?</li>
</ul>
	<li>5.3 Differences in Philosophy</li>
	<li>5.4 Basically Available, Soft State, Eventually Consistent (BASE)</li>
	<li>5.5 Column-Based Approach</li>
	<li>5.6 Examples of Column-Based Using Cassandra</li>
<ul>	<li>5.6.1 Cassandra's Basic Building Blocks</li>
<ul>	<li>5.6.1.1 Getting Hands-on with Cassandra</li>
</ul>
	<li>5.6.2 Data Sources</li>
	<li>5.6.3 Getting Started</li>
	<li>5.6.4 Creating the Column Family</li>
	<li>5.6.5 Inserting Data</li>
	<li>5.6.6 Retrieving Data</li>
	<li>5.6.7 Deleting Data and Removing Structures</li>
	<li>5.6.8 Command Line Script</li>
	<li>5.6.9 Shutdown</li>
</ul>
	<li>5.7 CQL</li>
<ul>	<li>5.7.1 Interactive CQL</li>
	<li>5.7.2 IF You Want to Check How Well You Now Know Cassandra &hellip;</li>
	<li>5.7.3 Timings</li>
</ul>
	<li>5.8 Document-Based Approach</li>
<ul>	<li>5.8.1 Examples of Document-Based Using MongoDB</li>
<ul>	<li>5.8.1.1 Getting Hands-on with MongoDB</li>
</ul>
	<li>5.8.2 Data Sources</li>
	<li>5.8.3 Getting Started</li>
	<li>5.8.4 Navigation</li>
	<li>5.8.5 Creating a Collection</li>
	<li>5.8.6 Simple Inserting and Reading of Data</li>
	<li>5.8.7 More on Retrieving Data</li>
	<li>5.8.8 Indexing</li>
	<li>5.8.9 Updating Data</li>
	<li>5.8.10 Moving Bulk Data into Mongo</li>
</ul>
	<li>5.9 IF You Want to Check How Well You Now Know MongoDB &hellip;</li>
<ul>	<li>5.9.1 Timings</li>
</ul>
	<li>5.10 Summary</li>
	<li>5.11 Review Questions</li>
	<li>5.12 Group Work Research Activities</li>
<ul>	<li>5.12.1 Sample Solutions</li>
	<li>5.12.2 MongoDB Crib</li>
</ul>
	<li>References</li>
</ul>
	<li>Chapter 6: Big Data</li>
<ul>	<li>6.1 What Is Big Data?</li>
	<li>6.2 The Datacentric View of Big Data</li>
<ul>	<li>6.2.1 The Four Vs</li>
<ul>	<li>6.2.1.1 Non-V</li>
</ul>
	<li>6.2.2 The Cloud Effect</li>
</ul>
	<li>6.3 The Analytics View of Big Data</li>
<ul>	<li>6.3.1 So Why Isn't Big Data Just Called Data Warehousing 2?</li>
	<li>6.3.2 What Is a Data Scientist?</li>
	<li>6.3.3 What Is Data Analysis for Big Data?</li>
</ul>
	<li>6.4 Big Data Tools</li>
<ul>	<li>6.4.1 MapReduce</li>
	<li>6.4.2 Hadoop</li>
<ul>	<li>6.4.2.1 If You Should Want to Explore Hadoop&hellip;</li>
</ul>
	<li>6.4.3 Hive, Pig and Other Tools</li>
</ul>
	<li>6.5 Getting Hands-on with MapReduce</li>
	<li>6.6 Using MongoDB's db.collection.mapReduce() Method</li>
<ul>	<li>6.6.1 And If You Have Time to Test Your MongoDB and JS Skills</li>
	<li>6.6.2 Sample Solutions</li>
</ul>
	<li>6.7 Summary</li>
	<li>6.8 Review Questions</li>
	<li>6.9 Group Work Research Activities</li>
	<li>References</li>
</ul>
	<li>Chapter 7: Object and Object Relational Databases</li>
<ul>	<li>7.1 Querying Data</li>
	<li>7.2 Problems with Relational Databases</li>
	<li>7.3 What Is an Object?</li>
	<li>7.4 An Object Oriented Solution</li>
	<li>7.5 XML</li>
	<li>7.6 Object Relational</li>
	<li>7.7 What Is Object Relational?</li>
	<li>7.8 Classes</li>
	<li>7.9 Pointers</li>
	<li>7.10 Hierarchies and Inheritance</li>
	<li>7.11 Aggregation</li>
	<li>7.12 Encapsulation and Polymorphism</li>
	<li>7.13 Polymorphism</li>
	<li>7.14 Support for Object Oriented and Object Relational Database Development</li>
	<li>7.15 Will Object Technology Ever Become Predominant in Database Systems?</li>
<ul>	<li>7.15.1 Review Questions</li>
	<li>7.15.2 Group Work Research Activities</li>
</ul>
	<li>References</li>
</ul>
	<li>Chapter 8: In-Memory Databases</li>
<ul>	<li>8.1 Introduction</li>
	<li>8.2 Origins</li>
	<li>8.3 Online Transaction Processing Versus Online Analytical Processing</li>
	<li>8.4 Interim Solution-Create a RAM Disk</li>
	<li>8.5 Interim Solution-Solid State Drive (SSD)</li>
	<li>8.6 In-Memory Databases-Some Misconceptions</li>
	<li>8.7 In-Memory Relational Database-The Oracle TimesTen Approach</li>
	<li>8.8 In-Memory Column Based Storage-The SAP HANA Approach</li>
	<li>8.9 In-Memory On-line Transaction Processing-The Starcounter Approach</li>
	<li>8.10 Applications Suited to In-Memory Databases</li>
	<li>8.11 In Memory Databases and Personal Computers</li>
	<li>8.12 Summary</li>
	<li>8.13 Review Questions</li>
	<li>8.14 Group Work Research Activities</li>
	<li>References</li>
</ul>
</ul>
	<li>Part III: What Database Professionals Worry About</li>
<ul>	<li>Chapter 9: Database Scalability</li>
<ul>	<li>9.1 What Do We Mean by Scalability?</li>
	<li>9.2 Coping with Growing Numbers of Users</li>
<ul>	<li>9.2.1 So Why Can't Access Cope with Lots of Users?</li>
	<li>9.2.2 Client/Server</li>
	<li>9.2.3 Scalability Eras</li>
<ul>	<li>9.2.3.1 Separating Users from Sessions</li>
	<li>9.2.3.2 Adding Tiers</li>
</ul>
</ul>
	<li>9.3 Coping with Growing Volumes of Data</li>
<ul>	<li>9.3.1 E-Commerce and Cloud Applications Need Scalable Solutions</li>
	<li>9.3.2 Vertical and Horizontal Scaling</li>
	<li>9.3.3 Database Scaling Issues</li>
	<li>9.3.4 Single Server Solutions</li>
<ul>	<li>9.3.4.1 Partitioning</li>
</ul>
	<li>9.3.5 Distributed RDBMS: Shared Nothing vs. Shared Disk</li>
	<li>9.3.6 Horizontal Scaling Solutions</li>
<ul>	<li>9.3.6.1 Cloud Computing</li>
</ul>
	<li>9.3.7 Scaling Down for Mobile</li>
</ul>
	<li>9.4 Summary</li>
	<li>9.5 Review Questions</li>
	<li>9.6 Group Work Research Activities</li>
	<li>References</li>
</ul>
	<li>Chapter 10: Database Availability</li>
<ul>	<li>10.1 What Do We Mean by Availability?</li>
	<li>10.2 Keeping the System Running-Immediate Solutions to Short Term Problems</li>
<ul>	<li>10.2.1 What Can Go Wrong?</li>
<ul>	<li>10.2.1.1 Database Software Related Problems</li>
	<li>10.2.1.2 Systems Design Problems and User Mistakes</li>
	<li>10.2.1.3 Technology Failures</li>
	<li>10.2.1.4 CPU, RAM and Motherboards</li>
	<li>10.2.1.5 Using RAID to Improve Availability</li>
	<li>10.2.1.6 Recovery at Startup</li>
	<li>10.2.1.7 Proactive Management</li>
	<li>10.2.1.8 Using Enterprise Manager</li>
</ul>
</ul>
	<li>10.3 Back-up and Recovery</li>
<ul>	<li>10.3.1 MTBF and MTTR</li>
<ul>	<li>10.3.1.1 Recovery</li>
	<li>10.3.1.2 RMAN and Enterprise Manager</li>
	<li>10.3.1.3 Storing the Back-Ups</li>
</ul>
</ul>
	<li>10.4 Disaster Recovery (DR)</li>
<ul>	<li>10.4.1 Business Impact</li>
	<li>10.4.2 High Availability for Critical Systems</li>
	<li>10.4.3 Trade-offs and Balances</li>
	<li>10.4.4 The Challenge or Opportunity of Mobile</li>
</ul>
	<li>10.5 Summary</li>
	<li>10.6 Review Questions</li>
	<li>10.7 Group Work Research Activities</li>
	<li>References</li>
</ul>
	<li>Chapter 11: Database Performance</li>
<ul>	<li>11.1 What Do We Mean by Performance?</li>
	<li>11.2 A Simpliï¬ed RDBMS Architecture</li>
	<li>11.3 Physical Storage</li>
<ul>	<li>11.3.1 Block Size</li>
	<li>11.3.2 Disk Arrays and RAID</li>
	<li>11.3.3 Alternatives to the HDD</li>
	<li>11.3.4 Operating System (OS)</li>
	<li>11.3.5 Database Server Processes</li>
	<li>11.3.6 Archive Manager</li>
	<li>11.3.7 Schema Level: Data Types, Location and Volumes</li>
	<li>11.3.8 SQL Optimisation</li>
	<li>11.3.9 Indexes</li>
<ul>	<li>11.3.9.1 B-Tree Indexes</li>
	<li>11.3.9.2 Non B-Tree Indexes-Bitmap Indexes</li>
	<li>11.3.9.3 Non B-Tree Indexes-Reverse Key Index</li>
	<li>11.3.9.4 Non B-Tree Indexes-Function-Based Indexes</li>
	<li>11.3.9.5 Indexes-Final Thoughts</li>
</ul>
	<li>11.3.10 Network</li>
	<li>11.3.11 Application</li>
<ul>	<li>11.3.11.1 Design with Performance in Mind</li>
</ul>
</ul>
	<li>11.4 Tuning</li>
<ul>	<li>11.4.1 What Is Database Tuning?</li>
	<li>11.4.2 Benchmarking</li>
	<li>11.4.3 Another Perspective</li>
</ul>
	<li>11.5 Tools</li>
<ul>	<li>11.5.1 Tuning and Performance Tools</li>
<ul>	<li>11.5.1.1 SQLPLUS Tools</li>
</ul>
	<li>11.5.2 Using the Built-in Advisers</li>
	<li>11.5.3 Over to You!</li>
<ul>	<li>11.5.3.1 Scenario One</li>
	<li>11.5.3.2 Scenario Two</li>
</ul>
</ul>
	<li>11.6 Summary</li>
	<li>11.7 Review Questions</li>
	<li>11.8 Group Work Research Activities</li>
	<li>Appendix:  Creation Scripts and Hints</li>
<ul>	<li>A.1 Hints on the Over to You Section</li>
<ul>	<li>A.1.1 Scenario One</li>
	<li>A.1.2 Scenario Two</li>
</ul>
</ul>
	<li>References</li>
</ul>
	<li>Chapter 12: Security</li>
<ul>	<li>12.1 Introduction</li>
	<li>12.2 Physical Security</li>
	<li>12.3 Software Security-Threats</li>
	<li>12.4 Privilege Abuse</li>
	<li>12.5 Platform Weaknesses</li>
	<li>12.6 SQL Injection</li>
	<li>12.7 Weak Audit</li>
	<li>12.8 Protocol Vulnerabilities</li>
	<li>12.9 Authentication Vulnerabilities</li>
	<li>12.10 Backup Data Exposure</li>
	<li>12.11 Mobile Device Based Threats</li>
	<li>12.12 Security Issues in Cloud Based Databases</li>
	<li>12.13 Policies and Procedures</li>
	<li>12.14 A Security Checklist</li>
	<li>12.15 Review Questions</li>
	<li>12.16 Group Work Research Activities</li>
	<li>References</li>
</ul>
</ul>
	<li>Index</li>
</ul>
</body></html>